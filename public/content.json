{"meta":{"title":"Honesty Wiki","subtitle":"知识库,沉淀文档","description":"Honesty 知识库,Honesty Wiki, Honesty blog,hehouhui 知识库,hehouhui Wiki, hehouhui blog","author":"Honesty","url":"https://docs.hehouhui.cn","root":"/"},"pages":[{"title":"关于","date":"2023-11-02T16:00:00.000Z","updated":"2023-11-03T11:18:01.239Z","comments":true,"path":"about.html","permalink":"https://docs.hehouhui.cn/about.html","excerpt":"","text":"努力学着变得勇敢 我是谁我是Honesty 作品 其他 听音乐 - 网易云音乐 看书、电影 - 豆瓣"},{"title":"文章归档","date":"2023-11-03T11:19:10.014Z","updated":"2023-11-03T11:19:10.014Z","comments":true,"path":"archive.html","permalink":"https://docs.hehouhui.cn/archive.html","excerpt":"","text":""},{"title":"","date":"2023-11-03T11:20:05.227Z","updated":"2023-11-03T11:20:05.227Z","comments":true,"path":"custom.css","permalink":"https://docs.hehouhui.cn/custom.css","excerpt":"","text":":root { --color-shadow: rgb(204 177 161 / 60%); } body { overflow-x: hidden; } #nexmoe-content .nexmoe-post-footer { background-color: transparent; } iframe { width: 100%!important; }"},{"title":"赞助我","date":"2019-11-30T15:00:09.000Z","updated":"2023-11-03T11:29:36.964Z","comments":true,"path":"donate.html","permalink":"https://docs.hehouhui.cn/donate.html","excerpt":"","text":"微信赞赏码 赞助单 时间 捐助人 金额"},{"title":"PY","date":"2023-11-03T11:10:36.000Z","updated":"2023-11-04T14:00:03.529Z","comments":true,"path":"friend.html","permalink":"https://docs.hehouhui.cn/friend.html","excerpt":"","text":"好玩的 朋友们 添加友链建议使用如下格式方便懒狗我复制粘贴 &#123; \"title\": \"Honesty\", \"link\": \"https://docs.hehouhui.cn\", \"img\": \"https://cdn.jsdelivr.net/gh/listener-He/images@default/202309111525908.jpeg\", \"des\": \"东风吹人醒,万事藏于心\" &#125; 需要更新头像或者链接请留言，有时间会处理的。 网站可访问却没有反链的，我会直接删除"},{"title":"links","date":"2023-09-22T04:08:00.000Z","updated":"2023-09-22T11:28:03.094Z","comments":true,"path":"links/index.html","permalink":"https://docs.hehouhui.cn/links/index.html","excerpt":"","text":"loadQexoFriends(\"qexo-friends\", \"https://editor.hehouhui.cn\")"},{"title":"talks","date":"2023-09-22T04:13:00.000Z","updated":"2023-09-22T11:28:30.928Z","comments":true,"path":"talks/index.html","permalink":"https://docs.hehouhui.cn/talks/index.html","excerpt":"","text":"showQexoTalks(\"qexot\", \"https://editor.hehouhui.cn\", 5)"}],"posts":[{"title":"Java异步编程方式介绍","slug":"archives/Java异步编程方式介绍","date":"2023-11-04T00:00:00.000Z","updated":"2023-11-08T07:58:00.000Z","comments":true,"path":"archives/java-sync-introduce-1104.html","link":"","permalink":"https://docs.hehouhui.cn/archives/java-sync-introduce-1104.html","excerpt":"","text":"异步执行对于开发者来说并不陌生，在实际的开发过程中，很多场景多会使用到异步，相比同步执行，异步可以大大缩短请求链路耗时时间，比如：发送短信、邮件、异步更新等，这些都是典型的可以通过异步实现的场景。 异步的八种实现方式 线程 Thread Future 异步框架 CompletableFuture Spring 注解@Async Spring ApplicationEvent 事件 消息队列 第三方异步框架，比如 Hutool 的 ThreadUtil Guava 异步 什么是异步？首先我们先看一个常见的用户下单的场景： 在同步操作中，我们执行到 发送短信 的时候，我们必须等待这个方法彻底执行完才能执行 赠送积分 这个操作，如果 赠送积分 这个动作执行时间较长，发送短信需要等待，这就是典型的同步场景。 实际上，发送短信和赠送积分没有任何的依赖关系，通过异步，我们可以实现赠送积分和发送短信这两个操作能够同时进行，比如： 这就是所谓的异步，是不是非常简单，下面就说说异步的几种实现方式吧。 异步编程4.1 线程异步public class AsyncThread extends Thread &#123; @Override public void run() &#123; System.out.println(\"Current thread name:\" + Thread.currentThread().getName() + \" Send email success!\"); &#125; public static void main(String[] args) &#123; AsyncThread asyncThread = new AsyncThread(); asyncThread.run(); &#125; &#125; 当然如果每次都创建一个Thread线程，频繁的创建、销毁，浪费系统资源，我们可以采用线程池： private ExecutorService executorService = Executors.newCachedThreadPool(); public void fun() &#123; executorService.submit(new Runnable() &#123; @Override public void run() &#123; log.info(\"执行业务逻辑...\"); &#125; &#125;); &#125; 可以将业务逻辑封装到Runnable或Callable中，交由线程池来执行。 4.2 Future 异步 import java.util.concurrent.*; import lombok.extern.slf4j.Slf4j; @Slf4j public class FutureManager &#123; public String execute() throws Exception &#123; ExecutorService executor = Executors.newFixedThreadPool(1); Future&lt;String> future = executor.submit(new Callable&lt;String>() &#123; @Override public String call() throws Exception &#123; System.out.println(\" --- task start --- \"); Thread.sleep(3000); System.out.println(\" --- task finish ---\"); return \"this is future execute final result!!!\"; &#125; &#125;); String result = future.get(); log.info(\"Future get result: &#123;&#125;\", result); return result; &#125; @SneakyThrows public static void main(String[] args) &#123; FutureManager manager = new FutureManager(); manager.execute(); &#125; &#125; 输出结果： Future get result: this is future execute final result!!! 4.2.1 Future 的不足之处Future 的不足之处的包括以下几点： 1️⃣ 无法被动接收异步任务的计算结果：虽然我们可以主动将异步任务提交给线程池中的线程来执行，但是待异步任务执行结束之后，主线程无法得到任务完成与否的通知，它需要通过 get 方法主动获取任务执行的结果。 2️⃣ Future 件彼此孤立：有时某一个耗时很长的异步任务执行结束之后，你想利用它返回的结果再做进一步的运算，该运算也会是一个异步任务，两者之间的关系需要程序开发人员手动进行绑定赋予，Future 并不能将其形成一个任务流（pipeline），每一个 Future 都是彼此之间都是孤立的，所以才有了后面的 CompletableFuture，CompletableFuture 就可以将多个 Future 串联起来形成任务流。 3️⃣ Futrue 没有很好的错误处理机制：截止目前，如果某个异步任务在执行发的过程中发生了异常，调用者无法被动感知，必须通过捕获 get 方法的异常才知晓异步任务执行是否出现了错误，从而在做进一步的判断处理。 4.3 CompletableFuture 实现异步 public class CompletableFutureCompose &#123; /** * thenAccept子任务和父任务公用同一个线程 */ @SneakyThrows public static void thenRunAsync() &#123; CompletableFuture&lt;Integer> cf1 = CompletableFuture.supplyAsync(() -> &#123; System.out.println(Thread.currentThread() + \" cf1 do something....\"); return 1; &#125;); CompletableFuture&lt;Void> cf2 = cf1.thenRunAsync(() -> &#123; System.out.println(Thread.currentThread() + \" cf2 do something...\"); &#125;); System.out.println(\"cf1结果->\" + cf1.get()); System.out.println(\"cf2结果->\" + cf2.get()); &#125; public static void main(String[] args) &#123; thenRunAsync(); &#125; &#125; 我们不需要显式使用 ExecutorService，CompletableFuture 内部使用了ForkJoinPool来处理异步任务，如果在某些业务场景我们想自定义自己的异步线程池也是可以的。 4.4 Spring 的@Async 异步4.4.1 自定义异步线程池/** * 线程池参数配置，多个线程池实现线程池隔离，@Async注解，默认使用系统自定义线程池，可在项目中设置多个线程池，在异步调用的时候，指明需要调用的线程池名称，比如：@Async(\"taskName\") * * @author: jacklin * @since: 2021/5/18 11:44 */ @EnableAsync @Configuration public class TaskPoolConfig &#123; /** * 自定义线程池 * * @author: jacklin * @since: 2021/11/16 17:41 */ @Bean(\"taskExecutor\") public Executor taskExecutor() &#123; int i = Runtime.getRuntime().availableProcessors(); System.out.println(\"系统最大线程数 ： \" + i); ThreadPoolTaskExecutor executor = new ThreadPoolTaskExecutor(); executor.setCorePoolSize(16); executor.setMaxPoolSize(20); executor.setQueueCapacity(99999); executor.setKeepAliveSeconds(60); executor.setThreadNamePrefix(\"asyncServiceExecutor -\"); executor.setAwaitTerminationSeconds(60); executor.setWaitForTasksToCompleteOnShutdown(true); return executor; &#125; &#125; 4.4.2 AsyncServicepublic interface AsyncService &#123; MessageResult sendSms(String callPrefix, String mobile, String actionType, String content); MessageResult sendEmail(String email, String subject, String content); &#125; @Slf4j @Service public class AsyncServiceImpl implements AsyncService &#123; @Autowired private IMessageHandler mesageHandler; @Override @Async(\"taskExecutor\") public MessageResult sendSms(String callPrefix, String mobile, String actionType, String content) &#123; try &#123; Thread.sleep(1000); mesageHandler.sendSms(callPrefix, mobile, actionType, content); &#125; catch (Exception e) &#123; log.error(\"发送短信异常 -> \", e) &#125; &#125; @Override @Async(\"taskExecutor\") public sendEmail(String email, String subject, String content) &#123; try &#123; Thread.sleep(1000); mesageHandler.sendsendEmail(email, subject, content); &#125; catch (Exception e) &#123; log.error(\"发送email异常 -> \", e) &#125; &#125; &#125; 在实际项目中， 使用@Async调用线程池，推荐等方式是是使用自定义线程池的模式，不推荐直接使用@Async 直接实现异步，因为 springboot 默认的线程池中只有一个常驻线程 4.5 Spring ApplicationEvent 事件实现异步4.5.1 定义事件private String email; // 主题 private String subject; // 内容 private String content; // 接收者 private String targetUserId; &#125; 4.5.2 定义事件处理器@Slf4j @Component public class AsyncSendEmailEventHandler implements ApplicationListener&lt;AsyncSendEmailEvent> &#123; @Autowired private IMessageHandler mesageHandler; @Async(\"taskExecutor\") @Override public void onApplicationEvent(AsyncSendEmailEvent event) &#123; if (event == null) &#123; return; &#125; String email = event.getEmail(); String subject = event.getSubject(); String content = event.getContent(); String targetUserId = event.getTargetUserId(); mesageHandler.sendsendEmailSms(email, subject, content, targerUserId); &#125; &#125; 另外，可能有些时候采用 ApplicationEvent 实现异步的使用，当程序出现异常错误的时候，需要考虑补偿机制，那么这时候可以结合 Spring Retry 重试来帮助我们避免这种异常造成数据不一致问题。 4.6 消息队列4.6.1 回调事件消息生产者@Slf4j @Component public class CallbackProducer &#123; @Autowired AmqpTemplate amqpTemplate; public void sendCallbackMessage(CallbackDTO allbackDTO, final long delayTimes) &#123; log.info(\"生产者发送消息，callbackDTO，&#123;&#125;\", callbackDTO); amqpTemplate.convertAndSend(CallbackQueueEnum.QUEUE_GENSEE_CALLBACK.getExchange(), CallbackQueueEnum.QUEUE_GENSEE_CALLBACK.getRoutingKey(), JsonMapper.getInstance().toJson(genseeCallbackDTO), new MessagePostProcessor() &#123; @Override public Message postProcessMessage(Message message) throws AmqpException &#123; message.getMessageProperties().setHeader(\"x-delay\", delayTimes); message.getMessageProperties().setCorrelationId(callbackDTO.getSdkId()); return message; &#125; &#125;); &#125; &#125; 4.6.2 回调事件消息消费者 @Slf4j @Component @RabbitListener(queues = \"message.callback\", containerFactory = \"rabbitListenerContainerFactory\") public class CallbackConsumer &#123; @Autowired private IGlobalUserService globalUserService; @RabbitHandler public void handle(String json, Channel channel, @Headers Map&lt;String, Object> map) throws Exception &#123; if (map.get(\"error\") != null) &#123; channel.basicNack((Long) map.get(AmqpHeaders.DELIVERY_TAG), false, true); return; &#125; try &#123; CallbackDTO callbackDTO = JsonMapper.getInstance().fromJson(json, CallbackDTO.class); globalUserService.execute(callbackDTO); channel.basicAck((Long) map.get(AmqpHeaders.DELIVERY_TAG), false); &#125; catch (Exception e) &#123; log.error(\"回调失败 -> &#123;&#125;\", e); &#125; &#125; &#125; 4.7 ThreadUtil 异步工具类@Slf4j public class ThreadUtils &#123; public static void main(String[] args) &#123; for (int i = 0; i &lt; 10; i++) &#123; ThreadUtil.execAsync(() -> &#123; ThreadLocalRandom threadLocalRandom = ThreadLocalRandom.current(); int number = threadLocalRandom.nextInt(20) + 1; System.out.println(number); &#125;); log.info(\"当前第：\" + i + \"个线程\"); &#125; log.info(\"task finish!\"); &#125; &#125; 4.8 Guava 异步Guava的ListenableFuture顾名思义就是可以监听的Future，是对 java 原生 Future 的扩展增强。我们知道 Future 表示一个异步计算任务，当任务完成时可以得到计算结果。如果我们希望一旦计算完成就拿到结果展示给用户或者做另外的计算，就必须使用另一个线程不断的查询计算状态。这样做，代码复杂，而且效率低下。使用Guava ListenableFuture可以帮我们检测 Future 是否完成了，不需要再通过 get()方法苦苦等待异步的计算结果，如果完成就自动调用回调函数，这样可以减少并发程序的复杂度。 ListenableFuture是一个接口，它从jdk的Future接口继承，添加了void addListener(Runnable listener, Executor executor)方法。 我们看下如何使用 ListenableFuture。首先需要定义 ListenableFuture 的实例: ListeningExecutorService executorService = MoreExecutors.listeningDecorator(Executors.newCachedThreadPool()); final ListenableFuture&lt;Integer> listenableFuture = executorService.submit(new Callable&lt;Integer>() &#123; @Override public Integer call() throws Exception &#123; log.info(\"callable execute...\"); TimeUnit.SECONDS.sleep(1); return 1; &#125; &#125;); 首先通过MoreExecutors类的静态方法listeningDecorator方法初始化一个ListeningExecutorService的方法，然后使用此实例的submit方法即可初始化ListenableFuture对象。 ListenableFuture要做的工作，在 Callable 接口的实现类中定义，这里只是休眠了 1 秒钟然后返回一个数字 1，有了 ListenableFuture 实例，可以执行此 Future 并执行 Future 完成之后的回调函数。 Futures.addCallback(listenableFuture, new FutureCallback&lt;Integer>() &#123; @Override public void onSuccess(Integer result) &#123; System.out.println(\"Get listenable future's result with callback \" + result); &#125; @Override public void onFailure(Throwable t) &#123; t.printStackTrace(); &#125; &#125;);","categories":[{"name":"技术分享","slug":"技术分享","permalink":"https://docs.hehouhui.cn/categories/%E6%8A%80%E6%9C%AF%E5%88%86%E4%BA%AB/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://docs.hehouhui.cn/tags/Java/"},{"name":"异步编程","slug":"异步编程","permalink":"https://docs.hehouhui.cn/tags/%E5%BC%82%E6%AD%A5%E7%BC%96%E7%A8%8B/"},{"name":"多线程","slug":"多线程","permalink":"https://docs.hehouhui.cn/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"}]},{"title":"Spring Boot 实现多租户架构：支持应用多租户部署和管理","slug":"archives/Spring Boot 实现多租户架构：支持应用多租户部署和管理","date":"2023-09-15T00:00:00.000Z","updated":"2023-11-02T08:23:00.000Z","comments":true,"path":"archives/spring-boot-tenant-202309.html","link":"","permalink":"https://docs.hehouhui.cn/archives/spring-boot-tenant-202309.html","excerpt":"","text":"1 什么是多租户架构？多租户架构是指在一个应用中支持多个租户（Tenant）同时访问，每个租户拥有独立的资源和数据，并且彼此之间完全隔离。通俗来说，多租户就是把一个应用按照客户的需求“分割”成多个独立的实例，每个实例互不干扰。 2 多租户架构的优势 更好地满足不同租户的个性化需求。 可以降低运维成本，减少硬件、网络等基础设施的投入。 节约开发成本，通过复用代码，快速上线新的租户实例。 增强了系统的可扩展性和可伸缩性，支持水平扩展，每个租户的数据和资源均可管理和控制。 3 实现多租户架构的技术选择对于实现多租户架构技术不是最重要的最重要的是正确的架构思路。但是选择正确的技术可以更快地实现多租户架构。 1 架构选型基于 Java 开发多租户应用推荐使用 Spring Boot 和 Spring Cloud。Spring Boot 能快速搭建应用并提供许多成熟的插件。Spring Cloud 则提供了许多实现微服务架构的工具和组件。 1.1 Spring Boot使用 Spring Boot 可以简化项目的搭建过程自动配置许多常见的第三方库和组件，减少了开发人员的工作量。 @RestController public class TenantController &#123; @GetMapping(\"/hello\") public String hello(@RequestHeader(\"tenant-id\") String tenantId) &#123; return \"Hello, \" + tenantId; &#125; &#125; 1.2 Spring Cloud在架构多租户的系统时 Spring Cloud 会更加有用。Spring Cloud 提供了一些成熟的解决方案，如 Eureka、Zookeeper、Consul 等，以实现服务发现、负载均衡等微服务功能。 2 数据库设计在多租户环境中数据库必须为每个租户分别存储数据并确保数据隔离。我们通常使用以下两种方式实现： 多个租户共享相同的数据库，每个表中都包含 tenant_id 这一列，用于区分不同租户的数据。 为每个租户创建单独的数据库，每个数据库内的表结构相同，但数据相互隔离。 3 应用多租户部署为了实现多租户在应用部署时我们需要考虑以下两个问题。 3.1 应用隔离在多租户环境中不同租户需要访问不同的资源，因此需要进行应用隔离。可以通过构建独立的容器或虚拟机、使用命名空间等方式实现。Docker 就是一种非常流行的隔离容器技术。 3.2 应用配置由于每个租户都有自己的配置需求因此需要为每个租户分别设置应用配置信息，例如端口号、SSL 证书等等。这些配置可以存储在数据库中，也可以存储在云配置中心中。 4 租户管理在多租户系统中需要能够管理不同租户的数据和资源，同时需要为每个租户分配相应的权限。解决方案通常包括以下两部分。 4.1 租户信息维护租户信息的维护包括添加、修改、删除、查询等操作，要求能够根据租户名称或租户 ID 快速查找对应的租户信息。 CREATE TABLE tenant ( id BIGINT AUTO_INCREMENT PRIMARY KEY, name VARCHAR(50) NOT NULL UNIQUE, description VARCHAR(255), created_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP, updated_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP ); 4.2 租户权限控制在多租户应用中必须为每个租户分别设置对系统资源的访问权限。例如，A 租户和 B 租户不能访问彼此的数据。 @EnableGlobalMethodSecurity(prePostEnabled = true) @Configuration public class SecurityConfig extends WebSecurityConfigurerAdapter &#123; @Override protected void configure(HttpSecurity http) throws Exception &#123; http.authorizeRequests() .antMatchers(\"/api/tenant/**\").hasRole(\"ADMIN\") .anyRequest().authenticated() .and() .formLogin(); &#125; @Autowired public void configureGlobal(AuthenticationManagerBuilder auth) throws Exception &#123; auth.userDetailsService(userDetailsService()) .passwordEncoder(new BCryptPasswordEncoder()) .and() .inMemoryAuthentication() .withUser(\"admin\") .password(new BCryptPasswordEncoder().encode(\"123456\")) .roles(\"ADMIN\"); &#125; &#125; 1 Spring Boot 中的多租户实现在 Spring Boot 中可以通过多数据源和动态路由来实现多租户机制。 1.1 多数据源实现多数据源是指为不同的租户配置不同的数据源，使得每个租户都可以访问自己的独立数据。具体实现方法如下： @Configuration public class DataSourceConfig &#123; @Bean(name = \"dataSourceA\") @ConfigurationProperties(prefix = \"spring.datasource.a\") public DataSource dataSourceA() &#123; return DataSourceBuilder.create().build(); &#125; @Bean(name = \"dataSourceB\") @ConfigurationProperties(prefix = \"spring.datasource.b\") public DataSource dataSourceB() &#123; return DataSourceBuilder.create().build(); &#125; @Bean(name = \"dataSourceC\") @ConfigurationProperties(prefix = \"spring.datasource.c\") public DataSource dataSourceC() &#123; return DataSourceBuilder.create().build(); &#125; &#125; 以上代码是配置了三个数据源分别对应三个租户。然后在使用时，可以使用注解标记需要连接的数据源。 @Service public class ProductService &#123; @Autowired @Qualifier(“dataSourceA”) private DataSource dataSource; // … &#125; 1.2 动态路由实现动态路由是指根据请求的 URL 或参数动态地切换到对应租户的数据源。具体实现如下： public class DynamicDataSource extends AbstractRoutingDataSource &#123; @Override protected Object determineCurrentLookupKey() &#123; return TenantContextHolder.getTenantId(); &#125; &#125; @Configuration public class DataSourceConfig &#123; @Bean(name = \"dataSource\") @ConfigurationProperties(prefix = \"spring.datasource\") public DataSource dataSource() &#123; return DataSourceBuilder.create().type(DynamicDataSource.class).build(); &#125; &#125; 以上是动态路由的核心代码DynamicDataSource继承自AbstractRoutingDataSource，通过determineCurrentLookupKey()方法动态获得租户 ID，然后切换到对应的数据源。 2 Spring Cloud 中的多租户实现在 Spring Cloud 中可以通过服务注册与发现、配置中心、负载均衡等方式实现多租户机制。 /** * 多租户配置类 */ @Configuration public class TenantConfig &#123; /** * 注册多租户拦截器 */ @Bean public TenantInterceptor tenantInterceptor() &#123; return new TenantInterceptor(); &#125; /** * 注册多租户数据源 */ @Bean @ConfigurationProperties(prefix = \"spring.datasource\") public DataSource dataSource() &#123; return DataSourceBuilder.create().build(); &#125; /** * 配置多租户Mybatis插件 */ @Bean public MybatisPlusInterceptor mybatisPlusInterceptor() &#123; MybatisPlusInterceptor interceptor = new MybatisPlusInterceptor(); interceptor.addInnerInterceptor(new TenantLineInnerInterceptor(new TenantHandler())); return interceptor; &#125; /** * 配置多租户Mybatis拦截器 */ @Bean public MybatisInterceptor mybatisInterceptor() &#123; MybatisInterceptor interceptor = new MybatisInterceptor(); interceptor.addInnerInterceptor(new TenantLineInnerInterceptor(new TenantHandler())); return interceptor; &#125; /** * 配置多租户Feign拦截器 */ @Bean public RequestInterceptor feignInterceptor() &#123; return new TenantFeignInterceptor(); &#125; /** * 配置多租户Ribbon拦截器 */ @Bean public LoadBalancerInterceptor ribbonInterceptor() &#123; return new TenantLoadBalancerInterceptor(); &#125; /** * 注册多租户Redis配置 */ @Bean public TenantRedisConfig tenantRedisConfig() &#123; return new TenantRedisConfig(); &#125; &#125; 2.1 服务注册与发现使用 Spring Cloud 中的 Eureka 实现服务注册与发现。每个租户的服务都在注册中心以不同的应用名称进行注册，客户端可以通过服务名称来访问对应租户的服务。 2.2 配置中心使用 Spring Cloud Config 作为配置中心。配置文件以租户 ID 进行区分，客户端通过读取对应租户的配置文件来获取配置信息。 2.3 负载均衡使用 Spring Cloud Ribbon 作为负载均衡器。根据请求的 URL 或参数选择对应租户的服务实例进行请求转发。 2.4 API在 API 网关层面实现多租户机制根据请求的 URL 或参数判断所属租户，并转发到对应租户的服务实例。 1 私有云环境私有云环境指的是由企业自行搭建的云环境，不对外提供服务，主要应用于企业内部的数据存储、管理、共享和安全控制。相较于公有云，私有云的优点在于可以更好地保护企业核心数据，同时也能够满足企业对于数据安全性和可控性的要求。 2 公有云环境公有云环境指的是由云服务商搭建并对外提供服务的云环境，用户可以根据需要购买相应的云服务，如云存储、云计算、云数据库等。相较于私有云，公有云的优点在于具有成本低廉、弹性伸缩、全球化部署等特点，能够更好地满足企业快速发展的需求。 3 企业级应用企业级应用是指面向企业客户的应用程序，主要包括 ERP、CRM、OA 等一系列应用系统。这类应用的特点在于功能强大、流程复杂、数据量大，需要满足企业的高效率、高可靠性、高安全性和易维护性等要求。在云计算环境下，企业可以将这些应用部署在私有云或公有云上，减少了硬件设备的投入和维护成本，提高了管理效率。 1 搭建 Spring Boot 和 Spring Cloud 环境首先需要在 Maven 项目中引入以下依赖： &lt;dependency> &lt;groupId>org.springframework.boot&lt;/groupId> &lt;artifactId>spring-boot-starter-web&lt;/artifactId> &lt;/dependency> &lt;dependencyManagement> &lt;dependencies> &lt;dependency> &lt;groupId>org.springframework.cloud&lt;/groupId> &lt;artifactId>spring-cloud-dependencies&lt;/artifactId> &lt;version>2020.0.3&lt;/version> &lt;type>pom&lt;/type> &lt;scope>import&lt;/scope> &lt;/dependency> &lt;/dependencies> &lt;/dependencyManagement> 然后需要在 application.yml 中配置相应的参数，如下所示： spring: datasource: url: jdbc:mysql://localhost:3306/appdb?useUnicode=true&amp;characterEncoding=utf-8&amp;serverTimezone=Asia/Shanghai username: root password: 123456 mybatis: type-aliases-package: com.example.demo.model mapper-locations: classpath:mapper/*.xml server: port: 8080 eureka: client: serviceUrl: defaultZone: http://localhost:8761/eureka/ management: endpoints: web: exposure: include: \"*\" 其中datasource.url为数据库连接的 URL，username 和 password 为数据库连接的账号和密码；server.port为 Spring Boot 应用启动的端口；eureka.client.serviceUrl.defaultZone为 Eureka 服务注册中心的 URL。 2 修改数据库设计接下来需要对数据库进行相应的修改，以支持多租户部署。具体来说，我们需要在数据库中添加一个与租户相关的字段，以便在应用中区分不同的租户。 3 实现应用多租户部署接着需要在代码中实现应用的多租户部署功能。具体来说，我们需要为每个租户实例化对应的 Spring Bean，并根据租户 ID 将请求路由到相应的 Bean 中去处理。 以下是一个简单的实现示例： @Configuration public class MultiTenantConfig &#123; // 提供对应租户的数据源 @Bean public DataSource dataSource(TenantRegistry tenantRegistry) &#123; return new TenantAwareDataSource(tenantRegistry); &#125; // 多租户Session工厂 @Bean(name = \"sqlSessionFactory\") public SqlSessionFactory sqlSessionFactory(DataSource dataSource) throws Exception &#123; SqlSessionFactoryBean sessionFactory = new SqlSessionFactoryBean(); sessionFactory.setDataSource(dataSource); return sessionFactory.getObject(); &#125; // 动态切换租户 @Bean public MultiTenantInterceptor multiTenantInterceptor(TenantResolver tenantResolver) &#123; MultiTenantInterceptor interceptor = new MultiTenantInterceptor(); interceptor.setTenantResolver(tenantResolver); return interceptor; &#125; // 注册拦截器 @Override public void addInterceptors(InterceptorRegistry registry) &#123; registry.addInterceptor(multiTenantInterceptor()); &#125; // 注册租户信息 @Bean public TenantRegistry tenantRegistry() &#123; return new TenantRegistryImpl(); &#125; // 解析租户ID @Bean public TenantResolver tenantResolver() &#123; return new HeaderTenantResolver(); &#125; &#125; 其中MultiTenantConfig是多租户部署的核心配置类，它提供了对应租户数据源、多租户 Session 工厂、动态切换租户等功能。 4 实现租户管理最后需要实现一个租户管理的功能，以便在系统中管理不同的租户。具体来说，我们可以使用 Spring Cloud 的服务注册与发现组件 Eureka 来注册每个租户的实例，并在管理界面中进行相应的操作。另外，我们还需要为每个租户提供一个独立的数据库，以保证数据隔离性。 本文详细介绍了如何使用 Spring Boot 和 Spring Cloud 实现一个支持多租户部署的应用。主要包括搭建 Spring Boot 和 Spring Cloud 环境、修改数据库设计、实现应用多租户部署、实现租户管理等方面。 应用场景主要包括 SaaS 应用、多租户云服务等。优劣势主要体现在提升了应用的可扩展性和可维护性，但也增加了部署和管理的复杂度。未来的改进方向可以考虑进一步提升多租户管理的自动化程度，减少人工干预和错误率。 添加 Eureka 依赖 &lt;dependency> &lt;groupId>org.springframework.cloud&lt;/groupId> &lt;artifactId>spring-cloud-starter-netflix-eureka-server&lt;/artifactId> &lt;/dependency> // 创建租户服务 // 添加Eureka依赖 &lt;dependency> &lt;groupId>org.springframework.cloud&lt;/groupId> &lt;artifactId>spring-cloud-starter-netflix-eureka-client&lt;/artifactId> &lt;/dependency> // 创建租户管理模块 // 添加Spring Data JPA依赖 &lt;dependency> &lt;groupId>org.springframework.boot&lt;/groupId> &lt;artifactId>spring-boot-starter-data-jpa&lt;/artifactId> &lt;/dependency> // 在Spring Boot和Spring Cloud环境下搭建多租户应用 // 修改数据库设计，实现应用多租户部署，实现租户管理 // 使用Eureka服务注册与发现组件注册每个租户的实例 // 启用Eureka @EnableEurekaServer @SpringBootApplication public class Application &#123; public static void main(String[] args) &#123; SpringApplication.run(Application.class, args); &#125; &#125; // 注册租户服务 @EnableDiscoveryClient @SpringBootApplication public class TenantServiceApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(TenantServiceApplication.class, args); &#125; &#125; // 配置文件 spring: application: name: tenant-service eureka: client: service-url: defaultZone: http://localhost:8761/eureka/ // 创建Tenant实体类 @Entity @Table(name = \"tenant\") public class Tenant &#123; @Id @GeneratedValue(strategy = GenerationType.IDENTITY) private Long id; private String name; private String databaseName; private String databaseUsername; private String databasePassword; // getter和setter &#125; // 创建TenantRepository接口 public interface TenantRepository extends JpaRepository&lt;Tenant, Long> &#123; &#125; // 创建TenantService类 @Service public class TenantService &#123; @Autowired private TenantRepository tenantRepository; // 添加租户 public Tenant addTenant(Tenant tenant) &#123; return tenantRepository.save(tenant); &#125; // 获取所有租户 public List&lt;Tenant> getAllTenants() &#123; return tenantRepository.findAll(); &#125; // 根据id获取租户 public Tenant getTenantById(Long id) &#123; return tenantRepository.findById(id).orElse(null); &#125; // 根据id删除租户 public void deleteTenantById(Long id) &#123; tenantRepository.deleteById(id); &#125; &#125; // 创建TenantController类 @RestController @RequestMapping(\"/tenants\") public class TenantController &#123; @Autowired private TenantService tenantService; // 添加租户 @PostMapping(\"\") public Tenant addTenant(@RequestBody Tenant tenant) &#123; return tenantService.addTenant(tenant); &#125; // 获取所有租户 @GetMapping(\"\") public List&lt;Tenant> getAllTenants() &#123; return tenantService.getAllTenants(); &#125; // 根据id获取租户 @GetMapping(\"/&#123;id&#125;\") public Tenant getTenantById(@PathVariable Long id) &#123; return tenantService.getTenantById(id); &#125; // 根据id删除租户 @DeleteMapping(\"/&#123;id&#125;\") public void deleteTenantById(@PathVariable Long id) &#123; tenantService.deleteTenantById(id); &#125; &#125; // 创建多租户数据库 public class MultiTenantConnectionProviderImpl implements MultiTenantConnectionProvider &#123; private Map&lt;String, DataSource> dataSourceMap = new HashMap&lt;>(); @Override public Connection getAnyConnection() throws SQLException &#123; return dataSourceMap.values().iterator().next().getConnection(); &#125; @Override public Connection getConnection(String tenantIdentifier) throws SQLException &#123; DataSource dataSource = dataSourceMap.get(tenantIdentifier); if (dataSource == null) &#123; throw new SQLException(\"No dataSource found for tenantIdentifier: \" + tenantIdentifier); &#125; return dataSource.getConnection(); &#125; @Override public void releaseAnyConnection(Connection connection) throws SQLException &#123; connection.close(); &#125; @Override public void releaseConnection(String tenantIdentifier, Connection connection) throws SQLException &#123; connection.close(); &#125; @Override public boolean supportsAggressiveRelease() &#123; return true; &#125; &#125; // 创建多租户实体类 @Entity @Table(name = \"customer\", schema = \"tenant1\") public class Customer &#123; @Id @GeneratedValue(strategy = GenerationType.IDENTITY) private Long id; private String name; private String email; // getter和setter &#125; // 创建多租户Repository接口 public interface CustomerRepository extends JpaRepository&lt;Customer, Long> &#123; List&lt;Customer> findByTenantId(Long tenantId); &#125; // 创建多租户Service类 @Service public class CustomerService &#123; @Autowired private CustomerRepository customerRepository; // 添加客户 public Customer addCustomer(Customer customer) &#123; return customerRepository.save(customer); &#125; // 获取所有客户 public List&lt;Customer> getAllCustomers(Long tenantId) &#123; return customerRepository.findByTenantId(tenantId); &#125; // 根据id获取客户 public Customer getCustomerById(Long id) &#123; return customerRepository.findById(id).orElse(null); &#125; // 根据id删除客户 public void deleteCustomerById(Long id) &#123; customerRepository.deleteById(id); &#125; &#125; // 创建多租户Controller类 @RestController @RequestMapping(\"/&#123;tenantId&#125;/customers\") public class CustomerController &#123; @Autowired private CustomerService customerService; // 添加客户 @PostMapping(\"\") public Customer addCustomer(@RequestBody Customer customer) &#123; return customerService.addCustomer(customer); &#125; // 获取所有客户 @GetMapping(\"\") public List&lt;Customer> getAllCustomers(@PathVariable Long tenantId) &#123; return customerService.getAllCustomers(tenantId); &#125; // 根据id获取客户 @GetMapping(\"/&#123;id&#125;\") public Customer getCustomerById(@PathVariable Long id) &#123; return customerService.getCustomerById(id); &#125; // 根据id删除客户 @DeleteMapping(\"/&#123;id&#125;\") public void deleteCustomerById(@PathVariable Long id) &#123; customerService.deleteCustomerById(id); &#125; &#125; // 配置文件 spring: application: name: eureka-server server: port: 8761","categories":[{"name":"技术分享","slug":"技术分享","permalink":"https://docs.hehouhui.cn/categories/%E6%8A%80%E6%9C%AF%E5%88%86%E4%BA%AB/"}],"tags":[{"name":"Spring","slug":"Spring","permalink":"https://docs.hehouhui.cn/tags/Spring/"},{"name":"微服务","slug":"微服务","permalink":"https://docs.hehouhui.cn/tags/%E5%BE%AE%E6%9C%8D%E5%8A%A1/"},{"name":"SAAS","slug":"SAAS","permalink":"https://docs.hehouhui.cn/tags/SAAS/"}]},{"title":"微服务之间的数据依赖问题，该如何解决？","slug":"archives/微服务之间的数据依赖问题，该如何解决？","date":"2023-09-15T00:00:00.000Z","updated":"2023-10-08T06:42:00.000Z","comments":true,"path":"archives/springcloud-data-202309.html","link":"","permalink":"https://docs.hehouhui.cn/archives/springcloud-data-202309.html","excerpt":"","text":"微服务，顾名思义，就是将我们程序拆分为最小化单元来提供服务。在一体化系统中，各个微服务也是不可能独立存在的，那么微服务之间涉及到的数据依赖问题，应该怎么处理呢？我们从场景入手来分析考虑此类问题。 一、场景在一个供应链系统中，存在商品、销售订单、采购三个微服务，他们的主数据部分数据结构如下： 商品： 订单和子订单： 采购单和子订单： 在设计这个供应链系统时，我们需要满足以下两个需求： 根据商品的型号&#x2F;分类&#x2F;生成年份&#x2F;编码等查找订单； 根据商品的型号&#x2F;分类&#x2F;生成年份&#x2F;编码等查找采购订单。 初期我们的方案是这样设计的：严格按照的微服务划分原则将商品相关的职责存放在商品系统中。因此，在查询订单与采购单时，如果查询字段包含商品字段，我们需要按照如下顺序进行查询： 先根据商品字段调用商品的服务，然后返回匹配的商品信息； 在订单或采购单中，通过 IN 语句匹配商品 ID，再关联查询对应的单据。 为了方便理解这个过程，订单查询流程图如下图所示： 图片 图片 初期方案设计完后，很快我们就遇到了一系列问题： 随着商品数量的增多，匹配的商品越来越多，于是订单服务中包含 IN 语句的查询效率越来越慢； 商品作为一个核心服务，依赖它的服务越来越多，同时随着商品数据量的增长，商品服务已不堪重负，响应速度也变慢，还存在请求超时的情况； 由于商品服务超时，相关服务处理请求经常失败。 结果就是业务方每次查询订单或采购单时，只要带上了商品这个关键字，查询效率就会很慢而且老是失败。于是，我们重新想了一个新方案——数据冗余，下面我们一起来看下。 欢迎关注 SpringForAll 社区（http://spring4all.com），专注分享关于Spring的一切，每周还有2-3次的福利赠书活动，一起来学习、分享、交流吧。 二、数据冗余的方案数据冗余说白了就是在订单、采购单中保存一些商品字段信息。 为了方便理解，我们借助上面实际业务场景具体说明下，看看两者的区别。 商品： 订单和子订单： 采购单和子订单： 调整架构方案后，每次查询时，我们就可以不再依赖商品服务了。 但是，如果商品进行了更新，我们如何同步冗余的数据呢？在此分享 2 种解决办法。 每次更新商品时，先调用订单与采购服务，再更新商品的冗余数据。 每次更新商品时，先发布一条消息，订单与采购服务各自订阅这条消息后，再各自更新商品冗余数据。 看到这里是不是觉得很眼熟了呢？没错，这就是我们上一篇提到过的数据一致性问题。那么这 2 种方案会出现哪些问题呢？ 如果商品服务每次更新商品都要调用订单与采购服务，然后再更新冗余数据，则会出现以下两种问题。 数据一致性问题：如果订单与采购的冗余数据更新失败了，整个操作都需要回滚。这时商品服务的开发人员肯定不乐意，因为冗余数据不是商品服务的核心需求，不能因为边缘流程阻断了自身的核心流程。 依赖问题：从职责来说，商品服务应该只关注商品本身，但是现在商品还需要调用订单与采购服务。而且，依赖商品这个核心服务的服务实在是太多了，也就导致后续商品服务每次更新商品时，都需要调用更新订单冗余数据、更新采购冗余数据、更新门店库存冗余数据、更新运营冗余数据等一大堆服务。那么商品到底是下游服务还是上游服务？还能不能安心当底层核心服务？ 因此，第一个解决办法直接被我们否决了，即我们采取的第二个解决办法——通过消息发布订阅的方案，因为它存在如下 2 点优势。 商品无须调用其他服务，它只需要关注自身逻辑即可，顶多多生成一条消息送到 MQ。 如果订单、采购等服务的更新冗余数据失败了，我们使用消息重试机制就可以了，最终能保证数据的一致性。 此时，我们的架构方案如下图所示： 这个方案看起来已经挺完美了，而且市面上基本也是这么做的，不过该方案存在如下几个问题。 1.在这个方案中，仅仅保存冗余数据还远远不够，我们还需要将商品分类与生产批号的清单进行关联查询。也就是说，每个服务不只是订阅商品变更这一种消息，还需要订阅商品分类、商品生产批号变更等消息。下面请注意查看订单表结构的加粗部分内容。 以上只是列举了一部分的结构，事实上，商品表中还有很多字段存在冗余，比如保修类型、包换类型等。为了更新这些冗余数据，采购服务与订单服务往往需要订阅近十种消息，因此，我们基本上需要把商品的一小半逻辑复制过来。 2.每个依赖的服务需要重复实现冗余数据更新同步的逻辑。前面我们讲了采购、订单及其他服务都需要依赖商品数据，因此每个服务需要将冗余数据的订阅、更新逻辑做一遍，最终重复的代码就会很多。 3.MQ 消息类型太多了：联调时最麻烦的是 MQ 之间的联动，如果是接口联调还好说，因为调用哪个服务器的接口相对可控而且比较好追溯；如果是消息联调就比较麻烦，因为我们常常不知道某条消息被哪台服务节点消费了，为了让特定的服务器消费特定的消息，我们就需要临时改动双方的代码。不过联调完成后，我们经常忘了改回原代码。 为此，我们不希望针对冗余数据这种非核心需求出现如此多的问题，最终决定使用一个特别的同步冗余数据方案，接下来我们进一步说明。 三、解耦业务逻辑的数据同步方案解耦业务逻辑的数据同步方案的设计思路是这样的： 将商品及商品相关的一些表（比如分类表、生产批号表、保修类型、包换类型等）实时同步到需要依赖使用它们的服务的数据库，并且保持表结构不变； 在查询采购、订单等服务时，直接关联同步过来的商品相关表； 不允许采购、订单等服务修改商品相关表。 此时，整个方案的架构如下图所示： 以上方案就能轻松解决如下两个问题： 商品无须依赖其他服务，如果其他服务的冗余数据同步失败，它也不需要回滚自身的流程； 采购、订单等服务无须关注冗余数据的同步。 不过，该方案的“缺点”是增加了订单、采购等数据库的存储空间（因为增加了商品相关表）。 仔细计算后，我们发现之前数据冗余的方案中每个订单都需要保存一份商品的冗余数据，假设订单总数是 N，商品总数是 M，而 N 一般远远大于 M。因此，在之前数据冗余的方案中，N 条订单就会产生 N 条商品的冗余数据。相比之下，解耦业务逻辑的数据同步方案更省空间，因为只增加了 M 条商品的数据。 此时问题又来了，如何实时同步相关表的数据呢？我们直接找一个现成的开源中间件就可以了，不过它需要满足支持实时同步、支持增量同步、不用写业务逻辑、支持 MySQL 之间同步、活跃度高这五点要求。 根据这五点要求，我们在市面上找了一圈，发现了 Canal、Debezium、DataX、Databus、Flinkx、Bifrost 这几款开源中间件，它们之间的区别如下表所示： 从对比表中来看，比较贴近我们需求的开源中间件是 Bifrost，原因如下： 它的界面管理不错； 它的架构比较简单，出现问题后，我们可以自行调查，之后就算作者不维护了也可以自我维护，相对比较可控。 作者更新活跃； 自带监控报警功能。 因此，最终我们使用了 Bifrost 开源中间件，此时整个方案的架构如下图所示： 四、上线效果整个架构方案上线后，商品数据的同步还算比较稳定，此时商品服务的开发人员只需要关注自身逻辑，无须再关注使用数据的人。如果需要关联使用商品数据的订单，采购服务的开发人员也无须关注商品数据的同步问题，只需要在查询时加上关联语句即可，实现了双赢。 然而，唯一让我们担心的是 Bifrost 不支持集群，没法保障高可用性。不过，到目前为止，它还没有出现宕机的情况，反而是那些部署多台节点负载均衡的后台服务常常会出现宕机。 最终，我们总算解决了服务之间数据依赖的问题。 五、总结这里我们探讨了服务间的数据依赖问题，并给出了目前较为合适的解决方案。其实这里提到的方案不是一个很大众的方案，肯定会存在一些遗漏的问题没考虑，如果你有更好的方案，欢迎留言讨论。","categories":[{"name":"技术分享","slug":"技术分享","permalink":"https://docs.hehouhui.cn/categories/%E6%8A%80%E6%9C%AF%E5%88%86%E4%BA%AB/"}],"tags":[{"name":"Spring","slug":"Spring","permalink":"https://docs.hehouhui.cn/tags/Spring/"},{"name":"微服务","slug":"微服务","permalink":"https://docs.hehouhui.cn/tags/%E5%BE%AE%E6%9C%8D%E5%8A%A1/"}]},{"title":"Redis 热key是什么问题，如何导致的？有什么解决方案？","slug":"archives/Redis 热key是什么问题，如何导致的？有什么解决方案？","date":"2023-09-12T00:00:00.000Z","updated":"2023-10-08T06:42:00.000Z","comments":true,"path":"archives/redis-key-202309.html","link":"","permalink":"https://docs.hehouhui.cn/archives/redis-key-202309.html","excerpt":"","text":"做一些 C 端业务，不可避免的要引入一级缓存来代替数据库的压力并且减少业务响应时间，其实每次引入一个中间件来解决问题的同时，必然会带来很多新的问题需要注意，比如缓存一致性问题。 那么其实还会有一些其他问题比如使用 Redis 作为一级缓存时可能带来的热 key、大 key 等问题，本文我们就热 key（hot key）问题来讨论，如何合理的解决热 key 问题。 背景 热 key 是什么问题，如何导致的？ 一般来说，我们使用的缓存 Redis 都是多节点的集群版，对某个 key 进行读写时，会根据该 key 的 hash 计算出对应的 slot，根据这个 slot 就能找到与之对应的分片(一个 master 和多个 slave 组成的一组 redis 集群)来存取该 K-V。但是在实际应用过程中，对于某些特定业务或者一些特定的时段（比如电商业务的商品秒杀活动），可能会发生大量的请求访问同一个 key。 所有的请求（且这类请求读写比例非常高）都会落到同一个 redis server 上，该 redis 的负载就会严重加剧，此时整个系统增加新 redis 实例也没有任何用处，因为根据 hash 算法，同一个 key 的请求还是会落到同一台新机器上，该机器依然会成为系统瓶颈 2，甚至造成整个集群宕掉，若此热点 key 的 value 也比较大，也会造成网卡达到瓶颈，这种问题称为 “热 key” 问题。 如下图 1、2 所示，分别是正常 redis cluster 集群和使用一层 proxy 代理的 redis 集群 key 访问。 如上所说，热 key 会给集群中的少部分节点带来超高的负载压力，如果不正确处理，那么这些节点宕机都有可能，从而会影响整个缓存集群的运作，因此我们必须及时发现热 key、解决热 key 问题。 1.热 key 探测热 key 探测，看到由于 redis 集群的分散性以及热点 key 带来的一些显著影响，我们可以通过由粗及细的思考流程来做热点 key 探测的方案。 1.1 集群中每个 slot 的 qps 监控热 key 最明显的影响是整个 redis 集群中的 qps 并没有那么大的前提下，流量分布在集群中 slot 不均的问题，那么我们可以最先想到的就是对于每个 slot 中的流量做监控，上报之后做每个 slot 的流量对比，就能在热 key 出现时发现影响到的具体 slot。虽然这个监控最为方便，但是粒度过于粗了，仅适用于前期集群监控方案，并不适用于精准探测到热 key 的场景。 1.2 proxy 的代理机制作为整个流量入口统计如果我们使用的是图 2 的 redis 集群 proxy 代理模式，由于所有的请求都会先到 proxy 再到具体的 slot 节点，那么这个热点 key 的探测统计就可以放在 proxy 中做，在 proxy 中基于时间滑动窗口，对每个 key 做计数，然后统计出超出对应阈值的 key。为了防止过多冗余的统计，还可以设定一些规则，仅统计对应前缀和类型的 key。这种方式需要至少有 proxy 的代理机制，对于 redis 架构有要求。 1.3 redis 基于 LFU 的热点 key 发现机制redis 4.0 以上的版本支持了每个节点上的基于 LFU 的热点 key 发现机制，使用 redis-cli –hotkeys 即可，执行 redis-cli 时加上–hotkeys 选项。可以定时在节点中使用该命令来发现对应热点 key。 如下所示，可以看到 redis-cli –hotkeys 的执行结果，热 key 的统计信息，这个命令的执行时间较长，可以设置定时执行来统计。 1.4 基于 Redis 客户端做探测由于 redis 的命令每次都是从客户端发出，基于此我们可以在 redis client 的一些代码处进行统计计数，每个 client 做基于时间滑动窗口的统计，超过一定的阈值之后上报至 server，然后统一由 server 下发至各个 client，并且配置对应的过期时间。 这个方式看起来更优美，其实在一些应用场景中并不是那么合适，因为在 client 端这一侧的改造，会给运行的进程带来更大的内存开销，更直接的来说，对于 Java 和 goLang 这种自动内存管理的语言，会更加频繁的创建对象，从而触发 gc 导致接口响应耗时增加的问题，这个反而是不太容易预料到的事情。 最终可以通过各个公司的基建，做出对应的选择。 2.热 key 解决通过上述几种方式我们探测到了对应热 key 或者热 slot，那么我们就要解决对应的热 key 问题。解决热 key 也有好几种思路可以参考，我们一个一个捋一下。 2.1 对特定 key 或 slot 做限流一种最简单粗暴的方式，对于特定的 slot 或者热 key 做限流，这个方案明显对于业务来说是有损的，所以建议只用在出现线上问题，需要止损的时候进行特定的限流。 2.2 使用二级（本地）缓存本地缓存也是一个最常用的解决方案，既然我们的一级缓存扛不住这么大的压力，就再加一个二级缓存吧。由于每个请求都是由 service 发出的，这个二级缓存加在 service 端是再合适不过了，因此可以在服务端每次获取到对应热 key 时，使用本地缓存存储一份，等本地缓存过期后再重新请求，降低 redis 集群压力。以 java 为例，guavaCache 就是现成的工具。以下示例： //本地缓存初始化以及构造 private static LoadingCache&lt;String, List> configCache = CacheBuilder.newBuilder() .concurrencyLevel(8) //并发读写的级别，建议设置cpu核数 .expireAfterWrite(10, TimeUnit.SECONDS) //写入数据后多久过期 .initialCapacity(10) //初始化cache的容器大小 .maximumSize(10)//cache的容器最大 .recordStats() // build方法中可以指定CacheLoader，在缓存不存在时通过CacheLoader的实现自动加载缓存 .build(new CacheLoader&lt;String, List>() &#123; @Override public List load(String hotKey) throws Exception &#123; &#125; &#125;); //本地缓存获取 Object result = configCache.get(key); 本地缓存对于我们的最大的影响就是数据不一致的问题，我们设置多长的缓存过期时间，就会导致最长有多久的线上数据不一致问题，这个缓存时间需要衡量自身的集群压力以及业务接受的最大不一致时间。 2.3 拆 key如何既能保证不出现热 key 问题，又能尽量的保证数据一致性呢？拆 key 也是一个好的解决方案。 我们在放入缓存时就将对应业务的缓存 key 拆分成多个不同的 key。如下图所示，我们首先在更新缓存的一侧，将 key 拆成 N 份，比如一个 key 名字叫做“good_100”，那我们就可以把它拆成四份，“good_100_copy1”、“good_100_copy2”、“good_100_copy3”、“good_100_copy4”，每次更新和新增时都需要去改动这 N 个 key，这一步就是拆 key。 对于 service 端来讲，我们就需要想办法尽量将自己访问的流量足够的均匀，如何给自己即将访问的热 key 上加入后缀。几种办法，根据本机的 ip 或 mac 地址做 hash，之后的值与拆 key 的数量做取余，最终决定拼接成什么样的 key 后缀，从而打到哪台机器上；服务启动时的一个随机数对拆 key 的数量做取余。 2.4 本地缓存的另外一种思路 配置中心对于熟悉微服务配置中心的伙伴来讲，我们的思路可以向配置中心的一致性转变一下。拿 nacos 来举例，它是如何做到分布式的配置一致性的，并且相应速度很快？那我们可以将缓存类比配置，这样去做。 长轮询+本地化的配置。首先服务启动时会初始化全部的配置，然后定时启动长轮询去查询当前服务监听的配置有没有变更，如果有变更，长轮询的请求便会立刻返回，更新本地配置；如果没有变更，对于所有的业务代码都是使用本地的内存缓存配置。这样就能保证分布式的缓存配置时效性与一致性。 2.5 其他可以提前做的预案上面的每一个方案都相对独立的去解决热 key 问题，那么如果我们真的在面临业务诉求时，其实会有很长的时间来考虑整体的方案设计。一些极端的秒杀场景带来的热 key 问题，如果我们预算充足，可以直接做服务的业务隔离、redis 缓存集群的隔离，避免影响到正常业务的同时，也会可以临时采取更好的容灾、限流措施。 一些整合的方案目前市面上已经有了不少关于 hotKey 相对完整的应用级解决方案，其中京东在这方面有开源的 hotkey 工具，原理就是在 client 端做洞察，然后上报对应 hotkey，server 端检测到后，将对应 hotkey 下发到对应服务端做本地缓存，并且这个本地缓存在远程对应的 key 更新后，会同步更新，已经是目前较为成熟的自动探测热 key、分布式一致性缓存解决方案 总结以上就是笔者大概了解或实践过的的如何应对热 key 的一些方案，从发现热 key 到解决热 key 的两个关键问题的应对。每一个方案都有优缺点，比如会带来业务的不一致性，实施起来较为困难等等，可以根据目前自身业务的特点、以及目前公司的基建去做对应的调整和改变。","categories":[{"name":"技术分享","slug":"技术分享","permalink":"https://docs.hehouhui.cn/categories/%E6%8A%80%E6%9C%AF%E5%88%86%E4%BA%AB/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"https://docs.hehouhui.cn/tags/Redis/"}]},{"title":"HttpClient? RestTemplate？WebClient? 不~是 RestClient","slug":"archives/HttpClient! RestTemplate？WebClient! 不~是 RestClient","date":"2023-09-06T00:00:00.000Z","updated":"2023-10-08T06:42:00.000Z","comments":true,"path":"archives/spring-restclient-2023.html","link":"","permalink":"https://docs.hehouhui.cn/archives/spring-restclient-2023.html","excerpt":"","text":"介绍Spring 框架一直提供了两种不同的客户端来执行 http 请求: RestTemplate: 它在 Spring 3 中被引入，提供同步的阻塞式通信。 WebClient: 它在 Spring 5 的 Spring WebFlux 库中作为一部分被发布。它提供了流式 API,遵循响应式模型。 RestTemplate 的方法暴露了太多的 HTTP 特性,导致了大量重载的方法，使用成本较高。WebClient 是 RestTemplate 的替代品,支持同步和异步调用。它是 Spring Web Reactive 项目的一部分。 现在 Spring 6.1 M1 版本引入了 RestClient。一个新的同步 http 客户端,其工作方式与 WebClient 类似,使用与 RestTemplate 相同的基础设施。 准备项目jar 依赖&lt;dependency> &lt;groupId>org.springframework.boot&lt;/groupId> &lt;artifactId>spring-boot-starter-web&lt;/artifactId> &lt;/dependency> 创建全局 RestClient创建 RestClient 实例有可用的静态方法: create(): 委托给默认的 rest 客户端。 create(String url): 接受一个默认的基础 url。 create(RestTemplate restTemplate): 基于给定 rest 模板的配置初始化一个新的 RestClient。 builder(): 允许使用 headers、错误处理程序、拦截器等选项自定义一个 RestClient。 builder(RestTemplate restTemplate): 基于给定 RestTemplate 的配置获取一个 RestClient builder。 让我们使用 builder 方法调用客户 API 来编写一个 RestClient: RestClient restClient = RestClient.builder() .baseUrl(properties.getUrl()) .defaultHeader(HttpHeaders.AUTHORIZATION, encodeBasic(\"pig\", \"pig\")) .build(); 参数说明: baseUrl 设置基础 url defaultHeader 允许设置一个默认 http 请求头 接收数据 retrieve下一步是使用客户端发送 http 请求并接收响应。RestClient 为每种 HTTP 方法都提供了方法。例如,要搜索所有活动客户,必须执行 GET 请求。retrieve 方法获取响应并声明如何提取它。 让我们从使用完整正文作为 String 的简单情况开始。 String data = restClient.get() .uri(\"?name=&#123;name&#125;&amp;type=&#123;type&#125;\", \"lengleng\", \"1\") .accept(MediaType.APPLICATION_JSON) .retrieve() .body(String.class); uri 方法可以设置 http 参数第一个参数(一个字符串模板)是附加到 RestClient 中定义的 base url 的查询字符串。第二个参数是模板的 uri 变量(varargs)。 我们还指定媒体类型为 JSON。输出显示在控制台中: [ &#123; \"id\": 1, \"name\": \"lengleng\", \"type\": \"1\" &#125; ] 如果需要检查响应状态码或响应头怎么办?别担心,toEntity 方法会返回一个 ResponseEntity。 ResponseEntity response = restClient .get() .uri(\"?name=&#123;name&#125;&amp;type=&#123;type&#125;\", \"lengleng\", \"1\") .accept(MediaType.APPLICATION_JSON) .retrieve() .toEntity(String.class); logger.info(\"Status \" + response.getStatusCode()); logger.info(\"Headers \" + response.getHeaders()); 结果转换 BeanRestClient 还可以将响应主体转换为 JSON 格式。Spring 将自动默认注册 MappingJackson2HttpMessageConverter 或 MappingJacksonHttpMessageConverter,如果在类路径中检测到 Jackson 2 库或 Jackson 库。但是你可以注册自己的消息转换器并覆盖默认设置。 在我们的例子中,响应可以直接转换为记录。例如,检索特定客户的 API: ReqUserResponse customer = restClient.get() .uri(\"/&#123;name&#125;\", \"lengleng\") .accept(MediaType.APPLICATION_JSON) .retrieve() .body(ReqUserResponse.class); logger.info(\"res name: \" + customer.personInfo().name()); 要搜索客户,我们只需要使用 List 类,如下所示: List&lt;ReqUserResponse> customers = restClient.get() .uri(\"?type=&#123;type&#125;\", \"1\") .accept(MediaType.APPLICATION_JSON) .retrieve() .body(List.class); logger.info(\"res size \" + customers.size()); 发布数据要发送 post 请求,只需调用 post 方法。下一段代码片段创建一个新客户。 ReqUserResponse customer = new ReqUserResponse(\"lengleng-plus\", \"1\"); ResponseEntity&lt;Void> response = restClient .post() .accept(MediaType.APPLICATION_JSON) .body(customer) .retrieve() .toBodilessEntity(); if (response.getStatusCode().is2xxSuccessful()) &#123; logger.info(\"Created \" + response.getStatusCode()); logger.info(\"New URL \" + response.getHeaders().getLocation()); &#125; 响应代码确认客户已成功创建: Created 201 CREATEDNew URL http://localhost:8080/api/v1/customers/11 要验证客户是否已添加,可以通过 postman 检索以上 URL: &#123; \"id\": 2, \"name\": \"lengleng-plus\", \"type\": \"1\"&#125; 当然,可以使用与前一节类似的代码通过 RestClient 获取它。 删除数据调用 delete 方法发出 HTTP delete 请求尝试删除资源非常简单。 ResponseEntity&lt;Void> response = restClient.delete() .uri(\"/&#123;id&#125;\", 2) .accept(MediaType.APPLICATION_JSON) .retrieve() .toBodilessEntity(); logger.info(\"Deleted with status \" + response.getStatusCode()); 值得一提的是,如果操作成功,响应主体将为空。对于这种情况,toBodilessEntity 方法非常方便。要删除的客户 ID 作为 uri 变量传递。 Deleted with status 204 NO_CONTENT 处理错误如果我们尝试删除或查询一个不存在的客户会发生什么?客户端点将返回一个 404 错误代码以及消息详细信息。然而,每当接收到客户端错误状态码(400-499)或服务器错误状态码(500-599)时,RestClient 将抛出 RestClientException 的子类。 要定义自定义异常处理程序,有两种选项适用于不同的级别: 在 RestClient 中使用 defaultStatusHandler 方法(对其发送的所有 http 请求) RestClient restClient = RestClient.builder() .baseUrl(properties.getUrl()) .defaultHeader(HttpHeaders.AUTHORIZATION, encodeBasic(\"pig\", \"pig\")) .defaultStatusHandler( HttpStatusCode::is4xxClientError, (request, response) -> &#123; logger.error(\"Client Error Status \" + response.getStatusCode()); logger.error(\"Client Error Body \" + new String(response.getBody().readAllBytes())); &#125; ) .build(); 在运行删除命令行运行程序后,控制台的输出如下: Client Error Status 404 NOT_FOUNDClient Error Body &#123;\"status\":404,\"message\":\"Entity Customer for id 2 was not found.\",\"timestamp\":\"2023-07-23T09:24:55.4088208\"&#125; 另一种选择是为删除操作实现 onstatus 方法。它优先于 RestClient 默认处理程序行为。 ResponseEntity response = restClient.delete() .uri(\"/&#123;id&#125;\", 2) .accept(MediaType.APPLICATION_JSON) .retrieve() .onStatus(HttpStatusCode::is4xxClientError, (req, res) -> logger.error(\"Couldn't delete \" + res.getStatusText())) .toBodilessEntity(); if (response.getStatusCode().is2xxSuccessful()) logger.info(\"Deleted with status \" + response.getStatusCode()); 现在控制台中的消息将是: Couldn't delete Not Found Exchange 方法当响应必须根据响应状态进行不同解码时,exchange 方法很有用。使用 exchange 方法时,状态处理程序将被忽略。 在这个虚构的示例代码中,响应基于状态映射到实体: SimpleResponse simpleResponse = restClient.get() .uri(\"/&#123;id&#125;\", 4) .accept(MediaType.APPLICATION_JSON) .exchange((req, res) -> switch (res.getStatusCode().value()) &#123; case 200 -> SimpleResponse.FOUND; case 404 -> SimpleResponse.NOT_FOUND; default -> SimpleResponse.ERROR; &#125;); 总结与旧的 RestTemplate 相比,这个新 API 更易于管理，官方重构的目的绝非是造轮子，而是目标去实现基于 Loom 标准的的 Http 客户端,更好的适配 JDK21 的虚拟线程，提供更高性能的客户端实现。","categories":[{"name":"技术分享","slug":"技术分享","permalink":"https://docs.hehouhui.cn/categories/%E6%8A%80%E6%9C%AF%E5%88%86%E4%BA%AB/"}],"tags":[{"name":"Spring","slug":"Spring","permalink":"https://docs.hehouhui.cn/tags/Spring/"},{"name":"Java","slug":"Java","permalink":"https://docs.hehouhui.cn/tags/Java/"}]},{"title":"上海咖啡文化周之薅瑞幸羊毛","slug":"archives/上海咖啡文化周之薅瑞幸羊毛","date":"2023-06-02T00:00:00.000Z","updated":"2023-10-08T06:42:00.000Z","comments":true,"path":"archives/lkcoffee-shanghai-coffee-activities-20230602.html","link":"","permalink":"https://docs.hehouhui.cn/archives/lkcoffee-shanghai-coffee-activities-20230602.html","excerpt":"","text":"首先声明：本人没有恶意利用这个漏洞去谋利，只是测试用了那么一次。不提倡大家去做这种违法行为，本文仅供学习 💡 最近瑞幸在搞活动，每天免费送 10000 份咖啡，我是个狂热喝咖啡的人儿，今天最后一天来整个活儿，点开瑞幸咖啡小程序主页，banner 栏轮播图中有一张海报入口，操作一通下来，果然，没抢到。手速不够快不是主要原因，手指操作延迟 + 系统页面跳转耗时加起来到 http 发出就已经耽误了 1 -2 秒钟了，这个时间才是关键，本文从技术角度探讨下怎么在最小成本比如几分钟内，实现一个小工具，来解决这个问题。 首先需要一个抓包工具，iphone 手机可以用 stream, 有几个注意点： 1、默认安装后是无法抓取 https 类型的，需要在设置里进行相关配置： 如果您要抓取 HTTPS 的请求，需要先启动抓包，然后安装 CA 证书后，去设置-通用-关于-证书信任设置 里信任 CA，才可以查看请求的内容。 2、注意小程序里面哦（原生的可能抓不到），拿到的接口名如下： https://mkt.lkcoffee.com/ladder/capi/resource/m/promo/activity/send stream 提供了 curl 的拷贝，将其复制并导入到 postman 中。 postman 导入&amp;复现点击 import 按钮，在弹窗中选择 raw text 将复制的 curl 字符串粘贴进去，点击确认，就成功的将 这个 http 接口导入到了 postman 中，尝试点击 send 按钮，发现拿到了正确的响应，验证了该接口已经可以正常使用。 自动化脚本？其实到这一步，已经实现了目标，点击 send 直接发送请求，大大提升了抢到的概率，如果你还想更进一步，那么可以尝试将其封装成 自动化脚本，来实现定时、自动、重复发送； 点开右侧代码块，选择语言，假设选择 python（也可以任意选择你擅长的语言），然后就自动生成 python 版本的可执行代码片段，我们就在这个基础上拓展功能； 示例代码如下： import requests import time url = \"http://example.com\" # 将此处的 URL 替换为你要请求的地址payload = &#123;&#125; headers = &#123; #将 postman 中的headers 复制过来&#125; start_time = \"09:59:55\" # 设置开始请求的时间end_time = \"10:00:30\" # 设置结束请求的时间def make_request(): response = requests.get(url, headers=headers, data=payload) if \"成功\" in response.text: print(\"响应内容:\", response.text) raise SystemExit # 中断程序while True: current_time = time.strftime(\"%H:%M:%S\", time.localtime()) if current_time >= start_time and current_time &lt;= end_time: make_request() time.sleep(1) # 每秒检查一次当前时间 将其保存到本地并通过 python 指令来执行，就可以运行了。 总结用今天的午睡时间，写了这篇文，以瑞幸的营销活动为例子，带你感受了下技术的魅力，其中涉及到了抓包、自动化脚本、定时任务、请求策略、stream 和 postman 等知识；","categories":[{"name":"碎片杂文","slug":"碎片杂文","permalink":"https://docs.hehouhui.cn/categories/%E7%A2%8E%E7%89%87%E6%9D%82%E6%96%87/"}],"tags":[{"name":"咖啡","slug":"咖啡","permalink":"https://docs.hehouhui.cn/tags/%E5%92%96%E5%95%A1/"},{"name":"整活","slug":"整活","permalink":"https://docs.hehouhui.cn/tags/%E6%95%B4%E6%B4%BB/"},{"name":"技术流","slug":"技术流","permalink":"https://docs.hehouhui.cn/tags/%E6%8A%80%E6%9C%AF%E6%B5%81/"}]},{"title":"Hutool 5.8.8  BeanUtil.copyProperties 致命异常","slug":"archives/Hutool 5.8.8  BeanUtil.copyProperties 致命异常","date":"2023-05-23T00:00:00.000Z","updated":"2023-10-08T06:42:00.000Z","comments":true,"path":"archives/hutool-beanutil-error.html","link":"","permalink":"https://docs.hehouhui.cn/archives/hutool-beanutil-error.html","excerpt":"","text":"前言最近项目上要求升级一个工具包hutool的版本，以解决安全漏洞问题，这不升级还好，一升级反而捅出了更大的篓子，究竟是怎么回事呢？ 事件回顾我们项目原先使用的hutool版本是 5.7.2，在代码中，我们的数据传输对象 DTO 和数据实体对象中大量使用了工具包中的BeanUtil.copyProperties(), 大体代码如下： 数据传输对象 @Data @ToString public class DiagramDTO &#123; // 前端生产的字符串id private String id; private String code; private String name; &#125; 数据实体对象 @Data @ToString public class Diagram &#123; private Integer id; private String code; private String name; &#125; 业务逻辑 public class BeanCopyTest &#123; public static void main(String[] args) &#123; // 前端传输的对象 DiagramDTO diagramDTO = new DiagramDTO(); // 如果前端传入的id事包含e的，升级后就会报错 diagramDTO.setId(\"3em3dgqsgmn0\"); diagramDTO.setCode(\"d1\"); diagramDTO.setName(\"图表\"); Diagram diagram = new Diagram(); // 关键点，数据拷贝 BeanUtil.copyProperties(diagramDTO, diagram); System.out.println(\"数据实体对象：\" + diagram); //设置id为空，自增 diagram.setId(null); //保存到数据库中 TODO //diagramMapper.save(diagram); &#125;&#125; 升级前，hutool是 5.7.2 版本下，执行结果如下图。 BeanUtil.copyProperties虽然字段类型不一样，但是做了兼容处理，所以业务没有影响业务逻辑。 升级后，hutool是 5.8.8 版本，执行结果如下图所示： 执行报错，因为升级后的版本修改了实现，增加了下面的逻辑，如果包含 E, 就会抛错，从而影响了业务逻辑，同时这个 id 是否包含 e 又是随机因素，到了生产才发现，就悲剧了。 分析探讨我发现大部分人写代码都喜欢偷懒，在上面的场景中，虽然BeanUtil.copyProperties用的一时爽，但有时候带来的后果是很严重的，所以很不推荐这种方式。为什么这么说呢？ 比如团队中的某些人偷偷改了数据传输对象 DTO，比如修改了类型、删去了某个字段。用BeanUtil.copyProperties的方式压根无法在编译阶段发现，更别提修改的影响范围了，这就只能把风险暴露到生产上去了。那有什么更好的方法呢？ 推荐方案 原始的get、set方式 我是比较推崇这种做法的，比如现在DiagramDTO删去某个字段，编译器就会报错，就会引起你的注意了，让问题提前暴露，无处遁形。 你可能觉得站着说话不腰疼，字段少好，如果字段很多还不得写死啊，我这里推荐一个 IDEA 的插件，可以帮你智能生成这样的代码。 话不多说，自己玩儿去~~ 使用开源库ModelMapper ModelMapper是一个开源库，可以很方便、简单地将对象从一种类型映射到另一种类型，底层是通过反射来自动确定对象之间的映射，还可以自定义映射规则。 private static void testModelMapper() &#123; ModelMapper modelMapper = new ModelMapper(); DiagramDTO diagramDTO = new DiagramDTO(); diagramDTO.setId(\"3em3dgqsgmn0\"); diagramDTO.setCode(\"d1\"); diagramDTO.setName(\"图表\"); Diagram diagram = modelMapper.map(diagramDTO, Diagram.class); &#125; 使用开源库MapStruct MapStruct也是 Java 中另外一个用于映射对象很流行的开源工具。它是在编译阶段生成对应的映射代码，相对于ModelMapper底层放射的方案，性能更好。 @Mapperpublic interface DiagramMapper &#123; DiagramMapper INSTANCE = Mappers.getMapper(DiagramMapper.class); DiagramDTO toDTO(Diagram diagram); Diagram toEntity(DiagramDTO diagram); &#125; private static void testMapStruct() &#123; DiagramDTO diagramDTO = new DiagramDTO(); diagramDTO.setId(\"3em3dgqsgmn0\"); diagramDTO.setCode(\"d1\"); diagramDTO.setName(\"图表\"); Diagram diagram = DiagramMapper.INSTANCE.toEntity(diagramDTO); &#125; DiagramMapper接口使用了@Mapper注解，用来表明使用MapStruct处理 MapStruct中更多高级特性大家自己探索一下。 总结小结一下，对象在不同层之间进行转换映射，很不建议使用BeanUtil.copyProperties这种方式，更加推荐使用原生的set, get方式，不容易出错。当然这不是将BeanUtil.copyProperties一棒子打死，毫无用武之地，在特定场景，比如方法内部对象的转换等影响小的范围还是很方便的。 原作者： https://blog.csdn.net/weiioy","categories":[{"name":"技术分享","slug":"技术分享","permalink":"https://docs.hehouhui.cn/categories/%E6%8A%80%E6%9C%AF%E5%88%86%E4%BA%AB/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://docs.hehouhui.cn/tags/Java/"},{"name":"BUG","slug":"BUG","permalink":"https://docs.hehouhui.cn/tags/BUG/"},{"name":"hutool","slug":"hutool","permalink":"https://docs.hehouhui.cn/tags/hutool/"}]},{"title":"使用 Keycloak REST API 进行身份验证和授权","slug":"archives/使用 Keycloak REST API 进行身份验证和授权","date":"2023-04-28T05:13:00.000Z","updated":"2023-10-08T06:42:00.000Z","comments":true,"path":"archives/authentication-and-authorization-using-the-keycloak-rest-api.html","link":"","permalink":"https://docs.hehouhui.cn/archives/authentication-and-authorization-using-the-keycloak-rest-api.html","excerpt":"","text":"启用身份验证和授权涉及复杂的功能，而不仅仅是简单的登录 API。在之前的文章（使用 Keycloak 的 API 登录和 JWT 令牌生成）中，我描述了 Keycloak REST 登录 API 端点，它只处理一些身份验证任务。在本文中，我描述了如何使用开箱即用的 Keycloak REST API 功能启用身份验证和授权的其他方面。 身份验证与授权但首先，身份验证和授权之间有什么区别？简单地说，身份验证意味着你是谁，而授权意味着你可以做什么，每种方法都使用不同的方法进行验证。例如，身份验证使用用户管理和登录表单，授权使用基于角色的访问控制 (RBAC) 或访问控制列表 (ACL)。幸运的是，这些验证方法在 Red Hat 的单点登录 (SSO) 工具中提供，或者在他们的上游开源项目 Keycloak 的 REST API 中提供。 Keycloak SSO 案例研究为了更好地理解使用 Keycloak 进行身份验证和授权，让我们从一个简单的案例研究开始。假设印度尼西亚教育部计划创建与多所学校的单点登录集成。他们计划使用集中式平台在多个学校维护学生和教师的单一帐户 ID。这让每个用户都具有相同的角色，但在每所学校具有不同的访问权限和特权，如图 1 所示。 图 1：每个用户都可以使用相同的角色，但在每所学校具有不同的访问权限和权限。”&gt; Keycloak SSO 演示让我们通过创建一个 Keycloak 领域来开始演示。为该部门使用 Add realm 对话框（如图 2 所示）。为领域命名education，将Enabled设置为ON，然后单击Create。 图 2：为教育部创建名为“教育”的 Keycloak 领域。”&gt; 接下来，转到 Roles 页面并确保选择了Realm Roles选项卡，如图 3 所示。 图 3：为教育领域创建两个独立的角色：教师和学生。 单击添加角色为该领域创建两个单独的角色，称为“教师”和“学生”。然后，这些新角色将出现在Realm Roles选项卡中，如图 4 所示。 图 4：添加教师和学生角色。”&gt; 然后，使用 Clients 页面，单击Create添加客户端，如图 5 所示。 图 5：添加客户端。 在 Add Client 页面上，创建一个名为“jakarta-school”的客户端，然后单击Save添加该客户端，如图 6 所示。 在 jakarta-school 详细信息页面上，转到“设置”选项卡并输入以下客户端配置，如图 7 所示： 客户 ID : jakarta-school 启用 ：开 需要同意 ：关闭 客户端协议 ：openid-connect 访问类型 ：机密 启用标准流量 ：开 启用影响流 ：关闭 已启用直接访问授权 ：ON 图 7：在 jakarta-school 详细信息页面上，输入客户端配置。 在同一页面的底部，在 Authentication Flow Overrides 部分，我们可以设置如下，如图 8 所示： 浏览器流程 ：浏览器 直接拨款流程 ：直接拨款 图 8：配置身份验证流程覆盖。”&gt; 转到Roles选项卡，单击Add Role，然后为此客户端创建 create-student-grade、view-student-grade 和 view-student-profile 角色，如图 9 所示。每个角色都应设置为 Composite False。 图 9：为此客户创建角色。 接下来，转到Client Scopes选项卡，在Default Client Scopes部分中，将“roles”和“profile”添加到Assigned Default Client Scopes中，如图 10 所示。 图 10：设置客户端范围。 在 jakarta-school 详情页面，选择Mappers，然后选择Create Protocol Mappers，设置 mappers 在 Userinfo API 上显示客户端角色，如图 11 所示： 名称 ：角色 映射器类型 ：用户领域角色 多值 ：开 令牌声明名称 ：角色 声明 JSON 类型 ：字符串 添加到 ID 令牌 ：关闭 添加到访问令牌 ：关闭 添加到用户信息 ：ON 图 11：设置映射器以显示客户端角色。 接下来，转到Users页面，选择Add user，创建新用户，然后单击Save，如图 12 所示： 用户名 ：埃德温 电子邮件 ：&#101;&#100;&#x77;&#105;&#x6e;&#x40;&#x72;&#x65;&#100;&#x68;&#97;&#x74;&#x2e;&#99;&#111;&#109; 名字 ：埃德温 姓 : M 用户启用 ：ON 电子邮件已验证 ：关闭 图 12：创建新用户。 最后，在Role Mappings选项卡中，为 jakarta-school 中的每个用户选择Client Roles，如图 13 所示。它们应该是 create-student-grade、view-student-grade 和 view-student-profile。 图 13：为 jakarta-school 中的每个用户选择客户端角色。 我的 Keycloak 配置演示到此结束。 使用 Java 应用程序的 Keycloak 连接现在我想演示如何开发一个非常简单的 Java 应用程序。此应用程序连接到您的 Keycloak 实例，并通过其 REST API 使用 Keycloak 的身份验证和授权功能。 首先，从 pom.xml 文件开始开发 Java 应用程序，如以下示例所示： &lt;?xml version=\"1.0\" encoding=\"UTF-8\"?> &lt;project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\"> &lt;modelVersion>4.0.0&lt;/modelVersion> &lt;groupId>EducationService&lt;/groupId> &lt;artifactId>com.edw&lt;/artifactId> &lt;version>1.0-SNAPSHOT&lt;/version> &lt;parent> &lt;groupId>org.springframework.boot&lt;/groupId> &lt;artifactId>spring-boot-starter-parent&lt;/artifactId> &lt;version>2.2.1.RELEASE&lt;/version> &lt;relativePath/> &lt;/parent> &lt;properties> &lt;java.version>11&lt;/java.version> &lt;/properties> &lt;dependencies> &lt;dependency> &lt;groupId>org.springframework.boot&lt;/groupId> &lt;artifactId>spring-boot-starter&lt;/artifactId> &lt;/dependency> &lt;dependency> &lt;groupId>org.springframework.boot&lt;/groupId> &lt;artifactId>spring-boot-starter-web&lt;/artifactId> &lt;/dependency> &lt;dependency> &lt;groupId>com.auth0&lt;/groupId> &lt;artifactId>jwks-rsa&lt;/artifactId> &lt;version>0.12.0&lt;/version> &lt;/dependency> &lt;dependency> &lt;groupId>com.auth0&lt;/groupId> &lt;artifactId>java-jwt&lt;/artifactId> &lt;version>3.8.3&lt;/version> &lt;/dependency> &lt;/dependencies> &lt;build> &lt;plugins> &lt;plugin> &lt;groupId>org.springframework.boot&lt;/groupId> &lt;artifactId>spring-boot-maven-plugin&lt;/artifactId> &lt;/plugin> &lt;/plugins> &lt;/build> &lt;/project> Java 应用程序还要求您开发一个简单的属性文件： server: error: whitelabel: enabled:false port: 8082 spring: mvc: favicon: enabled:false keycloak: client-id: jakarta-school client-secret: 197bc3b4-64b0-452f-9bdb-fcaea0988e90 scope: openid, profile authorization-grant-type: password authorization-uri: http://localhost:8080/auth/realms/education/protocol/openid-connect/auth user-info-uri: http://localhost:8080/auth/realms/education/protocol/openid-connect/userinfo token-uri: http://localhost:8080/auth/realms/education/protocol/openid-connect/token logout: http://localhost:8080/auth/realms/education/protocol/openid-connect/logout jwk-set-uri: http://localhost:8080/auth/realms/education/protocol/openid-connect/certs certs-id: vdaec4Br3ZnRFtZN-pimK9v1eGd3gL2MHu8rQ6M5SiE logging: level: root: INFO 接下来，从图 14 所示的表单中获取 Keycloak 证书 ID。 图 14：找到 Keycloak 证书 ID。 之后，最重要的是，您的下一个任务是开发集成代码；此操作涉及多个 Keycloak API。请注意，我没有详细介绍 Keycloak 登录 API，因为它已经在我之前的文章中进行了描述。 从一个简单的注销 API 开始： @Value(\"$&#123;keycloak.logout&#125;\") private String keycloakLogout; public void logout(String refreshToken) throws Exception &#123; MultiValueMap&lt;String, String> map = new LinkedMultiValueMap&lt;>(); map.add(\"client_id\",clientId); map.add(\"client_secret\",clientSecret); map.add(\"refresh_token\",refreshToken); HttpEntity&lt;MultiValueMap&lt;String, String>> request = new HttpEntity&lt;>(map, null); restTemplate.postForObject(keycloakLogout, request, String.class); &#125; 首先，我想指出，对于注销，使用参数refresh_token而不是是至关重要的access_token。现在，使用 API 检查不记名令牌是否有效和活跃，以验证请求是否带来有效凭证。 @Value(\"$&#123;keycloak.user-info-uri&#125;\") private String keycloakUserInfo; private String getUserInfo(String token) &#123; MultiValueMap&lt;String, String> headers = new LinkedMultiValueMap&lt;>(); headers.add(\"Authorization\", token); HttpEntity&lt;MultiValueMap&lt;String, String>> request = new HttpEntity&lt;>(null, headers); return restTemplate.postForObject(keycloakUserInfo, request, String.class); &#125; 对于授权，您可以使用两种方法来决定给定角色是否有资格访问特定 API。第一种方法是通过针对 Keycloak 的 userinfo API 进行验证来确定不记名令牌带来的角色，下一个方法是验证不记名令牌中的角色。 对于第一种方法，您可以期待 Keycloak 的以下响应： &#123; \"sub\": \"ef2cbe43-9748-40e5-aed9-fe981e3082d5\", \"roles\": [\"teacher\"], \"name\": \"Edwin M\", \"preferred_username\": \"edwin\", \"given_name\": \"Edwin \", \"family_name\": \"M\" &#125; 如您所见，那里有一个roles标签，一种方法是根据它来验证访问权限。缺点是每个请求在您的应用程序和 Keycloak 之间有多次往返请求，这会导致更高的延迟。 另一种方法是读取通过每个请求发送的 JWT 令牌的内容。为了成功解码您的 JWT 令牌，您必须知道用于对其签名的公钥。这就是 Keycloak 提供 JWKS 端点的原因。您可以使用 curl 命令查看其内容，如以下示例所示： &#123; \"keys\": [ &#123; \"kid\": \"vdaec4Br3ZnRFtZN-pimK9v1eGd3gL2MHu8rQ6M5SiE\", \"kty\": \"RSA\", \"alg\": \"RS256\", \"use\": \"sig\", \"n\": \"4OPCc_LDhU6ADQj7cEgRei4VUf4PZH8GYsxsR6RSdeKmDvZ48hCSEFiEgfc3FIfh-gC4r9PtKucc_nkRofrAKR4qL8KNNoSuzQAOC92Yz6r7Ao4HppHJ8-QVdo5H-d9wfNSlDLBSo5My4b4EnHb1HLuFxDqyhFpGvsoUJdgbt3m_Q3WAVb2yrM83S6HX__vrQvqUk2e7z5RNrI7LSsW3ZOz9fU7pvm8-kFFAIPJ7fOJIC7UQ9wBWg3YdwQ0B2b24jXjVr0QCGzqJ6o1G_UZYSJCDMGQDpDcEuYnvSKBLfVR-0EcAjolRhcSPjHlW0Cp0YU8qwWDHpjkbrMrFmxlO4Q\", \"e\": \"AQAB\" &#125; ] &#125; 请注意，在前面的示例中，kid 代表 key id，alg 代表加密算法，n 代表用于此领域的公钥。您可以使用此公钥轻松解码我们的 JWT 令牌，并从 JWT 声明中读取 roles。下面显示了解码后的样本 JWT 令牌： &#123; \"jti\": \"85edca8c-a4a6-4a4c-b8c0-356043e7ba7d\", \"exp\": 1598079154, \"nbf\": 0, \"iat\": 1598078854, \"iss\": \"http://localhost:8080/auth/realms/education\", \"sub\": \"ef2cbe43-9748-40e5-aed9-fe981e3082d5\", \"typ\": \"Bearer\", \"azp\": \"jakarta-school\", \"auth_time\": 0, \"session_state\": \"f8ab78f8-15ee-403d-8db7-7052a8647c65\", \"acr\": \"1\", \"realm_access\": &#123; \"roles\": [\"teacher\"] &#125;, \"resource_access\": &#123; \"jakarta-school\": &#123; \"roles\": [ \"create-student-grade\", \"view-student-profile\", \"view-student-grade\" ] &#125; &#125;, \"scope\": \"profile\", \"name\": \"Edwin M\", \"preferred_username\": \"edwin\", \"given_name\": \"Edwin\", \"family_name\": \"M\" &#125; 您可以使用以下示例中显示的代码读取角色标签： @GetMapping(\"/teacher\") public HashMap teacher(@RequestHeader(\"Authorization\") String authHeader) &#123; try &#123; DecodedJWT jwt = JWT.decode(authHeader.replace(\"Bearer\", \"\").trim()); // check JWT is valid Jwk jwk = jwtService.getJwk(); Algorithm algorithm = Algorithm.RSA256((RSAPublicKey) jwk.getPublicKey(), null); algorithm.verify(jwt); // check JWT role is correct List&lt;String> roles = ((List)jwt.getClaim(\"realm_access\").asMap().get(\"roles\")); if(!roles.contains(\"teacher\")) throw new Exception(\"not a teacher role\"); // check JWT is still active Date expiryDate = jwt.getExpiresAt(); if(expiryDate.before(new Date())) throw new Exception(\"token is expired\"); // all validation passed return new HashMap() &#123;&#123; put(\"role\", \"teacher\"); &#125;&#125;; &#125; catch (Exception e) &#123; logger.error(\"exception : &#123;&#125; \", e.getMessage()); return new HashMap() &#123;&#123; put(\"status\", \"forbidden\"); &#125;&#125;; &#125; &#125; 这种方法最好的部分是您可以将来自 Keycloak 的公钥放在缓存中，从而减少往返请求，这种做法最终会增加应用程序延迟和性能。可以在我的 GitHub 存储库中找到本文的完整代码。 结论总之，我准备这篇文章首先是为了解释启用身份验证和授权涉及复杂的功能，而不仅仅是一个简单的登录 API。然后我演示了如何使用开箱即用的 Keycloak REST API 功能启用身份验证和授权的许多方面。 此外，我还演示了如何开发一个连接到您的 Keycloak 实例的简单 Java 应用程序，并通过其 REST API 使用 Keycloak 的身份验证和授权功能。","categories":[{"name":"技术分享","slug":"技术分享","permalink":"https://docs.hehouhui.cn/categories/%E6%8A%80%E6%9C%AF%E5%88%86%E4%BA%AB/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://docs.hehouhui.cn/tags/Java/"},{"name":"建站","slug":"建站","permalink":"https://docs.hehouhui.cn/tags/%E5%BB%BA%E7%AB%99/"},{"name":"keycloak","slug":"keycloak","permalink":"https://docs.hehouhui.cn/tags/keycloak/"},{"name":"oauth","slug":"oauth","permalink":"https://docs.hehouhui.cn/tags/oauth/"},{"name":"开发","slug":"开发","permalink":"https://docs.hehouhui.cn/tags/%E5%BC%80%E5%8F%91/"}]},{"title":"Keycloak 客户端授权服务","slug":"archives/Keycloak 客户端授权服务","date":"2023-04-28T00:00:00.000Z","updated":"2023-11-02T08:24:00.000Z","comments":true,"path":"archives/keycloak-client-oauth-guide.html","link":"","permalink":"https://docs.hehouhui.cn/archives/keycloak-client-oauth-guide.html","excerpt":"","text":"授权服务概述编辑本节报告问题 Keycloak 支持细粒度的授权策略，并能够结合不同的访问控制机制，例如： 基于属性的访问控制 (ABAC) 基于角色的访问控制 (RBAC) 基于用户的访问控制 (UBAC) 基于上下文的访问控制 (CBAC) 基于规则的访问控制 使用 JavaScript 基于时间的访问控制 通过服务提供商接口 (SPI) 支持自定义访问控制机制 (ACM) Keycloak 基于一组管理 UI 和 RESTful API，并提供必要的方法来为您的受保护资源和范围创建权限，将这些权限与授权策略相关联，并在您的应用程序和服务中执行授权决策。 资源服务器（服务于受保护资源的应用程序或服务）通常依靠某种信息来决定是否应授予对受保护资源的访问权限。对于基于 RESTful 的资源服务器，该信息通常是从安全令牌中获取的，通常在每次向服务器发出请求时作为承载令牌发送。对于依赖会话来验证用户身份的 Web 应用程序，该信息通常存储在用户的会话中，并从那里为每个请求检索。 通常，资源服务器仅根据基于角色的访问控制 (RBAC) 执行授权决策，其中根据映射到这些相同资源的角色检查授予试图访问受保护资源的用户的角色。虽然角色非常有用并被应用程序使用，但它们也有一些限制： 资源和角色紧密耦合，角色的更改（例如添加、删除或更改访问上下文）会影响多个资源 对安全要求的更改可能意味着对应用程序代码进行深入更改以反映这些更改 根据您的应用程序大小，角色管理可能会变得困难且容易出错 它不是最灵活的访问控制机制。角色不代表您是谁，并且缺乏上下文信息。如果您被授予了一个角色，那么您至少拥有一些访问权限。 考虑到今天我们需要考虑用户分布在不同地域、本地策略不同、使用不同设备、信息共享需求高的异构环境，Keycloak 授权服务可以帮助您提升应用和服务的授权能力通过提供： 使用细粒度授权策略和不同访问控制机制的资源保护 集中的资源、权限和策略管理 中央政策决策点 基于一组基于 REST 的授权服务的 REST 安全性 授权工作流和用户管理的访问 有助于避免跨项目（和重新部署）代码复制并快速适应安全需求变化的基础设施。 从设计的角度来看，授权服务基于一组定义良好的授权模式，提供以下功能： 策略管理点 (PAP) 提供一组基于 Keycloak 管理控制台的 UI，用于管理资源服务器、资源、范围、权限和策略。其中一部分也通过使用Protection API远程完成。 政策决策点 (PDP) 提供一个可分发的策略决策点，指向发送授权请求的位置，并根据请求的权限相应地评估策略。有关详细信息，请参阅获取权限。 策略执行点 (PEP) 为不同环境提供实现，以在资源服务器端实际执行授权决策。Keycloak 提供了一些内置的Policy Enforcer。 政策信息点 (PIP) 基于 Keycloak Authentication Server，您可以在授权策略评估期间从身份和运行环境中获取属性。 授权流程三个主要流程定义了必要的步骤，以了解如何使用 Keycloak 为您的应用程序启用细粒度授权： 资源管理 权限和策略管理 政策执行 资源管理资源管理涉及定义受保护内容的所有必要步骤。 首先，您需要指定 Keycloak 您希望保护什么，这通常代表一个 Web 应用程序或一组一个或多个服务。有关资源服务器的更多信息，请参阅术语。 资源服务器使用 Keycloak 管理控制台进行管理。您可以在那里启用任何已注册的客户端应用程序作为资源服务器，并开始管理您要保护的资源和范围。 资源可以是网页、RESTFul 资源、文件系统中的文件、EJB 等等。它们可以代表一组资源（就像 Java 中的类），也可以代表单个特定资源。 例如，您可能拥有代表所有银行账户的银行账户资源，并使用它来定义所有银行账户通用的授权策略。但是，您可能希望为Alice 帐户（属于客户的资源实例）定义特定策略，其中仅允许所有者访问某些信息或执行操作。 可以使用 Keycloak 管理控制台或Protection API来管理资源。在后一种情况下，资源服务器能够远程管理它们的资源。 范围通常表示可以对资源执行的操作，但不限于此。您还可以使用范围来表示资源中的一个或多个属性。 权限和策略管理一旦定义了资源服务器和所有要保护的资源，就必须设置权限和策略。 此过程涉及实际定义管理资源的安全和访问要求的所有必要步骤。 策略定义了访问某物（资源或范围）或对其执行操作所必须满足的条件，但它们与所保护的内容无关。它们是通用的，可以重复用于构建权限或什至更复杂的策略。 例如，要仅允许具有“高级用户”角色的用户访问一组资源，您可以使用 RBAC（基于角色的访问控制）。 Keycloak 提供了一些内置的策略类型（及其各自的策略提供者），涵盖了最常见的访问控制机制。您甚至可以根据使用 JavaScript 编写的规则创建策略。 一旦您定义了策略，您就可以开始定义您的权限。权限与它们所保护的资源相结合。您可以在此处指定要保护的内容（资源或范围）以及授予或拒绝权限必须满足的策略。 政策执行策略执行涉及必要的步骤，以实际执行对资源服务器的授权决策。这是通过在资源服务器上启用策略执行点或 PEP 来实现的，该资源服务器能够与授权服务器通信，请求授权数据并根据服务器返回的决策和权限控制对受保护资源的访问。 Keycloak 提供了一些内置的Policy Enforcer实现，您可以使用它们来保护您的应用程序，具体取决于它们运行的平台。 授权服务授权服务由以下 RESTFul 端点组成： 令牌端点 资源管理端点 权限管理端点 这些服务中的每一个都提供一个特定的 API，涵盖授权过程中涉及的不同步骤。 令牌端点OAuth2 客户端（例如前端应用程序）可以使用令牌端点从服务器获取访问令牌，并使用这些相同的令牌访问受资源服务器保护的资源（例如后端服务）。同样，Keycloak 授权服务为 OAuth2 提供扩展，以允许根据与所请求的资源或范围关联的所有策略的处理来发布访问令牌。这意味着资源服务器可以根据服务器授予并由访问令牌持有的权限强制访问其受保护的资源。在 Keycloak 授权服务中，具有权限的访问令牌称为请求方令牌或简称 RPT。 额外资源 获取权限 保护 APIProtection API是一组符合 UMA 的端点，为资源服务器提供操作，以帮助它们管理与它们相关的资源、范围、权限和策略。仅允许资源服务器访问此 API，这也需要一个 uma_protection范围。 Protection API 提供的操作可以分为两大类： 资源管理 创建资源 删除资源 按 ID 查找 询问 权限管理 签发许可票 默认情况下，远程资源管理处于启用状态。您可以使用 Keycloak 管理控制台更改它，并且只允许通过控制台进行资源管理。 在使用 UMA 协议时，Protection API 签发 Permission Tickets 是整个授权流程的重要环节。如后续部分所述，它们表示客户端请求的权限，这些权限被发送到服务器以获得最终令牌，其中包含在评估与请求的资源和范围关联的权限和策略期间授予的所有权限。 额外资源 保护 API 术语在继续之前，了解 Keycloak 授权服务引入的这些术语和概念很重要。 资源服务器根据 OAuth2 术语，资源服务器是托管受保护资源并能够接受和响应受保护资源请求的服务器。 资源服务器通常依靠某种信息来决定是否应该授予对受保护资源的访问权限。对于基于 RESTful 的资源服务器，该信息通常在安全令牌中携带，通常作为不记名令牌与每个请求一起发送到服务器。依赖会话来验证用户身份的 Web 应用程序通常将该信息存储在用户的会话中，并从那里为每个请求检索它。 在 Keycloak 中，任何机密的客户端应用程序都可以充当资源服务器。此客户端的资源及其各自的范围受一组授权策略的保护和管理。 资源资源是应用程序和组织资产的一部分。它可以是一组一个或多个端点，一个经典的网络资源，如 HTML 页面，等等。在授权策略术语中，资源是受保护的对象。 每个资源都有一个唯一标识符，可以表示单个资源或一组资源。例如，您可以管理一个银行账户资源，它代表并定义了一组适用于所有银行账户的授权策略。但是您也可以拥有一个名为Alice’s Banking Account的不同资源，它代表单个客户拥有的单个资源，它可以有自己的一组授权策略。 范围资源的范围是可以对资源执行的访问的有界范围。在授权策略术语中，范围是逻辑上可应用于资源的潜在许多动词之一。 它通常指示可以使用给定资源做什么。范围的示例有查看、编辑、删除等。但是，范围也可以与资源提供的特定信息相关。在这种情况下，您可以拥有项目资源和成本范围，其中成本范围用于定义用户访问项目成本的特定策略和权限。 允许考虑这个简单且非常常见的权限： 权限将受保护的对象与必须评估以确定是否授予访问权限的策略相关联。 X可以对资源**Z 做****Y** 哪里…… X代表一个或多个用户、角色或组，或它们的组合。您还可以在此处使用声明和上下文。 Y表示要执行的操作，例如写入、查看等。 Z表示受保护的资源，例如“&#x2F;accounts”。 Keycloak 提供了一个丰富的平台，用于构建从简单到非常复杂的基于规则的动态权限的一系列权限策略。它提供了灵活性并有助于： 降低代码重构和权限管理成本 支持更灵活的安全模型，帮助您轻松适应安全需求的变化 在运行时进行更改；应用程序只关心受保护的资源和范围，而不关心如何保护它们。 政策策略定义了授予对象访问权限所必须满足的条件。与权限不同，您不指定受保护的对象，而是指定访问给定对象（例如，资源、范围或两者）必须满足的条件。策略与可用于保护资源的不同访问控制机制 (ACM) 密切相关。借助策略，您可以实施基于属性的访问控制 (ABAC)、基于角色的访问控制 (RBAC)、基于上下文的访问控制或它们的任意组合的策略。 Keycloak 通过提供聚合策略的概念来利用策略的概念以及您如何定义它们，您可以在其中构建“策略的策略”并仍然控制评估的行为。Keycloak 授权服务中的策略实现遵循分而治之技术，而不是编写一个包含访问给定资源必须满足的所有条件的大型策略。也就是说，您可以创建单独的策略，然后以不同的权限重复使用它们，并通过组合单独的策略来构建更复杂的策略。 政策提供者策略提供者是特定策略类型的实现。Keycloak 提供内置策略，由相应的策略提供者支持，您可以创建自己的策略类型来支持您的特定要求。 Keycloak 提供了一个 SPI（服务提供者接口），您可以使用它来插入您自己的策略提供者实现。 许可票权限票证是由用户管理的访问 (UMA) 规范定义的一种特殊类型的令牌，它提供了一种不透明的结构，其形式由授权服务器确定。此结构表示客户端请求的资源和&#x2F;或范围、访问上下文以及必须应用于授权数据请求（请求方令牌 [RPT]）的策略。 在 UMA 中，许可票证对于支持个人对个人共享以及个人对组织共享至关重要。将权限票证用于授权工作流可以实现从简单到复杂的一系列场景，在这些场景中，资源所有者和资源服务器可以根据管理对这些资源的访问的细粒度策略来完全控制他们的资源。 在 UMA 工作流中，权限票证由授权服务器颁发给资源服务器，资源服务器将权限票证返回给试图访问受保护资源的客户端。客户端收到票证后，可以通过将票证发送回授权服务器来请求 RPT（持有授权数据的最终令牌）。 有关权限票证的更多信息，请参阅用户管理的访问和UMA规范。 入门在使用本教程之前，您需要完成 Keycloak 的安装并创建初始管理员用户，如入门指南教程中所示。对此有一个警告。您必须在与 Keycloak 服务器相同的机器上运行一个单独的 WildFly 实例。这个单独的实例将运行您的 Java Servlet 应用程序。因此，您必须在不同的端口下运行 Keycloak，以便在同一台机器上运行时不会出现端口冲突。jboss.socket.binding.port-offset在命令行上使用系统属性。此属性的值是一个数字，将添加到 Keycloak 服务器打开的每个端口的基值中。 启动 Keycloak 服务器： Linux&#x2F;Unix $ .../bin/kc.sh start-dev --http-port 8180 视窗 > ...\\bin\\kc.bat start-dev --http-port 8180 安装并启动两台服务器后，您应该能够通过http://localhost:8180/auth/admin/访问 Keycloak 管理控制台，也可以通过http://localhost:8080 访问WildFly 实例 。 额外资源 有关安装和配置 WildFly 实例的更多详细信息，请参阅保护应用程序和服务指南。 保护 servlet 应用程序本入门指南的目的是让您尽快启动并运行，以便您可以试验和测试 Keycloak 提供的各种授权功能。此快速浏览在很大程度上依赖于默认数据库和服务器配置，并且不涵盖复杂的部署选项。有关功能或配置选项的更多信息，请参阅本文档中的相应部分。 本指南解释了有关 Keycloak 授权服务的关键概念： 为客户端应用程序启用细粒度授权 将客户端应用程序配置为具有受保护资源的资源服务器 定义权限和授权策略以管理对受保护资源的访问 在您的应用程序中启用策略实施。 创建领域和用户本教程的第一步是创建一个领域和该领域中的一个用户。然后，在领域内，我们将创建一个单一的客户端应用程序，然后成为您需要为其启用授权服务的资源服务器。 程序 创建一个名为hello-world-authz 的领域。创建完成后，会显示类似下图的页面： 领域 hello-world-authz 单击用户。 用户列表页面显示您可以在其中创建用户。 单击创建用户。 完成用户名、电子邮件、名字和姓氏字段。 将用户启用切换为ON。 单击创建。 添加用户 通过单击“凭据”选项卡为用户设置密码。 设置用户密码 完成New Password和Password Confirmation字段并将Temporary切换为OFF。 单击保存。 单击保存密码。 启用授权服务您可以在配置为使用 OpenID Connect 协议的现有客户端应用程序中启用授权服务。您还可以使用以下过程创建客户端。 程序 单击菜单中的客户端。 填写客户端类型。 单击下一步。 将客户端身份验证切换为ON。 将授权切换为ON。 单击保存。 向下滚动到功能配置部分。 填写根 URL字段。 单击保存。 创建客户端应用程序 将为客户端显示一个新的授权选项卡。 客户端设置 单击授权选项卡。 将显示类似于以下内容的授权设置页面： 授权设置 当您为客户端应用程序启用授权服务时，Keycloak 会自动为您的客户端授权配置创建几个默认设置。 额外资源 启用授权服务 默认配置 构建、部署和测试您的应用程序现在app-authz-vanilla资源服务器（或客户端）已正确配置并启用授权服务，可以将其部署到服务器。 您要部署的应用程序的项目和代码在Keycloak Quickstarts Repository中可用。在继续之前，您需要在计算机上安装以下内容并在您的 PATH 中可用： Java JDK 8 Apache Maven 3.1.1 或更高版本 Git 您可以通过在 https://github.com/keycloak/keycloak-quickstarts克隆存储库来获取代码。快速入门旨在与最新的 Keycloak 版本一起使用。 按照以下步骤下载代码。 克隆项目 git clone https://github.com/keycloak/keycloak-quickstarts 我们即将构建和部署的应用程序位于 cd keycloak-quickstarts/app-authz-jee-vanilla 获取适配器配置在构建和部署应用程序之前，您必须先获取适配器配置。 程序 登录管理控制台。 单击菜单中的客户端。 在客户端列表中，单击app-authz-vanilla客户端应用程序。客户端设置页面打开。 客户端设置 从操作列表中，选择下载适配器配置。 从格式选项列表中，选择Keycloak OIDC JSON。 适配器配置以 JSON 格式显示。 单击下载。 适配器配置 将文件移动keycloak.json到app-authz-jee-vanilla/config目录。 （可选）指定重定向 URL。 403默认情况下，当用户没有访问资源服务器上受保护资源的权限时，策略执行器会使用状态代码进行响应。但是，您也可以为未经授权的用户指定重定向 URL。要指定重定向 URL，请编辑您更新的keycloak.json文件并将policy-enforcer配置替换为以下内容： &#96;”policy-enforcer”: { “on-deny-redirect-to” : “&#x2F;app-authz-vanilla&#x2F;error.jsp” }&#96; /app-authz-vanilla/error.jsp如果用户没有访问受保护资源的必要权限，而不是无用的消息，则此更改指定策略执行者将用户重定向到页面403 Unauthorized。","categories":[{"name":"技术分享","slug":"技术分享","permalink":"https://docs.hehouhui.cn/categories/%E6%8A%80%E6%9C%AF%E5%88%86%E4%BA%AB/"}],"tags":[{"name":"建站","slug":"建站","permalink":"https://docs.hehouhui.cn/tags/%E5%BB%BA%E7%AB%99/"},{"name":"keycloak","slug":"keycloak","permalink":"https://docs.hehouhui.cn/tags/keycloak/"},{"name":"oauth","slug":"oauth","permalink":"https://docs.hehouhui.cn/tags/oauth/"}]},{"title":"使用 Keycloak 的 API 登录和 JWT 令牌生成","slug":"archives/使用 Keycloak 的 API 登录和 JWT 令牌生成","date":"2023-04-28T00:00:00.000Z","updated":"2023-10-08T06:42:00.000Z","comments":true,"path":"archives/api-login-and-jwt-token-generation-using-keycloak.html","link":"","permalink":"https://docs.hehouhui.cn/archives/api-login-and-jwt-token-generation-using-keycloak.html","excerpt":"","text":"Red Hat 单点登录(SSO) — 或其开源版本 Keycloak — 是 Web SSO 功能的领先产品之一，它基于流行的标准，例如安全断言标记语言 (SAML) 2.0、OpenID Connect 和 OAuth 2.0。Red Hat SSO 最强大的功能之一是我们可以通过多种方式直接访问 Keycloak，无论是通过简单的 HTML 登录表单，还是通过 API 调用。在以下场景中，我们将生成一个 JWT 令牌，然后对其进行验证。一切都将使用 API 调用来完成，因此 Keycloak 的 UI 不会直接暴露给公众。 设置用户 首先，我们将在 Keycloak 中创建一个简单的用户，如图 1 所示。 图 1：在 Keycloak 中创建用户。”&gt; 填写所有必填字段，例如Username、First Name和Last Name，如图 2 所示。 图 2：输入用户信息。”&gt; 设置用户密码，如图 3 所示。 图 3：设置用户密码。”&gt; 设置客户端下一步是在我们的领域中创建一个特定的客户端，如图 4 所示。Keycloak 中的客户端代表特定用户可以访问的资源，无论是用于验证用户身份、请求身份信息还是验证访问令牌。 图 4：查看您现有的客户。”&gt; 单击“创建” ，打开“添加客户端”对话框，如图 5 所示。 图 5：创建新客户端。”&gt; 填写客户表格中的所有必填字段。请特别注意Direct Grant Flow（如图 6 所示）并将其值设置为direct grant。此外，将访问类型更改为机密。 图 6：覆盖客户端的身份验证流程。”&gt; 最后，将 Client Authenticator字段中的客户端凭证更改为Client Id 和 Secret，如图 7 所示。 图 7：设置新客户的凭据。”&gt; 测试你的新客户现在我们可以通过 REST API 来测试我们新创建的客户端来模拟一个简单的登录。我们的身份验证 URL 是 `http://localhost:8080/auth/realms/&lt;your-realm-name&gt;/protocol/openid-connect/token` 填写参数并使用我们的用户名和密码设置 client_id 和 client_secret： curl -L -X POST 'http://localhost:8080/auth/realms/whatever-realm/protocol/openid-connect/token' \\ -H 'Content-Type: application/x-www-form-urlencoded' \\ --data-urlencode 'client_id=clientid-03' \\ --data-urlencode 'grant_type=password' \\ --data-urlencode 'client_secret=ec78c6bb-8339-4bed-9b1b-e973d27107dc' \\ --data-urlencode 'scope=openid' \\ --data-urlencode 'username=emuhamma' \\ --data-urlencode 'password=1' 或者，我们可以使用 Postman 等 REST API 工具来模拟 HTTP POST 请求，如图 8 所示。 图 8：我们模拟的 HTTP POST 请求。”&gt; 结果将是一个有效的 JWT 令牌： &#123; \"access_token\": \"eyJhbGciOiJSUzI1NiIsInR5cCIgOiAiSldUIiwia2lkIiAwNjEwLCJpc3MiOiJodHRwO.......wKRTus6PAoHMFlIlYQ75dYiLzzuRMvdXkHl6naLNQ8wYDv4gi7A3eJ163YzXSJf5PmQ\", \"expires_in\": 600, \"refresh_expires_in\": 1800, \"refresh_token\": \"eyJhbGciOiJIUzI1NiIsInR5cC.......IsInZpZXctcHJvZmlsZSJdfX0sInNjb3BlIjoib3BlbmlkIGVtYWlsIHByb2ZpbGUifQ.ePV2aqeDjlg6ih6SA7_x77gT4JYyv7HvK7PLQW-X1mM\", \" token_type\": \"bearer\", \"id_token\": \"eyJhbGciOiJSUz......A_d_LV96VCLBeTJSpqeqpMJYlh4AMJqN6kddtrI4ixZLfwAIj-Qwqn9kzGe-v1-oe80wQXrXzVBG7TJbKm4x5bgCO_B9lnDMrey9 0rvaKKr48K697ug\", \" not-before-policy\": 0, \"session_state\": \"22c8278b-3346-468e-9533-f41f22ed264f\", \"scope\": \"openid email profile\" &#125; 错误的用户名和密码组合会导致 HTTP 401 响应代码和如下响应正文： &#123; “error”：“invalid_grant”， “error_description”：“无效的用户凭证” &#125; 给你。现在您已经配置了一个登录 API，可以很好地与 Keycloak 配合使用。玩得开心！","categories":[{"name":"技术分享","slug":"技术分享","permalink":"https://docs.hehouhui.cn/categories/%E6%8A%80%E6%9C%AF%E5%88%86%E4%BA%AB/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://docs.hehouhui.cn/tags/Java/"},{"name":"建站","slug":"建站","permalink":"https://docs.hehouhui.cn/tags/%E5%BB%BA%E7%AB%99/"},{"name":"keycloak","slug":"keycloak","permalink":"https://docs.hehouhui.cn/tags/keycloak/"},{"name":"oauth","slug":"oauth","permalink":"https://docs.hehouhui.cn/tags/oauth/"},{"name":"开发","slug":"开发","permalink":"https://docs.hehouhui.cn/tags/%E5%BC%80%E5%8F%91/"}]},{"title":"物联网发展及前景如何？物联网开发有哪些板块？java 如何驱动万物互联？","slug":"archives/物联网发展及前景如何？物联网开发有哪些板块？java 如何驱动万物互联？","date":"2023-04-27T00:00:00.000Z","updated":"2023-10-08T06:42:00.000Z","comments":true,"path":"archives/iot-prospect-for-java.html","link":"","permalink":"https://docs.hehouhui.cn/archives/iot-prospect-for-java.html","excerpt":"","text":"物联网的未来发展将会越来越广泛和深入，涉及到包括家庭、工业、医疗、交通等各个方面。随着物联网技术的不断发展，将会给人们带来更加便利和舒适的生活方式。 🥅 物联网的应用场景非常广泛，例如智能家居、智慧城市、智能医疗、智能制造等。智能家居可以通过物联网技术实现家电之间的互联互通，例如智能门锁、智能音响、智能家电等，用户可以通过手机 APP 或者语音控制完成对家中设备的操作。智慧城市可以通过物联网技术实现城市基础设施之间的互联互通和数据共享，例如智能交通、智能停车、智能照明等，实现城市的智能化管理和优化。智能医疗可以通过物联网技术实现医疗设备之间的互联互通和数据共享，例如智能健康监测设备、智能手环、智能康复仪等，可以有效地提高医疗设备的使用效率和医疗服务的质量。智能制造可以通过物联网技术实现工业设备之间的互联互通和数据共享，例如智能机床、智能物流、智能质检等，可以提高生产线的自动化程度和生产效率。 总之，物联网技术的未来将会更加广阔和深入，它将会引领人们进入一个智能化的世界，为人们带来更加便利和舒适的生活方式，也将会对社会经济发展和生产力的提升起到重要的推动作用。 物联网开发物联网开发板块包括：硬件开发、通讯协议、后台平台、前端应用等。硬件开发包括传感器、嵌入式系统、无线通信模块等；通讯协议包括蓝牙、Wi-Fi、ZigBee、LoRa 等；后台平台包括云平台、数据中心等；前端应用包括手机 APP、智能家居设备、物流追踪等。 Java 是目前应用比较广泛的开发语言之一，它不仅可以在传统的软件开发领域中发挥重要作用，同时还能在物联网领域中有所作为。Java 所提供的多样化的技术可以实现物联网技术的全面应用与发展。 例如，在物联网应用领域中，Java EE 技术可以为传感器和设备的数据传输、处理、存储及应用提供服务器端的支持，以保证物联网系统的稳定性和效率。Java ME 技术则可以专门针对嵌入式设备进行优化，尤其在轻量级设备领域中有着广泛的应用。而 Java SE 技术则可以用于物联网领域中的通信管理，使得数据的传输变得更为安全与高效。 同时，Java 还可以通过容器化部署、服务治理等技术来为物联网提供更完善的支持。容器化部署可以将物联网系统进行模块化处理，从而保证系统的灵活性和扩展性。而服务治理则可以为物联网系统提供更加规范化的服务管理，从而能够更好地保证系统的稳定性和安全性。 接入第三方云平台，如 AWS、Azure 等，则可以让 Java 更好地实现物联网中的云端计算、数据存储和分析等功能，并且还能够提供更加高效和可靠的服务。同时，Java 的多线程技术也可以实现物联网中的即时通信和实时数据传输，从而能够更加快速地进行数据同步和处理。 总之，Java 中众多的技术可以使物联网系统更加智能化、高效化、可扩展，并且还可以实现更好的数据管理、存储和分析等功能。因此，Java 在物联网领域中具有广泛的应用前景和发展空间。 Java 在 iot 中技术中常用 MQTT 协议进行交互，除了 MQTT 之外，Java 在 IoT 中还有许多其他知名的技术栈和框架。其中一个重要的技术是 CoAP（Constrained Application Protocol），它是一种轻量级的 Web 协议，用于将资源暴露给 IoT 设备，并且可以负责设备的发现和管理。此外，Java 还有很多基于 RESTful 架构的框架，如 Jersey 和 Spring，可以用于快速构建可扩展的 Web 应用程序，同时提供与 IoT 设备交互的 API。还有一个名为 Apache Camel 的框架，它提供了一种用于数据传输和转换的规则引擎，可以很好地用于 IoT 设备之间的消息传递和数据交换。总之，Java 在 IoT 中技术栈非常丰富，包括不同的框架和协议，可以帮助开发人员构建最适合他们需求的应用程序。","categories":[{"name":"学习思考","slug":"学习思考","permalink":"https://docs.hehouhui.cn/categories/%E5%AD%A6%E4%B9%A0%E6%80%9D%E8%80%83/"}],"tags":[{"name":"思考","slug":"思考","permalink":"https://docs.hehouhui.cn/tags/%E6%80%9D%E8%80%83/"},{"name":"物联网","slug":"物联网","permalink":"https://docs.hehouhui.cn/tags/%E7%89%A9%E8%81%94%E7%BD%91/"}]},{"title":"主动阅读：成为更好的阅读者","slug":"archives/主动阅读：成为更好的阅读者","date":"2023-04-25T00:00:00.000Z","updated":"2023-10-08T06:42:00.000Z","comments":true,"path":"archives/active-reading-becoming-better-reader.html","link":"","permalink":"https://docs.hehouhui.cn/archives/active-reading-becoming-better-reader.html","excerpt":"","text":"什么是主动阅读？被动阅读的典型代表便是应试教育之下的各种填鸭式阅读。 如果阅读者在阅读过程中的阅读心理和阅读行为都是被动的，那么对于知识和信息的掌握必然是低效的。 高效阅读者都会使用一系列被称为主动阅读的阅读策略。通过主动阅读，我们更加全面、更加深入地理解和掌握知识。 主动阅读意味着阅读者在阅读某些内容时，积极地与文本或者作者进行对话，尝试去理解、评估、记忆、反思所读内容。 在《如何阅读一本书》这本经典著作中，作者区分了基础阅读、检视阅读、分析阅读、主题阅读四种阅读层次。 其中，基础阅读以外的其他三种阅读层次都属于主动阅读。在作者看来，一个好的读者自然应该是主动阅读者。阅读的艺术便是需要通过批判性思维以适当的顺序提出适当的问题。 具体而言，主动阅读的基础包含四个基础问题： 整体而言，这本书在谈论什么？（关于主题与关键议题） 作者具体说了什么，怎么说的？（关于作者的想法、声明与论点） 这本书说的是否有道理？全部有道理，还是部分有道理？（对书进行批判性评价） 这本书跟你有什么关系？为你提供了资讯？启发了你？ 如果你想成为主动阅读者，那基本要求便是勇于承担阅读者的责任，在阅读过程中利用批判性思维对这四个基础问题进行自我提问并认真回答。 其中，关于批判性思维与阅读方法，具体可以移步阅读我以前的博文《批判性阅读和批判性写作》。 14 种主动阅读策略理解作者的意图作者的目标是告知、论证某个理论和观点，还是想要宣传他们的产品或服务吗？花几分钟认真阅读文本资料的内容介绍（前言、摘要、说明文档等），以了解作者写作的原因和意图。 调整阅读速度不要使用固定速度阅读文本内容，而是应该让自己的阅读速度适应当前正在阅读的内容。 这意味着在阅读一些你已经熟悉的内容时加快速度，而遇到一些新的信息、知识时降低速度进行细读。 标记文本必要时给段落进行编号。圈出阅读过程中比较重要的单词、短语、名称、日期等重要信息。在作者的观点、声明等重要信息下面划线。 注释内容记笔记是与阅读内容保持联系、进行对话的好方法。请在纸质页面空白处或者编辑器页面随时写下你在阅读某些内容时突然想到的各种想法。 标记文本只是辅助，重要的是注释和评论。 在阅读过程中，可以尝试总结文本、提出问题、对某个观点表示同意或者质疑。当然，你也可以写下一些关键词帮助你讨论或者后面回忆重点内容。 或者，你可以将文章的标题、副标题、章节和段落标题更改为问题，引导自己进行积极思考。上面的这些要求都是要求主动阅读者尽量与作者进行对话。 释义当在阅读过程中，遇到一个你感觉比较难以掌握的新概念时，建议停下来尝试使用自己的话对这个概念进行解释。 总结段落思想仔细阅读每个段落，使用你自己的话表达段落的主要思想。然后分析文章的主要观点是什么，以及具体是如何论证的。 书写章节摘要使用你自己的话写出一篇文章或某个章节的摘要。在摘要中使用凝练的语言概括阅读内容的基本思想，并罗列出关键词。这是一种能检验自己是否真正理解所读内容的有效方法。 建立信息或者知识块将信息或者知识分解成更小的块，以便更为轻松地保留。比如，以概念或者理论为单位进行分解成知识块，后期可以较为方便地对这些知识块进行排列组合。 连接节点当遇到新的知识，尝试将文本内容与个人经验或者其他知识内容进行联系。事实上，这是在释义、信息&#x2F;知识块的基础上，连接节点可以建立知识网络。 直观地组织信息将文本内容可视化，以便更好地查看、新建和发现知识之间的联系。换而言之，知识内容图谱化。 目前，已经有一些编辑器提供了知识图谱辅助功能。不过，任何知识图谱只有阅读者的主动建构才有意义和价值。 利用大纲列表、思维导图、流程图、数字白板、图表等工具，我们也可以直观地梳理我们的想法。 获取参考信息每当你有疑问时，请使用字典或其他参考资料工具，以此确保你能准确理解所读汉字、单词、专业术语的含义，以及掌握相关的背景知识信息。 总结内容一旦你读完一本书，那最好坐下来写你自己的总结。如果你在网上发布你的阅读体验，你还可以获取其他读者的反馈。 通过与他人观点的交汇和碰撞，我们对于阅读内容的理解和反思会更加深入。 评价内容每隔一段时间，后退几步——从阅读内容中跳出来，然后批判性地思考你正在阅读的内容。思考一下，阅读资料的结构是否合理？论点是否存在漏洞？作者是否有明显或者潜在的偏见？ 通过输出增强理解尝试将自己的所掌握的新知教给别人（输出）。大量研究明确表明，教学是最有效的学习方式之一。 如果你无法表达已读内容，那说明你对这些知识掌握并不牢靠。给别人讲解所读内容，是一种非常好的检验以及增强自己对于已读内容的自我学习方式。 甚至在这个过程中，还可能激发你对这些已读内容产生新的理解。 学习金字塔：主动学习和消极学习所获得知识的内容留存率差别悬殊 上述主动阅读策略基本上展示了主动阅读的学习流程。需要说明的是，这 14 种阅读策略里面有一些是为学生等群体进行准备的。 因此，在进行阅读时，并没有必要使用所有的主动阅读策略。当然，如果你想对某个文本进行认真研读，那这些阅读策略基本上都是适用的。 注释 文章转载自 Effie Blog, 已经获得授权。 Effie 官方介绍：无论是严肃写作，随手记录，亦或是把逻辑完善成思维导图，Effie 都是您明智的选择。 Effie 简要介绍：一款思写合一的专业写作软件，拥有 Markdown 语法支持、全平台、极简沉浸设计等基本要求，同时内置了思维导图，允许用户将大纲列表一键转化为思维导图模式。 Effie 官网| 把思想变为价值 Effie 海外英文官网 Effie Blog Effie 新手入门完全指南 Effie：为深度创作，也为记录灵感","categories":[{"name":"学习思考","slug":"学习思考","permalink":"https://docs.hehouhui.cn/categories/%E5%AD%A6%E4%B9%A0%E6%80%9D%E8%80%83/"}],"tags":[{"name":"文字","slug":"文字","permalink":"https://docs.hehouhui.cn/tags/%E6%96%87%E5%AD%97/"},{"name":"思考","slug":"思考","permalink":"https://docs.hehouhui.cn/tags/%E6%80%9D%E8%80%83/"},{"name":"学习","slug":"学习","permalink":"https://docs.hehouhui.cn/tags/%E5%AD%A6%E4%B9%A0/"}]},{"title":"Notion AI平替 Writely 基于chatGPT免费实现的写作神器","slug":"archives/Notion AI平替 Writely 基于chatGPT免费实现的写作神器","date":"2023-04-23T16:00:00.000Z","updated":"2023-10-08T06:42:00.000Z","comments":true,"path":"archives/notion-ai-writely.html","link":"","permalink":"https://docs.hehouhui.cn/archives/notion-ai-writely.html","excerpt":"","text":"AI 技术的迅速发展和普及，为人类的生活带来了很多便利。在写作领域，也出现了不少基于人工智能技术的写作工具。其中最近推出的一款基于 chatGPT 技术的写作神器 Writely，引起了广泛关注。 Writely 可以说是一款非常优秀的写作工具，它可以自动生成文章内容，并进行语法纠错。而且，它提供的多种模板和主题，使得写作更加高效实用。 这款工具的优势不仅仅在于自动生成文章内容，而且在于生成的内容质量非常高。它可以根据你的写作目的和文章类型，去自动匹配适合的模板和主题，从而为你提供更加具有针对性的文章内容。同时，这款工具还可以进行语法纠错，并提供优化建议，使文章的表达更加准确清晰。 Writely 的问世，既是技术的一次升级，也是写作工具创新的一次尝试。它为初学者和专业写作者都提供了一种全新的写作方式，使得写作变得更加轻松高效。尤其是对于一些需要大量写作的职业人士，使用 Writely 工具可以大大提高效率，降低工作量。 除了对写作领域的影响外，Writely 也预示着未来 AI 技术将在更多领域有所应用。可以预见，在未来，人工智能将越来越多地渗透到我们的生活和工作中，为我们带来更多便利和可能性。 总之，Writely 是一款非常优秀的基于 chatGPT 技术的写作工具，它为写作人士提供了一种全新的写作方式，同时也预示着未来 AI 技术的不断发展和普及。 💡 以上内容使用：Writely 自动生成 特性1.🔥 基于 Open AI GPT 模型，带来了全新的智能写作体验。 2.✍️ 支持在互联网上的任何编辑器网页上进行写作辅助，有效提高用户的写作效率和质量。 3.📖 该产品可以执行查询翻译和阅读辅助功能，大大减少用户的阅读时间并提高理解能力。 使用方法安装Chrome 插件： Firefox 扩展： 配置 获取 Open AI API Key。 如果您没有，请在 https://platform.openai.com/account/api-keys 上进行申请。 单击插件图标，然后单击“设置”图标。 进行配置。 将鼠标滑动到任何网页上的单词上，一个“W”图标将出现在鼠标附近，单击以使用。 GitHub 地址 : bookmark 原作者开源地址： bookmark 如浏览器查询无法安装从这下载谷歌浏览器插件下载地址","categories":[{"name":"创作分享","slug":"创作分享","permalink":"https://docs.hehouhui.cn/categories/%E5%88%9B%E4%BD%9C%E5%88%86%E4%BA%AB/"}],"tags":[{"name":"文字","slug":"文字","permalink":"https://docs.hehouhui.cn/tags/%E6%96%87%E5%AD%97/"},{"name":"工具","slug":"工具","permalink":"https://docs.hehouhui.cn/tags/%E5%B7%A5%E5%85%B7/"},{"name":"chatgpt","slug":"chatgpt","permalink":"https://docs.hehouhui.cn/tags/chatgpt/"},{"name":"notion","slug":"notion","permalink":"https://docs.hehouhui.cn/tags/notion/"}]},{"title":"你有什么值得分享的高效学习方法？","slug":"archives/你有什么值得分享的高效学习方法？","date":"2023-04-21T00:00:00.000Z","updated":"2023-10-08T06:42:00.000Z","comments":true,"path":"archives/efficient-learning-methods-pq4r.html","link":"","permalink":"https://docs.hehouhui.cn/archives/efficient-learning-methods-pq4r.html","excerpt":"","text":"在 21 世纪的今天，我们每天都要面临大量的知识和信息。我们每个人都需要不断学习新知识、新思想和进行新的实践。 这意味着，学习不再是学生阶段才需要做的事情，终身教育应该成为我们工作和生活的有机组成部分。 在自我学习的过程中，很多人都会尝试寻找高效的学习方法，以便提升自己的学习能力和学习效率。 接下来我分享的学习方法，不仅是我自己切身实践有效，也有科学依据作为支撑。 PQ4R 阅读法PQ4R 是英文首字母缩写，代表了阅读理解流程中的六个阶段。 预览 Preview：在开始阅读之前预览信息，以了解主题是什么。略读材料并查看文章标题、副标题和一些加粗重点文本。 问题 Question：问自己与该主题相关的问题，例如，我希望学到什么？关于这个主题，我已经知道什么？ 阅读 Read：每次阅读一条信息，并尝试找出问题的答案。 反思 Reflect：你回答所有问题了吗？如果没有，请返回原文查看，看看是否可以找到答案。 背诵 Recite：用你自己的话，说出或写下你刚刚阅读内容的简要摘要。 复习 Review：再看一遍材料，回答哪些尚未回答的问题。 间隔练习 Spaced Practice间隔练习，英文为 Spaced Practice, 也被称为分散式练习 distributed practice. 这是一种被科学证明富有效率的记忆方法。 近年来，随着 Anki 等软件的普及，间隔练习的理念也逐步为学习者所熟悉。 间隔练习反对学习者对于知识进行突击式的死记硬背，而是鼓励学习者在更长的时间段内进行学习。当我们的大脑几乎忘记某事时，大脑会更加努力地回忆起这些信息。 间隔学习根据记忆规律，将学习按照一定的时间进行间隔，这样将会促使我们的大脑在知识和想法之间建立联系，并且轻松回忆知识内容。 间隔练习要求学习者以下列时间表的间隔时间回顾我们的学习内容：第 1 天：学习所学知识。随后第 2 天、第 3 天、一周后、两周后各进行一次回顾与复习。 使用费曼技巧术语或者概念是知识的基本单位，相当于构建成知识大厦的砖瓦。如果对于一个术语或概念一知半解，那最后肯定会影响我们对于知识掌握的深度和广度。 而费曼技巧是一种学习各种术语和概念的有效方法。 费曼技巧的基本思想：如果我们想很好地理解某事，那么试着简单地解释它。这意味着，我们可以尝试通过自己的话解释某个概念，这样我们可能会更为快速更加深刻地理解这个概念。 费曼技巧的基本流程是： 首先在一张纸的顶部写下你正在研究的主题&#x2F;概念。 然后，用你自己的话解释这个主题和概念，就像你在正在教别人一样。 随后，查看你所写的内容并找出任何错误的地方，并且回到笔记或阅读材料中找出正确的答案。 最后，如果你在写作中有任何使用专业术语的地方，而你的读者不是专业人士的话，最好使用一些更为简单的术语重新表达你的写作内容。 善用思维导图使用思维导图记录和整理笔记，不仅可以提高我们对于所学习内容的理解能力，并且通过思维导图能够有效地了解各种想法之间的层次结构关系。 如果你是视觉型学习者，那强烈建议你使用思维导图。打开思维导图工具，新建文稿，在中心写下你的学习主题，然后建立子节点。使用思维导图，最好选择合适的思维导图工具。 在我看来，使用传统的思维导图工具，往往还需要导出为图片等文件，再根据思维导图的内容记录笔记或者进行写作。 在这样的场景中，思维导图和文本编辑器是割裂的，不利于在记笔记或者写作时随时查看、编辑思维导图。 因此，我选择了专业写作工具 Effie, 其内置了大纲一键转化为思维导图的功能。如此，我便可以在学习过程中随时使用思维导图了。 睡眠与学习：保障睡眠，睡前学习高效的学习方法，不是无限延长学习时间，而是学会劳逸结合。如果你想最大限度地利用时间，那也要保障每天睡眠 6-8 小时。这是大量科学研究证明的结果。 不要再去检索如何每天只睡 4 个小时又同时保障精力充沛这样的问题了。 每天只睡如此少的时间，不适用于绝大多数人。并且，每天只睡眠 4 个小时是成功人的特质，这样的说法基本上属于身居高位者对于打工人的心灵鸡汤。 足够的睡眠除了对身体健康有重要影响外，睡眠对大脑功能、记忆形成和学习也至关重要。睡眠会使得大脑进行一定程度的休息，并且会优化和完善在清醒状态时所获取的知识和信息。 此外，睡前学习是一种良好的学习习惯。无论是复习笔记，还是复习抽认卡，这都有助于提高我们的记忆。与其熬夜，不如在睡前几个小时学习，然后在早上查看回顾知识。 运动与学习在学习方法的分享中，很少有人会讨论运动和学习之间的关系。究其原因，可能很多人潜意识里认为运动会占用学习时间。然而，事实上，运动与学习之间关系密切。 很多人在进行长时段学习后，经常会出现精力不济、学习效率下降的现象。 如果每天保持适当的锻炼，不仅可以对抗疲劳，提升我们的记忆力和认知能力，而且会促使大脑释放脑内啡等化合物，改善我们的情绪并且降低我们的精神压力水平。 注释 文章转载自 Effie Blog, 已经获得授权。 Effie 官方介绍：无论是严肃写作，随手记录，亦或是把逻辑完善成思维导图，Effie 都是您明智的选择。 Effie 简要介绍：一款思写合一的专业写作软件，拥有 Markdown 语法支持、全平台、极简沉浸设计等基本要求，同时内置了思维导图，允许用户将大纲列表一键转化为思维导图模式。 Effie 官网| 把思想变为价值 Effie 海外英文官网 Effie Blog Effie 新手入门完全指南 Effie：为深度创作，也为记录灵感","categories":[{"name":"学习思考","slug":"学习思考","permalink":"https://docs.hehouhui.cn/categories/%E5%AD%A6%E4%B9%A0%E6%80%9D%E8%80%83/"}],"tags":[{"name":"文字","slug":"文字","permalink":"https://docs.hehouhui.cn/tags/%E6%96%87%E5%AD%97/"},{"name":"思考","slug":"思考","permalink":"https://docs.hehouhui.cn/tags/%E6%80%9D%E8%80%83/"},{"name":"学习","slug":"学习","permalink":"https://docs.hehouhui.cn/tags/%E5%AD%A6%E4%B9%A0/"}]},{"title":"Redis 入手知识点","slug":"archives/Redis 入手知识点","date":"2023-04-15T00:00:00.000Z","updated":"2023-10-08T06:42:00.000Z","comments":true,"path":"archives/redis-01.html","link":"","permalink":"https://docs.hehouhui.cn/archives/redis-01.html","excerpt":"","text":"Redis 面试宝典 🤪 💡 Redis 是一种高性能的 key-value 存储系统，具有以下几个特点： 内存存储：Redis 将数据存储在内存中，因此读写速度非常快，通常比基于磁盘的存储系统快几个数量级。 数据结构多样：Redis 支持多种数据结构，包括字符串、哈希、列表、集合、有序集合等，丰富的数据结构使 Redis 可以支持更多的应用场景。 持久化：Redis 支持将数据持久化到磁盘上，以保证数据的可靠性。 高并发性：Redis 采用单线程模型，减少了多线程之间的竞争，从而使得 Redis 具有更高的并发性。 分布式：Redis 提供了集群模式，可以将数据分布到不同的节点上，从而实现水平扩展。 1. 内存存储：Redis将数据存储在内存中，因此读写速度非常快，通常比基于磁盘的存储系统快几个数量级。 2. 数据结构多样：Redis支持多种数据结构，包括字符串、哈希、列表、集合、有序集合等，丰富的数据结构使Redis可以支持更多的应用场景。 3. 持久化：Redis支持将数据持久化到磁盘上，以保证数据的可靠性。 4. 高并发性：Redis采用单线程模型，减少了多线程之间的竞争，从而使得Redis具有更高的并发性。 5. 分布式：Redis提供了集群模式，可以将数据分布到不同的节点上，从而实现水平扩展。 💡 Redis 的应用场景非常广泛，主要包括以下几个方面： 缓存：Redis 常用于作为缓存系统，用于加速数据的读取。通过将热数据缓存到内存中，可以减少磁盘读取次数，从而提高应用程序的响应速度。 计数器：Redis 支持原子操作，可以用于实现计数器功能。 消息队列：Redis 支持发布&#x2F;订阅模式，可以用于实现简单的消息队列功能。 分布式锁：Redis 可以通过原子操作实现分布式锁，用于实现分布式系统中的互斥访问。 会话管理：Redis 可以用于存储会话数据，从而实现分布式应用程序的会话管理。 Redis的应用场景非常广泛，主要包括以下几个方面： 1. 缓存：Redis常用于作为缓存系统，用于加速数据的读取。通过将热数据缓存到内存中，可以减少磁盘读取次数，从而提高应用程序的响应速度。 2. 计数器：Redis支持原子操作，可以用于实现计数器功能。 3. 消息队列：Redis支持发布/订阅模式，可以用于实现简单的消息队列功能。 4. 分布式锁：Redis可以通过原子操作实现分布式锁，用于实现分布式系统中的互斥访问。 5. 会话管理：Redis可以用于存储会话数据，从而实现分布式应用程序的会话管理。 一致性 hash 在 Redis 集群模式 Cluster 中，Redis 采用的是分片 Sharding 的方式，也就是将数据采用一定的分区策略，分发到相应的集群节点中。但是我们使用上述 HASH 算法进行缓存时，会出现一些缺陷，主要体现在服务器数量变动的时候，所有缓存的位置都要发生改变！具体来讲就是说第一当缓存服务器数量发生变化时，会引起缓存的雪崩，可能会引起整体系统压力过大而崩溃（大量缓存同一时间失效）。第二当缓存服务器数量发生变化时，几乎所有缓存的位置都会发生改变。 一致性 Hash 算法也是使用取模的方法，只是，刚才描述的取模法是对服务器的数量进行取模，而一致性 Hash 算法是对 232 取模，什么意思呢？简单来说，一致性 Hash 算法将整个哈希值空间组织成一个虚拟的圆环，如假设某哈希函数 H 的值空间为 0-232-1（即哈希值是一个 32 位无符号整形），整个哈希环如下： 整个空间按顺时针方向组织，圆环的正上方的点代表 0，0 点右侧的第一个点代表 1，以此类推，2、3、4、5、6……直到 232-1，也就是说 0 点左侧的第一个点代表 232-1， 0 和 232-1 在零点中方向重合，我们把这个由 232 个点组成的圆环称为 Hash 环。 那么，一致性哈希算法与上图中的圆环有什么关系呢？我们继续聊，仍然以之前描述的场景为例，假设我们有 4 台缓存服务器，服务器 A、服务器 B、服务器 C，服务器 D，那么，在生产环境中，这 4 台服务器肯定有自己的 IP 地址或主机名，我们使用它们各自的 IP 地址或主机名作为关键字进行哈希计算，使用哈希后的结果对 2^32 取模，可以使用如下公式示意： $$hash（服务器A的IP地址） % 2^32$$ 通过上述公式算出的结果一定是一个 0 到 232-1 之间的一个整数，我们就用算出的这个整数，代表服务器 A，既然这个整数肯定处于 0 到 232-1 之间，那么，上图中的 hash 环上必定有一个点与这个整数对应，而我们刚才已经说明，使用这个整数代表服务器 A，那么，服务器 A 就可以映射到这个环上。 以此类推，下一步将各个服务器使用类似的 Hash 算式进行一个哈希，这样每台机器就能确定其在哈希环上的位置，这里假设将上文中四台服务器使用 IP 地址哈希后在环空间的位置如下： 接下来使用如下算法定位数据访问到相应服务器： 将数据 key 使用相同的函数 Hash 计算出哈希值，并确定此数据在环上的位置，从此位置沿环顺时针“行走”，第一台遇到的服务器就是其应该定位到的服务器！ Hash 算法的容错性和可扩展性现假设 Node C 不幸宕机，可以看到此时对象 A、B、D 不会受到影响，只有 C 对象被重定位到 Node D。一般的，在一致性 Hash 算法中，如果一台服务器不可用，则受影响的数据仅仅是此服务器到其环空间中前一台服务器（即沿着逆时针方向行走遇到的第一台服务器）之间数据，其它不会受到影响，如下所示： 下面考虑另外一种情况，如果在系统中增加一台服务器 Node X，如下图所示： 此时对象 Object A、B、D 不受影响，只有对象 C 需要重定位到新的 Node X ！一般的，在一致性 Hash 算法中，如果增加一台服务器，则受影响的数据仅仅是新服务器到其环空间中前一台服务器（即沿着逆时针方向行走遇到的第一台服务器）之间数据，其它数据也不会受到影响。 综上所述，一致性 Hash 算法对于节点的增减都只需重定位环空间中的一小部分数据，具有较好的容错性和可扩展性。 数据倾斜问题一致性 Hash 算法在服务节点太少时，容易因为节点分部不均匀而造成数据倾斜（被缓存的对象大部分集中缓存在某一台服务器上）问题，例如系统中只有两台服务器，其环分布如下： 此时必然造成大量数据集中到 Node A 上，而只有极少量会定位到 Node B 上，从而出现 hash 环偏斜的情况，当 hash 环偏斜以后，缓存往往会极度不均衡的分布在各服务器上，如果想要均衡的将缓存分布到 2 台服务器上，最好能让这 2 台服务器尽量多的、均匀的出现在 hash 环上，但是，真实的服务器资源只有 2 台，我们怎样凭空的让它们多起来呢，没错，就是凭空的让服务器节点多起来，既然没有多余的真正的物理服务器节点，我们就只能将现有的物理节点通过虚拟的方法复制出来。 这些由实际节点虚拟复制而来的节点被称为”虚拟节点”，即对每一个服务节点计算多个哈希，每个计算结果位置都放置一个此服务节点，称为虚拟节点。具体做法可以在服务器 IP 或主机名的后面增加编号来实现。 例如上面的情况，可以为每台服务器计算三个虚拟节点，于是可以分别计算 “Node A#1”、“Node A#2”、“Node A#3”、“Node B#1”、“Node B#2”、“Node B#3”的哈希值，于是形成六个虚拟节点： 同时数据定位算法不变，只是多了一步虚拟节点到实际节点的映射，例如定位到“Node A#1”、“Node A#2”、“Node A#3”三个虚拟节点的数据均定位到 Node A 上。这样就解决了服务节点少时数据倾斜的问题。在实际应用中，通常将虚拟节点数设置为 32 甚至更大，因此即使很少的服务节点也能做到相对均匀的数据分布。 集群情况下什么时候不可用 如果集群任意master挂掉*,且当前master没有slave.集群进入fail*状态 有 A,B,C 三个节点的集群，在没有复制模型的情况下，如果节点 B 失败了，那么整个集群就会以为缺少 5501-11000 这个范围的槽而不可用。 如果集群超过半数以上master挂掉，无论是否有slave集群进入fail状态 集群某一节点的主从全数宕机 （与 2 相似） 故障的处理过程 查看业务日志（微服务） 首先查看 redis 集群状态。 继续查看 redis 集群节点的状态。 处理过程 1、先停止所有 redis 节点。2、删除每个节点的缓存文件，包括 node-6380.conf dump.rdp 等文件。3、重启每个 redis 节点。4、重新创建 redis 集群。 集群 发现我们当前项目用的 redis 是主从，但是跟单点其实没有什么区别，因为我们在应用层面没有做读写分离，所以其实从服务器只是做了一个主从复制的工作，其他的什么都没有做。 那么如果我们的系统升级，用户量上升，那么一主一从可能扛不住那么大的压力，可能需要一主多从做备机，那么假如主服务器宕机了，选举哪台从服务器做主呢？这就是一个问题，需要一个第三个人来解决，所以我查了一下，哨兵模式可以解决这个问题。哨兵模式的细节下面会讲到。 然后我又想了，那如果单台服务器无法承受100%的存储压力，那就应该将存储压力分散开来，所以集群就可以解决这个问题 了，比如我们用6台服务器做集群，3主3从，那么每台服务器只需要存储1/3即可。好，那么我们就来详细看一下这些具体怎么做的。 单点主从基本上就是一主一从，我们应用层主要使用的是主节点，从节点的主要工作是从主节点做主从复制。关键时刻，如果主服务器挂掉，可以手动启动从服务器，然后更改应用层的 redis 的 ip 即可 实现主从复制（Master-Slave Replication）的工作原理：Slave 从节点服务启动并连接到 Master 之后，它将主动发送一个 SYNC 命令。Master 服务主节点收到同步命令后将启动后台存盘进程，同时收集所有接收到的用于修改数据集的命令，在后台进程执行完毕后，Master 将传送整个数据库文件到 Slave，以完成一次完全同步。而 Slave 从节点服务在接收到数据库文件数据之后将其存盘并加载到内存中。此后，Master 主节点继续将所有已经收集到的修改命令，和新的修改命令依次传送给 Slaves，Slave 将在本次执行这些数据修改命令，从而达到最终的数据同步。 如果 Master 和 Slave 之间的链接出现断连现象，Slave 可以自动重连 Master，但是在连接成功之后，一次完全同步将被自动执行。 主从复制配置 修改从节点的配置文件：slaveof masterip masterport如果设置了密码，就要设置：masterauth master-password 主从模式的优缺点 优点： 同一个 Master 可以同步多个 Slaves。 Slave 同样可以接受其它 Slaves 的连接和同步请求，这样可以有效的分载 Master 的同步压力。因此我们可以将 Redis 的 Replication 架构视为图结构。 Master Server 是以非阻塞的方式为 Slaves 提供服务。所以在 Master-Slave 同步期间，客户端仍然可以提交查询或修改请求。 Slave Server 同样是以非阻塞的方式完成数据同步。在同步期间，如果有客户端提交查询请求，Redis 则返回同步之前的数据 为了分载 Master 的读操作压力，Slave 服务器可以为客户端提供只读操作的服务，写服务仍然必须由 Master 来完成。即便如此，系统的伸缩性还是得到了很大的提高。 Master 可以将数据保存操作交给 Slaves 完成，从而避免了在 Master 中要有独立的进程来完成此操作。支持主从复制，主机会自动将数据同步到从机，可以进行读写分离。 缺点： Redis 不具备自动容错和恢复功能，主机从机的宕机都会导致前端部分读写请求失败，需要等待机器重启或者手动切换前端的 IP 才能恢复。 主机宕机，宕机前有部分数据未能及时同步到从机，切换 IP 后还会引入数据不一致的问题，降低了系统的可用性。 Redis 的主从复制采用全量复制，复制过程中主机会 fork 出一个子进程对内存做一份快照，并将子进程的内存快照保存为文件发送给从机，这一过程需要确保主机有足够多的空余内存。若快照文件较大，对集群的服务能力会产生较大的影响，而且复制过程是在从机新加入集群或者从机和主机网络断开重连时都会进行，也就是网络波动都会造成主机和从机间的一次全量的数据复制，这对实际的系统运营造成了不小的麻烦。 Redis 较难支持在线扩容，在集群容量达到上限时在线扩容会变得很复杂。为避免这一问题，运维人员在系统上线时必须确保有足够的空间，这对资源造成了很大的浪费。 其实 redis 的主从模式很简单，在实际的生产环境中是很少使用的，我也不建议在实际的生产环境中使用主从模式来提供系统的高可用性，之所以不建议使用都是由它的缺点造成的，在数据量非常大的情况，或者对系统的高可用性要求很高的情况下，主从模式也是不稳定的。 读写分离 对于读占比较高的场景，可以通过把一部分流量分摊导出从节点(salve) 来减轻主节点（master）压力，同时需要主要只对主节点执行写操作。 常见的应用场景下我觉得 redis 没必要进行读写分离。 先来讨论一下为什么要读写分离： 读写分离使用于大量读请求的情况，通过多个 slave 分摊了读的压力，从而增加了读的性能。过多的 select 会阻塞住数据库，使你增删改不能执行，而且到并发量过大时，数据库会拒绝服务。 因而通过读写分离，从而增加性能，避免拒绝服务的发生。 我认为需要读写分离的应用场景是：写请求在可接受范围内，但读请求要远大于写请求的场景。 再来讨论一下 redis 常见的应用场景： 缓存 排名型的应用，访问计数型应用 实时消息系统 首先说一下缓存集群，这也是非常常见的应用场景： 缓存主要解决的是用户访问时，怎么以更快的速度得到数据。 单机的内存资源是很有限的，所以缓存集群会通过某种算法将不同的数据放入到不同的机器中。 不同持久化数据库，一般来说，内存数据库单机可以支持大量的增删查改。 如果一台机器支持不住，可以用主从复制，进行缓存的方法解决。 综上，在这个场景下应用 redis 进行读写分离，完全就失去了读写分离的意义。 当然，也有可能考虑不到的地方需要读写分离，毕竟“存在即合理”嘛，那么就来介绍一下这个读写分离吧。 当使用从节点响应读请求时，业务端可能会遇到以下问题 复制数据延迟 读到过期数据 从节点故障 数据延迟 Redis 复制数的延迟由于异步复制特性是无法避免的，延迟取决于网络带宽和命令阻塞情况，对于无法容忍大量延迟场景，可以编写外部监控程序监听主从节点的复制偏移量，当延迟较大时触发报警或者通知客户端避免读取延迟过高的从节点，实现逻辑如下： 监控程序(monitor) 定期检查主从节点的偏移量，主节点偏移量在 info replication 的 master_repl_offset 指标记录，从节点 偏移量可以查询主节点的 slave0 字段的 offset 指标，它们的差值就是主从节点延迟的字节 量。 当延迟字节量过高时，比如超过 10M。监控程序触发报警并通知客户端从节点延迟过高。可以采用 Zookeeper 的监听回调机制实现客户端通知。 客户端接到具体的从节点高延迟通知后，修改读命令路由到其他从节点或主节点上。当延迟回复后，再次通知客户端，回复从节点的读命令请求。 这种方案成本较高，需要单独修改适配 Redis 的客户端类库。 读到过期数据 当主节点存储大量设置超时的数据时，如缓存数据，Redis 内部需要维护过期数据删除策略，删除策略主要有两种：惰性删除和定时删除。 惰性删除：主节点每次处理读取命令时，都会检查键是否超时，如果超时则执行 del 命令删除键对象那个，之后 del 命令也会异步 发送给 从节点 需要注意的是为了保证复制的一致性，从节点自身永远不会主动删除超时数据， 定时删除： Redis 主节点在内部定时任务会循环采样一定数量的键，当发现采样的键过期就执行 del 命令，之后再同步给从节点 如果此时 数据的大量超时，主节点采样速度跟不上过期速度且主节点没有读取过期键的操作，那么从节点将无法收到 del 命令，这时在从节点 上可以读取到已经超时的数据。Redis 在 3.2 版本解决了这个问题，从节点 读取数据之前会检查键的过期时间来决定是否返回数据，可以升级到 3.2 版本来规避这个问题。 哨兵模式 该模式是从 Redis 的 2.6 版本开始提供的，但是当时这个版本的模式是不稳定的，直到 Redis 的 2.8 版本以后，这个哨兵模式才稳定下来，无论是主从模式，还是哨兵模式，这两个模式都有一个问题，不能水平扩容，并且这两个模式的高可用特性都会受到 Master 主节点内存的限制。 Sentinel(哨兵)进程是用于监控 redis 集群中 Master 主服务器工作的状态，在 Master 主服务器发生故障的时候，可以实现 Master 和 Slave 服务器的切换，保证系统的高可用。 Sentinel（哨兵）进程的作用 监控(Monitoring): 哨兵(sentinel) 会不断地检查你的 Master 和 Slave 是否运作正常。 提醒(Notification)：当被监控的某个 Redis 节点出现问题时, 哨兵(sentinel) 可以通过 API 向管理员或者其他应用程序发送通知。 自动故障迁移(Automatic failover)： 当一个 Master 不能正常工作时，哨兵(sentinel) 会开始一次自动故障迁移操作，它会将失效 Master 的其中一个 Slave 升级为新的 Master, 并让失效 Master 的其他 Slave 改为复制新的 Master；当客户端试图连接失效的 Master 时，集群也会向客户端返回新 Master 的地址，使得集群可以使用现在的 Master 替换失效 Master。 Master和Slave服务器切换后，Master的redis.conf、Slave的redis.conf和sentinel.conf的配置文件的内容都会发生相应的改变，即，Master主服务器的redis.conf配置文件中会多一行slaveof的配置，sentinel.conf的监控目标会随之调换。 Sentinel（哨兵）进程的工作方式 每个 Sentinel（哨兵）进程以每秒钟一次的频率向整个集群中的 Master 主服务器，Slave 从服务器以及其他 Sentinel（哨兵）进程发送一个 PING 命令。 如果一个实例（instance）距离最后一次有效回复 PING 命令的时间超过 down-after-milliseconds 选项所指定的值， 则这个实例会被 Sentinel（哨兵）进程标记为主观下线（SDOWN） 如果一个 Master 主服务器被标记为主观下线（SDOWN），则正在监视这个 Master 主服务器的所有 Sentinel（哨兵）进程要以每秒一次的频率确认 Master 主服务器的确进入了主观下线状态 当有足够数量的 Sentinel（哨兵）进程（大于等于配置文件指定的值）在指定的时间范围内确认 Master 主服务器进入了主观下线状态（SDOWN）， 则 Master 主服务器会被标记为客观下线（ODOWN） 在一般情况下， 每个 Sentinel（哨兵）进程会以每 10 秒一次的频率向集群中的所有 Master 主服务器、Slave 从服务器发送 INFO 命令。 当 Master 主服务器被 Sentinel（哨兵）进程标记为客观下线（ODOWN）时，Sentinel（哨兵）进程向下线的 Master 主服务器的所有 Slave 从服务器发送 INFO 命令的频率会从 10 秒一次改为每秒一次。 若没有足够数量的 Sentinel（哨兵）进程同意 Master 主服务器下线， Master 主服务器的客观下线状态就会被移除。若 Master 主服务器重新向 Sentinel（哨兵）进程发送 PING 命令返回有效回复，Master 主服务器的主观下线状态就会被移除。哨兵模式的优缺点 优点: 哨兵集群模式是基于主从模式的，所有主从的优点，哨兵模式同样具有。 主从可以切换，故障可以转移，系统可用性更好。 哨兵模式是主从模式的升级，系统更健壮，可用性更高。 缺点: Redis 较难支持在线扩容，在集群容量达到上限时在线扩容会变得很复杂。为避免这一问题，运维人员在系统上线时必须确保有足够的空间，这对资源造成了很大的浪费。 配置复杂 工作原理: 用户链接时先通过哨兵获取主机 Master 的信息 获取 Master 的链接后实现 redis 的操作(set&#x2F;get) 当 master 出现宕机时,哨兵的心跳检测发现主机长时间没有响应.这时哨兵会进行推选.推选出新的主机完成任务. 当新的主机出现时,其余的全部机器都充当该主机的从机 这就有一个问题，就是添加哨兵以后，所有的请求都会经过哨兵询问当前的主服务器是谁，所以如果哨兵部在主服务器上面的话可能会增加服务器的压力，所以最好是将哨兵单独放在一个服务器上面。以分解压力。 然后可能还有人担心哨兵服务器宕机了怎么办啊，首先哨兵服务器宕机的可能性很小，然后是如果哨兵服务器宕机了，使用人工干预重启即可，就会导致主从服务器监控的暂时不可用，不影响主从服务器的正常运行。 先配置服务器（本地）哨兵模式，直接从 redis 官网下载安装或者解压版，安装后的目录结构 然后配置哨兵模式 测试采用 3 个哨兵，1 个主 redis，2 个从 redis。 复制 6 份 redis.windows.conf 文件并重命名如下（开发者可根据自己的开发习惯进行重命名） 配置 master.6378.conf port:6379 #设置连接密码 requirepass:grs #连接密码 masterauth:grs slave.6380.conf 配置 port:6380 dbfilename dump6380.rdb #配置master slaveof 127.0.0.1 6379 slave.6381.conf 配置 port 6381 slaveof 127.0.0.1 6379 dbfilename \"dump.rdb\" 配置哨兵 sentinel.63791.conf（其他两个哨兵配置文件一致，只修改端口号码即可） port 63791 #主master，2个sentinel选举成功后才有效，这里的master-1是名称，在整合的时候需要一致，这里可以随便更改 sentinel monitor master-1 127.0.0.1 6379 2 #判断主master的挂机时间（毫秒），超时未返回正确信息后标记为sdown状态 sentinel down-after-milliseconds master-1 5000 #若sentinel在该配置值内未能完成failover操作（即故障时master/slave自动切换），则认为本次failover失败。 sentinel failover-timeout master-1 18000 #选项指定了在执行故障转移时， 最多可以有多少个从服务器同时对新的主服务器进行同步，这个数字越小，完成故障转移所需的时间就越长 sentinel config-epoch master-1 2 需要注意的地方 1、若通过 redis-cli -h 127.0.0.1 -p 6379 连接，无需改变配置文件，配置文件默认配置为 bind 127.0.0.1(只允许 127.0.0.1 连接访问）若通过 redis-cli -h 192.168.180.78 -p 6379 连接，需改变配置文件，配置信息为 bind 127.0.0.1 192.168.180.78（只允许 127.0.0.1 和 192.168.180.78 访问）或者将 bind 127.0.0.1 注释掉（允许所有远程访问） 2、masterauth 为所要连接的 master 服务器的 requirepass,如果一个 redis 集群中有一个 master 服务器，两个 slave 服务器，当 master 服务器挂掉时，sentinel 哨兵会随机选择一个 slave 服务器充当 master 服务器，鉴于这种机制，解决办法是将所有的主从服务器的 requirepass 和 masterauth 都设置为一样。 3、sentinel monitor master-1 127.0.0.1 6379 2 行尾最后的一个 2 代表什么意思呢？我们知道，网络是不可靠的，有时候一个 sentinel 会因为网络堵塞而误以为一个 master redis 已经死掉了，当 sentinel 集群式，解决这个问题的方法就变得很简单，只需要多个 sentinel 互相沟通来确认某个 master 是否真的死了，这个 2 代表，当集群中有 2 个 sentinel 认为 master 死了时，才能真正认为该 master 已经不可用了。（sentinel 集群中各个 sentinel 也有互相通信，通过 gossip 协议）。 依次启动 redis redis-server master.6379.conf redis-server slave.6380.conf redis-server slave.6381.conf redis-server sentinel.63791.conf –sentinel（linux:redis-sentinel sentinel.63791.conf）其他两个哨兵也这样启动 使用客户端查看一下 master 状态 查看一下哨兵状态 现在就可以在 master 插入数据，所有的 redis 服务都可以获取到，slave 只能读 整合 spring，导入依赖 &lt;dependency> &lt;groupId>redis.clients&lt;/groupId> &lt;artifactId>jedis&lt;/artifactId> &lt;version>2.8.0&lt;/version> &lt;/dependency> &lt;!-- spring-redis --> &lt;dependency> &lt;groupId>org.springframework.data&lt;/groupId> &lt;artifactId>spring-data-redis&lt;/artifactId> &lt;version>1.6.4.RELEASE&lt;/version> &lt;/dependency> &lt;dependency> &lt;groupId>org.apache.commons&lt;/groupId> &lt;artifactId>commons-pool2&lt;/artifactId> &lt;version>2.4.2&lt;/version> &lt;/dependency> redis.properties #redis中心 redis.host=127.0.0.1 #redis.host=10.75.202.11 redis.port=6379 redis.password= #redis.password=123456 redis.maxTotal=200 redis.maxIdle=100 redis.minIdle=8 redis.maxWaitMillis=100000 redis.maxActive=300 redis.testOnBorrow=true redis.testOnReturn=true #Idle时进行连接扫描 redis.testWhileIdle=true #表示idle object evitor两次扫描之间要sleep的毫秒数 redis.timeBetweenEvictionRunsMillis=30000 #表示idle object evitor每次扫描的最多的对象数 redis.numTestsPerEvictionRun=10 #表示一个对象至少停留在idle状态的最短时间，然后才能被idle object evitor扫描并驱逐；这一项只有在timeBetweenEvictionRunsMillis大于0时才有意义 redis.minEvictableIdleTimeMillis=60000 redis.timeout=100000 Cluster Redis3.0 版本之后支持 Cluster. redis cluster 的现状 目前 redis 支持的 cluster 特性： 1):节点自动发现 2):slave-&gt;master 选举,集群容错 3):Hot resharding:在线分片 4):进群管理:cluster xxx 5):基于配置(nodes-port.conf)的集群管理 6):ASK 转向&#x2F;MOVED 转向机制. redis cluster 架构 1)redis-cluster 架构图 在这个图中，每一个蓝色的圈都代表着一个 redis 的服务器节点。它们任何两个节点之间都是相互连通的。客户端可以与任何一个节点相连接，然后就可以访问集群中的任何一个节点。对其进行存取和其他操作。 架构细节: 所有的 redis 节点彼此互联(PING-PONG 机制),内部使用二进制协议优化传输速度和带宽. 节点的 fail 是通过集群中超过半数的节点检测失效时才生效. 客户端与 redis 节点直连,不需要中间 proxy 层.客户端不需要连接集群所有节点,连接集群中任何一个可用节点即可 redis-cluster 把所有的物理节点映射到[0-16383]slot 上,cluster 负责维护 node&lt;-&gt;slot&lt;-&gt;value redis-cluster 选举:容错 领着选举过程是集群中所有 master 参与,如果半数以上 master 节点与 master 节点通信超过(cluster-node-timeout),认为当前 master 节点挂掉. 什么时候整个集群不可用(cluster_state:fail),当集群不可用时,所有对集群的操作做都不可用，收到((error) CLUSTERDOWN The cluster is down)错误 如果集群任意 master 挂掉,且当前 master 没有 slave.集群进入 fail 状态,也可以理解成进群的 slot 映射[0-16383]不完成时进入 fail 状态. 如果进群超过半数以上 master 挂掉，无论是否有 slave 集群进入 fail 状态. 它们之间通过互相的 ping-pong 判断是否节点可以连接上。如果有一半以上的节点去 ping 一个节点的时候没有回应，集群就认为这个节点宕机了，然后去连接它的备用节点。 如果某个节点和所有从节点全部挂掉，我们集群就进入faill状态。还有就是如果有一半以上的主节点宕机，那么我们集群同样进入发力了状态。这就是我们的redis的投票机制 Redis 3.0 的集群方案有以下两个问题。 一个Redis实例具备了“数据存储”和“路由重定向”，完全去中心化的设计。 这带来的好处是部署非常简单，直接部署 Redis 就行，不像 Codis 有那么多的组件和依赖。但带来的问题是很难对业务进行无痛的升级，如果哪天 Redis 集群出了什么严重的 Bug，就只能回滚整个 Redis 集群。对协议进行了较大的修改，对应的 Redis 客户端也需要升级。升级 Redis 客户端后谁能确保没有 Bug？而且对于线上已经大规模运行的业务，升级代码中的 Redis 客户端也是一个很麻烦的事情。Redis Cluster 是 Redis 3.0 以后才正式推出，时间较晚，目前能证明在大规模生产环境下成功的案例还不是很多，需要时间检验。 Jedis sharding Redis Sharding 可以说是在 Redis cluster 出来之前业界普遍的采用方式，其主要思想是采用 hash 算法将存储数据的 key 进行 hash 散列，这样特定的 key 会被定为到特定的节点上。 庆幸的是，Java Redis 客户端驱动 Jedis 已支持 Redis Sharding 功能，即 ShardedJedis 以及结合缓存池的 ShardedJedisPool Jedis的Redis Sharding实现具有如下特点： 采用一致性哈希算法，将 key 和节点 name 同时 hashing，然后进行映射匹配，采用的算法是 MURMUR_HASH。采用一致性哈希而不是采用简单类似哈希求模映射的主要原因是当增加或减少节点时，不会产生由于重新匹配造成的 rehashing。一致性哈希只影响相邻节点 key 分配，影响量小。 为了避免一致性哈希只影响相邻节点造成节点分配压力，ShardedJedis 会对每个 Redis 节点根据名字(没有，Jedis 会赋予缺省名字)会虚拟化出 160 个虚拟节点进行散列。根据权重 weight，也可虚拟化出 160 倍数的虚拟节点。用虚拟节点做映射匹配，可以在增加或减少 Redis 节点时，key 在各 Redis 节点移动再分配更均匀，而不是只有相邻节点受影响。 ShardedJedis 支持 keyTagPattern 模式，即抽取 key 的一部分 keyTag 做 sharding，这样通过合理命名 key，可以将一组相关联的 key 放入同一个 Redis 节点，这在避免跨节点访问相关数据时很重要。 当然，Redis Sharding 这种轻量灵活方式必然在集群其它能力方面做出妥协。比如扩容，当想要增加 Redis 节点时，尽管采用一致性哈希，毕竟还是会有 key 匹配不到而丢失，这时需要键值迁移。作为轻量级客户端 sharding，处理 Redis 键值迁移是不现实的，这就要求应用层面允许 Redis 中数据丢失或从后端数据库重新加载数据。但有些时候，击穿缓存层，直接访问数据库层，会对系统访问造成很大压力。 利用中间件代理中间件的作用是将我们需要存入 redis 中的数据的 key 通过一套算法计算得出一个值。然后根据这个值找到对应的 redis 节点，将这些数据存在这个 redis 的节点中。 常用的中间件有这几种 Twemproxy Codis nginx 具体用法就不赘述了，可以自行百度。 总结 客户端分片（sharding）需要客户端维护分片算法，这是一种静态的分片方案，需要增加或者减少 Redis 实例的数量，需要手工调整分片的程序。 利用中间件的情况则会影响到 redis 的性能，具体看中间件而定，毕竟所有请求都要经过中间件一层过滤 官方提供方案 （Cluster），现时点成功案例不多。 Redis 分片Redis 的分片承担着两个主要目标： 允许使用很多电脑的内存总和来支持更大的数据库。没有分片，你就被局限于单机能支持的内存容量 允许伸缩计算能力到多核或多服务器，伸缩网络带宽到多服务器或多网络适配器范围分片的替代方案是哈希分片(hash partitioning)。这种模式适用于任何键 哈希槽设置 key–&gt;hashcode–&gt;16384 在 redis 的每一个节点上，都有这么两个东西 一个是插槽（slot）可以理解为是一个可以存储两个数值的一个变量这个变量的取值范围是：0-16383。 还有一个就是cluster我个人把这个cluster理解为是一个集群管理的插件。 当我们的存取的key到达的时候，redis会根据crc16的算法得出一个结果，然后把结果对 16384 求余数，这样每个 key 都会对应一个编号在 0-16383 之间的哈希槽，通过这个值，去找到对应的插槽所对应的节点，然后直接自动跳转到这个对应的节点上进行存取操作。 还有就是因为如果集群的话，是有好多个redis一起工作的，那么，就需要这个集群不是那么容易挂掉，所以呢，理论上就应该给集群中的每个节点至少一个备用的redis服务。这个备用的redis称为从节点（slave）。那么这个集群是如何判断是否有某个节点挂掉了呢？ 首先要说的是，每一个节点都存有这个集群所有主节点以及从节点的信息。 Redis 持久化RDB Redis Database，就是快照 snapshots。缺省情况情况下，Redis 把数据快照存放在磁盘上的二进制文件中，文件名为 dump.rdb。可以配置 Redis 的持久化策略，例如数据集中每 N 秒钟有超过 M 次更新，就将数据写入磁盘；或者你可以手工调用命令 SAVE 或 BGSAVE。 Redis 是使用 fork 函数复制一份当前进程(父进程)的副本(子进程) 子进程开始将数据写到临时 RDB 文件中 当子进程完成写 RDB 文件，用新文件替换老文件 这种方式可以使 Redis 使用 copy-on-write 技术。 AOF Append Only File。快照模式并不十分健壮，当系统停止或者无意中 Redis 被 kill 掉，最后写入 Redis 的数据就会丢失。这对某些应用也许不是大问题，但对于要求高可靠性的应用来说，Redis 就不是一个合适的选择。Append-only 文件模式是另一种选择。可以在配置文件中打开 AOF 模式Redis 中提供了 3 中同步策略，即每秒同步、每修改同步和不同步。事实上每秒同步也是异步完成的，其效率也是非常高的，所差的是一旦系统出现宕机现象，那么这一秒钟之内修改的数据将会丢失。 appendfsync always 每次有数据修改发生时都会写入 AOF 文件 appendfsync everysec 每秒钟同步一次，该策略为 AOF 的缺省策略。在性能和持久化方面作了很好的折中 appendfsync no 从不同步。高效但是数据不会被持久化。 虚拟内存方式 当 key 很小而 value 很大时，使用 VM 的效果会比较好，因为这样节约的内存比较大。当 key 不小时可以考虑使用一些非常方法将很大的 key 变成很大的 value，如可以考虑将 key&#x2F;value 组合成一个新 value.vm-max-threads 这个参数,可以设置访问 swap 文件的线程数,设置最好不要超过机器的核数,如果设置为 0,那么所有对 swap 文件的操作都是串行的.可能会造成比较长时间的延迟,但是对数据完整性有很好的保证.用虚拟内存性能也不错。如果数据量很大，可以考虑分布式或者其他数据库 redis.windows.confdaemonize no 默认情况下 redis 不是作为守护进程运行的，如果想让它在后台运行，就把它改成 yes。当 redis 作为守护进程运行的时候，它会写一个 pid 到&#x2F;var&#x2F;run&#x2F;redis.pid 文件里面 建议 更新频繁: 一致性要求比较高，AOF 策略为主 更新不频繁: 可以容忍少量数据丢失或错误，snapshot（快照）策略为主 Redis 事务redis 事务是通过 MULTI，EXEC，DISCARD 和 WATCH 四个原语实现的。 MULTI命令用于开启一个事务，它总是返回OK。 MULTI执行之后，客户端可以继续向服务器发送任意多条命令，这些命令不会立即被执行，而是被放到一个队列中，当EXEC命令被调用时，所有队列中的命令才会被执行。 另一方面，通过调用DISCARD，客户端可以清空事务队列，并放弃执行事务。 redis事务范围 从multi命令开始，到exec或者discard为止，整个操作过程是原子性的，不能打乱顺序，也不能插入操作 但是出错之前的操作会正常提交 WATCH 命令可以为 Redis 事务提供 check-and-set （CAS）行为。 被 WATCH 的键会被监视，并会发觉这些键是否被改动过了。 如果有至少一个被监视的键在 EXEC 执行之前 被修改了， 那么整个事务都会被取消， EXEC 返回空多条批量回复（null multi-bulk reply）来表示事务已 经失败。 使用 Redis 实现分布式锁1、向Redis中存放固定key的值，如果key不存在则实现存放并获取锁；如果key已经存在则不能获取锁 （依靠Redis中的原子操作进行CAS比对，实现锁的互斥） 2、获取key所对应的时间，时间是锁预期的实效时间，如果已经实效，则存储新值，并获取锁 3、否则获取锁失败 解锁： 删除指定key的redis列 抢购、秒杀是如今很常见的一个应用场景，主要需要解决的问题有两个： 高并发对数据库产生的压力 竞争状态下如何解决库存的正确减少（”超卖”问题） 对于第一个问题，已经很容易想到用缓存来处理抢购，避免直接操作数据库，例如使用 Redis。 Redis 使用 watch 完成秒杀抢购功能：使用 redis 中两个 key 完成秒杀抢购功能，mywatchkey 用于存储抢购数量和 mywatchlist 用户存储抢购列表。 优点： 1、首先选用内存数据库来抢购速度极快 2、速度快并发自然没不是问题 3、使用悲观锁，会迅速增加系统资源 4、比队列强的多，队列会使内存数据库资源瞬间爆棚 5、使用乐观锁，达到综合需求 与关系型数据库的区别数据 bai 存储方式不同。关系型和非关系型数据库的主要差异是数据存储的方式。关系型数据天然就是表格式的，因此存储在数据表的行和列中。数据表可以彼此关联协作存储，也很容易提取数据。 与其相反，非关系型数据不适合存储在数据表的行和列中，而是大块组合在一起。非关系型数据通常存储在数据集中，就像文档、键值对或者图结构。你的数据及其特性是选择数据存储和提取方式的首要影响因素。 扩展方式不同。SQL 和 NoSQL 数据库最大的差别可能是在扩展方式上，要支持日益增长的需求当然要扩展。 要支持更多并发量，SQL 数据库是纵向扩展，也就是说提高处理能力，使用速度更快速的计算机，这样处理相同的数据集就更快了。 因为数据存储在关系表中，操作的性能瓶颈可能涉及很多个表，这都需要通过提高计算机性能来客服。虽然 SQL 数据库有很大扩展空间，但最终肯定会达到纵向扩展的上限。而 NoSQL 数据库是横向扩展的。 而非关系型数据存储天然就是分布式的，NoSQL 数据库的扩展可以通过给资源池添加更多普通的数据库服务器(节点)来分担负载。 对事务性的支持不同。如果数据操作需要高事务性或者复杂数据查询需要控制执行计划，那么传统的 SQL 数据库从性能和稳定性方面考虑是你的最佳选择。SQL 数据库支持对事务原子性细粒度控制，并且易于回滚事务。 虽然 NoSQL 数据库也可以使用事务操作，但稳定性方面没法和关系型数据库比较，所以它们真正闪亮的价值是在操作的扩展性和大数据量处理方面。 关系型优点： 易于维护：都是使用表结构，格式一致； 使用方便：SQL 语言通用，可用于复杂查询； 复杂操作：支持 SQL，可用于一个表以及多个表之间非常复杂的查询。 缺点： 读写性能比较差，尤其是海量数据的高效率读写； 固定的表结构，灵活度稍欠； 高并发读写需求，传统关系型数据库来说，硬盘 I&#x2F;O 是一个很大的瓶颈。 非关系型非关系型数据库严格上不是一种数据库，应该是一种数据结构化存储方法的集合，可以是文档或者键值对等。 优点： 格式灵活：存储数据的格式可以是 key,value 形式、文档形式、图片形式等等，文档形式、图片形式等等，使用灵活，应用场景广泛，而关系型数据库则只支持基础类型。 速度快：nosql 可以使用硬盘或者随机存储器作为载体，而关系型数据库只能使用硬盘； 高扩展性； 成本低：nosql 数据库部署简单，基本都是开源软件。 缺点： 不提供 sql 支持，学习和使用成本较高； 无事务处理； 数据结构相对复杂，复杂查询方面稍欠。","categories":[{"name":"技术分享","slug":"技术分享","permalink":"https://docs.hehouhui.cn/categories/%E6%8A%80%E6%9C%AF%E5%88%86%E4%BA%AB/"}],"tags":[{"name":"微服务","slug":"微服务","permalink":"https://docs.hehouhui.cn/tags/%E5%BE%AE%E6%9C%8D%E5%8A%A1/"},{"name":"分布式","slug":"分布式","permalink":"https://docs.hehouhui.cn/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"},{"name":"Redis","slug":"Redis","permalink":"https://docs.hehouhui.cn/tags/Redis/"},{"name":"开发","slug":"开发","permalink":"https://docs.hehouhui.cn/tags/%E5%BC%80%E5%8F%91/"},{"name":"缓存","slug":"缓存","permalink":"https://docs.hehouhui.cn/tags/%E7%BC%93%E5%AD%98/"}]},{"title":"silk-v3-decoder 一款微信音频转码的工具","slug":"archives/silk-v3-decoder 一款微信音频转码的工具","date":"2022-03-17T00:00:00.000Z","updated":"2023-10-08T06:42:00.000Z","comments":true,"path":"archives/41.html","link":"","permalink":"https://docs.hehouhui.cn/archives/41.html","excerpt":"","text":"SILK v3 编码是 Skype 向第三方开发人员和硬件制造商提供免版税认证(RF)的 Silk 宽带音频编码器，Skype 后来将其开源。具体可见 Wikipedia。 之前一直使用 ffmpeg 来进行格式转换，但是将微信的 amr 转为 mp3 后语音质量不理想（也可能是我参数没有调正确 🤪）。于是就继续想解决办法，后来在 github 瞎逛时看到可以使用 silk-v3-decoder 来做这件事情。虽然本质上还是使用的 ffmpeg 来转的，只是封装了一下。 (前往 silk-v3-decoder) ++环境要求 gcc 和 ffmpeg，所以还是得要安装 ffmpeg，gcc 是拿来编译 silk-v3-decoder 源码，ffmpeg 是拿来转换格式的。++ 安装 gccyum -y install gcc yum -y install gcc-c++ ffmpeg 安装打开官网地址，进入下载页：https://ffmpeg.org/download.html#build-linux 选择 Linux Static Builds 下的构建选项，进入详情页 在列表中选择适合自己的版本，鼠标右键，复制链接地址 # 下载文件 wget &lt;https://johnvansickle.com/ffmpeg/builds/ffmpeg-git-amd64-static.tar.xz> # 解压 xz -d ffmpeg-git-amd64-static.tar.xz # 再次解压 tar -xvf ffmpeg-git-amd64-static.tar 得到目录 ffmpeg 和 ffprobe 都在这里 如果想要 ffmpeg 命令全局可用，可以在 bin 目录加个链接。比如，分别执行如下命令，即可在:&#x2F;usr&#x2F;bin 目录下创建 ffmpeg 和 ffprobe 软链接。 cd /usr/bin ln -s 解压目录/ffmpeg ffmpeg ln -s 解压目录/ffprobe ffprobe 下载 silk-v3-decoder 源码&lt;https://ghproxy.com/https://github.com/kn007/silk-v3-decoder/archive/refs/heads/master.zip> 给脚步赋执行权限 chmod +x converter.sh chmod +x converter_beta.sh 使用silk-v3-decoder目录/converter.sh silk音频文件路径 mp3 第一个为执行脚本 第二个为 silk 音频如 amr 文件路径 第三个为需要转换为的音频格式","categories":[{"name":"技术分享","slug":"技术分享","permalink":"https://docs.hehouhui.cn/categories/%E6%8A%80%E6%9C%AF%E5%88%86%E4%BA%AB/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://docs.hehouhui.cn/tags/Python/"},{"name":"健康","slug":"健康","permalink":"https://docs.hehouhui.cn/tags/%E5%81%A5%E5%BA%B7/"}]},{"title":"wkhtmltopdf 安装","slug":"archives/wkhtmltopdf 安装","date":"2022-03-15T00:00:00.000Z","updated":"2023-10-08T06:42:00.000Z","comments":true,"path":"archives/40.html","link":"","permalink":"https://docs.hehouhui.cn/archives/40.html","excerpt":"","text":"wkhtmltopdf “wkhtmltopdf”，是一个能够把网页&#x2F;文件转换成 PDF 的工具。工具全名叫 “wkhtmltopdf” ; 是一个使用 Qt WebKit 引擎做渲染的，能够把 html 文档转换成 pdf 文档 或 图片(image) 的“命令行工具”。 支持多个平台，可在win，linux，os x 等系统下运行。 安装 通过文章底部的下载连接选择适合的版本**注意系统版本** 此处按照liunx版本来讲解 liunx版本的安装相对比较简单直接解压即可 解压后目录为use，重命名为wkhtmltopdf（强迫症）。 目录结构 /wkhtmltopdf/local/bin/ wkhtmltopdf 把一个文件或网页转成 pdf wkhtmltoimage 把一个文件或网页转成图片 妥妥的见名知意 使用把一个 html 文件转换成 PDF命令格式 ：wkhtmltopdf xxx.html xxx.pdf wkhtmltopdf 1.html 1.pdf Loading pages (1/6) Counting pages (2/6) Resolving links (4/6) Loading headers and footers (5/6) Printing pages (6/6) Done 当你看到类似上面的内容时，说转换已经完成了，去打开转换好的 pdf 文档慢慢研究吧。 把一个 url 指向的网页转换成 PDF命令格式 ： wkhtmltopdf url xxx.pdf wkhtmltopdf www.yioks.com yioks.pdf Loading pages (1/6) QFont::setPixelSize: Pixel size &lt;= 0 (0) ] 55% Counting pages (2/6) QFont::setPixelSize: Pixel size &lt;= 0 (0)=====================] Object 1 of 1 Resolving links (4/6) Loading headers and footers (5/6) Printing pages (6/6) Done 当你看到如上信息时代表转换成功，是不是很酷。 把 html 文件 和 url 指向的网页 转换成图片命令格式 ： wkhtmltoimage xxx.html xxx.jpg wkhtmltoimage url xxx.jpg 其实和转 pdf 时的参数是一样的，只是命令和输出文件的扩展名变了。上的命令格式中，我是把图片保存成了 jpg 格式，当然，如果你愿意也可以保存成其他图片格式(如：png)，但文件可能会变大很多倍。在我的测试中，jpg 格式文件是最小的。 使用过程中的问题 liunx 环境下中文乱码 部分 liunx 服务器缺少依赖 lib *中文乱码 ** liunx 中缺少中文（宋体）字体文件， (下载字体文件) 放置服务器中 /usr/share/fonts目录下 缺少 lib libjpeg.so.62 的解决方案 执行如下命令下载依赖 yum install fontconfig freetype libpng libjpeg libX11 libXext libXrender xorg-x11-fonts-Type1 xorg-x11-fonts-75dpi 其他的 html 转 pdf 方案 itextpdf.html2pdf 可转换样式简单的 html 使用简单速度快，对 CSS 样式支持不是很好。在 contract 中的实现为 PdfGeneratorIText lowagie.itext 可转换较复杂的 html，但对格式极其严苛。速度快 也有其他几款转换工具但都有或多或少的缺陷，wkhtmltopdf 虽说慢至少样式还行 下载链接【官网】:【http://wkhtmltopdf.org/】 【下载列表】 (详细使用)","categories":[{"name":"技术分享","slug":"技术分享","permalink":"https://docs.hehouhui.cn/categories/%E6%8A%80%E6%9C%AF%E5%88%86%E4%BA%AB/"}],"tags":[{"name":"图像处理","slug":"图像处理","permalink":"https://docs.hehouhui.cn/tags/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/"}]},{"title":"wkhtmltopdf详细使用","slug":"archives/wkhtmltopdf详细使用","date":"2022-03-15T00:00:00.000Z","updated":"2023-10-08T06:42:00.000Z","comments":true,"path":"archives/39.html","link":"","permalink":"https://docs.hehouhui.cn/archives/39.html","excerpt":"","text":"wkhtmltopdf 精讲(原文) 作者：JSON_NULL 术语定义文档对象“文档对象”是指 PDF 文档中的文档对象，共有三种类型的“文档对象”，他们分别是“页面对象”，“封面对象”和“目录对象”。 页面对象“页面对象”是指以页面的形式在 PDF 文档中呈现的对象，这个是相对于“封面对象”和“目录对象”来讲的。此类对象会成为 PDF 文档中内容。 封面对象“封面对象”是指以封面的形式在 PDF 文档中呈现的对象。这类对象会成为 PDF 文档中的封面。 目录对象“目录对象”是以目录的形式在 PDF 文档中呈现的对象，又叫“TOC 对象”。这类对象会成为 PDF 文档中的目录。 大纲“大纲”是指阅读 PDF 文档时的书签导航。 外链“外链”是指所有在这个页面中且不指向它自身页面中锚点的超链接。 内链“内链”是指在这个页面中且指向的目标页面是这个页面本身中的一个锚点的超链接。 命令格式wkhtmltopdf [GLOBAL OPTION]... [OBJECT]... &lt;output file> 上述代码就是 wkhtmltopdf 的命令行格式，看似简单，其实在 [GLOBAL OPTION] 和 [OBJECT] 中还别有洞天。预知详情，且听我慢慢道来。 文档对象简介wkhtmltopdf 能够把多个“对象”合并生成一个 pdf 文档，这些“对象”可以是“页面对象”、“封面对象”、或是“目录对象”。这些对象在 pdf 文档中的顺序可以通过命令行参数来指定。命令行参数包括两部分，一种是针对某一特定“对象”的命令行参数，另一种是全局的命令行参数。并且全局的命令行参数只能放在全局参数区([GLOBAL OPTION])中指定。 页面对象简介“页面对象”作用是用来把一个网页作为内容输出到 PDF 文档中。 (page)? &lt;input url/file name> [PAGE OPTION]... “页面对象”的参数可以放在“全局参数域([GLOBAL OPTIONS])”和“页面参数域([PAGE OPTIONS])”。程序会根据实际情况在所有参数中找到合适的参数应用到页面、页眉和页脚。 封面对象简介“封面对象”用来把一个网页作为封面输出到 PDF 文档中，输出的页面不会在 TOC 中出现，并且不会包含页眉和页脚。 cover &lt;input url/file name> [PAGE OPTION]... 所有能够在“页面对象”中使用的参数都可以用到“封面对象” 目录对象简介“目录对象”的作用是输出一个目录到 PDF 文件中。 toc [TOC OPTION]... 所有能够在“页面对象”中使用的参数都可以用到“TOC 对象”，并且还有许多的针对“TOC 对象”的参数可以应用到“TOC 对象”中。目录是通过 XSLT 生成的，这就意味着它可以被定义成任何你想看到的样子。你可以通过命令行参数 --dump-default-toc-xsl 输出默认的 XSLT 文档，通过 --dump-outline 命令行参数 可指定以 XML 格式输出当前处理文档的目录到指定文件。更多详细内容请查看后面介绍的 目录对象参数 命令参数命令参数包含五部分，分别是“全局参数”，“大纲参数选项”，“页面对象参数”，“页眉和页脚参数选项”和“目录对象参数”。 全局参数 --collate 当输出多个副本时进行校验(这是默认设置) --no-collate 当输出多个副本时不进行校验 --cookie-jar &lt;path> 从提供的JAR文件中读写cookie数据 --copies &lt;number> 设置输出副本的数量(默认主1)，其实为1就够了 -d, --dpi &lt;dpi> 指定一个要分辨率(这在 X11 系统中并没有什么卵用) -H, --extended-help 相对 -h 参数，显示更详细的说明文档 -g, --grayscale 指定以灰度图生成PDF文档。占用的空间更小 -h, --help 显示帮助信息 --htmldoc 输出程序的html帮助文档 --image-dpi &lt;integer> 当页面中有内嵌的图片时， 会下载此命令行参数指定尺寸的图片(默认值是 600) --image-quality &lt;interger> 当使用 jpeg 算法压缩图片时使用这个参数指定的质量(默认为 94) --license 输出授权信息并退出 -l, --lowquality 生成低质量的 PDF/PS ,能够很好的节约最终生成文档所占存储空间 --manpage 输出程序的手册页 -B, --margin-bottom &lt;unitreal> 设置页面的 底边距 -L, --margin-left &lt;unitreal> 设置页面的 左边距 (默认是 10mm) -R, --margin-right &lt;unitreal> 设置页面的 右边距 (默认是 10mm) -T, --margin-top &lt;unitreal> 设置页面的 上边距 -O, --orientation &lt;orientation> 设置为“风景(Landscape)”或“肖像(Portrait)”模式, 默认是肖像模块(Portrait) --page-height &lt;unitreal> 页面高度 -s, --page-size &lt;Size> 设置页面的尺寸，如：A4,Letter等，默认是：A4 --page-width &lt;unitreal> 页面宽度 --no-pdf-compression 不对PDF对象使用丢失少量信息的压缩算法，不建议使用些参数， 因为生成的PDF文件会非常大。 -q, --quiet 静态模式，不在标准输出中打印任何信息 --read-args-from-stdin 从标准输入中读取命令行参数，后续会有针对此指令的详细介绍， 请参见 **从标准输入获取参数** --readme 输出程序的 readme 文档 --title &lt;text> 生成的PDF文档的标题，如果不指定则使用第一个文档的标题 -V, --version 输出版本信息后退出 上述代码区是所有全局参数及注释，下面简单说一下个别参数的意义及用法。 -copies NN 是一个正整数。 这个选项可以先不用关心了，因为你这辈子可能都用不到。他的作用是在生成的 PDF 文档中，把内容重复输出 N 份。也就是说，你将得到一个 PDF 文档，这个文档中的大小、内容量都将是不使用此参数时的 N 倍。然而重复的内容对你来说并没有什么卵用。 如果不使用 --copies 参数，那么 --collate 和 --no-collate 参数就不用了解了，因为他们只在 --copies 参数中的 N 大于 1 时才有意义。 g, –grayscale这个参数非常有用，使用这个参数可以有效压缩生成的 PDF 所占用的存储空间。当然这个压缩是要付出一定代价的，那就是最终生成的 PDF 文档将是灰度的，没有任何色彩。如果你能接受灰度 PDF 文档，并不影响实际使用，那就请使用这个参数吧。生成的 PDF 文档越大，使用此参数获得的惊喜就越大。 l, –lowquality这个参数与 -g 参数有异曲同工之妙， -l 参数也会大大压缩 PDF 文档所占用的存储空间。只是它是通过降低 PDF 文档的质量来完成这一任务的。这个参数也值得推荐，你最好先尝试一下，看看使用此参数后生成的 PDF 文档与不使用此参数的区别再做决定。我可以告诉你的是，在纯文字的情况下他们的差别不大，此参数只是降低了 PDF 文档的质量，看上去是糙了一些，但不会影响阅读。如果你是一个追求感官享受，或是你生成的 PDF 文档中有大量图片，那就不要使用此参数了。 -no-pdf-compression这个参数强烈建议不要使用，最好这辈子都不要去了解他的好，因为对于你来说肯定用不到。它的作用就是在输出 PDF 文档时，不使用任何的压缩。这将会导致输出的 PDF 文档特别的大，质量是无损的，但是对于人类来说从感观上根本察觉不到压缩前后的质量变化的。如果你的感观超乎于常人，压缩之后的体验对你来说无法接受，那我收回前面的话，你就尽情使用此参数吧。 q, –quiet使用这个参数后，你将得到一个干净的命令行输出，就连程序处理的进度和状态都没有。这个参数会抑制所有命令行输出，在程序的工作过程中，你看不到任何输出。建议不会使用此参数，因为程序输出一些进度和状态信息还是非常有用的。万一程序工作到某处死了呢(嘿嘿)，在 -q 模式下你是无法分辨是否程序死掉了的。 大纲参数选项--dump-default-toc-xsl 输出默认的 TOC xsl 样式表到标准输出 --dump-outline &lt;file> 输出“大纲”到指定的文件(文件内容为xml) --outline 在生成的PDF文档中输出“大纲”(这是默认设置) --no-outline 不在pdf文档中输出大纲 --outline-depth &lt;level> 设置生成大纲的深度(默认为 4) 大纲参数中唯一需要特别说一下的是 --outline-depth ，其他参数默认就好了。 何为大纲 大纲 如上图所示，其实我更喜欢称之为目录或导航。大纲是根据你 HTML 中的标题(Hn 标签)自动生成的。 -outline-depth -outline-depth 用来指定生成的大纲的深度。默认值为 4。你可以指定一个大一些的数字，以保证所有在 HTML 中指定的 H 标签都能在大纲中生成对应的项，方便阅读时快速跳转。 当指定了 --no-outline 参数时， 将不会输出大纲到 PDF 文档，所以再指定 --outline-depth 也就没有意义了。 页面对象参数 --allow &lt;path> 指定加载HTML中相对路径文件的目录(可重复使用此参数指定多个 目录)，这个参数会在后面进行更详细的讲解 --background 输出页面背景到PDF文档(这是默认设置) --no-background 不输出页面背景到PDF文档 --cache-dir &lt;path> 网页的缓存目录 --checkbox-checked-svg &lt;path> 使用指定的SVG文件渲染选中的复选框 --checkbox-svg &lt;path> 使用指定的SVG文件渲染未选中的筛选框 --cookie &lt;name> &lt;value> 设置访问网页时的cookie,value 需要进行url编码 (可重复使用此参数指定多个cookie) --custom-header &lt;name> &lt;value> 设置访问网页时的HTTP头(可重复使用此参数指定多个HTTP头) --custom-header-propagation 为每个要加载的资源添加由 --custom-header 指定的HTTP头 --no-custom-header-propagation 不为每个要加载的资源添加由 --custom-header 指定的HTTP头 --debug-javascript 显示javascript调试输出的信息 --no-debug-javascript 不显示javascript调试输出的信息(这是默认设置) --default-header 添加一个默认的“头”，在页面的左头显示页面的名字， 在页面的右头显示页码，这相对于进行了如下设置： --header-left='[webpage]' --header-right='[page]/[toPage]' --top 2cm --header-line --encoding &lt;encoding> 为输入的文本设置默认的编码方式 --disable-external-links 禁止页面中的外链生成超链接 --enable-external-links 允许页面中的外链生成超链接(这是默认设置) --disable-forms 不转换HTML表单为PDF表单(这是默认设置) --enable-forms 转换HTML表单为PDF表单 --images 加载图片并输出到PDF文档(这是默认设置) --no-images 在生成的PDF文档中过滤掉图片 --disable-internal-links 禁止页面中的内链生成超链接 --enable-internal-links 允许页面中的内链生成超连接(这是默认设置) -n, --disable-javascript 禁止WEB页面执行 javascript --enable-javascript 允许WEB页面执行 javascript(这是默认设置) --javascript-delay &lt;msec> 延迟一定的毫秒等待javascript 执行完成(默认值是200) --load-error-handling &lt;handler> 指定当页面加载失败后的动作，可以指定为：abort(中止)、 ignore(忽略)、skip(跳过)；(默认值是：abort) --load-media-error-handling &lt;handler> 指定当媒体文件加载失败后的动作，可以指定为： abort(中止)、ignore(忽略)、skip(跳过)； (默认值是：ignore) --disable-local-file-access 不允许一个本地文件加载其他的本地文件，使用命令行参数 `--allow` 指定的目录除外。 --enable-local-file-access 允许本地文件加载其他的本地文件(这是默认设置) --minimum-font-size &lt;int> 设置最小的字号，除非必要不推荐使用该参数 --exclude-from-outline 拒绝加载当前页面到PDF文档的目录和大纲中 --include-in-outline 加载当前页面到PDF文档的目录和大纲中(这是默认设置) --page-offset &lt;offset> 设置页码的起始值(默认值为0) --password &lt;password> HTTP身份认证的密码 --disable-plugins 禁止使用插件(这是默认设置) --enable-plugins 允许使用插件，但插件可能并不工作 --post &lt;name> &lt;value> 添加一个POST字段，可以重复使用该参数添加多个POST字段。 --post-file &lt;name> &lt;value> 添加一个POST文件，可以重复使用该参数添加多个文件。 --print-media-type 用显示媒体类型代替屏幕 --no-print-media-type 不用显示媒体类型代替屏幕 -p, --proxy &lt;proxy> 使用代理 --radiobutton-checked-svg &lt;path> 使用指定的SVG文件渲染选中的单选框 --radiobutton-svg &lt;path> 使用指定的SVG文件渲染未选中的单选框 --run-sript &lt;js> 页面加载完成后执行一个附加的JS文件，可以重复使用此参数指定 多个要在页面加载完成后要执行的JS文件。 --disable-smart-shrinking 不使用智能收缩策略 --enable-smart-shrinking 使用智能收缩策略(这是默认设置) --stop-slow-scripts 停止运行缓慢的javascript代码(这是默认设置) --no-stop-slow-scripts 不停止运行缓慢的javascript代码 --disable-toc-back-links 禁止从标题链接到目录(这是默认设置) --enable-toc-back-links 允许从标题链接到目录 --user-style-sheet &lt;url> 设置一个在每个页面都加载的用户自定义样式表 --username &lt;username> HTTP身谁的用户名 --viewport-size &lt;> 设置窗口大小,需要你自定义滚动条或css属性来自适应窗口大小。 --window-status &lt;windowStatus> Wait until window.status is equal to this string before rendering page --zoom &lt;float> 设置转换成PDF时页面的缩放比例(默认为1) 上面代码段中只是对所有 页面对象参数 做了个大概的说明，下面针对个别主要参数做更详细的讲解。 -allow这个参数只在“页面对象”是一个文件时有效，在“页面对象”是一个 url 时此参数无效。 这个参数的作用是为 HTML 页面中使用相对路径引用的文件指定一个加载文件的基目录。也就是说 HTML 文件中所有以相对路径指定的文件都会从 --allow 参数指定的目录进行加载。其实在 HTML 中指定 base 标签可以达到同样的目的。如果两者(--allow参数和base标签)都没有指定，则使用当前处理的 HTML 文件所在的目录作为基目录加载当前处理的 HTML 中相对路径指定的文件。 -background AND –no-background这两个参数是一对，用来指定是否在生成的 PDF 中应用网页的背景，默认 --background 参数是开启的，也就是说默认生成的 PDF 文档中是带有 HTML 页面的背景图片或背景色的。如果开启 --no-backgroupd 参数，则生成的 PDF 文档中不会有 HTML 页面中的背景图片和背景色。 -debug-javascript ADN –no-debug-javascript这两个参数用来指定是否在标准输出中输出 javascript 的调试信息，默认 --no-debug-javasript 参数是开启的，也就是说默认不会输出 javascript 的调试信息。下图是打开 --debug-javascript 参数的演示。 -debug-javascript -disable-external-links AND –enable-external-links这两个参数是用来设置在页面中的外链是否以超链接的形式出现在 PDF 文档中。关于“外链”的定义请移架 术语定义 。默认 --enable-external-links 参数被打开，所以默认情况是页面中的外链是以超链接的形式出现的 PDF 文档中的，点击可以打开指定的网页。 -exclude-from-outline AND –include-in-outline这两个参数用来设置当前页面对象是否包含到目录和大纲中。默认情况下 --include-in-outline 参数是打开的。也就是说默认情况下生成的 PDF 文档目录和大纲中是包含当前页面的，如果你不想让当前页面加到目录和大纲中可以打开 --exclude-from-outline 参数。 -post AND –post-file当目标页面需要接受 POST 表单才能正确得到响应时，可以用这两个参数。这两个参数都是可以重复使用的。 还有一个应用场景是，用于自动化的 WEB 应用测试中。可以得到 PDF 文档作为测试报告。 -post-file 也可以用于自动批量上传文件的场景。 -run-sript当需要对页面进行一定的预处理后再生成 PDF 文档的场景，使用该参数再合适不过了。这个参数可以重复使用指定多个需要在页面加载完成后执行的 JS 代码。你可以在这些 JS 中对页面的结构和内容进处理，JS 执行完成后才会把对应的页面生成 PDF 文档。 -disable-internal-links AND –enable-internal-links这两个参数是用来设置在页面中的内链是否以超链接的形式出现在 PDF 文档中。关于“内链”的定义请移架 术语定义 。默认 --enable-internal-links 参数被打开，所以默认情况是页面中的内链是以超链接的形式出现的 PDF 文档中的，点击在当前 PDF 中跳转到指定锚点。 -enable-toc-back-links AND –disable-toc-back-links这组参数用来设置，是否在 PDF 内容中的 H 标签处生成超链接。生成的超链接点击后会跳转到目录和大纲中该 H 标签对应的锚点位置。默认情况下 --disable-toc-back-links 参数被打开，不会在 PDF 文档的 H 标签处生成超链接。 如果你需要在阅读 PDF 文档的内容时快速回到目录，你可以打开 --enable-toc-back-links 参数。 -user-style-sheet这个参数用来加载一个用户自定义的样式表，用来改变 HTML 页面原有的样式。需要高度自定义页面新式的同学可以尝试使用这个参数达到目的。 页眉和页脚参数选项--footer-center &lt;text> 在页脚的居中部分显示页脚文本 &lt;text> --footer-font-name &lt;name> 设置页脚的字体 (默认为 Arial) --footer-font-size &lt;size> 设置页脚的字体大小 (默认为 12) --footer-html &lt;url> 添加一个html作为页脚 --footer-left &lt;text> 在页脚的居左部分显示页脚文本 &lt;text> --footer-line 在页脚上方显示一条直线分隔正文 --no-footer-line 不使用直线分隔页脚与正文(这是默认设置) --footer-right &lt;text> 在页脚的居右部分显示页脚文本 &lt;text> --footer-spacing &lt;real> 页脚与正文之间的距离(默认为零) --header-center &lt;text> 在页眉的居中部分显示页眉文本 &lt;text> --header-font-name &lt;name> 设置页眉的字体 (默认为 Arial) --header-font-size &lt;size> 设置页眉的字体大小 (默认为 12) --header-html &lt;url> 添加一个html作为页眉 --header-left &lt;text> 在页眉的居左部分显示页眉文本 &lt;text> --header-line 在页眉下方显示一条直线分隔正文 --no-header-line 不使用直线分隔页眉与正文(这是默认设置) --header-right &lt;text> 在页眉的居右部分显示页眉文本 &lt;text> --header-spacing &lt;real> 页眉与正文之间的距离(默认为零) 页眉页脚的设置比较简单，看上述代码段中的解释已经非常明了，所以不再赘述。后面还有针对页眉与页脚的其他相关介绍。 目录对象参数--disable-dotted-lines 在目录中不使用虚线 --toc-header-text &lt;text> 设置目录的页眉文本 --toc-level-indentation &lt;width> 第级标题在目录中的缩进宽度(默认为1em) --disable-toc-links 在目录中不生成指向内容锚点的超链接 --toc-text-size-shrink &lt;real> 在目录中每级标题的缩放比例(默认为0.8) --xsl-style-sheet &lt;file> 使用自定义的 XSL 样式表显示目录内容 “目录对象”我们一般用不到，上述代码段中的讲解也不难懂，所以不针对每一个具体参数再做详细的讲解。 关于页面尺寸说明默认的页面尺寸是 A4，你可以使用 --page-size 参数指定你想要的页面尺寸，如：A3，Letter 和 Legal 等。想要查看本程序支持的所有页面尺寸，请访问 http://qt-project.org/doc/qt-4.8/qprinter.html#PaperSize-enum 你还可以使用 --page-height 和 --page-width 对页面尺寸进行更精细的控制。 从标准输入获取参数如果你需要对许多页面进行批量的处理，并且感觉 wkhtmltopdf 开启比较慢，你可以尝试使用 --read-args-from-stdin 参数。 wkhtmltopdf 命令会为 --read-args-from-stdin 参数发送过来的每一行进行一次单独命令调用。也就是说此参数每读取一行都会执行一次 wkhtmltopdf 命令。而最终执行的命令中的参数是命令行中参数与此参数读取的标准输入流中参数的结合。 下面的代码段是一个例子: echo \"&lt;http://qt-project.org/doc/qt-4.8/qapplication.html> qapplication.pdf\" >> cmds echo \"cover google.com &lt;http://en.wikipedia.org/wiki/Qt_(software)> qt.pdf\" >> cmds wkhtmltopdf --read-args-from-stdin --book &lt; cmds 指令一个代理默认情况下代理信息将读取环境变量:proxy、all_proxy 和 http_proxy,代理选项还可以通过指定 -p 参数开启。 使用 BNF 对代理的定义如下： &lt;type> := \"http://\" | \"socks5://\" &lt;serif> := &lt;username> (\":\" &lt;password>)? \"@\" &lt;proxy> := \"None\" | &lt;type>? &lt;serif>? &lt;host> (\":\" &lt;port>)? 如果你不熟悉 BNF 的话，下面的代码段中是三个例子: &lt;http://user:password@myproxyserver:8080> socks5://myproxyserver None 页眉和页脚页眉和页脚可以使用参数 --header-* 和 --footer-* 添加到文件中。有些参数(如 --footer-left)需要提供一个字符串text作为参数值。你可以在 text中插入下述变量，他们将会被替换成对应的值。 [page] 当前正在被输出页面的页码 [frompage] 第一页在文档中的页码 [topage] 最后一面在文档中的页码 [webpage] 当前正在被输出页面的URL [section] 当前正在被输出的章节的名字 [subsection] 当前正在被输出的小节的名字 [date] 本地系统格式的当前日期 [isodate] ISO 8601 格式的当前日期 [time] 本地系统格式的当前时间 [title] 当前对象的标题 [doctitle] 输出文档的标题 [sitepage] 当前正在处理的对象中当前页面的页码 [sitepages] 当前正在处理的对象中的总页数 举一个例子来说明吧，--header-right &quot;Page [page] of [toPage]&quot;, 会在页面的右上角生成一个类似 Page x of y 的字符串，其中 x 是当前页面的页码， y 是当前文档最后一页的页码。 页眉和页脚也可以通过 HTML 文档来提供。 同样举一个例子，使用命令行参数 --header-html header.html 来生成页眉，而 header.html 的内容如下: &lt;html>&lt;head>&lt;script> function subst() &#123; var vars=&#123;&#125;; var x=window.location.search.substring(1).split('&amp;'); for (var i in x) &#123;var z=x[i].split('=',2);vars[z[0]] = unescape(z[1]);&#125; var x=['frompage','topage','page','webpage','section','subsection','subsubsection']; for (var i in x) &#123; var y = document.getElementsByClassName(x[i]); for (var j=0; j&lt;y.length; ++j) y[j].textContent = vars[x[i]]; &#125; &#125; &lt;/script>&lt;/head>&lt;body style=\"border:0; margin: 0;\" onload=\"subst()\"> &lt;table style=\"border-bottom: 1px solid black; width: 100%\"> &lt;tr> &lt;td class=\"section\">&lt;/td> &lt;td style=\"text-align:right\"> Page &lt;span class=\"page\">&lt;/span> of &lt;span class=\"topage\">&lt;/span> &lt;/td> &lt;/tr> &lt;/table> &lt;/body>&lt;/html> 大纲(Outlines)wkhtmltopdf 可以使用 --outline 命令行参数来指定在 PDF 就要中输出像书本中目录一样的“大纲”，“大纲”是基本 HTML 文档中 H 标签生成的，具体的大纲的层级和尝试请移步 目录 如果 HTML 文档中的 H 标签等级比较多，就可以生成深层级树形结构的“大纲”，而生成“大纲”的真实深度是通过 --outline-depth 参数来控制。 目录通过在命令行中添加 TOC 对象 可以把一个目录添加到生成的 PDF 文档中，例如下面的代码段： wkhtmltopdf toc &lt;http://qt-project.org/doc/qt-4.8/qstring.html> qstring.pdf 生成的目录也是基于 HTML 文档的 H 标签。过程是首先生成一个 XML 文档,然后使用 XSLT 转换为 HTML。生成的 XML 文档可以通过 --dump-outline 参数查看。 wkhtmltopdf --dump-outline toc.xml &lt;http://qt-project.org/doc/qt-4.8/qstring.html> qstring.pdf 你如果想要使用自定义的 XSLT 文档可以通过 --xsl-style-sheet 参数指定 wkhtmltopdf toc --xsl-style-sheet my.xsl &lt;http://qt-project.org/doc/qt-4.8/qstring.html> qstring.pdf 你可以使用 --dump-default-toc-xsl 参数把默认的 XSLT 文档打印到标准输出，然后基于它创建你的自定义 XSLT 文档。 wkhtmltopdf --dump-default-toc-xsl","categories":[{"name":"技术分享","slug":"技术分享","permalink":"https://docs.hehouhui.cn/categories/%E6%8A%80%E6%9C%AF%E5%88%86%E4%BA%AB/"}],"tags":[{"name":"图像处理","slug":"图像处理","permalink":"https://docs.hehouhui.cn/tags/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/"}]},{"title":"API网关之Gateway","slug":"archives/API网关之Gateway","date":"2022-03-05T00:00:00.000Z","updated":"2023-10-08T06:42:00.000Z","comments":true,"path":"archives/46.html","link":"","permalink":"https://docs.hehouhui.cn/archives/46.html","excerpt":"","text":"Gateway什么是服务网关API Gateway，顾名思义，是出现在系统边界上的一个面向 API 的、串行集中式的强管控服务，这里的边界是系统服务的边界，可以理解为服务防火墙，主要起到隔离外部访问与内部系统的作用。在微服务概念的流行之前，API 网关就已经诞生了，例如银行、证券等领域常见的前置机系统，它也是解决访问认证、报文转换、访问统计等问题的。 API 网关的流行，源于近几年来移动应用与企业间互联需求的兴起。移动应用、企业互联，使得后台服务支持的对象，从以前单一的 Web 应用，扩展到多种使用场景，且每种使用场景对后台服务的要求都不尽相同。这不仅增加了后台服务的响应量，还增加了后台服务的复杂性。随着微服务架构概念的提出，API网关成为了微服务架构的一个标配组件。 API 网关是一个服务器，是系统对外的唯一入口。API 网关封装了系统内部架构，为每个客户端提供定制的 API。所有的客户端和消费端都通过统一的网关接入微服务，在网关层处理所有非业务功能。API 网关并不是微服务场景中必须的组件，如下图，不管有没有 API 网关，后端微服务都可以通过 API 很好地支持客户端的访问。 API 网关出现的原因是微服务架构的出现，不同的微服务一般会有不同的网络地址，而外部客户端可能需要调用多个服务的接口才能完成一个业务需求，如果让客户端直接与各个微服务通信，会有以下的问题： 客户端会多次请求不同的微服务，增加了客户端的复杂性。 存在跨域请求，在一定场景下处理相对复杂。 认证复杂，每个服务都需要独立认证。 难以重构，随着项目的迭代，可能需要重新划分微服务。例如，可能将多个服务合并成一个或者将一个服务拆分成多个。如果客户端直接与微服务通信，那么重构将会很难实施。 某些微服务可能使用了防火墙 &#x2F; 浏览器不友好的协议，直接访问会有一定的困难。 API 网关方式的核心要点是，所有的客户端和消费端都通过统一的网关接入微服务， 在网关层处理所有的非业务功能。通常，网关也是提供 REST&#x2F;HTTP 的访问 API。 网关解决了什么问题但对于服务数量众多、复杂度比较高、规模比较大的业务来说，引入 API 网关也有一系列的好处： 聚合接口使得服务对调用者透明，客户端与后端的耦合度降低 聚合后台服务，节省流量，提高性能，提升用户体验 提供安全、流控、过滤、缓存、计费、监控等 API 管理功能 网关应当具备以下功能 性能：API 高可用，负载均衡，容错机制。 安全：权限身份认证、脱敏，流量清洗分发，后端签名（保证全链路可信调用），黑名单（非法调用的限制）。 日志：日志记录，一旦涉及分布式，全链路跟踪必不可少。 缓存：数据缓存。 监控：记录请求响应数据，API 耗时分析，性能监控。 限流：流量控制，错峰流控，可以定义多种限流规则。 灰度：线上灰度部署，可以减小风险。 路由：动态路由规则。 主要功能 数据平面主要功能是接入用户的 HTTP 请求和微服务被拆分后的聚合。使用微服务网关统一对外暴露后端服务的 API 和契约，路由和过滤功能正是网关的核心能力模块。另外，微服务网关可以实现拦截机制和专注跨横切面的功能，包括协议转换、安全认证、熔断限流、灰度发布、日志管理、流量监控等。 控制平面主要功能是对后端服务做统一的管控和配置管理。例如，可以控制网关的弹性伸缩；可以统一下发配置；可以对网关服务添加标签； Spring Cloud Gateway Spring Cloud Gateway 是基于 Spring 生态系统之上构建的 API 网关，包括：Spring 5，Spring Boot 2 和 Project Reactor。Spring Cloud Gateway 旨在提供一种简单而有效的方法来路由到 API，并为它们提供跨领域的关注点，例如：安全性，监视&#x2F;指标，限流等。 由于 Spring 5.0 支持 Netty，Http2，而 Spring Boot 2.0 支持 Spring 5.0，因此 Spring Cloud Gateway 天然支持 Netty 和 Http2。 Spring Cloud Gateway 建立在Spring Boot 2.x、Spring WebFlux和Project Reactor之上（基于⾼性能的 Reactor 模式响应式通信框架 Netty，异步⾮阻塞模型）。因此当使用 Spring Cloud Gateway 时，许多熟悉的同步库（例如 Spring Data 和 Spring Security）和模式可能并不适用。 Spring Cloud Gateway 需要 Spring Boot 和 Spring Webflux 提供的 Netty 运行时。它不适用于传统的 Servlet 容器或构建为 WAR 时。 核心概念 Route: The basic building block of the gateway. It is defined by an ID, a destination URI, a collection of predicates, and a collection of filters. A route is matched if the aggregate predicate is true. 路由是构建网关的基本模块，它由 ID，目标 URI，一系列的断言和过滤器组成，如果断言为 true 则匹配该路由 Predicate: This is a Java 8 Function Predicate. The input type is a Spring Framework ServerWebExchange. This lets you match on anything from the HTTP request, such as headers or parameters. 参考的是 java8 的 java.util.function.Predicate 开发人员可以匹配 HTTP 请求中的所有内容（例如请求头或请求参数），如果请求与断言相匹配则进行路由 Filter: These are instances of GatewayFilter that have been constructed with a specific factory. Here, you can modify requests and responses before or after sending the downstream request. 指的是 Spring 框架中 GatewayFilter 的实例，使用过滤器，可以在请求被路由前或者之后对请求进行修改。 工作流程 如上图所示，客户端向 Spring Cloud Gateway 发出请求。再由网关处理程序 Gateway Handler Mapping 映射确定与请求相匹配的路由，将其发送到网关 Web 处理程序 Gateway Web Handler。该处理程序通过指定的过滤器链将请求发送到我们实际的服务执行业务逻辑，然后返回。过滤器由虚线分隔的原因是，过滤器可以在发送代理请求之前和之后运行逻辑。所有 pre 过滤器逻辑均被执行。然后发出代理请求。发出代理请求后，将运行 post 过滤器逻辑。 Predicate 断言SpringCloud Gateway 包括许多内置的 Route Predicate 工厂。所有这些 Predicate 都与 HTTP 请求的不同属性匹配。多个 Route Predicate 工厂可以进行组合。 SpringCloud Gateway 创建 Route 对象时，使用 RoutePredicateFactory 创建 Predicate 对象，Predicate 对象可以赋值给 Route。SpringCloud Gateway 包含许多内置的 RoutePredicateFactories。 所有这些断言都匹配 HTTP 请求的不同属性。多种断言工厂可以组合，并通过逻辑 and。 SpringCloud Gateway 将路由匹配作为 Spring WebFlux HandlerMapping 基础架构的一部分。 server: port: 11001 spring: application: name: gateway cloud: gateway: discovery: locator: enabled: true #开启从注册中心动态创建路由的功能，利用微服务名进行路由 routes: - id: blog-admin #路由的ID，没有固定规则但要求唯一，建议配合服务名 #uri: &lt;http://localhost:8001> #匹配后提供服务的路由地址 uri: lb://blog-service predicates: - Path=/api/admin/** #断言,路径相匹配的进行路由 - id: blog #uri: &lt;http://localhost:8001> #匹配后提供服务的路由地址 uri: lb://blog-service predicates: - Path=/archives/** #断言,路径相匹配的进行路由 #- After=2020-03-08T10:59:34.102+08:00[Asia/Shanghai] #- Cookie=username,zhangshuai #并且Cookie是username=zhangshuai才能访问 #- Header=X-Request-Id, \\\\d+ #请求头中要有X-Request-Id属性并且值为整数的正则表达式 #- Host=**.atguigu.com #- Method=GET #- Query=username, \\\\d+ #要有参数名称并且是正整数才能路由 内置谓词断言 After Route Predicate Factory 匹配在指定日期时间之后发生的请求 参数：datetime 注意时间格式带时区 ZonedDateTime spring: cloud: gateway: routes: - id: after_route_test uri: &lt;https://www.hehouhui.cn&gt; predicates: - After=2022-07-10T17:42:00.789-07:00[Asia/Shanghai] Before Route Predicate Factory 匹配在指定日期时间之前发生的请求 参数：datetime 注意时间格式带时区 ZonedDateTime spring: cloud: gateway: routes: - id: before_route_test uri: &lt;https://www.hehouhui.cn> predicates: - Before=2022-07-10T17:43:48.789-07:00[Asia/Shanghai] Between Route Predicate Factory 匹配在指定范围日期时间发生的请求 参数：datetime1 开始 datetime2 结束 注意时间格式带时区 ZonedDateTime spring: cloud: gateway: routes: - id: between_route_test uri: &lt;https://www.hehouhui.cn&gt; predicates: - Between=2022-07-10T17:42:30.789-07:00[Asia/Shanghai],2022-07-10T17:42:59.789-07:00[Asia/Shanghai] Cookie Route Predicate Factory 匹配在指定包含指定 cookie 值的请求 参数：name cookie 名称 regexp 匹配值的正则 spring: cloud: gateway: routes: - id: cookie_route_test uri: &lt;https://www.hehouhui.cn&gt; predicates: - Cookie=token,allow_account Header Route Predicate Factory 匹配在指定包含指定请求头的请求 参数：name 请求头的名称 regexp 匹配值的正则 spring: cloud: gateway: routes: - id: handler_route_test uri: &lt;https://www.hehouhui.cn> predicates: - Handler=User-Agent,*Mac OS* Host Route Predicate Factory 请求主机地址匹配 参数： patterns 正则集合 spring: cloud: gateway: routes: - id: host_route_test uri: &lt;https://www.hehouhui.cn> predicates: - Host=blog.hehouhui.cn Method Route Predicate Factory 请求方法的匹配 参数： methods 集合 spring: cloud: gateway: routes: - id: method_route_test uri: &lt;https://www.hehouhui.cn> predicates: - Method=Get,Post Path Route Predicate Factory 请求路由规则匹配 参数：patterns 匹配集合 spring: cloud: gateway: routes: - id: path_route_test uri: &lt;https://www.hehouhui.cn> predicates: - Path=/test/** Query Route Predicate Factory 请求参数 url ？后的参数 参数 param 参数名 regexp 参数匹配值，如果为空的情况下表示任何值 spring: cloud: gateway: routes: - id: query_route_test uri: &lt;https://www.hehouhui.cn> predicates: - Query=name,laiweihua RemoteAddr Route Predicate Factory 客户端请求 IP 匹配 参数：sources 注意此处为cidr格式 CIDR 表示法（IPv4 或 IPv6）字符串 spring: cloud: gateway: routes: - id: remoteAddr_route_test uri: &lt;https://www.hehouhui.cn> predicates: - RemoteAddr=192.168.1.1/24 默认情况下，RemoteAddr 路由谓词工厂使用来自传入请求的远程地址。如果 Spring Cloud Gateway 位于代理层后面，这可能与实际客户端 IP 地址不匹配。 可以通过设置自定义来自定义解析远程地址的方式RemoteAddressResolver。Spring Cloud Gateway 带有一个基于X-Forwarded-For 标头的非默认远程地址解析器，XForwardedRemoteAddressResolver. XForwardedRemoteAddressResolver有两个静态构造方法，它们采用不同的安全方法： XForwardedRemoteAddressResolver::trustAll返回RemoteAddressResolver始终采用在X-Forwarded-For标头中找到的第一个 IP 地址的 a。这种方法容易受到欺骗，因为恶意客户端可以为 设置初始值，X-Forwarded-For解析器会接受该值。 XForwardedRemoteAddressResolver::maxTrustedIndex采用与 Spring Cloud Gateway 前运行的受信任基础架构数量相关的索引。例如，如果 Spring Cloud Gateway 只能通过 HAProxy 访问，则应使用值 1。如果在访问 Spring Cloud Gateway 之前需要两跳可信基础架构，则应使用值 2。 Weight Route Predicate Factory 权重路由匹配 参数： group 自定义分组名称 weight 权重 spring: cloud: gateway: routes: - id: weight_high uri: &lt;https://www.hehouhui.cn> predicates: - Weight=group, 8 - id: weight_low uri: &lt;https://blog.hehouhui.cn> predicates: - Weight=group, 2 该路由会将约 80% 的流量转发到 &lt;https://www.hehouhui.cn>，将约 20% 的流量转发到https://blog.hehouhui.cn XForwarded Remote Addr Route Predicate Factory 根据 X-Forwarded-For 请求头匹配 参数：sources 注意此处为 cidr 格式 spring: cloud: gateway: routes: - id: xforwarded_route uri: &lt;https://www.hehouhui.cn> predicates: - XForwardedRemoteAddr=192.168.1.1/24 Filter 过滤器GlobaFilter 全局过滤器 GlobaFilter 全局过滤器，不需要在配置文件中配置，系统初始化时加载，并作用在每个路由上。 SpringCloud Gateway 核心的功能也是通过内置的全局过滤器来完成 类名 作用 CustomGlobalFilter 组合所有过滤器的排序,包含 GlobaFilter 和 GatewayFilter。Gateway 区分过滤器逻辑执行的“前”和“后”阶段，具有最高优先级的过滤器是“前”阶段的第一个和“后”阶段的最后一个 -阶段。 ForwardRoutingFilter forward:&#x2F;&#x2F;开头的 url 协议将会被转发 ReactiveLoadBalancerClientFilter 将以 lb:&#x2F;&#x2F;开头的 url 经过 ReactorLoadBalancer 路由加载转换 NettyRoutingFilter http 或者 https 协议 由 Netty 过滤器运行。它使用 NettyHttpClient 发出下游代理请求 NettyWriteResponseFilter 并将代理响应写回网关客户端响应 RouteToRequestUrlFilter 将 Route 对象转为 URL,并存储原 Route 信息 WebsocketRoutingFilter 将以 ws,wss 或请求头 Upgrade &#x3D; WebSocket 的 http,https 以 We bsocket 协议执行 GatewayMetricsFilter 路由指标数据收集用于集成 Grafana 仪表板 AbstractGatewayFilterFactory 局部过滤器工厂 GatewayFilter 局部过滤器，是针对单个路由的过滤器。 在 SpringCloud Gateway 组件中提供了大量内置的局部过滤器，对请求和响应坐过滤操作。 遵循约定大于配置的思想，只需要在配置文件配置局部过滤器名称，并为其制定对应的值。就可以让其生效。 名称 作用 参数 AddRequestHeader 为路由添加请求头 name,value AddRequestParameter 为路由添加请求 query 参数(拼接在 url 后) name,value AddResponseHeader 为路由添加响应头 name,value DedupeResponseHeader 响应头去重,默认取响应头的首个 value，最主要是用来处理自 2.2.5 版本后网关和下游服务都处理了 CORS 的 bug name,strategy CircuitBreaker 路由断路器，在服务异常或者返回指定状态码时转发路由至指定 fallbackUri name,fallbackUri,routeId,statusCodes FallbackHeaders 在执行请求转发时添加指定的请求头信息 MapRequestHeader copy 请求头中指定值到新的请求头中 fromHeader,toHeader PrefixPath 为路由 URL 添加统一前缀 prefix PreserveHostHeader 无特殊逻辑，只是标记一下路由为原始请求 RequestRateLimiter 路由限流，由 KeyResolver 解析出路由细粒度，RateLimiter 进行限流处理 keyResolver, rate-limiter.replenish-rate ,rate-limiter.burst-capacity,rate-limiter.requested-tokens。requestedTokens 每次请求消耗多少令牌，burstCapacity 令牌桶总容量，replenishRate 每多少秒生产一枚令牌 RedirectTo 请求转发 RemoveRequestHeader 删除请求头 RemoveResponseHeader 删除响应头 RemoveRequestParameter 删除请求参数 RequestHeaderSize 累计请求头 value bytes 大小不超过最大值 RewritePath 重写 Path RewriteLocationResponseHeader 修改 Location 的值 RewriteResponseHeader 重写响应头值 SaveSession 保存 session SecureHeaders 安全生命请求头 SetPath 设置新 Path SetRequestHeader 设置请求头 SetResponseHeader 设置响应头 SetStatus 设置状态码 StripPrefix 路由摘除以&#x2F;分割 parts Retry 路由重试 retries：重试次数,statuses: 判断可重试的状态吗默认 5xx,methods: 支持重试的请求方法,exceptions: 重试的抛出异常列表 RequestSize 通过 Content-length 判断是否超过最大请求大小 SetRequestHostHeader 清除原请求头 Host，并添加新值 ModifyRequestBody 修改请求体 ModifyResponseBody 修改响应体 TokenRelay OAuth2 标准认证服务，执行授权认证后并将信息传递下游服务 CacheRequestBody 缓存请求体 RouteLocator路由配置加载 ConfigurationService路由&amp;谓词&amp;过滤器工程等动态配置等实现","categories":[{"name":"技术分享","slug":"技术分享","permalink":"https://docs.hehouhui.cn/categories/%E6%8A%80%E6%9C%AF%E5%88%86%E4%BA%AB/"}],"tags":[{"name":"Spring","slug":"Spring","permalink":"https://docs.hehouhui.cn/tags/Spring/"},{"name":"微服务","slug":"微服务","permalink":"https://docs.hehouhui.cn/tags/%E5%BE%AE%E6%9C%8D%E5%8A%A1/"},{"name":"分布式","slug":"分布式","permalink":"https://docs.hehouhui.cn/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"}]},{"title":"Mysql数据结构&锁","slug":"archives/Mysql数据结构&锁","date":"2022-02-21T00:00:00.000Z","updated":"2023-10-08T06:42:00.000Z","comments":true,"path":"archives/38.html","link":"","permalink":"https://docs.hehouhui.cn/archives/38.html","excerpt":"","text":"Mysql 数据库存储引擎MyISAM 引擎不支持事务 支持表级锁（MySql 支持两种表级锁，表共享读锁和表独占写锁），但不支持行级锁 存储表的总行数 一个 MyISAM 表有三个文件：索引文件（.MYI），表结构文件(.frm)，数据文件(.MYD) 采用非聚集索引：即索引文件和数据文件是分开的，索引文件的数据域存储指向数据文件的指针 跨平台应用更方便（表保存为文件形式） 支持三种不同的存储格式： （1）静态表：存储迅速，容易缓存，出现故障容易恢复；但是占用内存多，因为会按列宽度补足空格 （2）动态表：占用空间少，但是频繁更新和删除容易产生碎片，需要定期整理（OPTIMIZE TABLE），并且出现故障难以恢复。 （3）压缩表：占据的磁盘空间非常小（每个记录被单独压缩，所以访问开支很小）。 InnoDb 引擎支持事务 支持行级锁（仅在条件语句中包括主键索引时） 内存使用率低 查询效率和写的效率更低 采用聚集索引，索引和数据存在一起，叶子结点直接存的是数据。 支持外键 注：MyISAM 在查询时的性能比 InnoDB 高，因为它采用的辅索引和主键索引类似，所以通过辅索引查找数据时只需要通过辅索引树就可以查找到，而 InnoDB 需要先通过辅索引查找到主索引，再通过主索引树查找到数据。 MyISAM 和 InnoDB 区别MyISAM 是 MySQL 的默认数据库引擎（5.5 版之前）。虽然性能极佳，而且提供了大量的特性，包括全文索引、压缩、空间函数等，但 MyISAM 不支持事务和行级锁，而且最大的缺陷就是崩溃后无法安全恢复。不过，5.5 版本之后，MySQL 引入了 InnoDB（事务性数据库引擎），MySQL 5.5 版本后默认的存储引擎为 InnoDB。 大多数时候我们使用的都是 InnoDB 存储引擎，但是在某些情况下使用 MyISAM 也是合适的比如读密集的情况下。（如果你不介意 MyISAM 崩溃恢复问题的话）。 两者的对比： 是否支持行级锁 : MyISAM 只有表级锁(table-level locking)，而 InnoDB 支持行级锁(row-level locking)和表级锁,默认为行级锁。 是否支持事务和崩溃后的安全恢复： MyISAM 强调的是性能，每次查询具有原子性,其执行速度比 InnoDB 类型更快，但是不提供事务支持。但是InnoDB 提供事务支持，外部键等高级数据库功能。 具有事务(commit)、回滚(rollback)和崩溃修复能力(crash recovery capabilities)的事务安全(transaction-safe (ACID compliant))型表。 是否支持外键： MyISAM 不支持，而 InnoDB 支持。 是否支持 MVCC ：仅 InnoDB 支持。应对高并发事务, MVCC 比单纯的加锁更高效;MVCC 只在 READ COMMITTED 和 REPEATABLE READ 两个隔离级别下工作;MVCC 可以使用 乐观(optimistic)锁 和 悲观(pessimistic)锁来实现;各数据库中 MVCC 实现并不统一。推荐阅读：MySQL-InnoDB-MVCC 多版本并发控制 …… 《MySQL 高性能》上面有一句话这样写到: 不要轻易相信“MyISAM 比 InnoDB 快”之类的经验之谈，这个结论往往不是绝对的。在很多我们已知场景中，InnoDB 的速度都可以让 MyISAM 望尘莫及，尤其是用到了聚簇索引，或者需要访问的数据都可以放入内存的应用。 一般情况下我们选择 InnoDB 都是没有问题的，但是某些情况下你并不在乎可扩展能力和并发能力，也不需要事务支持，也不在乎崩溃后的安全恢复问题的话，选择 MyISAM 也是一个不错的选择。但是一般情况下，我们都是需要考虑到这些问题的。 MEMORY只对应一个磁盘文件.frm，用来存储表结构 访问速度非常快，因为他的数据存在内存中，但是一旦服务关闭，数据就会丢失 可以指定 Hash 索引或 BTREE 索引 默认存储数据大小不超过 16MB，但可以调整 应用场景：比如作为统计操作的的中间结果表，便于高效地对中间结果分析并得到最终结果。 MERGE一组 MyISAM 表的组合，这些 MyISAM 表结构必须完全相同 对 MERGE 表的操作实际上是对其子表进行的 可以通过指定 INSERT_METHOD&#x3D;LAST 来制定插入数据的表（这里指定为最后一个表） 参考[1] 《深入浅出 MySQL》 [2] 平衡查找树之 B 树：http://www.cnblogs.com/yangecnu/p/Introduce-B-Tree-and-B-Plus-Tree.html [3] B 树，B+树，B*树：https://www.jianshu.com/p/db226e0196b4 数据结构B 树（B-Tree）B 树是 2-3 树的一种扩展，对于 M 阶(M 就是树的高度，比如下图为一个四阶的树)的 B 树来说： （1）根节点至少有两个子节点 （2）每个节点至多有 M-1 个 key，以升序排列，以及 Nk+1 个指针，其中 Nk 代表 key 的数量。 （3）对于一个 key1 来说，它左侧的指针指向的子节点的 key 值&lt;&#x3D;key1,右侧子指针指向的子节点的 key 值&gt;key1（详见下图） （4）其他节点至少有 M&#x2F;2 个子节点 他的插入过程和红黑树很相似，总结一下就是： （1）当要插入一个新值时，首先根据第三条原则找到他在叶子节点的位置并插入 （2）如果当前叶子节点的 key 数目等于 M，那么就要拆分，拆分的过程就是把所有 key 通过一个中间值（如 M&#x3D;4 取第二个数，M&#x3D;5 取第三个数），分成相同的两份（如果 M 是偶数会相差一），然后中间数为父节点，两边的树作为左右子节点，但要注意中间数是插入到他们的父节点中的，而不是新生成一棵树，这也就意味着如果这时候父节点的 key 树等于 M，那么就要通过相同的变换把 key 值接着向上传递，直到 key 数&lt;M。 B 树由来 定义：B-树是一类树，包括 B-树、B+树、B*树等，是一棵自平衡的搜索树，它类似普通的平衡二叉树，不同的一点是 B-树允许每个节点有更多的子节点。B-树是专门为外部存储器设计的，如磁盘，它对于读取和写入大块数据有良好的性能，所以一般被用在文件系统及数据库中。 定义只需要知道 B-树允许每个节点有更多的子节点即可（多叉树）。子节点数量一般在上千，具体数量依赖外部存储器的特性。 先来看看为什么会出现 B-树这类数据结构。 传统用来搜索的平衡二叉树有很多，如 AVL 树，红黑树等。这些树在一般情况下查询性能非常好，但当数据非常大的时候它们就无能为力了。 原因当数据量非常大时，内存不够用，大部分数据只能存放在磁盘上，只有需要的数据才加载到内存中。一般而言内存访问的时间约为 50 ns，而磁盘在 10 ms 左右。速度相差了近 5 个数量级，磁盘读取时间远远超过了数据在内存中比较的时间。 这说明程序大部分时间会阻塞在磁盘 IO 上。那么我们如何提高程序性能？减少磁盘 IO 次数，像 AVL 树，红黑树这类平衡二叉树从设计上无法“迎合”磁盘。 上图是一颗简单的平衡二叉树，平衡二叉树是通过旋转来保持平衡的，而旋转是对整棵树的操作，若部分加载到内存中则无法完成旋转操作。其次平衡二叉树的高度相对较大为 log n（底数为 2），这样逻辑上很近的节点实际可能非常远，无法很好的利用磁盘预读（局部性原理），所以这类平衡二叉树在数据库和文件系统上的选择就被 pass 了。 空间局部性原理：如果一个存储器的某个位置被访问，那么将它附近的位置也会被访问。 我们从“迎合”磁盘的角度来看看 B-树的设计。 索引的效率依赖与磁盘 IO 的次数，快速索引需要有效的减少磁盘 IO 次数，如何快速索引呢？ 索引的原理其实是不断的缩小查找范围，就如我们平时用字典查单词一样，先找首字母缩小范围，再第二个字母等等。 平衡二叉树是每次将范围分割为两个区间。为了更快，B-树每次将范围分割为多个区间，区间越多，定位数据越快越精确。那么如果节点为区间范围，每个节点就较大了。所以新建节点时，直接申请页大小的空间（磁盘存储单位是按 block 分的，一般为 512 Byte。 磁盘 IO 一次读取若干个 block，我们称为一页，具体大小和操作系统有关，一般为 4 k，8 k或 16 k），计算机内存分配是按页对齐的，这样就实现了一个节点只需要一次 IO。 上图是一棵简化的 B-树，多叉的好处非常明显，有效的降低了 B-树的高度，为底数很大的 log n，底数大小与节点的子节点数目有关，一般一棵 B-树的高度在 3 层左右。 层数低，每个节点区确定的范围更精确，范围缩小的速度越快（比二叉树深层次的搜索肯定快很多）。上面说了一个节点需要进行一次 IO，那么总 IO 的次数就缩减为了 log n 次。B-树的每个节点是 n 个有序的序列(a1,a2,a3…an)，并将该节点的子节点分割成 n+1 个区间来进行索引(X1&lt; a1, a2 &lt; X2 &lt; a3, … , an+1 &lt; Xn &lt; anXn+1 &gt; an)。 点评：B 树的每个节点，都是存多个值的，不像二叉树那样，一个节点就一个值，B 树把每个节点都给了一点的范围区间，区间更多的情况下，搜索也就更快了，比如：有 1-100 个数，二叉树一次只能分两个范围，0-50 和 51-100，而 B 树，分成 4 个范围 1-25， 25-50，51-75，76-100 一次就能筛选走四分之三的数据。所以作为多叉树的 B 树是更快的 B+Tree（树）存储引擎常用的一种数据结构，一种多叉平衡查找树，特点（对于 M 阶的 B+树）： （1）除叶子结点外所有节点都有 M 个键以及 M 个指向子节点的指针。 （2）所有叶子节点都在同一层 （3）非叶子结点的子树指针与关键字（Key）个数相同； （4）非叶子结点的子树指针 P[i]，指向关键字值属于[K[i], K[i+1])的子树（B-树是开区间）； （5）为所有叶子结点增加一个链指针； （6）所有关键字（key）都在叶子结点出现；，因此所有查找只会在叶子结点命中 结构如下图所示： 因为内节点并不存储 data，所以一般 B+树的叶节点和内节点大小不同，而 B-树的每个节点大小一般是相同的，为一页。 为了增加 区间访问性，一般会对 B+树做一些优化。如下图带顺序访问的 B+树。 相比 B 树的优点： （1）支持范围查找 （2）遍历更方便 （3）节省空间：因为 B+树只有叶子节点才存数据，因此内部节点不需要只想关键字具体信息的指针。 （4）所有查询操作都需要命中子节点，所以是相同的。 PS: B*树就是在 B+树基础上，为非叶子结点也增加链表指针 B 树与 B+Tree 的区别1 . B+树内节点不存储数据，所有 data 存储在叶节点导致查询时间复杂度固定为 log n。而 B-树查询时间复杂度不固定，与 key 在树中的位置有关，最好为 O(1)。 如下所示 B-树&#x2F;B+树查询节点 key 为 50 的 data。 B-树： 从上图可以看出，key 为 50 的节点就在第一层，B-树只需要一次磁盘 IO 即可完成查找。所以说 B-树的查询最好时间复杂度是 O(1)。 B+树： 由于 B+树所有的 data 域都在根节点，所以查询 key 为 50 的节点必须从根节点索引到叶节点，时间复杂度固定为 O(log n)。 点评：B 树的由于每个节点都有 key 和 data，所以查询的时候可能不需要 O(logn)的复杂度，甚至最好的情况是 O(1)就可以找到数据，而 B+树由于只有叶子节点保存了 data，所以必须经历 O(logn)复杂度才能找到数据 B+树叶节点两两相连可大大增加区间访问性，可使用在范围查询等，而 B-树每个节点 key 和 data 在一起，则无法区间查找。 根据空间局部性原理：如果一个存储器的某个位置被访问，那么将它附近的位置也会被访问。 B+树可以很好的利用局部性原理，若我们访问节点 key 为 50，则 key 为 55、60、62 的节点将来也可能被访问，我们可以利用磁盘预读原理提前将这些数据读入内存，减少了磁盘 IO 的次数。当然 B+树也能够很好的完成范围查询。比如查询 key 值在 50-70 之间的节点。 点评：由于 B+树的叶子节点的数据都是使用链表连接起来的，而且他们在磁盘里是顺序存储的，所以当读到某个值的时候，磁盘预读原理就会提前把这些数据都读进内存，使得范围查询和排序都很快 B+树更适合外部存储。由于内节点无 data 域，每个节点能索引的范围更大更精确 这个很好理解，由于 B-树节点内部每个 key 都带着 data 域，而 B+树节点只存储 key 的副本，真实的 key 和 data 域都在叶子节点存储。 前面说过磁盘是分 block 的，一次磁盘 IO 会读取若干个 block，具体和操作系统有关，那么由于磁盘 IO 数据大小是固定的，在一次 IO 中，单个元素越小，量就越大。 这就意味着 B+树单次磁盘 IO 的信息量大于 B-树，从这点来看 B+树相对 B-树磁盘 IO 次数少。 点评：由于 B 树的节点都存了 key 和 data，而 B+树只有叶子节点存 data，非叶子节点都只是索引值，没有实际的数据，这就时 B+树在一次 IO 里面，能读出的索引值更多。从而减少查询时候需要的 IO 次数！ 从上图可以看出相同大小的区域，B-树仅有 2 个 key，而 B+树有 3 个 key。 为什么使用 B-Tree（B+Tree） `上文说过，红黑树等数据结构也可以用来实现索引，但是文件系统及数据库系统普遍采用B-/+Tree作为索引结构，这一节将结合计算机组成原理相关知识讨论B-/+Tree作为索引的理论基础。` 一般来说，索引本身也很大，不可能全部存储在内存中，因此索引往往以索引文件的形式存储的磁盘上。这样的话，索引查找过程中就要产生磁盘 I&#x2F;O 消耗，相对于内存存取，I&#x2F;O 存取的消耗要高几个数量级，所以评价一个数据结构作为索引的优劣最重要的指标就是在查找过程中磁盘 I&#x2F;O 操作次数的渐进复杂度。 换句话说，索引的结构组织要尽量减少查找过程中磁盘I/O的存取次数。下面先介绍内存和磁盘存取原理，然后再结合这些原理分析B-/+Tree作为索引的效率。 存储数据最小单元我们都知道计算机在存储数据的时候，有最小存储单元，这就好比我们今天进行现金的流通最小单位是一毛。 在计算机中磁盘存储数据最小单元是扇区，一个扇区的大小是 512 字节，而文件系统（例如 XFS&#x2F;EXT4）他的最小单元是块，一个块的大小是 4k 而对于我们的 InnoDB 存储引擎也有自己的最小储存单元——页（Page），一个页的大小是 16K。 下面几张图可以帮你理解最小存储单元： 文件系统中一个文件大小只有 1 个字节，但不得不占磁盘上 4KB 的空间。 磁盘扇区、文件系统、InnoDB 存储引擎都有各自的最小存储单元。 在 MySQL 中我们的 InnoDB 页的大小默认是 16k，当然也可以通过参数设置： 数据表中的数据都是存储在页中的，所以一个页中能存储多少行数据呢？假设一行数据的大小是 1k，那么一个页可以存放 16 行这样的数据。 主存存取原理目前计算机使用的主存基本都是随机读写存储器（RAM），现代 RAM 的结构和存取原理比较复杂，这里本文抛却具体差别，抽象出一个十分简单的存取模型来说明 RAM 的工作原理。 从抽象角度看，主存是一系列的存储单元组成的矩阵，每个存储单元存储固定大小的数据。每个存储单元有唯一的地址，现代主存的编址规则比较复杂 这里将其简化成一个`二维地址`：通过一个行地址和一个列地址可以唯一定位到一个存储单元。图5展示了一个4 x 4的主存模型。 主存的存取过程如下： 当系统需要读取主存时，则将地址信号放到地址总线上传给主存，主存读到地址信号后，解析信号并定位到指定存储单元，然后将此存储单元数据放到数据总线上，供其它部件读取。 写主存的过程类似，系统将要写入单元地址和数据分别放在地址总线和数据总线上，主存读取两个总线的内容，做相应的写操作。 这里可以看出，主存存取的时间仅与存取次数呈线性关系，因为不存在机械操作，两次存取的数据的“距离”不会对时间有任何影响，例如，先取 A0 再取 A1 和先取 A0 再取 D3 的时间消耗是一样的。 磁盘存取原理上文说过，索引一般以文件形式存储在磁盘上，索引检索需要磁盘 I&#x2F;O 操作。与主存不同，磁盘 I&#x2F;O 存在机械运动耗费，因此磁盘 I&#x2F;O 的时间消耗是巨大的。 图 6 是磁盘的整体结构示意图。 一个磁盘由大小相同且同轴的圆形盘片组成，磁盘可以转动（各个磁盘必须同步转动）。在磁盘的一侧有磁头支架，磁头支架固定了一组磁头，每个磁头负责存取一个磁盘的内容。磁头不能转动，但是可以沿磁盘半径方向运动（实际是斜切向运动），每个磁头同一时刻也必须是同轴的，即从正上方向下看，所有磁头任何时候都是重叠的（不过目前已经有多磁头独立技术，可不受此限制）。 图 7 是磁盘结构的示意图。 盘片被划分成一系列同心环，圆心是盘片中心，每个同心环叫做一个磁道，所有半径相同的磁道组成一个柱面。磁道被沿半径线划分成一个个小的段，每个段叫做一个扇区，每个扇区是磁盘的最小存储单元。为了简单起见，我们下面假设磁盘只有一个盘片和一个磁头。 当需要从磁盘读取数据时，系统会将数据逻辑地址传给磁盘，磁盘的控制电路按照寻址逻辑将逻辑地址翻译成物理地址，即确定要读的数据在哪个磁道，哪个扇区。 为了读取这个扇区的数据，需要将磁头放到这个扇区上方，为了实现这一点，磁头需要移动对准相应磁道，这个过程叫做寻道，所耗费时间叫做寻道时间，然后磁盘旋转将目标扇区旋转到磁头下，这个过程耗费的时间叫做旋转时间。 局部性原理与磁盘预读 由于存储介质的特性，磁盘本身存取就比主存慢很多，再加上机械运动耗费，磁盘的存取速度往往是主存的几百分分之一，因此为了提高效率，要尽量减少磁盘 I&#x2F;O。为了达到这个目的，磁盘往往不是严格按需读取，而是每次都会预读，即使只需要一个字节，磁盘也会从这个位置开始，顺序向后读取一定长度的数据放入内存。这样做的理论依据是计算机科学中著名的局部性原理： 当一个数据被用到时，其附近的数据也通常会马上被使用。 程序运行期间所需要的数据通常比较集中。 由于磁盘顺序读取的效率很高（不需要寻道时间，只需很少的旋转时间），因此对于具有局部性的程序来说，预读可以提高 I&#x2F;O 效率。 预读的长度一般为页（page）的整倍数。页是计算机管理存储器的逻辑块，硬件及操作系统往往将主存和磁盘存储区分割为连续的大小相等的块，每个存储块称为一页（在许多操作系统中，页得大小通常为 4k），主存和磁盘以页为单位交换数据。当程序要读取的数据不在主存中时，会触发一个缺页异常，此时系统会向磁盘发出读盘信号，磁盘会找到数据的起始位置并向后连续读取一页或几页载入内存中，然后异常返回，程序继续运行。 所以 IO 一次就是读一页的大小 索引MySQL 索引使用的数据结构主要有BTree 索引 和 哈希索引 。对于哈希索引来说，底层的数据结构就是哈希表，因此在绝大多数需求为单条记录查询的时候，可以选择哈希索引，查询性能最快；其余大部分场景，建议选择 BTree 索引。 MySQL 的 BTree 索引使用的是 B 树中的 B+Tree，但对于主要的两种存储引擎的实现方式是不同的。 MyISAM: B+Tree 叶节点的 data 域存放的是数据记录的地址。在索引检索的时候，首先按照 B+Tree 搜索算法搜索索引，如果指定的 Key 存在，则取出其 data 域的值，然后以 data 域的值为地址读取相应的数据记录。这被称为“非聚簇索引”。 InnoDB: 其数据文件本身就是索引文件。相比 MyISAM，索引文件和数据文件是分离的，其表数据文件本身就是按 B+Tree 组织的一个索引结构，树的叶节点 data 域保存了完整的数据记录。这个索引的 key 是数据表的主键，因此 InnoDB 表数据文件本身就是主索引。这被称为“聚簇索引（或聚集索引）”。而其余的索引都作为辅助索引，辅助索引的 data 域存储相应记录主键的值而不是地址，这也是和 MyISAM 不同的地方。在根据主索引搜索时，直接找到 key 所在的节点即可取出数据；在根据辅助索引查找时，则需要先取出主键的值，再走一遍主索引。 因此，在设计表的时候，不建议使用过长的字段作为主键，也不建议使用非单调的字段作为主键，这样会造成主索引频繁分裂。 PS：整理自《Java 工程师修炼之道》 Hash 索引 Hash 索引结构的特殊性，其检索效率非常高，索引的检索可以一次定位，不像 B-Tree 索引需要从根节点到枝节点，最后才能访问到页节点这样多次的 IO 访问，所以 Hash 索引的查询效率要远高于 B-Tree 索引。 可 能很多人又有疑问了，既然 Hash 索引的效率要比 B-Tree 高很多，为什么大家不都用 Hash 索引而还要使用 B-Tree 索引呢？任何事物都是有两面性的，Hash 索引也一样，虽然 Hash 索引效率高，但是 Hash 索引本身由于其特殊性也带来了很多限制和弊端，主要有以下这些。 （1）Hash 索引仅仅能满足”&#x3D;”,”IN”和”&lt;&#x3D;&gt;”查询，不能使用范围查询。 由于 Hash 索引比较的是进行 Hash 运算之后的 Hash 值，所以它只能用于等值的过滤，不能用于基于范围的过滤，因为经过相应的 Hash 算法处理之后的 Hash 值的大小关系，并不能保证和 Hash 运算前完全一样。 （2）Hash 索引无法被用来避免数据的排序操作。 由于 Hash 索引中存放的是经过 Hash 计算之后的 Hash 值，而且 Hash 值的大小关系并不一定和 Hash 运算前的键值完全一样，所以数据库无法利用索引的数据来避免任何排序运算； （3）Hash 索引不能利用部分索引键查询。 对于组合索引，Hash 索引在计算 Hash 值的时候是组合索引键合并后再一起计算 Hash 值，而不是单独计算 Hash 值，所以通过组合索引的前面一个或几个索引键进行查询的时候，Hash 索引也无法被利用。 （4）Hash 索引在任何时候都不能避免表扫描。 前面已经知道，Hash 索引是将索引键通过 Hash 运算之后，将 Hash 运算结果的 Hash 值和所对应的行指针信息存放于一个 Hash 表中，由于不同索引键存在相同 Hash 值，所以即使取满足某个 Hash 键值的数据的记录条数，也无法从 Hash 索引中直接完成查询，还是要通过访问表中的实际数据进行相应的比较，并得到相应的结果。 （5）Hash 索引遇到大量 Hash 值相等的情况后性能并不一定就会比 B-Tree 索引高。 对于选择性比较低的索引键，如果创建 Hash 索引，那么将会存在大量记录指针信息存于同一个 Hash 值相关联。这样要定位某一条记录时就会非常麻烦，会浪费多次表数据的访问，而造成整体性能低下。 (6) hash 索引查找数据基本上能一次定位数据，当然有大量碰撞的话性能也会下降。而 btree 索引就得在节点上挨着查找了，很明显在数据精确查找方面 hash 索引的效率是要高于 btree 的； (7) 那么不精确查找呢，也很明显，因为 hash 算法是基于等值计算的，所以对于“like”等范围查找 hash 索引无效，不支持； (8) 对于 btree 支持的联合索引的最优前缀，hash 也是无法支持的，联合索引中的字段要么全用要么全不用。提起最优前缀居然都泛起迷糊了，看来有时候放空得太厉害； (9) hash 不支持索引排序，索引值和计算出来的 hash 值大小并不一定一致。 B-Tree 索引B-tree 索引可以用于使用 &#x3D;, &gt;, &gt;&#x3D;, &lt;, &lt;&#x3D; 或者 BETWEEN 运算符的列比较。如果 LIKE 的参数是一个没有以通配符起始的常量字符串的话也可以使用这种索引。比如，以下 SELECT 语句就使用索引： 1. SELECT * FROM tbl_name WHERE key_col LIKE 'Patrick%'; 2. SELECT * FROM tbl_name WHERE key_col LIKE 'Pat%_ck%'; 在第一个句子中，只会考虑 ‘Patrick’ &lt;&#x3D; key_col &lt; ‘Patricl’ 的记录。第二句中，则只会考虑 ‘Pat’ &lt;&#x3D; key_col &lt; ‘Pau’ 的记录。以下 SELECT 语句不使用索引： 1. SELECT * FROM tbl_name WHERE key_col LIKE '%Patrick%'; 2. SELECT * FROM tbl_name WHERE key_col LIKE other_col; 第一句里面，LIKE 的值起始于一个通配符。在第二句里，LIKE 的值不是一个常量。如果你这样使用： … LIKE ‘%string%’，其中的 string 不大于三个字符，MySql 将使用 Turbo Boyer-Moore 算法来对该字符串表达式进行初始化，并使用这种表达式来让查询更加迅速。如果 col_name 列创建了索引，那么一个使用了 col_name IS NULL 的查询是可以使用该索引的。任何没有涵盖 WHERE 从句中所有 AND 级别的条件的索引将不会被使用。换句话讲，要想使用索引，该索引的前导列必须在每一个 AND 组合中使用到。以下 WHERE 从句使用索引： 1. ... WHERE index_part1=1 AND index_part2=2 AND other_column=3 4. /* index = 1 OR index = 2 */ 5. ... WHERE index=1 OR A=10 AND index=2 8. /* optimized like \"index_part1='hello'\" */ 9. ... WHERE index_part1='hello' AND index_part3=5 12. /* Can use index on index1 but not on index2 or index3 */ 13. ... WHERE index1=1 AND index2=2 OR index1=3 AND index3=3; 这些 WHERE 从句不使用索引： 1. /* index_part1 is not used */ 2. WHERE index_part2=1 AND index_part3=2 5. /* Index is not used in both parts of the WHERE clause */ 6. WHERE index=1 OR A=10 9. /* No index spans all rows */ 10. WHERE index_part1=1 OR index_part2=10 有时，即使有索引可以使用，MySQL 也不使用任何索引。发生这种情况的场景之一就是优化器估算出使用该索引将要求 MySql 去访问这张表的绝大部分记录。这种情况下，一个表扫描可能更快，因为它要求更少量的查询。但是，如果这样的一个查询使用了 LIMIT 来检索只是少量的记录时，MySql 还是会使用索引，因为它能够更快地找到这点记录并将其返回。 事务事务是逻辑上的一组操作，要么都执行，要么都不执行。 事务最经典也经常被拿出来说例子就是转账了。假如小明要给小红转账 1000 元，这个转账会涉及到两个关键操作就是：将小明的余额减少 1000 元，将小红的余额增加 1000 元。万一在这两个操作之间突然出现错误比如银行系统崩溃，导致小明余额减少而小红的余额没有增加，这样就不对了。事务就是保证这两个关键操作要么都成功，要么都要失败。 事务的四大特性(ACID) 事务处理可以确保除非事务性单元内的所有操作都成功完成，否则不会永久更新面向数据的资源。通过将一组相关操作组合为一个要么全部成功要么全部失败的单元，可以简化错误恢复并使应用程序更加可靠。一个逻辑工作单元要成为事务，必须满足所谓的 ACID(原子性、一致性、隔离性和持久性)属性： 原子性(Atomicity) 事务是最小的执行单位，不允许分割。事务的原子性确保动作要么全部完成，要么完全不起作用； 事务必须是原子工作单元；对于其数据修改，要么全都执行，要么全都不执行。通常，与某个事务关联的操作具有共同的目标，并且是相互依赖的。如果系统只执行这些操作的一个子集，则可能会破坏事务的总体目标。原子性消除了系统处理操作子集的可能性。 一致性(Consistency) 执行事务后，数据库从一个正确的状态变化到另一个正确的状态； 事务在完成时，必须使所有的数据都保持一致状态。在相关数据库中，所有规则都必须应用于事务的修改，以保持所有数据的完整性。事务结束时，所有的内部数据结构（如 B 树索引或双向链表）都必须是正确的。某些维护一致性的责任由应用程序开发人员承担，他们必须确保应用程序已强制所有已知的完整性约束。例如，当开发用于转帐的应用程序时，应避免在转帐过程中任意移动小数点。 隔离性(Isolation) 并发访问数据库时，一个用户的事务不被其他事务所干扰，各并发事务之间数据库是独立的； 由并发事务所作的修改必须与任何其它并发事务所作的修改隔离。事务查看数据时数据所处&gt;的状态，要么是另一并发事务修改它之前的状态，要么是另一事务修改它之后的状态，事务不会查看中间状态的数据。这称为可串行性，因为它能够重新装载起始数据，并且重播一系列事务，以使数据结束时的状态与原始事务执行的状态相同。当事务可序列化时将获得最高的隔离级别。在此级别上，从一组可并行执行的事务获得的结果与通过连续运行每个事务所获得的结果相同。由于高度隔离会限制可并行执行的事务数，所以一些应用程序降低隔离级别以换取更大的吞吐量。 · 持久性(Durability） 一个事务被提交之后。它对数据库中数据的改变是持久的，即使数据库发生故障也不应该对其有任何影响。 事务完成之后，它对于系统的影响是永久性的。该修改即使出现致命的系统故障也将一直保持。 并发事务带来哪些问题?在典型的应用程序中，多个事务并发运行，经常会操作相同的数据来完成各自的任务（多个用户对同一数据进行操作）。并发虽然是必须的，但可能会导致以下的问题。 脏读（Dirty read）: 当一个事务正在访问数据并且对数据进行了修改，而这种修改还没有提交到数据库中，这时另外一个事务也访问了这个数据，然后使用了这个数据。因为这个数据是还没有提交的数据，那么另外一个事务读到的这个数据是“脏数据”，依据“脏数据”所做的操作可能是不正确的。 丢失修改（Lost to modify）: 指在一个事务读取一个数据时，另外一个事务也访问了该数据，那么在第一个事务中修改了这个数据后，第二个事务也修改了这个数据。这样第一个事务内的修改结果就被丢失，因此称为丢失修改。 例如：事务 1 读取某表中的数据 A&#x3D;20，事务 2 也读取 A&#x3D;20，事务 1 修改 A&#x3D;A-1，事务 2 也修改 A&#x3D;A-1，最终结果 A&#x3D;19，事务 1 的修改被丢失。 不可重复读（Unrepeatableread）: 指在一个事务内多次读同一数据。在这个事务还没有结束时，另一个事务也访问该数据。那么，在第一个事务中的两次读数据之间，由于第二个事务的修改导致第一个事务两次读取的数据可能不太一样。这就发生了在一个事务内两次读到的数据是不一样的情况，因此称为不可重复读。 幻读（Phantom read）: 幻读与不可重复读类似。它发生在一个事务（T1）读取了几行数据，接着另一个并发事务（T2）插入了一些数据时。在随后的查询中，第一个事务（T1）就会发现多了一些原本不存在的记录，就好像发生了幻觉一样，所以称为幻读。 不可重复读和幻读区别： 不可重复读的重点是修改比如多次读取一条记录发现其中某些列的值被修改，幻读的重点在于新增或者删除比如多次读取一条记录发现记录增多或减少了。 事务隔离级别有哪些?MySQL 的默认隔离级别是?SQL 标准定义了四个隔离级别： READ-UNCOMMITTED(读取未提交)： 最低的隔离级别，允许读取尚未提交的数据变更，可能会导致脏读、幻读或不可重复读。 READ-COMMITTED(读取已提交)： 允许读取并发事务已经提交的数据，可以阻止脏读，但是幻读或不可重复读仍有可能发生。 REPEATABLE-READ(可重复读)： 对同一字段的多次读取结果都是一致的，除非数据是被本身事务自己所修改，可以阻止脏读和不可重复读，但幻读仍有可能发生。 SERIALIZABLE(可串行化)： 最高的隔离级别，完全服从 ACID 的隔离级别。所有的事务依次逐个执行，这样事务之间就完全不可能产生干扰，也就是说，该级别可以防止脏读、不可重复读以及幻读。 隔离级别 脏读 不可重复读 幻影读 READ-UNCOMMITTED √ √ √ READ-COMMITTED × √ √ REPEATABLE-READ × × √ SERIALIZABLE × × × MySQL InnoDB 存储引擎的默认支持的隔离级别是 REPEATABLE-READ（可重读）。我们可以通过SELECT @@tx_isolation;命令来查看，MySQL 8.0 该命令改为SELECT @@transaction_isolation; 事务保存点(SavePoint) 有一个数据库管理员，在早上的时候，设置了一个保存点 a1，然后工作（对数据库进行增删改查），到了中午他又设置了一个保存点 a2，到了下午他又继续工作（还是对数据库进行增删改查），然后到了晚上。突然之间，他执行了一个很愚蠢的操作，把数据库里的数据全部给删除了！！！！ 那这个时候怎么办呢？好在他在早上和中午的时候设置了保存点，然后只要回到保存点就可以了。 rollback to xxxSavePoint 当事务 commit 之后，事务中设置的 SavePoint 就取消了 事务并发两个并发事务同时访问数据库表相同的行时，可能存在以下三个问题： 1、幻想读：事务 T1 读取一条指定 where 条件的语句，返回结果集。此时事务 T2 插入一行新记录，恰好满足 T1 的 where 条件。然后 T1 使用相同的条件再次查询，结果集中可以看到 T2 插入的记录，这条新纪录就是幻想。 2、不可重复读取：事务 T1 读取一行记录，紧接着事务 T2 修改了 T1 刚刚读取的记录，然后 T1 再次查询，发现与第一次读取的记录不同，这称为不可重复读。 3、脏读：事务 T1 更新了一行记录，还未提交所做的修改，这个 T2 读取了更新后的数据，然后 T1 执行回滚操作，取消刚才的修改，所以 T2 所读取的行就无效，也就是脏数据。 事务实现原理数据库系统是通过并发控制技术和日志恢复技术来避免这种情况发生的。 并发控制技术保证了事务的隔离性,使数据库的一致性状态不会因为并发执行的操作被破坏。日志恢复技术保证了事务的原子性,使一致性状态不会因事务或系统故障被破坏。同时使已提交的对数据库的修改不会因系统崩溃而丢失,保证了事务的持久性。 并发异常 在讲解并发控制技术前,先简单介绍下数据库常见的并发异常。 脏写脏写是指事务回滚了其他事务对数据项的已提交修改,比如下面这种情况 丢失更新 丢失更新是指事务覆盖了其他事务对数据的已提交修改,导致这些修改好像丢失了一样。 事务 1 和事务 2 读取 A 的值都为 10,事务 2 先将 A 加上 10 并提交修改,之后事务 2 将 A 减少 10 并提交修改,A 的值最后为,导致事务 2 对 A 的修改好像丢失了一样 脏读 脏读是指一个事务读取了另一个事务未提交的数据 在事务 1 对 A 的处理过程中,事务 2 读取了 A 的值,但之后事务 1 回滚,导致事务 2 读取的 A 是未提交的脏数据。 不可重复读 不可重复读是指一个事务对同一数据的读取结果前后不一致。脏读和不可重复读的区别在于:前者读取的是事务未提交的脏数据,后者读取的是事务已经提交的数据,只不过因为数据被其他事务修改过导致前后两次读取的结果不一样,比如下面这种情况 由于事务 2 对 A 的已提交修改,事务 1 前后两次读取的结果不一致。 幻读 幻读是指事务读取某个范围的数据时，因为其他事务的操作导致前后两次读取的结果不一致。幻读和不可重复读的区别在于,不可重复读是针对确定的某一行数据而言,而幻读是针对不确定的多行数据。因而幻读通常出现在带有查询条件的范围查询中,比如下面这种情况: 事务 1 查询 A&lt;5 的数据,由于事务 2 插入了一条 A&#x3D;4 的数据,导致事务 1 两次查询得到的结果不一样 并发控制 并发控制技术是实现事务隔离性以及不同隔离级别的关键,实现方式有很多,按照其对可能冲突的操作采取的不同策略可以分为乐观并发控制和悲观并发控制两大类。 乐观并发控制:对于并发执行可能冲突的操作,假定其不会真的冲突,允许并发执行,直到真正发生冲突时才去解决冲突,比如让事务回滚。 悲观并发控制:对于并发执行可能冲突的操作,假定其必定发生冲突,通过让事务等待(锁)或者中止(时间戳排序)的方式使并行的操作串行执行。 基于封锁的并发控制核心思想:对于并发可能冲突的操作,比如读-写,写-读,写-写,通过锁使它们互斥执行。锁通常分为共享锁和排他锁两种类型 1.共享锁(S):事务 T 对数据 A 加共享锁,其他事务只能对 A 加共享锁但不能加排他锁。 2.排他锁(X):事务 T 对数据 A 加排他锁,其他事务对 A 既不能加共享锁也不能加排他锁 基于锁的并发控制流程: 事务根据自己对数据项进行的操作类型申请相应的锁(读申请共享锁,写申请排他锁) 申请锁的请求被发送给锁管理器。锁管理器根据当前数据项是否已经有锁以及申请的和持有的锁是否冲突决定是否为该请求授予锁。 若锁被授予,则申请锁的事务可以继续执行;若被拒绝,则申请锁的事务将进行等待,直到锁被其他事务释放。 可能出现的问题: 死锁:多个事务持有锁并互相循环等待其他事务的锁导致所有事务都无法继续执行。 饥饿:数据项 A 一直被加共享锁,导致事务一直无法获取 A 的排他锁。 对于可能发生冲突的并发操作,锁使它们由并行变为串行执行,是一种悲观的并发控制。 基于时间戳的并发控制核心思想:对于并发可能冲突的操作,基于时间戳排序规则选定某事务继续执行,其他事务回滚。 系统会在每个事务开始时赋予其一个时间戳,这个时间戳可以是系统时钟也可以是一个不断累加的计数器值,当事务回滚时会为其赋予一个新的时间戳,先开始的事务时间戳小于后开始事务的时间戳。 每一个数据项 Q 有两个时间戳相关的字段:W-timestamp(Q):成功执行 write(Q)的所有事务的最大时间戳R-timestamp(Q):成功执行 read(Q)的所有事务的最大时间戳 时间戳排序规则如下: 假设事务 T 发出 read(Q),T 的时间戳为 TSa.若 TS(T)&lt;W-timestamp(Q),则 T 需要读入的 Q 已被覆盖。此read 操作将被拒绝,T 回滚。b.若 TS(T)&gt;&#x3D;W-timestamp(Q),则执行 read 操作,同时把R-timestamp(Q)设置为 TS(T)与 R-timestamp(Q)中的最大值 假设事务 T 发出 write(Q)a.若 TS(T)&lt;R-timestamp(Q),write 操作被拒绝,T 回滚。b.若 TS(T)&lt;W-timestamp(Q),则 write 操作被拒绝,T 回滚。c.其他情况:系统执行 write 操作,将 W-timestamp(Q)设置为 TS(T)。 基于时间戳排序和基于锁实现的本质一样:对于可能冲突的并发操作,以串行的方式取代并发执行,因而它也是一种悲观并发控制。它们的区别主要有两点: 基于锁是让冲突的事务进行等待,而基于时间戳排序是让冲突的事务回滚。 基于锁冲突事务的执行次序是根据它们申请锁的顺序,先申请的先执行;而基于时间戳排序是根据特定的时间戳排序规则。 基于有效性检查的并发控制核心思想:事务对数据的更新首先在自己的工作空间进行,等到要写回数据库时才进行有效性检查,对不符合要求的事务进行回滚。 基于有效性检查的事务执行过程会被分为三个阶段: 读阶段:数据项被读入并保存在事务的局部变量中。所有 write 操作都是对局部变量进行,并不对数据库进行真正的更新。 有效性检查阶段:对事务进行有效性检查,判断是否可以执行 write 操作而不违反可串行性。如果失败,则回滚该事务。 写阶段:事务已通过有效性检查,则将临时变量中的结果更新到数据库中。 有效性检查通常也是通过对事务的时间戳进行比较完成的,不过和基于时间戳排序的规则不一样。 该方法允许可能冲突的操作并发执行,因为每个事务操作的都是自己工作空间的局部变量,直到有效性检查阶段发现了冲突才回滚。因而这是一种乐观的并发策略。 基于快照隔离的并发控制快照隔离是多版本并发控制(mvcc)的一种实现方式。 其核心思想是:数据库为每个数据项维护多个版本(快照),每个事务只对属于自己的私有快照进行更新,在事务真正提交前进行有效性检查,使得事务正常提交更新或者失败回滚。 由于快照隔离导致事务看不到其他事务对数据项的更新,为了避免出现丢失更新问题,可以采用以下两种方案避免： 先提交者获胜:对于执行该检查的事务 T,判断是否有其他事务已经将更新写入数据库,是则 T 回滚否则 T 正常提交。 先更新者获胜:通过锁机制保证第一个获得锁的事务提交其更新,之后试图更新的事务中止。 事务间可能冲突的操作通过数据项的不同版本的快照相互隔离,到真正要写入数据库时才进行冲突检测。因而这也是一种乐观并发控制。 事务的执行过程以及可能产生的问题 事务的执行过程可以简化如下: 系统会为每个事务开辟一个私有工作区 事务读操作将从磁盘中拷贝数据项到工作区中,在执行写操作前所有的更新都作用于工作区中的拷贝. 事务的写操作将把数据输出到内存的缓冲区中,等到合适的时间再由缓冲区管理器将数据写入到磁盘。 由于数据库存在立即修改和延迟修改,所以在事务执行过程中可能存在以下情况: 在事务提交前出现故障,但是事务对数据库的部分修改已经写入磁盘数据库中。这导致了事务的原子性被破坏。 在系统崩溃前事务已经提交,但数据还在内存缓冲区中,没有写入磁盘。系统恢复时将丢失此次已提交的修改。这是对事务持久性的破坏。 日志的种类和格式 &#96;&#96;:描述一次数据库写操作,T 是执行写操作的事务的唯一标识,X 是要写的数据项,V1 是数据项的旧值,V2 是数据项的新值。 &#96;&#96;:对数据库写操作的撤销操作,将事务 T 的 X 数据项恢复为旧值 V1。在事务恢复阶段插入。 &#96;&#96;: 事务 T 开始 &#96;&#96;: 事务 T 提交 &#96;&#96;: 事务 T 中止 关于日志,有以下两条规则 1.系统在对数据库进行修改前会在日志文件末尾追加相应的日志记录。 2.当一个事务的 commit 日志记录写入到磁盘成功后,称这个事务已提交,但事务所做的修改可能并未写入磁盘 日志恢复的核心思想 撤销事务 undo:将事务更新的所有数据项恢复为日志中的旧值,事务撤销完毕时将插入一条&#96;&#96;记录。 重做事务 redo:将事务更新的所有数据项恢复为日志中的新值。 事务正常回滚&#x2F;因事务故障中止将进行 redo系统从崩溃中恢复时将先进行 redo 再进行 undo。 以下事务将进行 undo:日志中只包括记录,但既不包括记录也不包括&#96;&#96;记录. 以下事务将进行 redo:日志中包括记录,也包括记录或&#96;&#96;记录。 假设系统从崩溃中恢复时日志记录如下 &lt;T0 start> &lt;T0,A,1000,950> &lt;T0,B,2000,2050> &lt;T0 commit> &lt;T1 start> &lt;T1,C,700,600> 由于 T0 既有 start 记录又有 commit 记录,将会对事务 T0 进行重做,执行相应的 redo 操作。由于 T1 只有 start 记录,将会对 T1 进行撤销,执行相应的 undo 操作,撤销完毕将写入一条 abort 记录。 事务故障中止&#x2F;正常回滚的恢复流程 从后往前扫描日志,对于事务 T 的每个形如&#96;&#96;的记录,将旧值 V1 写入数据项 X 中。 往日志中写一个特殊的只读记录&#96;&#96;,表示将数据项恢复成旧值 V1,这是一个只读的补偿记录,不需要根据它进行 undo。 一旦发现了日志记录,就停止继续扫描,并往日志中写一个 日志记录。 系统崩溃时的恢复过程(带检查点)检查点是形如&#96;&#96;的特殊的日志记录,L 是写入检查点记录时还未提交的事务的集合,系统保证在检查点之前已经提交的事务对数据库的修改已经写入磁盘,不需要进行 redo。检查点可以加快恢复的过程。 系统奔溃时的恢复过程分为两个阶段:重做阶段和撤销阶段。 重做阶段: 系统从最后一个检查点开始正向的扫描日志,将要重做的事务的列表 undo-list 设置为检查点日志记录中的 L 列表。 发现的更新记录或的补偿撤销记录,就重做该操作。 发现&#96;&#96;记录,就把 T 加入到 undo-list 中。 发现或记录,就把 T 从 undo-list 中去除。 撤销阶段: 系统从尾部开始反向扫描日志 发现属于 undo-list 中的事务的日志记录,就执行 undo 操作 发现 undo-list 中事务的 T 的记录,就写入一条记录,并把 T 从 undo-list 中去除。 4.undo-list 为空,则撤销阶段结束 总结:先将日志记录中所有事务的更新按顺序重做一遍,在针对需要撤销的事务按相反的顺序执行其更新操作的撤销操作。 总结事务是数据库系统进行并发控制的基本单位,是数据库系统进行故障恢复的基本单位,从而也是保持数据库状态一致性的基本单位。ACID 是事务的基本特性,数据库系统是通过并发控制技术和日志恢复技术来对事务的 ACID 进行保证的,从而可以得到如下的关于数据库事务的概念体系结构。 故障与故障恢复技术数据库运行过程中可能会出现故障,这些故障包括事务故障和系统故障两大类 事务故障:比如非法输入,系统出现死锁,导致事务无法继续执行。 系统故障:比如由于软件漏洞或硬件错误导致系统崩溃或中止。 这些故障可能会对事务和数据库状态造成破坏,因而必须提供一种技术来对各种故障进行恢复,保证数据库一致性,事务的原子性以及持久性。数据库通常以日志的方式记录数据库的操作从而在故障时进行恢复,因而可以称之为日志恢复技术。 锁机制与 InnoDB 锁算法MyISAM 和 InnoDB 存储引擎使用的锁： MyISAM 采用表级锁(table-level locking)。 InnoDB 支持行级锁(row-level locking)和表级锁,默认为行级锁 表级锁和行级锁对比： 表级锁： MySQL 中锁定 粒度最大 的一种锁，对当前操作的整张表加锁，实现简单，资源消耗也比较少，加锁快，不会出现死锁。其锁定粒度最大，触发锁冲突的概率最高，并发度最低，MyISAM 和 InnoDB 引擎都支持表级锁。 行级锁： MySQL 中锁定 粒度最小 的一种锁，只针对当前操作的行进行加锁。 行级锁能大大减少数据库操作的冲突。其加锁粒度最小，并发度高，但加锁的开销也最大，加锁慢，会出现死锁。 详细内容可以参考： MySQL 锁机制简单了解一下：https://blog.csdn.net/qq_34337272&#x2F;article&#x2F;details&#x2F;80611486 行锁A recordlockisalockonanindexrecord.Recordlocksalwayslockindexrecords, evenifatableisdefinedwithnoindexes.Forsuch cases,InnoDBcreates a hidden clusteredindexanduses thisindexforrecordlocking. 上文出自 MySQL 的官方文档，从这里我们可以看出行锁是作用在索引上的，哪怕你在建表的时候没有定义一个索引，InnoDB 也会创建一个聚簇索引并将其作为锁作用的索引。 这里还是讲一下 InnoDB 中的聚簇索引。每一个 InnoDB 表都需要一个聚簇索引，有且只有一个。如果你为该表表定义一个主键，那么 MySQL 将使用主键作为聚簇索引；如果你不为定义一个主键，那么 MySQL 将会把第一个唯一索引(而且要求 NOT NULL)作为聚簇索引；如果上诉两种情况都 GG，那么 MySQL 将自动创建一个名字为 的隐藏聚簇索引。 因为是聚簇索引，所以 B+树上的叶子节点都存储了数据行，那么如果现在是二级索引呢？InnoDB 中的二级索引的叶节点存储的是主键值(或者说聚簇索引的值)，所以通过二级索引查询数据时，还需要将对应的主键去聚簇索引中再次进行查询。 关于索引的问题就到这，我们用一张直观的图来表示行锁： 接下来以两条 SQL 的执行为例，讲解一下 InnoDB 对于单行数据的加锁原理： updateusersetage &#x3D;10whereid&#x3D;49; updateusersetage &#x3D;10wherename&#x3D;’Tom’; 第一条 SQL 使用主键查询，只需要在 id &#x3D; 49 这个主键索引上加上锁。第二条 SQL 使用二级索引来查询，那么首先在 name &#x3D; Tom 这个索引上加写锁，然后由于使用 InnoDB 二级索引还需再次根据主键索引查询，所以还需要在 id &#x3D; 49 这个主键索引上加锁。 也就是说使用主键索引需要加一把锁，使用二级索引需要在二级索引和主键索引上各加一把锁。 根据索引对单行数据进行更新的加锁原理了解了，那如果更新操作涉及多个行呢，比如下面 SQL 的执行场景。 updateusersetage &#x3D;10whereid&gt;49; 上述 SQL 的执行过程如下图所示。MySQL Server 会根据 WHERE 条件读取第一条满足条件的记录，然后 InnoDB 引擎会将第一条记录返回并加锁，接着 MySQL Server 发起更新改行记录的 UPDATE 请求，更新这条记录。一条记录操作完成，再读取下一条记录，直至没有匹配的记录为止。 InnoDB 存储引擎的锁的算法有三种： Record lock：单个行记录上的锁 Gap lock：间隙锁，锁定一个范围，不包括记录本身 Next-key lock：record+gap 锁定一个范围，包含记录本身 相关知识点： innodb 对于行的查询使用 next-key lock Next-locking keying 为了解决 Phantom Problem 幻读问题 当查询的索引含有唯一属性时，将 next-key lock 降级为 record key Gap 锁设计的目的是为了阻止多个事务将记录插入到同一范围内，而这会导致幻读问题的产生 有两种方式显式关闭 gap 锁：（除了外键约束和唯一性检查外，其余情况仅使用 record lock） A. 将事务隔离级别设置为 RC B. 将参数 innodb_locks_unsafe_for_binlog 设置为 1 表锁上面我们讲解行锁的时候，操作语句中的条件判断列都是有建立索引的，那么如果现在的判断列不存在索引呢？InnoDB 既支持行锁，也支持表锁，当没有查询列没有索引时，InnoDB 就不会去搞什么行锁了，毕竟行锁一定要有索引，所以它现在搞表锁，把整张表给锁住了。那么具体啥是表锁？还有其他什么情况下也会进行锁表呢？ 表锁使用的是一次性锁技术，也就是说，在会话开始的地方使用 lock 命令将后续需要用到的表都加上锁，在表释放前，只能访问这些加锁的表，不能访问其他表，直到最后通过 unlock tables 释放所有表锁。 除了使用 unlock tables 显示释放锁之外，会话持有其他表锁时执行 lock table 语句会释放会话之前持有的锁；会话持有其他表锁时执行 start transaction 或者 begin 开启事务时，也会释放之前持有的锁。 表锁由 MySQL Server 实现，行锁则是存储引擎实现，不同的引擎实现的不同。在 MySQL 的常用引擎中 InnoDB 支持行锁，而 MyISAM 则只能使用 MySQL Server 提供的表锁。 日志MySQL 中有以下日志文件，分别是： 1：重做日志（redo log） 2：回滚日志（undo log） 3：二进制日志（binlog） 4：错误日志（errorlog） 5：慢查询日志（slow query log） 6：一般查询日志（general log） 7：中继日志（relay log）。 其中重做日志和回滚日志与事务操作息息相关，二进制日志也与事务操作有一定的关系，这三种日志，对理解 MySQL 中的事务操作有着重要的意义。 重做日志（redo log）作用： 确保事务的持久性。redo 日志记录事务执行后的状态，用来恢复未写入 data file 的已成功事务更新的数据。防止在发生故障的时间点，尚有脏页未写入磁盘，在重启mysql服务的时候，根据 redo log 进行重做，从而达到事务的持久性这一特性。 内容： 物理格式的日志，记录的是物理数据页面的修改的信息，其 redo log 是顺序写入 redo log file 的物理文件中去的。 什么时候产生： 事务开始之后就产生 redo log，redo log 的落盘并不是随着事务的提交才写入的，而是在事务的执行过程中，便开始写入 redo log 文件中。 什么时候释放： 当对应事务的脏页写入到磁盘之后，redo log 的使命也就完成了，重做日志占用的空间就可以重用（被覆盖）。 对应的物理文件： 默认情况下，对应的物理文件位于数据库的 data 目录下的 ib_logfile1&amp;ib_logfile2 innodb_log_group_home_dir 指定日志文件组所在的路径，默认.&#x2F; ，表示在数据库的数据目录下。 innodb_log_files_in_group 指定重做日志文件组中文件的数量，默认 2 关于文件的大小和数量，由以下两个参数配置： innodb_log_file_size 重做日志文件的大小。 innodb_mirrored_log_groups 指定了日志镜像文件组的数量，默认 1 其他： 很重要一点，redo log 是什么时候写盘的？前面说了是在事物开始之后逐步写盘的。 之所以说重做日志是在事务开始之后逐步写入重做日志文件，而不一定是事务提交才写入重做日志缓存，原因就是，重做日志有一个缓存区 Innodb_log_buffer，Innodb_log_buffer 的默认大小为 8M(这里设置的 16M),Innodb 存储引擎先将重做日志写入 innodb_log_buffer 中。 然后会通过以下三种方式将 innodb 日志缓冲区的日志刷新到磁盘 Master Thread 每秒一次执行刷新 Innodb_log_buffer 到重做日志文件。 每个事务提交时会将重做日志刷新到重做日志文件。 当重做日志缓存可用空间 少于一半时，重做日志缓存被刷新到重做日志文件 由此可以看出，重做日志通过不止一种方式写入到磁盘，尤其是对于第一种方式，Innodb_log_buffer 到重做日志文件是 Master Thread 线程的定时任务。 因此重做日志的写盘，并不一定是随着事务的提交才写入重做日志文件的，而是随着事务的开始，逐步开始的。 另外引用《MySQL 技术内幕 Innodb 存储引擎》（page37）上的原话： 即使某个事务还没有提交，Innodb 存储引擎仍然每秒会将重做日志缓存刷新到重做日志文件。 这一点是必须要知道的，因为这可以很好地解释再大的事务的提交（commit）的时间也是很短暂的。 回滚日志（undo log）作用： 保证数据的原子性，保存了事务发生之前的数据的一个版本，可以用于回滚，同时可以提供多版本并发控制下的读（MVCC），也即非锁定读 内容： 逻辑格式的日志，在执行 undo 的时候，仅仅是将数据从逻辑上恢复至事务之前的状态，而不是从物理页面上操作实现的，这一点是不同于 redo log 的。 什么时候产生： 事务开始之前，将当前是的版本生成 undo log，undo 也会产生 redo 来保证 undo log 的可靠性 什么时候释放： 当事务提交之后，undo log 并不能立马被删除，而是放入待清理的链表，由 purge 线程判断是否由其他事务在使用 undo 段中表的上一个事务之前的版本信息，决定是否可以清理 undo log 的日志空间。 对应的物理文件： MySQL5.6 之前，undo 表空间位于共享表空间的回滚段中，共享表空间的默认的名称是 ibdata，位于数据文件目录中。 MySQL5.6 之后，undo 表空间可以配置成独立的文件，但是提前需要在配置文件中配置，完成数据库初始化后生效且不可改变 undo log 文件的个数 如果初始化数据库之前没有进行相关配置，那么就无法配置成独立的表空间了。 关于 MySQL5.7 之后的独立 undo 表空间配置参数如下： innodb_undo_directory &#x3D; &#x2F;data&#x2F;undospace&#x2F; –undo 独立表空间的存放目录 innodb_undo_logs &#x3D; 128 –回滚段为 128KB innodb_undo_tablespaces &#x3D; 4 –指定有 4 个 undo log 文件 如果 undo 使用的共享表空间，这个共享表空间中又不仅仅是存储了 undo 的信息，共享表空间的默认为与 MySQL 的数据目录下面，其属性由参数 innodb_data_file_path 配置。 其他： undo 是在事务开始之前保存的被修改数据的一个版本，产生 undo 日志的时候，同样会伴随类似于保护事务持久化机制的 redolog 的产生。 默认情况下 undo 文件是保持在共享表空间的，也即 ibdatafile 文件中，当数据库中发生一些大的事务性操作的时候，要生成大量的 undo 信息，全部保存在共享表空间中的。 因此共享表空间可能会变的很大，默认情况下，也就是 undo 日志使用共享表空间的时候，被“撑大”的共享表空间是不会也不能自动收缩的。 因此，mysql5.7 之后的“独立 undo 表空间”的配置就显得很有必要了。 二进制日志（binlog）作用： 用于复制，在主从复制中，从库利用主库上的 binlog 进行重播，实现主从同步。 用于数据库的基于时间点的还原。 内容： 逻辑格式的日志，可以简单认为就是执行过的事务中的 sql 语句。 但又不完全是 sql 语句这么简单，而是包括了执行的 sql 语句（增删改）反向的信息，也就意味着 delete 对应着 delete 本身和其反向的 insert；update 对应着 update 执行前后的版本的信息；insert 对应着 delete 和 insert 本身的信息。 在使用 mysqlbinlog 解析 binlog 之后一些都会真相大白。 因此可以基于 binlog 做到类似于 oracle 的闪回功能，其实都是依赖于 binlog 中的日志记录。 什么时候产生： 事务提交的时候，一次性将事务中的 sql 语句（一个事物可能对应多个 sql 语句）按照一定的格式记录到 binlog 中。 这里与 redo log 很明显的差异就是 redo log 并不一定是在事务提交的时候刷新到磁盘，redo log 是在事务开始之后就开始逐步写入磁盘。 因此对于事务的提交，即便是较大的事务，提交（commit）都是很快的，但是在开启了 bin_log 的情况下，对于较大事务的提交，可能会变得比较慢一些。 这是因为 binlog 是在事务提交的时候一次性写入的造成的，这些可以通过测试验证。 什么时候释放： binlog 的默认是保持时间由参数 expire_logs_days 配置，也就是说对于非活动的日志文件，在生成时间超过 expire_logs_days 配置的天数之后，会被自动删除。 对应的物理文件： 配置文件的路径为 log_bin_basename，binlog 日志文件按照指定大小，当日志文件达到指定的最大的大小之后，进行滚动更新，生成新的日志文件。 对于每个 binlog 日志文件，通过一个统一的 index 文件来组织。 其他： 二进制日志的作用之一是还原数据库的，这与 redo log 很类似，很多人混淆过，但是两者有本质的不同 作用不同：redo log 是保证事务的持久性的，是事务层面的，binlog 作为还原的功能，是数据库层面的（当然也可以精确到事务层面的），虽然都有还原的意思，但是其保护数据的层次是不一样的。 内容不同：redo log 是物理日志，是数据页面的修改之后的物理记录，binlog 是逻辑日志，可以简单认为记录的就是 sql 语句 另外，两者日志产生的时间，可以释放的时间，在可释放的情况下清理机制，都是完全不同的。 恢复数据时候的效率，基于物理日志的 redo log 恢复数据的效率要高于语句逻辑日志的 binlog 关于事务提交时，redo log 和 binlog 的写入顺序，为了保证主从复制时候的主从一致（当然也包括使用 binlog 进行基于时间点还原的情况），是要严格一致的，MySQL 通过两阶段提交过程来完成事务的一致性的，也即 redo log 和 binlog 的一致性的，理论上是先写 redo log，再写 binlog，两个日志都提交成功（刷入磁盘），事务才算真正的完成。 错误日志错误日志记录着 mysqld 启动和停止,以及服务器在运行过程中发生的错误的相关信息。在默认情况下，系统记录错误日志的功能是关闭的，错误信息被输出到标准错误输出。 指定日志路径两种方法: 编辑 my.cnf 写入 log-error&#x3D;[path] 通过命令参数错误日志 mysqld_safe –user&#x3D;mysql –log-error&#x3D;[path] &amp; 显示错误日志的命令（如下图所示） 普通查询日志记录了服务器接收到的每一个查询或是命令，无论这些查询或是命令是否正确甚至是否包含语法错误，general log 都会将其记录下来 ，记录的格式为 {Time ，Id ，Command，Argument }。也正因为 mysql 服务器需要不断地记录日志，开启 General log 会产生不小的系统开销。 因此，Mysql 默认是把 General log 关闭的。 查看日志的存放方式：show variables like ‘log_output’; 如果设置 mysql&gt; set global log_output&#x3D;’table’ 的话，则日志结果会记录到名为 gengera_log 的表中，这表的默认引擎都是 CSV 如果设置表数据到文件 set global log_output&#x3D;file; 设置 general log 的日志文件路径： set global general_log_file&#x3D;’&#x2F;tmp&#x2F;general.log’; 开启 general log： set global general_log&#x3D;on; 关闭 general log： set global general_log&#x3D;off; 然后在用：show global variables like ‘general_log’ 慢查询日志慢日志记录执行时间过长和没有使用索引的查询语句，报错 select、update、delete 以及 insert 语句，慢日志只会记录执行成功的语句。 1. 查看慢查询时间： show variables like “long_query_time”;默认 10s 查看慢查询配置情况： show status like “%slow_queries%”; 查看慢查询日志路径： show variables like “%slow%”; 开启慢日志 查看已经开启： 优化大表优化 当 MySQL 单表记录数过大时，数据库的 CRUD 性能会明显下降，一些常见的优化措施如下： 1. 限定数据的范围务必禁止不带任何限制数据范围条件的查询语句。比如：我们当用户在查询订单历史的时候，我们可以控制在一个月的范围内； 2. 读&#x2F;写分离经典的数据库拆分方案，主库负责写，从库负责读； 3. 垂直分区根据数据库里面数据表的相关性进行拆分。 例如，用户表中既有用户的登录信息又有用户的基本信息，可以将用户表拆分成两个单独的表，甚至放到单独的库做分库。 简单来说垂直拆分是指数据表列的拆分，把一张列比较多的表拆分为多张表。 如下图所示，这样来说大家应该就更容易理解了。 垂直拆分的优点： 可以使得列数据变小，在查询时减少读取的 Block 数，减少 I&#x2F;O 次数。此外，垂直分区可以简化表的结构，易于维护。 垂直拆分的缺点： 主键会出现冗余，需要管理冗余列，并会引起 Join 操作，可以通过在应用层进行 Join 来解决。此外，垂直分区会让事务变得更加复杂； 4. 水平分区保持数据表结构不变，通过某种策略存储数据分片。这样每一片数据分散到不同的表或者库中，达到了分布式的目的。 水平拆分可以支撑非常大的数据量。 水平拆分是指数据表行的拆分，表的行数超过 200 万行时，就会变慢，这时可以把一张的表的数据拆成多张表来存放。举个例子：我们可以将用户信息表拆分成多个用户信息表，这样就可以避免单一表数据量过大对性能造成影响。 水平拆分可以支持非常大的数据量。需要注意的一点是：分表仅仅是解决了单一表数据过大的问题，但由于表的数据还是在同一台机器上，其实对于提升 MySQL 并发能力没有什么意义，所以 水平拆分最好分库 。 水平拆分能够 支持非常大的数据量存储，应用端改造也少，但 分片事务难以解决 ，跨节点 Join 性能较差，逻辑复杂。《Java 工程师修炼之道》的作者推荐 尽量不要对数据进行分片，因为拆分会带来逻辑、部署、运维的各种复杂度 ，一般的数据表在优化得当的情况下支撑千万以下的数据量是没有太大问题的。如果实在要分片，尽量选择客户端分片架构，这样可以减少一次和中间件的网络 I&#x2F;O。 补充一下数据库分片的两种常见方案： 客户端代理： 分片逻辑在应用端，封装在 jar 包中，通过修改或者封装 JDBC 层来实现。 当当网的 Sharding-JDBC 、阿里的 TDDL 是两种比较常用的实现。 中间件代理： 在应用和数据中间加了一个代理层。分片逻辑统一维护在中间件服务中。 我们现在谈的 Mycat 、360 的 Atlas、网易的 DDB 等等都是这种架构的实现。","categories":[{"name":"技术分享","slug":"技术分享","permalink":"https://docs.hehouhui.cn/categories/%E6%8A%80%E6%9C%AF%E5%88%86%E4%BA%AB/"}],"tags":[{"name":"mysql","slug":"mysql","permalink":"https://docs.hehouhui.cn/tags/mysql/"}]},{"title":"Java基础-锁","slug":"archives/Java基础-锁","date":"2022-02-08T00:00:00.000Z","updated":"2023-10-08T06:42:00.000Z","comments":true,"path":"archives/37.html","link":"","permalink":"https://docs.hehouhui.cn/archives/37.html","excerpt":"","text":"synchronizedsynchronized 关键字解决的是多个线程之间访问资源的同步性，**synchronized**关键字可以保证被它修饰的方法或者代码块在任意时刻只能有一个线程执行。 另外，在 Java 早期版本中，synchronized 属于 重量级锁，效率低下。 为什么呢？ 因为监视器锁（monitor）是依赖于底层的操作系统的 Mutex Lock 来实现的，Java 的线程是映射到操作系统的原生线程之上的。如果要挂起或者唤醒一个线程，都需要操作系统帮忙完成，而操作系统实现线程之间的切换时需要从用户态转换到内核态，这个状态之间的转换需要相对比较长的时间，时间成本相对较高。 庆幸的是在 Java 6 之后 Java 官方对从 JVM 层面对 synchronized 较大优化，所以现在的 synchronized 锁效率也优化得很不错了。JDK1.6 对锁的实现引入了大量的优化，如自旋锁、适应性自旋锁、锁消除、锁粗化、偏向锁、轻量级锁等技术来减少锁操作的开销。 所以，你会发现目前的话，不论是各种开源框架还是 JDK 源码都大量使用了 synchronized 关键字。 使用方式synchronized 关键字最主要的三种使用方式： 1.修饰实例方法: 作用于当前对象实例加锁，进入同步代码前要获得 当前对象实例的锁 synchronized void method() &#123; //业务代码 &#125;Copy to clipboardErrorCopied 2.修饰静态方法: 也就是给当前类加锁，会作用于类的所有对象实例 ，进入同步代码前要获得 当前 class 的锁。因为静态成员不属于任何一个实例对象，是类成员（ _static 表明这是该类的一个静态资源，不管 new 了多少个对象，只有一份_）。所以，如果一个线程 A 调用一个实例对象的非静态 synchronized 方法，而线程 B 需要调用这个实例对象所属类的静态 synchronized 方法，是允许的，不会发生互斥现象，因为访问静态 synchronized 方法占用的锁是当前类的锁，而访问非静态 synchronized 方法占用的锁是当前实例对象锁。 synchronized void staic method() &#123; //业务代码 &#125;Copy to clipboardErrorCopied 3.修饰代码块 ：指定加锁对象，对给定对象&#x2F;类加锁。synchronized(this|object) 表示进入同步代码库前要获得给定对象的锁。synchronized(类.class) 表示进入同步代码前要获得 当前 class 的锁 synchronized(this) &#123; //业务代码 &#125;Copy to clipboardErrorCopied 总结： synchronized 关键字加到 static 静态方法和 synchronized(class) 代码块上都是是给 Class 类上锁。 synchronized 关键字加到实例方法上是给对象实例上锁。 尽量不要使用 synchronized(String a) 因为 JVM 中，字符串常量池具有缓存功能！ 下面我以一个常见的面试题为例讲解一下 synchronized 关键字的具体使用。 面试中面试官经常会说：“单例模式了解吗？来给我手写一下！给我解释一下双重检验锁方式实现单例模式的原理呗！” 双重校验锁实现对象单例（线程安全） public class Singleton &#123; private volatile static Singleton uniqueInstance; private Singleton() &#123; &#125; public static Singleton getUniqueInstance() &#123; //先判断对象是否已经实例过，没有实例化过才进入加锁代码 if (uniqueInstance == null) &#123; //类对象加锁 synchronized (Singleton.class) &#123; if (uniqueInstance == null) &#123; uniqueInstance = new Singleton(); &#125; &#125; &#125; return uniqueInstance; &#125; &#125;Copy to clipboardErrorCopied 另外，需要注意 uniqueInstance 采用 volatile 关键字修饰也是很有必要。 uniqueInstance 采用 volatile 关键字修饰也是很有必要的， uniqueInstance = new Singleton(); 这段代码其实是分为三步执行： 为 uniqueInstance 分配内存空间 初始化 uniqueInstance 将 uniqueInstance 指向分配的内存地址 但是由于 JVM 具有指令重排的特性，执行顺序有可能变成 1-&gt;3-&gt;2。指令重排在单线程环境下不会出现问题，但是在多线程环境下会导致一个线程获得还没有初始化的实例。例如，线程 T1 执行了 1 和 3，此时 T2 调用 getUniqueInstance() 后发现 uniqueInstance 不为空，因此返回 uniqueInstance，但此时 uniqueInstance 还未被初始化。 使用 volatile 可以禁止 JVM 的指令重排，保证在多线程环境下也能正常运行。 底层原理synchronized 关键字底层原理属于 JVM 层面。 代码块 public class SynchronizedDemo &#123; public void method() &#123; synchronized (this) &#123; System.out.println(\"synchronized 代码块\"); &#125; &#125; &#125; 通过 JDK 自带的 javap 命令查看 SynchronizedDemo 类的相关字节码信息：首先切换到类的对应目录执行 javac SynchronizedDemo.java 命令生成编译后的 .class 文件，然后执行javap -c -s -v -l SynchronizedDemo.class。 从上面我们可以看出： synchronized 同步语句块的实现使用的是 monitorenter 和 monitorexit 指令，其中 monitorenter 指令指向同步代码块的开始位置，**monitorexit** 指令则指明同步代码块的结束位置。 当执行 monitorenter 指令时，线程试图获取锁也就是获取 对象监视器 monitor 的持有权。 在 Java 虚拟机(HotSpot)中，Monitor 是基于 C++实现的，由 ObjectMonitor 实现的。每个对象中都内置了一个 ObjectMonitor 对象。 另外，**`wait/notify`****等方法也依赖于****`monitor`****对象，这就是为什么只有在同步的块或者方法中才能调用****`wait/notify`****等方法，否则会抛出****`java.lang.IllegalMonitorStateException`****的异常的原因。** 在执行monitorenter时，会尝试获取对象的锁，如果锁的计数器为 0 则表示锁可以被获取，获取后将锁计数器设为 1 也就是加 1。 在执行 monitorexit 指令后，将锁计数器设为 0，表明锁被释放。如果获取对象锁失败，那当前线程就要阻塞等待，直到锁被另外一个线程释放为止。 修饰方法 public class SynchronizedDemo2 &#123; public synchronized void method() &#123; System.out.println(\"synchronized 方法\"); &#125; &#125; Copy to clipboardErrorCopied synchronized 修饰的方法并没有 monitorenter 指令和 monitorexit 指令，取得代之的确实是 ACC_SYNCHRONIZED 标识，该标识指明了该方法是一个同步方法。JVM 通过该 ACC_SYNCHRONIZED 访问标志来辨别一个方法是否声明为同步方法，从而执行相应的同步调用。 锁乐观锁和悲观锁 乐观锁对应于生活中乐观的人总是想着事情往好的方向发展，悲观锁对应于生活中悲观的人总是想着事情往坏的方向发展。这两种人各有优缺点，不能不以场景而定说一种人好于另外一种人。 悲观锁 总是假设最坏的情况，每次去拿数据的时候都认为别人会修改，所以每次在拿数据的时候都会上锁，这样别人想拿这个数据就会阻塞直到它拿到锁（共享资源每次只给一个线程使用，其它线程阻塞，用完后再把资源转让给其它线程）。传统的关系型数据库里边就用到了很多这种锁机制，比如行锁，表锁等，读锁，写锁等，都是在做操作之前先上锁。Java 中 synchronized 和 ReentrantLock 等独占锁就是悲观锁思想的实现。 乐观锁 总是假设最好的情况，每次去拿数据的时候都认为别人不会修改，所以不会上锁，但是在更新的时候会判断一下在此期间别人有没有去更新这个数据，可以使用版本号机制和 CAS 算法实现。乐观锁适用于多读的应用类型，这样可以提高吞吐量，像数据库提供的类似于 write_condition 机制，其实都是提供的乐观锁。在 Java 中 java.util.concurrent.atomic 包下面的原子变量类就是使用了乐观锁的一种实现方式 CAS 实现的。 使用场景从上面对两种锁的介绍，我们知道两种锁各有优缺点，不可认为一种好于另一种，像乐观锁适用于写比较少的情况下（多读场景），即冲突真的很少发生的时候，这样可以省去了锁的开销，加大了系统的整个吞吐量。但如果是多写的情况，一般会经常产生冲突，这就会导致上层应用会不断的进行 retry，这样反倒是降低了性能，所以一般多写的场景下用悲观锁就比较合适。 乐观锁版本号机制一般是在数据表中加上一个数据版本号 version 字段，表示数据被修改的次数，当数据被修改时，version 值会加一。当线程 A 要更新数据值时，在读取数据的同时也会读取 version 值，在提交更新时，若刚才读取到的 version 值为当前数据库中的 version 值相等时才更新，否则重试更新操作，直到更新成功。 举一个简单的例子： 假设数据库中帐户信息表中有一个 version 字段，当前值为 1 ；而当前帐户余额字段（ balance ）为 $100 。 操作员 A 此时将其读出（ version&#x3D;1 ），并从其帐户余额中扣除 $50（ $100-$50 ）。 在操作员 A 操作的过程中，操作员 B 也读入此用户信息（ version&#x3D;1 ），并从其帐户余额中扣除 $20 （ $100-$20 ）。 操作员 A 完成了修改工作，将数据版本号（ version&#x3D;1 ），连同帐户扣除后余额（ balance&#x3D;$50 ），提交至数据库更新，此时由于提交数据版本等于数据库记录当前版本，数据被更新，数据库记录 version 更新为 2 。 操作员 B 完成了操作，也将版本号（ version&#x3D;1 ）试图向数据库提交数据（ balance&#x3D;$80 ），但此时比对数据库记录版本时发现，操作员 B 提交的数据版本号为 1 ，数据库记录当前版本也为 2 ，不满足 “ 提交版本必须等于当前版本才能执行更新 “ 的乐观锁策略，因此，操作员 B 的提交被驳回。 这样，就避免了操作员 B 用基于 version&#x3D;1 的旧数据修改的结果覆盖操作员 A 的操作结果的可能 CAS 算法即compare and swap（比较与交换），是一种有名的无锁算法。无锁编程，即不使用锁的情况下实现多线程之间的变量同步，也就是在没有线程被阻塞的情况下实现变量的同步，所以也叫非阻塞同步（Non-blocking Synchronization）。CAS 算法涉及到三个操作数 需要读写的内存值 V 进行比较的值 A 拟写入的新值 B 当且仅当 V 的值等于 A 时，CAS 通过原子方式用新值 B 来更新 V 的值，否则不会执行任何操作（比较和替换是一个原子操作）。一般情况下是一个自旋操作，即不断的重试。 关于自旋锁，大家可以看一下这篇文章，非常不错：《 面试必备之深入理解自旋锁》 缺点 ABA 问题是乐观锁一个常见的问题 ABA 问题如果一个变量 V 初次读取的时候是 A 值，并且在准备赋值的时候检查到它仍然是 A 值，那我们就能说明它的值没有被其他线程修改过了吗？很明显是不能的，因为在这段时间它的值可能被改为其他值，然后又改回 A，那 CAS 操作就会误认为它从来没有被修改过。这个问题被称为 CAS 操作的 “ABA”问题。 JDK 1.5 以后的 AtomicStampedReference 类就提供了此种能力，其中的 compareAndSet 方法就是首先检查当前引用是否等于预期引用，并且当前标志是否等于预期标志，如果全部相等，则以原子方式将该引用和该标志的值设置为给定的更新值。 开销大自旋 CAS（也就是不成功就一直循环执行直到成功）如果长时间不成功，会给 CPU 带来非常大的执行开销。 如果 JVM 能支持处理器提供的 pause 指令那么效率会有一定的提升，pause 指令有两个作用，第一它可以延迟流水线执行指令（de-pipeline）,使 CPU 不会消耗过多的执行资源，延迟的时间取决于具体实现的版本，在一些处理器上延迟时间是零。第二它可以避免在退出循环的时候因内存顺序冲突（memory order violation）而引起 CPU 流水线被清空（CPU pipeline flush），从而提高 CPU 的执行效率。 只能一个变量原子操作CAS 只对单个共享变量有效，当操作涉及跨多个共享变量时 CAS 无效。但是从 JDK 1.5 开始，提供了AtomicReference类来保证引用对象之间的原子性，你可以把多个变量放在一个对象里来进行 CAS 操作.所以我们可以使用锁或者利用AtomicReference类把多个共享变量合并成一个共享变量来操作。 CAS 与synchronized的使用情景 简单的来说 CAS 适用于写比较少的情况下（多读场景，冲突一般较少），synchronized 适用于写比较多的情况下（多写场景，冲突一般较多） 对于资源竞争较少（线程冲突较轻）的情况，使用synchronized同步锁进行线程阻塞和唤醒切换以及用户态内核态间的切换操作额外浪费消耗 cpu 资源；而 CAS 基于硬件实现，不需要进入内核，不需要切换线程，操作自旋几率较少，因此可以获得更高的性能。 对于资源竞争严重（线程冲突严重）的情况，CAS 自旋的概率会比较大，从而浪费更多的 CPU 资源，效率低于 synchronized。 补充： Java 并发编程这个领域中synchronized关键字一直都是元老级的角色，很久之前很多人都会称它为 “重量级锁” 。但是，在 JavaSE 1.6 之后进行了主要包括为了减少获得锁和释放锁带来的性能消耗而引入的 偏向锁 和 轻量级锁 以及其它各种优化之后变得在某些情况下并不是那么重了。synchronized的底层实现主要依靠 Lock-Free 的队列，基本思路是 自旋后阻塞，竞争切换后继续竞争锁，稍微牺牲了公平性，但获得了高吞吐量。在线程冲突较少的情况下，可以获得和 CAS 类似的性能；而线程冲突严重的情况下，性能远高于 CAS。 死锁线程死锁描述的是这样一种情况：多个线程同时被阻塞，它们中的一个或者全部都在等待某个资源被释放。由于线程被无限期地阻塞，因此程序不可能正常终止。 如下图所示，线程 A 持有资源 2，线程 B 持有资源 1，他们同时都想申请对方的资源，所以这两个线程就会互相等待而进入死锁状态。 线程 A 通过 synchronized (resource1) 获得 resource1 的监视器锁，然后通过Thread.sleep(1000);让线程 A 休眠 1s 为的是让线程 B 得到执行然后获取到 resource2 的监视器锁。线程 A 和线程 B 休眠结束了都开始企图请求获取对方的资源，然后这两个线程就会陷入互相等待的状态，这也就产生了死锁。上面的例子符合产生死锁的四个必要条件。 学过操作系统的朋友都知道产生死锁必须具备以下四个条件： 互斥条件：该资源任意一个时刻只由一个线程占用。 请求与保持条件：一个进程因请求资源而阻塞时，对已获得的资源保持不放。 不剥夺条件:线程已获得的资源在未使用完之前不能被其他线程强行剥夺，只有自己使用完毕后才释放资源。 循环等待条件:若干进程之间形成一种头尾相接的循环等待资源关系。 如何避免死锁我上面说了产生死锁的四个必要条件，为了避免死锁，我们只要破坏产生死锁的四个条件中的其中一个就可以了。现在我们来挨个分析一下： 破坏互斥条件 ：这个条件我们没有办法破坏，因为我们用锁本来就是想让他们互斥的（临界资源需要互斥访问）。 破坏请求与保持条件 ：一次性申请所有的资源。 破坏不剥夺条件 ：占用部分资源的线程进一步申请其他资源时，如果申请不到，可以主动释放它占有的资源。 破坏循环等待条件 ：靠按序申请资源来预防。按某一顺序申请资源，释放资源则反序释放。破坏循环等待条件。 AQS AQS ，AbstractQueuedSynchronizer ，即队列同步器。它是构建锁或者其他同步组件的基础框架（如 ReentrantLock、ReentrantReadWriteLock、Semaphore 等），J.U.C 并发包的作者（Doug Lea）期望它能够成为实现大部分同步需求的基础。 它是 J.U.C 并发包中的核心基础组件。在介绍 Lock 之前，我们需要先熟悉一个非常重要的组件，掌握了该组件 J.U.C 包下面很多问题都不在是问题了。该组件就是 AQS 。 AQS 核心思想是，如果被请求的共享资源空闲，则将当前请求资源的线程设置为有效的工作线程，并且将共享资源设置为锁定状态。如果被请求的共享资源被占用，那么就需要一套线程阻塞等待以及被唤醒时锁分配的机制，这个机制 AQS 是用 CLH 队列锁实现的，即将暂时获取不到锁的线程加入到队列中。 CLH(Craig,Landin,and Hagersten)队列是一个虚拟的双向队列（虚拟的双向队列即不存在队列实例，仅存在结点之间的关联关系）。AQS 是将每条请求共享资源的线程封装成一个 CLH 锁队列的一个结点（Node）来实现锁的分配。 看个 AQS(AbstractQueuedSynchronizer)原理图： AQS 使用一个 int 成员变量来表示同步状态，通过内置的 FIFO 队列来完成获取资源线程的排队工作。AQS 使用 CAS 对该同步状态进行原子操作实现对其值的修改。 private volatile int state;//共享变量，使用volatile修饰保证线程可见性Copy to clipboardErrorCopied 状态信息通过 protected 类型的getState，setState，compareAndSetState进行操作 //返回同步状态的当前值 protected final int getState() &#123; return state; &#125; // 设置同步状态的值 protected final void setState(int newState) &#123; state = newState; &#125; //原子地（CAS操作）将同步状态值设置为给定值update如果当前同步状态的值等于expect（期望值） protected final boolean compareAndSetState(int expect, int update) &#123; return unsafe.compareAndSwapInt(this, stateOffset, expect, update); &#125; 优势 AQS 解决了在实现同步器时涉及当的大量细节问题，例如获取同步状态、FIFO 同步队列。基于 AQS 来构建同步器可以带来很多好处。它不仅能够极大地减少实现工作，而且也不必处理在多个位置上发生的竞争问题。 在基于 AQS 构建的同步器中，只能在一个时刻发生阻塞，从而降低上下文切换的开销，提高了吞吐量。同时在设计 AQS 时充分考虑了可伸缩性，因此 J.U.C 中，所有基于 AQS 构建的同步器均可以获得这个优势。 同步状态 AQS 的主要使用方式是继承，子类通过继承同步器，并实现它的抽象方法来管理同步状态。 AQS 使用一个 int 类型的成员变量 state 来表示同步状态： 当 state &gt; 0 时，表示已经获取了锁。当 state &#x3D; 0 时，表示释放了锁。它提供了三个方法，来对同步状态 state 进行操作，并且 AQS 可以确保对 state 的操作是安全的： getState()setState(int newState)compareAndSetState(int expect, int update) 同步队列AQS 通过内置的 FIFO 同步队列来完成资源获取线程的排队工作： 如果当前线程获取同步状态失败（锁）时，AQS 则会将当前线程以及等待状态等信息构造成一个节点（Node）并将其加入同步队列，同时会阻塞当前线程当同步状态释放时，则会把节点中的线程唤醒，使其再次尝试获取同步状态。如图： 它维护了一个 volatile int state（代表共享资源）和一个 FIFO 线程等待队列（多线程争用资源被阻塞时会进入此队列）。这里 volatile 是核心关键词，具体 volatile 的语义，在此不述。state 的访问方式有三种（也就 第 3 点 的三个方法） getState()setState(int newState)compareAndSetState(int expect, int update) 主要内置方法AQS 主要提供了如下方法： getState()：返回同步状态的当前值。setState(int newState)：设置当前同步状态。compareAndSetState(int expect, int update)：使用 CAS 设置当前状态，该方法能够保证状态设置的原子性。【可重写】#tryAcquire(int arg)：独占式获取同步状态，获取同步状态成功后，其他线程需要等待该线程释放同步状态才能获取同步状态。【可重写】#tryRelease(int arg)：独占式释放同步状态。 -【可重写】#tryAcquireShared(int arg)：共享式获取同步状态，返回值大于等于 0 ，则表示获取成功；否则，获取失败。【可重写】#tryReleaseShared(int arg)：共享式释放同步状态。【可重写】#isHeldExclusively()：当前同步器是否在独占式模式下被线程占用，一般该方法表示是否被当前线程所独占。acquire(int arg)：独占式获取同步状态。如果当前线程获取同步状态成功，则由该方法返回；否则，将会进入同步队列等待。该方法将会调用可重写的 #tryAcquire(int arg) 方法；acquireInterruptibly(int arg)：与 #acquire(int arg) 相同，但是该方法响应中断。当前线程为获取到同步状态而进入到同步队列中，如果当前线程被中断，则该方法会抛出 InterruptedException 异常并返回。tryAcquireNanos(int arg, long nanos)：超时获取同步状态。如果当前线程在 nanos 时间内没有获取到同步状态，那么将会返回 false ，已经获取则返回 true 。acquireShared(int arg)：共享式获取同步状态，如果当前线程未获取到同步状态，将会进入同步队列等待，与独占式的主要区别是在同一时刻可以有多个线程获取到同步状态；acquireSharedInterruptibly(int arg)：共享式获取同步状态，响应中断。tryAcquireSharedNanos(int arg, long nanosTimeout)：共享式获取同步状态，增加超时限制。release(int arg)：独占式释放同步状态，该方法会在释放同步状态之后，将同步队列中第一个节点包含的线程唤醒。releaseShared(int arg)：共享式释放同步状态。 从上面的方法看下来，基本上可以分成 3 类： 独占式获取与释放同步状态 共享式获取与释放同步状态 查询同步队列中的等待线程情况 资源的共享方式AQS 定义两种资源共享方式 1)Exclusive（独占） 只有一个线程能执行，如 ReentrantLock。又可分为公平锁和非公平锁,ReentrantLock 同时支持两种锁,下面以 ReentrantLock 对这两种锁的定义做介绍： 公平锁：按照线程在队列中的排队顺序，先到者先拿到锁 非公平锁：当线程要获取锁时，先通过两次 CAS 操作去抢锁，如果没抢到，当前线程再加入到队列中等待唤醒。 说明：下面这部分关于 ReentrantLock 源代码内容节选自：https://www.javadoop.com/post/AbstractQueuedSynchronizer-2 ，这是一篇很不错文章，推荐阅读。 下面来看 ReentrantLock 中相关的源代码： ReentrantLock 默认采用非公平锁，因为考虑获得更好的性能，通过 boolean 来决定是否用公平锁（传入 true 用公平锁）。 /** Synchronizer providing all implementation mechanics */ private final Sync sync; public ReentrantLock() &#123; // 默认非公平锁 sync = new NonfairSync(); &#125; public ReentrantLock(boolean fair) &#123; sync = fair ? new FairSync() : new NonfairSync(); &#125;Copy to clipboardErrorCopied ReentrantLock 中公平锁的 lock 方法 static final class FairSync extends Sync &#123; final void lock() &#123; acquire(1); &#125; // AbstractQueuedSynchronizer.acquire(int arg) public final void acquire(int arg) &#123; if (!tryAcquire(arg) &amp;&amp; acquireQueued(addWaiter(Node.EXCLUSIVE), arg)) selfInterrupt(); &#125; protected final boolean tryAcquire(int acquires) &#123; final Thread current = Thread.currentThread(); int c = getState(); if (c == 0) &#123; // 1. 和非公平锁相比，这里多了一个判断：是否有线程在等待 if (!hasQueuedPredecessors() &amp;&amp; compareAndSetState(0, acquires)) &#123; setExclusiveOwnerThread(current); return true; &#125; &#125; else if (current == getExclusiveOwnerThread()) &#123; int nextc = c + acquires; if (nextc &lt; 0) throw new Error(\"Maximum lock count exceeded\"); setState(nextc); return true; &#125; return false; &#125; &#125;Copy to clipboardErrorCopied 非公平锁的 lock 方法： static final class NonfairSync extends Sync &#123; final void lock() &#123; // 2. 和公平锁相比，这里会直接先进行一次CAS，成功就返回了 if (compareAndSetState(0, 1)) setExclusiveOwnerThread(Thread.currentThread()); else acquire(1); &#125; // AbstractQueuedSynchronizer.acquire(int arg) public final void acquire(int arg) &#123; if (!tryAcquire(arg) &amp;&amp; acquireQueued(addWaiter(Node.EXCLUSIVE), arg)) selfInterrupt(); &#125; protected final boolean tryAcquire(int acquires) &#123; return nonfairTryAcquire(acquires); &#125; &#125; /** * Performs non-fair tryLock. tryAcquire is implemented in * subclasses, but both need nonfair try for trylock method. */ final boolean nonfairTryAcquire(int acquires) &#123; final Thread current = Thread.currentThread(); int c = getState(); if (c == 0) &#123; // 这里没有对阻塞队列进行判断 if (compareAndSetState(0, acquires)) &#123; setExclusiveOwnerThread(current); return true; &#125; &#125; else if (current == getExclusiveOwnerThread()) &#123; int nextc = c + acquires; if (nextc &lt; 0) // overflow throw new Error(\"Maximum lock count exceeded\"); setState(nextc); return true; &#125; return false; &#125;Copy to clipboardErrorCopied 总结：公平锁和非公平锁只有两处不同： 非公平锁在调用 lock 后，首先就会调用 CAS 进行一次抢锁，如果这个时候恰巧锁没有被占用，那么直接就获取到锁返回了。 非公平锁在 CAS 失败后，和公平锁一样都会进入到 tryAcquire 方法，在 tryAcquire 方法中，如果发现锁这个时候被释放了（state &#x3D;&#x3D; 0），非公平锁会直接 CAS 抢锁，但是公平锁会判断等待队列是否有线程处于等待状态，如果有则不去抢锁，乖乖排到后面。 公平锁和非公平锁就这两点区别，如果这两次 CAS 都不成功，那么后面非公平锁和公平锁是一样的，都要进入到阻塞队列等待唤醒。 相对来说，非公平锁会有更好的性能，因为它的吞吐量比较大。当然，非公平锁让获取锁的时间变得更加不确定，可能会导致在阻塞队列中的线程长期处于饥饿状态。 2)Share（共享） 多个线程可同时执行，如 Semaphore&#x2F;CountDownLatch。Semaphore、CountDownLatCh、 CyclicBarrier、ReadWriteLock 我们都会在后面讲到。 ReentrantReadWriteLock 可以看成是组合式，因为 ReentrantReadWriteLock 也就是读写锁允许多个线程同时对某一资源进行读。 不同的自定义同步器争用共享资源的方式也不同。自定义同步器在实现时只需要实现共享资源 state 的获取与释放方式即可，至于具体线程等待队列的维护（如获取资源失败入队&#x2F;唤醒出队等），AQS 已经在上层已经帮我们实现好了。 底层模版方法模式同步器的设计是基于模板方法模式的，如果需要自定义同步器一般的方式是这样（模板方法模式很经典的一个应用）： 使用者继承 AbstractQueuedSynchronizer 并重写指定的方法。（这些重写方法很简单，无非是对于共享资源 state 的获取和释放） 将 AQS 组合在自定义同步组件的实现中，并调用其模板方法，而这些模板方法会调用使用者重写的方法。 这和我们以往通过实现接口的方式有很大区别，这是模板方法模式很经典的一个运用，下面简单的给大家介绍一下模板方法模式，模板方法模式是一个很容易理解的设计模式之一。 模板方法模式是基于”继承“的，主要是为了在不改变模板结构的前提下在子类中重新定义模板中的内容以实现复用代码。举个很简单的例子假如我们要去一个地方的步骤是：购票 buyTicket()-&gt;安检 securityCheck()-&gt;乘坐某某工具回家 ride()-&gt;到达目的地 arrive()。我们可能乘坐不同的交通工具回家比如飞机或者火车，所以除了 ride()方法，其他方法的实现几乎相同。我们可以定义一个包含了这些方法的抽象类，然后用户根据自己的需要继承该抽象类然后修改 ride()方法。 AQS 使用了模板方法模式，自定义同步器时需要重写下面几个 AQS 提供的模板方法： isHeldExclusively()//该线程是否正在独占资源。只有用到condition才需要去实现它。 tryAcquire(int)//独占方式。尝试获取资源，成功则返回true，失败则返回false。 tryRelease(int)//独占方式。尝试释放资源，成功则返回true，失败则返回false。 tryAcquireShared(int)//共享方式。尝试获取资源。负数表示失败；0表示成功，但没有剩余可用资源；正数表示成功，且有剩余资源。 tryReleaseShared(int)//共享方式。尝试释放资源，成功则返回true，失败则返回false。 Copy to clipboardErrorCopied 默认情况下，每个方法都抛出 UnsupportedOperationException。 这些方法的实现必须是内部线程安全的，并且通常应该简短而不是阻塞。AQS 类中的其他方法都是 final ，所以无法被其他类使用，只有这几个方法可以被其他类使用。 以 ReentrantLock 为例，state 初始化为 0，表示未锁定状态。A 线程 lock()时，会调用 tryAcquire()独占该锁并将 state+1。此后，其他线程再 tryAcquire()时就会失败，直到 A 线程 unlock()到 state&#x3D;0（即释放锁）为止，其它线程才有机会获取该锁。当然，释放锁之前，A 线程自己是可以重复获取此锁的（state 会累加），这就是可重入的概念。但要注意，获取多少次就要释放多么次，这样才能保证 state 是能回到零态的。 再以 CountDownLatch 以例，任务分为 N 个子线程去执行，state 也初始化为 N（注意 N 要与线程个数一致）。这 N 个子线程是并行执行的，每个子线程执行完后 countDown()一次，state 会 CAS(Compare and Swap)减 1。等到所有子线程都执行完后(即 state&#x3D;0)，会 unpark()主调用线程，然后主调用线程就会从 await()函数返回，继续后余动作。 一般来说，自定义同步器要么是独占方法，要么是共享方式，他们也只需实现tryAcquire-tryRelease、tryAcquireShared-tryReleaseShared中的一种即可。但 AQS 也支持自定义同步器同时实现独占和共享两种方式，如ReentrantReadWriteLock。 推荐两篇 AQS 原理和相关源码分析的文章： http://www.cnblogs.com/waterystone/p/4920797.html https://www.cnblogs.com/chengxiao/archive/2017/07/24/7141160.html 信号量（允许多个线程同时访问）synchronized 和 ReentrantLock 都是一次只允许一个线程访问某个资源，Semaphore(信号量)可以指定多个线程同时访问某个资源。 示例代码如下： /** * * @author Snailclimb * @date 2018年9月30日 * @Description: 需要一次性拿一个许可的情况 */ public class SemaphoreExample1 &#123; // 请求的数量 private static final int threadCount = 550; public static void main(String[] args) throws InterruptedException &#123; // 创建一个具有固定线程数量的线程池对象（如果这里线程池的线程数量给太少的话你会发现执行的很慢） ExecutorService threadPool = Executors.newFixedThreadPool(300); // 一次只能允许执行的线程数量。 final Semaphore semaphore = new Semaphore(20); for (int i = 0; i &lt; threadCount; i++) &#123; final int threadnum = i; threadPool.execute(() -> &#123;// Lambda 表达式的运用 try &#123; semaphore.acquire();// 获取一个许可，所以可运行线程数量为20/1=20 test(threadnum); semaphore.release();// 释放一个许可 &#125; catch (InterruptedException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; &#125;); &#125; threadPool.shutdown(); System.out.println(\"finish\"); &#125; public static void test(int threadnum) throws InterruptedException &#123; Thread.sleep(1000);// 模拟请求的耗时操作 System.out.println(\"threadnum:\" + threadnum); Thread.sleep(1000);// 模拟请求的耗时操作 &#125; &#125;Copy to clipboardErrorCopied 执行 acquire 方法阻塞，直到有一个许可证可以获得然后拿走一个许可证；每个 release 方法增加一个许可证，这可能会释放一个阻塞的 acquire 方法。然而，其实并没有实际的许可证这个对象，Semaphore 只是维持了一个可获得许可证的数量。 Semaphore 经常用于限制获取某种资源的线程数量。 当然一次也可以一次拿取和释放多个许可，不过一般没有必要这样做： semaphore.acquire(5);// 获取5个许可，所以可运行线程数量为20/5=4 test(threadnum); semaphore.release(5);// 获取5个许可，所以可运行线程数量为20/5=4Copy to clipboardErrorCopied 除了 acquire方法之外，另一个比较常用的与之对应的方法是tryAcquire方法，该方法如果获取不到许可就立即返回 false。 Semaphore 有两种模式，公平模式和非公平模式。 公平模式： 调用 acquire 的顺序就是获取许可证的顺序，遵循 FIFO； 非公平模式： 抢占式的。 Semaphore 对应的两个构造方法如下： public Semaphore(int permits) &#123; sync = new NonfairSync(permits); &#125; public Semaphore(int permits, boolean fair) &#123; sync = fair ? new FairSync(permits) : new NonfairSync(permits); &#125;Copy to clipboardErrorCopied 这两个构造方法，都必须提供许可的数量，第二个构造方法可以指定是公平模式还是非公平模式，默认非公平模式。 issue645 补充内容 ：Semaphore 与 CountDownLatch 一样，也是共享锁的一种实现。它默认构造 AQS 的 state 为 permits。当执行任务的线程数量超出 permits,那么多余的线程将会被放入阻塞队列 Park,并自旋判断 state 是否大于 0。只有当 state 大于 0 的时候，阻塞的线程才能继续执行,此时先前执行任务的线程继续执行 release 方法，release 方法使得 state 的变量会加 1，那么自旋的线程便会判断成功。 如此，每次只有最多不超过 permits 数量的线程能自旋成功，便限制了执行任务线程的数量。 由于篇幅问题，如果对 Semaphore 源码感兴趣的朋友可以看下这篇文章：https://juejin.im/post/5ae755366fb9a07ab508adc6 CountDownLatch 倒计时器CountDownLatch 允许 count 个线程阻塞在一个地方，直至所有线程的任务都执行完毕。 CountDownLatch 是共享锁的一种实现,它默认构造 AQS 的 state 值为 count。当线程使用 countDown() 方法时,其实使用了tryReleaseShared方法以 CAS 的操作来减少 state,直至 state 为 0 。当调用 await() 方法的时候，如果 state 不为 0，那就证明任务还没有执行完毕，await() 方法就会一直阻塞，也就是说 await() 方法之后的语句不会被执行。然后，CountDownLatch 会自旋 CAS 判断 state == 0，如果 state == 0 的话，就会释放所有等待的线程，await() 方法之后的语句 **CountDownLatch****两种经典算法** 某一线程在开始运行前等待 n 个线程执行完毕。将 CountDownLatch 的计数器初始化为 n ：new CountDownLatch(n)，每当一个任务线程执行完毕，就将计数器减 1 countdownlatch.countDown()，当计数器的值变为 0 时，在CountDownLatch上 await() 的线程就会被唤醒。一个典型应用场景就是启动一个服务时，主线程需要等待多个组件加载完毕，之后再继续执行。 实现多个线程开始执行任务的最大并行性。注意是并行性，不是并发，强调的是多个线程在某一时刻同时开始执行。类似于赛跑，将多个线程放到起点，等待发令枪响，然后同时开跑。做法是初始化一个共享的 CountDownLatch 对象，将其计数器初始化为 1 ：new CountDownLatch(1)，多个线程在开始执行任务前首先 coundownlatch.await()，当主线程调用 countDown() 时，计数器变为 0，多个线程同时被唤醒。 使用示范 /** * * @author SnailClimb * @date 2018年10月1日 * @Description: CountDownLatch 使用方法示例 */ public class CountDownLatchExample1 &#123; // 请求的数量 private static final int threadCount = 550; public static void main(String[] args) throws InterruptedException &#123; // 创建一个具有固定线程数量的线程池对象（如果这里线程池的线程数量给太少的话你会发现执行的很慢） ExecutorService threadPool = Executors.newFixedThreadPool(300); final CountDownLatch countDownLatch = new CountDownLatch(threadCount); for (int i = 0; i &lt; threadCount; i++) &#123; final int threadnum = i; threadPool.execute(() -> &#123;// Lambda 表达式的运用 try &#123; test(threadnum); &#125; catch (InterruptedException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; finally &#123; countDownLatch.countDown();// 表示一个请求已经被完成 &#125; &#125;); &#125; countDownLatch.await(); threadPool.shutdown(); System.out.println(\"finish\"); &#125; public static void test(int threadnum) throws InterruptedException &#123; Thread.sleep(1000);// 模拟请求的耗时操作 System.out.println(\"threadnum:\" + threadnum); Thread.sleep(1000);// 模拟请求的耗时操作 &#125; &#125; Copy to clipboardErrorCopied 上面的代码中，我们定义了请求的数量为 550，当这 550 个请求被处理完成之后，才会执行System.out.println(&quot;finish&quot;);。 与 CountDownLatch 的第一次交互是主线程等待其他线程。主线程必须在启动其他线程后立即调用 CountDownLatch.await() 方法。这样主线程的操作就会在这个方法上阻塞，直到其他线程完成各自的任务。 其他 N 个线程必须引用闭锁对象，因为他们需要通知 CountDownLatch 对象，他们已经完成了各自的任务。这种通知机制是通过 CountDownLatch.countDown()方法来完成的；每调用一次这个方法，在构造函数中初始化的 count 值就减 1。所以当 N 个线程都调 用了这个方法，count 的值等于 0，然后主线程就能通过 await()方法，恢复执行自己的任务。 再插一嘴：CountDownLatch 的 await() 方法使用不当很容易产生死锁，比如我们上面代码中的 for 循环改为： for (int i = 0; i &lt; threadCount-1; i++) &#123; ....... &#125;Copy to clipboardErrorCopied 这样就导致 count 的值没办法等于 0，然后就会导致一直等待。 如果对 CountDownLatch 源码感兴趣的朋友，可以查看： 【JUC】JDK1.8 源码分析之 CountDownLatch（五）","categories":[{"name":"技术分享","slug":"技术分享","permalink":"https://docs.hehouhui.cn/categories/%E6%8A%80%E6%9C%AF%E5%88%86%E4%BA%AB/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://docs.hehouhui.cn/tags/Java/"}]},{"title":"springBoot三剑客","slug":"archives/springBoot三剑客","date":"2021-12-27T00:00:00.000Z","updated":"2023-10-08T06:42:00.000Z","comments":true,"path":"archives/24.html","link":"","permalink":"https://docs.hehouhui.cn/archives/24.html","excerpt":"","text":"springBoot 三板斧AOP aop 是一种面向切面编程 能够将那些与业务无关，却为业务模块所共同调用的逻辑或责任（缓存，锁） 封装起来，便于减少系统的重复代码，降低模块间的耦合度，并有利于未来的可拓展性和可维护性 Spring AOP 就是基于动态代理的，如果要代理的对象，实现了某个接口，那么 Spring AOP 会使用 JDK Proxy，去创建代理对象，而对于没有实现接口的对象，就无法使用 JDK Proxy 去进行代理了，这时候 Spring AOP 会使用 Cglib ，这时候 Spring AOP 会使用 Cglib 生成一个被代理对象的子类来作为代理，如下图所示： bean 作用域 singleton : 唯一 bean 实例，Spring 中的 bean 默认都是单例的。 prototype : 每次请求都会创建一个新的 bean 实例。 request : 每一次 HTTP 请求都会产生一个新的 bean，该 bean 仅在当前 HTTP request 内有效。 session : 每一次 HTTP 请求都会产生一个新的 bean，该 bean 仅在当前 HTTP session 内有效。 global-session： 全局 session 作用域，仅仅在基于 portlet 的 web 应用中才有意义，Spring5 已经没有了。Portlet 是能够生成语义代码(例如：HTML)片段的小型 Java Web 插件。它们基于 portlet 容器，可以像 servlet 一样处理 HTTP 请求。但是，与 servlet 不同，每个 portlet 都有不同的会话 bean 是否线程安全的确是存在安全问题的。因为，当多个线程操作同一个对象的时候，对这个对象的成员变量的写操作会存在线程安全问题。 但是，一般情况下，我们常用的 Controller、Service、Dao 这些 Bean 是无状态的。无状态的 Bean 不能保存数据，因此是线程安全的。 常见的有 2 种解决办法： 在类中定义一个 ThreadLocal 成员变量，将需要的可变成员变量保存在 ThreadLocal 中（推荐的一种方式）。 改变 Bean 的作用域为 “prototype”：每次请求都会创建一个新的 bean 实例，自然不会存在线程安全问题。 生命周期 Bean 容器找到配置文件中 Spring Bean 的定义。 Bean 容器利用 Java Reflection API 创建一个 Bean 的实例。 如果涉及到一些属性值 利用 set()方法设置一些属性值。 如果 Bean 实现了 BeanNameAware 接口，调用 setBeanName()方法，传入 Bean 的名字。 如果 Bean 实现了 BeanClassLoaderAware 接口，调用 setBeanClassLoader()方法，传入 ClassLoader对象的实例。 与上面的类似，如果实现了其他 .Aware接口，就调用相应的方法。 如果有和加载这个 Bean 的 Spring 容器相关的 BeanPostProcessor 对象，执行postProcessBeforeInitialization() 方法 如果 Bean 实现了InitializingBean接口，执行afterPropertiesSet()方法。 如果 Bean 在配置文件中的定义包含 init-method 属性，执行指定的方法。 如果有和加载这个 Bean 的 Spring 容器相关的 BeanPostProcessor 对象，执行postProcessAfterInitialization() 方法 当要销毁 Bean 的时候，如果 Bean 实现了 DisposableBean 接口，执行 destroy() 方法。 当要销毁 Bean 的时候，如果 Bean 在配置文件中的定义包含 destroy-method 属性，执行指定的方法。 MVC 流程说明（重要）： 客户端（浏览器）发送请求，直接请求到 DispatcherServlet。 DispatcherServlet 根据请求信息调用 HandlerMapping，解析请求对应的 Handler。 解析到对应的 Handler（也就是我们平常说的 Controller 控制器）后，开始由 HandlerAdapter 适配器处理。 HandlerAdapter 会根据 Handler 来调用真正的处理器来处理请求，并处理相应的业务逻辑。 处理器处理完业务后，会返回一个 ModelAndView 对象，Model 是返回的数据对象，View 是个逻辑上的 View。 ViewResolver 会根据逻辑 View 查找实际的 View。 DispaterServlet 把返回的 Model 传给 View（视图渲染）。 把 View 返回给请求者（浏览器） 有哪些设计模式 工厂设计模式 : Spring 使用工厂模式通过 BeanFactory、ApplicationContext 创建 bean 对象。 代理设计模式 : Spring AOP 功能的实现。 单例设计模式 : Spring 中的 Bean 默认都是单例的。 模板方法模式 : Spring 中 jdbcTemplate、hibernateTemplate 等以 Template 结尾的对数据库操作的类，它们就使用到了模板模式。 包装器设计模式 : 我们的项目需要连接多个数据库，而且不同的客户在每次访问中根据需要会去访问不同的数据库。这种模式让我们可以根据客户的需求能够动态切换不同的数据源。 观察者模式: Spring 事件驱动模型就是观察者模式很经典的一个应用。 适配器模式 :Spring AOP 的增强或通知(Advice)使用到了适配器模式、spring MVC 中也是用到了适配器模式适配Controller。 事务事务方式 编程式事务，在代码中硬编码。(不推荐使用) 声明式事务，在配置文件中配置（推荐使用） 声明式事务又分为两种： 基于 XML 的声明式事务 基于注解的声明式事务 隔离级别ransactionDefinition 接口中定义了五个表示隔离级别的常量： TransactionDefinition.ISOLATION_DEFAULT: 使用后端数据库默认的隔离级别，Mysql 默认采用的 REPEATABLE_READ 隔离级别 Oracle 默认采用的 READ_COMMITTED 隔离级别. TransactionDefinition.ISOLATION_READ_UNCOMMITTED: 最低的隔离级别，允许读取尚未提交的数据变更，可能会导致脏读、幻读或不可重复读 TransactionDefinition.ISOLATION_READ_COMMITTED: 允许读取并发事务已经提交的数据，可以阻止脏读，但是幻读或不可重复读仍有可能发生 TransactionDefinition.ISOLATION_REPEATABLE_READ: 对同一字段的多次读取结果都是一致的，除非数据是被本身事务自己所修改，可以阻止脏读和不可重复读，但幻读仍有可能发生。 TransactionDefinition.ISOLATION_SERIALIZABLE: 最高的隔离级别，完全服从 ACID 的隔离级别。所有的事务依次逐个执行，这样事务之间就完全不可能产生干扰，也就是说，该级别可以防止脏读、不可重复读以及幻读。但是这将严重影响程序的性能。通常情况下也不会用到该级别。 传播行为支持当前事务的情况： TransactionDefinition.PROPAGATION_REQUIRED： 如果当前存在事务，则加入该事务；如果当前没有事务，则创建一个新的事务。 TransactionDefinition.PROPAGATION_SUPPORTS： 如果当前存在事务，则加入该事务；如果当前没有事务，则以非事务的方式继续运行。 TransactionDefinition.PROPAGATION_MANDATORY： 如果当前存在事务，则加入该事务；如果当前没有事务，则抛出异常。（mandatory：强制性） 不支持当前事务的情况： TransactionDefinition.PROPAGATION_REQUIRES_NEW： 创建一个新的事务，如果当前存在事务，则把当前事务挂起。 TransactionDefinition.PROPAGATION_NOT_SUPPORTED： 以非事务方式运行，如果当前存在事务，则把当前事务挂起。 TransactionDefinition.PROPAGATION_NEVER： 以非事务方式运行，如果当前存在事务，则抛出异常。 其他情况： TransactionDefinition.PROPAGATION_NESTED： 如果当前存在事务，则创建一个事务作为当前事务的嵌套事务来运行；如果当前没有事务，则该取值等价于 TransactionDefinition.PROPAGATION_REQUIRED。 IOC IOC 是一种设计思想，它有一个容器用来存放对象引用。IoC 容器是 Spring 用来实现 IoC 的载体 将原本在程序中手动创建对象的控制权，交由 Spring 框架来管理 IoC 容器实际上就是个 Map（key，value）,Map 中存放的是各种对象。 Spring IoC 的初始化过程： spring 怎么解决循环依赖Spring 整个解决循环依赖问题的实现思路已经比较清楚了。对于整体过程，读者朋友只要理解两点： Spring 是通过递归的方式获取目标 bean 及其所依赖的 bean 的； Spring 实例化一个 bean 的时候，是分两步进行的，首先实例化目标 bean，然后为其注入属性。 结合这两点，也就是说，Spring 在实例化一个 bean 的时候，是首先递归的实例化其所依赖的所有 bean，直到某个 bean 没有依赖其他 bean，此时就会将该实例返回，然后反递归的将获取到的 bean 设置为各个上层 bean 的属性的。 三级缓存 如何解决循环依赖，Spring 主要的思路就是依据三级缓存，在实例化 A 时调用 doGetBean，发现 A 依赖的 B 的实例，此时调用 doGetBean 去实例 B，实例化的 B 的时候发现又依赖 A，如果不解决这个循环依赖的话此时的 doGetBean 将会无限循环下去，导致内存溢出，程序奔溃。spring 引用了一个早期对象，并且把这个”早期引用”并将其注入到容器中，让 B 先完成实例化，此时 A 就获取 B 的引用，完成实例化。 Spring 能够轻松的解决属性的循环依赖正式用到了三级缓存，在 AbstractBeanFactory 中有详细的注释。 一级缓存：singletonObjects，存放完全实例化属性赋值完成的 Bean，直接可以使用。二级缓存：earlySingletonObjects，存放早期 Bean 的引用，尚未属性装配的 Bean 三级缓存：singletonFactories，三级缓存，存放实例化完成的 Bean 工厂。 根据以上的分析，大概清楚了 Spring 是如何解决循环依赖的。假设 A 依赖 B，B 依赖 A（注意：这里是 set 属性依赖）分以下步骤执行：A 依次执行doGetBean、查询缓存、createBean创建实例，实例化完成放入三级缓存 singletonFactories 中，接着执行populateBean方法装配属性，但是发现有一个属性是 B 的对象。因此再次调用 doGetBean 方法创建 B 的实例，依次执行 doGetBean、查询缓存、createBean 创建实例，实例化完成之后放入三级缓存 singletonFactories 中，执行 populateBean 装配属性，但是此时发现有一个属性是 A 对象。因此再次调用 doGetBean 创建 A 的实例，但是执行到 getSingleton 查询缓存的时候，从三级缓存中查询到了 A 的实例(早期引用，未完成属性装配)，此时直接返回 A，不用执行后续的流程创建 A 了，那么 B 就完成了属性装配，此时是一个完整的对象放入到一级缓存 singletonObjects 中。B 创建完成了，则 A 自然完成了属性装配，也创建完成放入了一级缓存 singletonObjects 中。Spring 三级缓存的应用完美的解决了循环依赖的问题，下面是循环依赖的解决流程图。 SPI（自动装配)1）SPI 思想 SPI 的全名为 Service Provider Interface.这个是针对厂商或者插件的。 SPI 的思想：系统里抽象的各个模块，往往有很多不同的实现方案，比如日志模块的方案，xml 解析模块、jdbc 模块的方案等。面向的对象的设计里，我们一般推荐模块之间基于接口编程，模块之间不对实现类进行硬编码。一旦代码里涉及具体的实现类，就违反了可拔插的原则，如果需要替换一种实现，就需要修改代码。为了实现在模块装配的时候能不在程序里动态指明，这就需要一种服务发现机制。java spi 就是提供这样的一个机制：为某个接口寻找服务实现的机制 （2）SPI 约定 当服务的提供者，提供了服务接口的一种实现之后，在 jar 包的 META-INF&#x2F;services&#x2F;目录里同时创建一个以服务接口命名的文件。该文件里就是实现该服务接口的具体实现类。而当外部程序装配这个模块的时候，就能通过该 jar 包 META-INF&#x2F;services&#x2F;里的配置文件找到具体的实现类名，并装载实例化，完成模块的注入。通过这个约定，就不需要把服务放在代码中了，通过模块被装配的时候就可以发现服务类了。 springboot 最重要的特性就是自动配置，许多功能不需要手动开启，会自动帮助程序员开启，如果想扩展某些 第三方组件的功能，例如mybatis，只需要配置依赖，就可以了，这其中都是需要SPI支持实现的，下面我们从源码 层面看看springboot如何通过spi机制实现自动配置的。 spring.factories加载所有工程中META-INF/spring.factories文件中的配置 创建 SpringApplication 对象： 我们随便看一个工程的目录：spring-boot-autoconfigure 工程下 META-INF 目录下的 spring.factories 文件的内容， 配置类似 map ，key 为某一项，value 为实现集合 这就是 Spi 的加载机制，可以通过配置的方式实现和业务代码的解耦，需要增加时直接配置到文件内。 这一步是在容器启动的时候，加载所有的 factoryname 的值到缓存，包括自动配置的。 如何寻找并注册看一下这个注解@SpringBootApplication 看一下这个类的内容：类里面有获取自动配置的方法 getAutoConfigrationEntry 就是从上一步缓存 result 中查询所有的 EnableAutoConfiguration 的 value 集合， 这一节主要分析 spi 机制加载 spring.factories 的配置项，下一节我们来分析自动配置的加载流程","categories":[{"name":"技术分享","slug":"技术分享","permalink":"https://docs.hehouhui.cn/categories/%E6%8A%80%E6%9C%AF%E5%88%86%E4%BA%AB/"}],"tags":[{"name":"Spring","slug":"Spring","permalink":"https://docs.hehouhui.cn/tags/Spring/"},{"name":"Java","slug":"Java","permalink":"https://docs.hehouhui.cn/tags/Java/"}]},{"title":"Redis集群与特性","slug":"archives/Redis集群与特性","date":"2021-12-18T00:00:00.000Z","updated":"2023-10-08T06:42:00.000Z","comments":true,"path":"archives/23.html","link":"","permalink":"https://docs.hehouhui.cn/archives/23.html","excerpt":"","text":"一致性 hash 在 Redis 集群模式 Cluster 中，Redis 采用的是分片 Sharding 的方式，也就是将数据采用一定的分区策略，分发到相应的集群节点中。但是我们使用上述 HASH 算法进行缓存时，会出现一些缺陷，主要体现在服务器数量变动的时候，所有缓存的位置都要发生改变！具体来讲就是说第一当缓存服务器数量发生变化时，会引起缓存的雪崩，可能会引起整体系统压力过大而崩溃（大量缓存同一时间失效）。第二当缓存服务器数量发生变化时，几乎所有缓存的位置都会发生改变。 一致性 Hash 算法也是使用取模的方法，只是，刚才描述的取模法是对服务器的数量进行取模，而一致性 Hash 算法是对 232 取模，什么意思呢？简单来说，一致性 Hash 算法将整个哈希值空间组织成一个虚拟的圆环，如假设某哈希函数 H 的值空间为 0-232-1（即哈希值是一个 32 位无符号整形），整个哈希环如下： 整个空间按顺时针方向组织，圆环的正上方的点代表 0，0 点右侧的第一个点代表 1，以此类推，2、3、4、5、6……直到 232-1，也就是说 0 点左侧的第一个点代表 232-1， 0 和 232-1 在零点中方向重合，我们把这个由 232 个点组成的圆环称为 Hash 环。 那么，一致性哈希算法与上图中的圆环有什么关系呢？我们继续聊，仍然以之前描述的场景为例，假设我们有 4 台缓存服务器，服务器 A、服务器 B、服务器 C，服务器 D，那么，在生产环境中，这 4 台服务器肯定有自己的 IP 地址或主机名，我们使用它们各自的 IP 地址或主机名作为关键字进行哈希计算，使用哈希后的结果对 2^32 取模，可以使用如下公式示意： hash（服务器 A 的 IP 地址） % 2^32 通过上述公式算出的结果一定是一个 0 到 232-1 之间的一个整数，我们就用算出的这个整数，代表服务器 A，既然这个整数肯定处于 0 到 232-1 之间，那么，上图中的 hash 环上必定有一个点与这个整数对应，而我们刚才已经说明，使用这个整数代表服务器 A，那么，服务器 A 就可以映射到这个环上。 以此类推，下一步将各个服务器使用类似的 Hash 算式进行一个哈希，这样每台机器就能确定其在哈希环上的位置，这里假设将上文中四台服务器使用 IP 地址哈希后在环空间的位置如下： 接下来使用如下算法定位数据访问到相应服务器： 将数据 key 使用相同的函数 Hash 计算出哈希值，并确定此数据在环上的位置，从此位置沿环顺时针“行走”，第一台遇到的服务器就是其应该定位到的服务器！ Hash 算法的容错性和可扩展性现假设 Node C 不幸宕机，可以看到此时对象 A、B、D 不会受到影响，只有 C 对象被重定位到 Node D。一般的，在一致性 Hash 算法中，如果一台服务器不可用，则受影响的数据仅仅是此服务器到其环空间中前一台服务器（即沿着逆时针方向行走遇到的第一台服务器）之间数据，其它不会受到影响，如下所示： ![image-20210228225729174](&#x2F;Users&#x2F;hehui&#x2F;Library&#x2F;Application Support&#x2F;typora-user-images&#x2F;image-20210228225729174.png) 下面考虑另外一种情况，如果在系统中增加一台服务器 Node X，如下图所示： ![image-20210228225716137](&#x2F;Users&#x2F;hehui&#x2F;Library&#x2F;Application Support&#x2F;typora-user-images&#x2F;image-20210228225716137.png) 此时对象 Object A、B、D 不受影响，只有对象 C 需要重定位到新的 Node X ！一般的，在一致性 Hash 算法中，如果增加一台服务器，则受影响的数据仅仅是新服务器到其环空间中前一台服务器（即沿着逆时针方向行走遇到的第一台服务器）之间数据，其它数据也不会受到影响。 综上所述，一致性 Hash 算法对于节点的增减都只需重定位环空间中的一小部分数据，具有较好的容错性和可扩展性。 数据倾斜问题一致性 Hash 算法在服务节点太少时，容易因为节点分部不均匀而造成数据倾斜（被缓存的对象大部分集中缓存在某一台服务器上）问题，例如系统中只有两台服务器，其环分布如下： ![image-20210228225828563](&#x2F;Users&#x2F;hehui&#x2F;Library&#x2F;Application Support&#x2F;typora-user-images&#x2F;image-20210228225828563.png) 此时必然造成大量数据集中到 Node A 上，而只有极少量会定位到 Node B 上，从而出现 hash 环偏斜的情况，当 hash 环偏斜以后，缓存往往会极度不均衡的分布在各服务器上，如果想要均衡的将缓存分布到 2 台服务器上，最好能让这 2 台服务器尽量多的、均匀的出现在 hash 环上，但是，真实的服务器资源只有 2 台，我们怎样凭空的让它们多起来呢，没错，就是凭空的让服务器节点多起来，既然没有多余的真正的物理服务器节点，我们就只能将现有的物理节点通过虚拟的方法复制出来。 这些由实际节点虚拟复制而来的节点被称为”虚拟节点”，即对每一个服务节点计算多个哈希，每个计算结果位置都放置一个此服务节点，称为虚拟节点。具体做法可以在服务器 IP 或主机名的后面增加编号来实现。 例如上面的情况，可以为每台服务器计算三个虚拟节点，于是可以分别计算 “Node A#1”、“Node A#2”、“Node A#3”、“Node B#1”、“Node B#2”、“Node B#3”的哈希值，于是形成六个虚拟节点： ![image-20210228225807543](&#x2F;Users&#x2F;hehui&#x2F;Library&#x2F;Application Support&#x2F;typora-user-images&#x2F;image-20210228225807543.png) 同时数据定位算法不变，只是多了一步虚拟节点到实际节点的映射，例如定位到“Node A#1”、“Node A#2”、“Node A#3”三个虚拟节点的数据均定位到 Node A 上。这样就解决了服务节点少时数据倾斜的问题。在实际应用中，通常将虚拟节点数设置为 32 甚至更大，因此即使很少的服务节点也能做到相对均匀的数据分布。 集群情况下什么时候不可用 如果集群任意master挂掉*,且当前_master没有*slave.集群进入_fail状态 有 A,B,C 三个节点的集群，在没有复制模型的情况下，如果节点 B 失败了，那么整个集群就会以为缺少 5501-11000 这个范围的槽而不可用。 如果集群超过半数以上master挂掉，无论是否有slave集群进入fail状态 集群某一节点的主从全数宕机 （与 2 相似） 故障的处理过程 查看业务日志（微服务） 首先查看 redis 集群状态。 继续查看 redis 集群节点的状态。 处理过程 1、先停止所有 redis 节点。2、删除每个节点的缓存文件，包括 node-6380.conf dump.rdp 等文件。3、重启每个 redis 节点。4、重新创建 redis 集群。 集群 发现我们当前项目用的 redis 是主从，但是跟单点其实没有什么区别，因为我们在应用层面没有做读写分离，所以其实从服务器只是做了一个主从复制的工作，其他的什么都没有做。 那么如果我们的系统升级，用户量上升，那么一主一从可能扛不住那么大的压力，可能需要一主多从做备机，那么假如主服务器宕机了，选举哪台从服务器做主呢？这就是一个问题，需要一个第三个人来解决，所以我查了一下，哨兵模式可以解决这个问题。哨兵模式的细节下面会讲到。 然后我又想了，那如果单台服务器无法承受100%的存储压力，那就应该将存储压力分散开来，所以集群就可以解决这个问题 了，比如我们用6台服务器做集群，3主3从，那么每台服务器只需要存储1/3即可。好，那么我们就来详细看一下这些具体怎么做的。 单点主从基本上就是一主一从，我们应用层主要使用的是主节点，从节点的主要工作是从主节点做主从复制。关键时刻，如果主服务器挂掉，可以手动启动从服务器，然后更改应用层的 redis 的 ip 即可 实现主从复制（Master-Slave Replication）的工作原理：Slave 从节点服务启动并连接到 Master 之后，它将主动发送一个 SYNC 命令。Master 服务主节点收到同步命令后将启动后台存盘进程，同时收集所有接收到的用于修改数据集的命令，在后台进程执行完毕后，Master 将传送整个数据库文件到 Slave，以完成一次完全同步。而 Slave 从节点服务在接收到数据库文件数据之后将其存盘并加载到内存中。此后，Master 主节点继续将所有已经收集到的修改命令，和新的修改命令依次传送给 Slaves，Slave 将在本次执行这些数据修改命令，从而达到最终的数据同步。 如果 Master 和 Slave 之间的链接出现断连现象，Slave 可以自动重连 Master，但是在连接成功之后，一次完全同步将被自动执行。 主从复制配置 修改从节点的配置文件：slaveof masterip masterport如果设置了密码，就要设置：masterauth master-password 主从模式的优缺点 优点： 同一个 Master 可以同步多个 Slaves。 Slave 同样可以接受其它 Slaves 的连接和同步请求，这样可以有效的分载 Master 的同步压力。因此我们可以将 Redis 的 Replication 架构视为图结构。 Master Server 是以非阻塞的方式为 Slaves 提供服务。所以在 Master-Slave 同步期间，客户端仍然可以提交查询或修改请求。 Slave Server 同样是以非阻塞的方式完成数据同步。在同步期间，如果有客户端提交查询请求，Redis 则返回同步之前的数据 为了分载 Master 的读操作压力，Slave 服务器可以为客户端提供只读操作的服务，写服务仍然必须由 Master 来完成。即便如此，系统的伸缩性还是得到了很大的提高。 Master 可以将数据保存操作交给 Slaves 完成，从而避免了在 Master 中要有独立的进程来完成此操作。支持主从复制，主机会自动将数据同步到从机，可以进行读写分离。 缺点： Redis 不具备自动容错和恢复功能，主机从机的宕机都会导致前端部分读写请求失败，需要等待机器重启或者手动切换前端的 IP 才能恢复。 主机宕机，宕机前有部分数据未能及时同步到从机，切换 IP 后还会引入数据不一致的问题，降低了系统的可用性。 Redis 的主从复制采用全量复制，复制过程中主机会 fork 出一个子进程对内存做一份快照，并将子进程的内存快照保存为文件发送给从机，这一过程需要确保主机有足够多的空余内存。若快照文件较大，对集群的服务能力会产生较大的影响，而且复制过程是在从机新加入集群或者从机和主机网络断开重连时都会进行，也就是网络波动都会造成主机和从机间的一次全量的数据复制，这对实际的系统运营造成了不小的麻烦。 Redis 较难支持在线扩容，在集群容量达到上限时在线扩容会变得很复杂。为避免这一问题，运维人员在系统上线时必须确保有足够的空间，这对资源造成了很大的浪费。 其实 redis 的主从模式很简单，在实际的生产环境中是很少使用的，我也不建议在实际的生产环境中使用主从模式来提供系统的高可用性，之所以不建议使用都是由它的缺点造成的，在数据量非常大的情况，或者对系统的高可用性要求很高的情况下，主从模式也是不稳定的。 读写分离 对于读占比较高的场景，可以通过把一部分流量分摊导出从节点(salve) 来减轻主节点（master）压力，同时需要主要只对主节点执行写操作。 常见的应用场景下我觉得 redis 没必要进行读写分离。 先来讨论一下为什么要读写分离： 读写分离使用于大量读请求的情况，通过多个 slave 分摊了读的压力，从而增加了读的性能。过多的 select 会阻塞住数据库，使你增删改不能执行，而且到并发量过大时，数据库会拒绝服务。 因而通过读写分离，从而增加性能，避免拒绝服务的发生。 我认为需要读写分离的应用场景是：写请求在可接受范围内，但读请求要远大于写请求的场景。 再来讨论一下 redis 常见的应用场景： 缓存 排名型的应用，访问计数型应用 实时消息系统 首先说一下缓存集群，这也是非常常见的应用场景： 缓存主要解决的是用户访问时，怎么以更快的速度得到数据。 单机的内存资源是很有限的，所以缓存集群会通过某种算法将不同的数据放入到不同的机器中。 不同持久化数据库，一般来说，内存数据库单机可以支持大量的增删查改。 如果一台机器支持不住，可以用主从复制，进行缓存的方法解决。 综上，在这个场景下应用 redis 进行读写分离，完全就失去了读写分离的意义。 当然，也有可能考虑不到的地方需要读写分离，毕竟“存在即合理”嘛，那么就来介绍一下这个读写分离吧。 当使用从节点响应读请求时，业务端可能会遇到以下问题 复制数据延迟 读到过期数据 从节点故障 数据延迟 Redis 复制数的延迟由于异步复制特性是无法避免的，延迟取决于网络带宽和命令阻塞情况，对于无法容忍大量延迟场景，可以编写外部监控程序监听主从节点的复制偏移量，当延迟较大时触发报警或者通知客户端避免读取延迟过高的从节点，实现逻辑如下： 监控程序(monitor) 定期检查主从节点的偏移量，主节点偏移量在 info replication 的 master_repl_offset 指标记录，从节点 偏移量可以查询主节点的 slave0 字段的 offset 指标，它们的差值就是主从节点延迟的字节 量。 当延迟字节量过高时，比如超过 10M。监控程序触发报警并通知客户端从节点延迟过高。可以采用 Zookeeper 的监听回调机制实现客户端通知。 客户端接到具体的从节点高延迟通知后，修改读命令路由到其他从节点或主节点上。当延迟回复后，再次通知客户端，回复从节点的读命令请求。 这种方案成本较高，需要单独修改适配 Redis 的客户端类库。 读到过期数据 当主节点存储大量设置超时的数据时，如缓存数据，Redis 内部需要维护过期数据删除策略，删除策略主要有两种：惰性删除和定时删除。 惰性删除：主节点每次处理读取命令时，都会检查键是否超时，如果超时则执行 del 命令删除键对象那个，之后 del 命令也会异步 发送给 从节点 需要注意的是为了保证复制的一致性，从节点自身永远不会主动删除超时数据， 定时删除： Redis 主节点在内部定时任务会循环采样一定数量的键，当发现采样的键过期就执行 del 命令，之后再同步给从节点 如果此时 数据的大量超时，主节点采样速度跟不上过期速度且主节点没有读取过期键的操作，那么从节点将无法收到 del 命令，这时在从节点 上可以读取到已经超时的数据。Redis 在 3.2 版本解决了这个问题，从节点 读取数据之前会检查键的过期时间来决定是否返回数据，可以升级到 3.2 版本来规避这个问题。 哨兵模式 该模式是从 Redis 的 2.6 版本开始提供的，但是当时这个版本的模式是不稳定的，直到 Redis 的 2.8 版本以后，这个哨兵模式才稳定下来，无论是主从模式，还是哨兵模式，这两个模式都有一个问题，不能水平扩容，并且这两个模式的高可用特性都会受到 Master 主节点内存的限制。 Sentinel(哨兵)进程是用于监控 redis 集群中 Master 主服务器工作的状态，在 Master 主服务器发生故障的时候，可以实现 Master 和 Slave 服务器的切换，保证系统的高可用。 Sentinel（哨兵）进程的作用 监控(Monitoring): 哨兵(sentinel) 会不断地检查你的 Master 和 Slave 是否运作正常。 提醒(Notification)：当被监控的某个 Redis 节点出现问题时, 哨兵(sentinel) 可以通过 API 向管理员或者其他应用程序发送通知。 自动故障迁移(Automatic failover)： 当一个 Master 不能正常工作时，哨兵(sentinel) 会开始一次自动故障迁移操作，它会将失效 Master 的其中一个 Slave 升级为新的 Master, 并让失效 Master 的其他 Slave 改为复制新的 Master；当客户端试图连接失效的 Master 时，集群也会向客户端返回新 Master 的地址，使得集群可以使用现在的 Master 替换失效 Master。 Master和Slave服务器切换后，Master的redis.conf、Slave的redis.conf和sentinel.conf的配置文件的内容都会发生相应的改变，即，Master主服务器的redis.conf配置文件中会多一行slaveof的配置，sentinel.conf的监控目标会随之调换。 Sentinel（哨兵）进程的工作方式 每个 Sentinel（哨兵）进程以每秒钟一次的频率向整个集群中的 Master 主服务器，Slave 从服务器以及其他 Sentinel（哨兵）进程发送一个 PING 命令。 如果一个实例（instance）距离最后一次有效回复 PING 命令的时间超过 down-after-milliseconds 选项所指定的值， 则这个实例会被 Sentinel（哨兵）进程标记为主观下线（SDOWN） 如果一个 Master 主服务器被标记为主观下线（SDOWN），则正在监视这个 Master 主服务器的所有 Sentinel（哨兵）进程要以每秒一次的频率确认 Master 主服务器的确进入了主观下线状态 当有足够数量的 Sentinel（哨兵）进程（大于等于配置文件指定的值）在指定的时间范围内确认 Master 主服务器进入了主观下线状态（SDOWN）， 则 Master 主服务器会被标记为客观下线（ODOWN） 在一般情况下， 每个 Sentinel（哨兵）进程会以每 10 秒一次的频率向集群中的所有 Master 主服务器、Slave 从服务器发送 INFO 命令。 当 Master 主服务器被 Sentinel（哨兵）进程标记为客观下线（ODOWN）时，Sentinel（哨兵）进程向下线的 Master 主服务器的所有 Slave 从服务器发送 INFO 命令的频率会从 10 秒一次改为每秒一次。 若没有足够数量的 Sentinel（哨兵）进程同意 Master 主服务器下线， Master 主服务器的客观下线状态就会被移除。若 Master 主服务器重新向 Sentinel（哨兵）进程发送 PING 命令返回有效回复，Master 主服务器的主观下线状态就会被移除。哨兵模式的优缺点 优点: 哨兵集群模式是基于主从模式的，所有主从的优点，哨兵模式同样具有。 主从可以切换，故障可以转移，系统可用性更好。 哨兵模式是主从模式的升级，系统更健壮，可用性更高。 缺点: Redis 较难支持在线扩容，在集群容量达到上限时在线扩容会变得很复杂。为避免这一问题，运维人员在系统上线时必须确保有足够的空间，这对资源造成了很大的浪费。 配置复杂 工作原理: 用户链接时先通过哨兵获取主机 Master 的信息 获取 Master 的链接后实现 redis 的操作(set&#x2F;get) 当 master 出现宕机时,哨兵的心跳检测发现主机长时间没有响应.这时哨兵会进行推选.推选出新的主机完成任务. 当新的主机出现时,其余的全部机器都充当该主机的从机 这就有一个问题，就是添加哨兵以后，所有的请求都会经过哨兵询问当前的主服务器是谁，所以如果哨兵部在主服务器上面的话可能会增加服务器的压力，所以最好是将哨兵单独放在一个服务器上面。以分解压力。 然后可能还有人担心哨兵服务器宕机了怎么办啊，首先哨兵服务器宕机的可能性很小，然后是如果哨兵服务器宕机了，使用人工干预重启即可，就会导致主从服务器监控的暂时不可用，不影响主从服务器的正常运行。 先配置服务器（本地）哨兵模式，直接从 redis 官网下载安装或者解压版，安装后的目录结构 然后配置哨兵模式 测试采用 3 个哨兵，1 个主 redis，2 个从 redis。 复制 6 份 redis.windows.conf 文件并重命名如下（开发者可根据自己的开发习惯进行重命名） 配置 master.6378.conf port:6379 #设置连接密码 requirepass:grs #连接密码 masterauth:grs slave.6380.conf 配置 port:6380 dbfilename dump6380.rdb #配置master slaveof 127.0.0.1 6379 slave.6381.conf 配置 port 6381 slaveof 127.0.0.1 6379 dbfilename \"dump.rdb\" 配置哨兵 sentinel.63791.conf（其他两个哨兵配置文件一致，只修改端口号码即可） port 63791 #主master，2个sentinel选举成功后才有效，这里的master-1是名称，在整合的时候需要一致，这里可以随便更改 sentinel monitor master-1 127.0.0.1 6379 2 #判断主master的挂机时间（毫秒），超时未返回正确信息后标记为sdown状态 sentinel down-after-milliseconds master-1 5000 #若sentinel在该配置值内未能完成failover操作（即故障时master/slave自动切换），则认为本次failover失败。 sentinel failover-timeout master-1 18000 #选项指定了在执行故障转移时， 最多可以有多少个从服务器同时对新的主服务器进行同步，这个数字越小，完成故障转移所需的时间就越长 sentinel config-epoch master-1 2 需要注意的地方 1、若通过 redis-cli -h 127.0.0.1 -p 6379 连接，无需改变配置文件，配置文件默认配置为 bind 127.0.0.1(只允许 127.0.0.1 连接访问）若通过 redis-cli -h 192.168.180.78 -p 6379 连接，需改变配置文件，配置信息为 bind 127.0.0.1 192.168.180.78（只允许 127.0.0.1 和 192.168.180.78 访问）或者将 bind 127.0.0.1 注释掉（允许所有远程访问） 2、masterauth 为所要连接的 master 服务器的 requirepass,如果一个 redis 集群中有一个 master 服务器，两个 slave 服务器，当 master 服务器挂掉时，sentinel 哨兵会随机选择一个 slave 服务器充当 master 服务器，鉴于这种机制，解决办法是将所有的主从服务器的 requirepass 和 masterauth 都设置为一样。 3、sentinel monitor master-1 127.0.0.1 6379 2 行尾最后的一个 2 代表什么意思呢？我们知道，网络是不可靠的，有时候一个 sentinel 会因为网络堵塞而误以为一个 master redis 已经死掉了，当 sentinel 集群式，解决这个问题的方法就变得很简单，只需要多个 sentinel 互相沟通来确认某个 master 是否真的死了，这个 2 代表，当集群中有 2 个 sentinel 认为 master 死了时，才能真正认为该 master 已经不可用了。（sentinel 集群中各个 sentinel 也有互相通信，通过 gossip 协议）。 依次启动 redis redis-server master.6379.conf redis-server slave.6380.conf redis-server slave.6381.conf redis-server sentinel.63791.conf –sentinel（linux:redis-sentinel sentinel.63791.conf）其他两个哨兵也这样启动 使用客户端查看一下 master 状态 查看一下哨兵状态 现在就可以在 master 插入数据，所有的 redis 服务都可以获取到，slave 只能读 整合 spring，导入依赖 &lt;dependency> &lt;groupId>redis.clients&lt;/groupId> &lt;artifactId>jedis&lt;/artifactId> &lt;version>2.8.0&lt;/version> &lt;/dependency> &lt;!-- spring-redis --> &lt;dependency> &lt;groupId>org.springframework.data&lt;/groupId> &lt;artifactId>spring-data-redis&lt;/artifactId> &lt;version>1.6.4.RELEASE&lt;/version> &lt;/dependency> &lt;dependency> &lt;groupId>org.apache.commons&lt;/groupId> &lt;artifactId>commons-pool2&lt;/artifactId> &lt;version>2.4.2&lt;/version> &lt;/dependency> redis.properties #redis中心 redis.host=127.0.0.1 #redis.host=10.75.202.11 redis.port=6379 redis.password= #redis.password=123456 redis.maxTotal=200 redis.maxIdle=100 redis.minIdle=8 redis.maxWaitMillis=100000 redis.maxActive=300 redis.testOnBorrow=true redis.testOnReturn=true #Idle时进行连接扫描 redis.testWhileIdle=true #表示idle object evitor两次扫描之间要sleep的毫秒数 redis.timeBetweenEvictionRunsMillis=30000 #表示idle object evitor每次扫描的最多的对象数 redis.numTestsPerEvictionRun=10 #表示一个对象至少停留在idle状态的最短时间，然后才能被idle object evitor扫描并驱逐；这一项只有在timeBetweenEvictionRunsMillis大于0时才有意义 redis.minEvictableIdleTimeMillis=60000 redis.timeout=100000 Cluster Redis3.0 版本之后支持 Cluster. redis cluster 的现状 目前 redis 支持的 cluster 特性： 1):节点自动发现 2):slave-&gt;master 选举,集群容错 3):Hot resharding:在线分片 4):进群管理:cluster xxx 5):基于配置(nodes-port.conf)的集群管理 6):ASK 转向&#x2F;MOVED 转向机制. redis cluster 架构 1)redis-cluster 架构图 在这个图中，每一个蓝色的圈都代表着一个 redis 的服务器节点。它们任何两个节点之间都是相互连通的。客户端可以与任何一个节点相连接，然后就可以访问集群中的任何一个节点。对其进行存取和其他操作。 架构细节: 所有的 redis 节点彼此互联(PING-PONG 机制),内部使用二进制协议优化传输速度和带宽. 节点的 fail 是通过集群中超过半数的节点检测失效时才生效. 客户端与 redis 节点直连,不需要中间 proxy 层.客户端不需要连接集群所有节点,连接集群中任何一个可用节点即可 redis-cluster 把所有的物理节点映射到[0-16383]slot 上,cluster 负责维护 node&lt;-&gt;slot&lt;-&gt;value redis-cluster 选举:容错 领着选举过程是集群中所有 master 参与,如果半数以上 master 节点与 master 节点通信超过(cluster-node-timeout),认为当前 master 节点挂掉. 什么时候整个集群不可用(cluster_state:fail),当集群不可用时,所有对集群的操作做都不可用，收到((error) CLUSTERDOWN The cluster is down)错误 如果集群任意 master 挂掉,且当前 master 没有 slave.集群进入 fail 状态,也可以理解成进群的 slot 映射[0-16383]不完成时进入 fail 状态. 如果进群超过半数以上 master 挂掉，无论是否有 slave 集群进入 fail 状态. 它们之间通过互相的 ping-pong 判断是否节点可以连接上。如果有一半以上的节点去 ping 一个节点的时候没有回应，集群就认为这个节点宕机了，然后去连接它的备用节点。 如果某个节点和所有从节点全部挂掉，我们集群就进入faill状态。还有就是如果有一半以上的主节点宕机，那么我们集群同样进入发力了状态。这就是我们的redis的投票机制 Redis 3.0 的集群方案有以下两个问题。 一个Redis实例具备了“数据存储”和“路由重定向”，完全去中心化的设计。 这带来的好处是部署非常简单，直接部署 Redis 就行，不像 Codis 有那么多的组件和依赖。但带来的问题是很难对业务进行无痛的升级，如果哪天 Redis 集群出了什么严重的 Bug，就只能回滚整个 Redis 集群。对协议进行了较大的修改，对应的 Redis 客户端也需要升级。升级 Redis 客户端后谁能确保没有 Bug？而且对于线上已经大规模运行的业务，升级代码中的 Redis 客户端也是一个很麻烦的事情。Redis Cluster 是 Redis 3.0 以后才正式推出，时间较晚，目前能证明在大规模生产环境下成功的案例还不是很多，需要时间检验。 Jedis sharding Redis Sharding 可以说是在 Redis cluster 出来之前业界普遍的采用方式，其主要思想是采用 hash 算法将存储数据的 key 进行 hash 散列，这样特定的 key 会被定为到特定的节点上。 庆幸的是，Java Redis 客户端驱动 Jedis 已支持 Redis Sharding 功能，即 ShardedJedis 以及结合缓存池的 ShardedJedisPool Jedis的Redis Sharding实现具有如下特点： 采用一致性哈希算法，将 key 和节点 name 同时 hashing，然后进行映射匹配，采用的算法是 MURMUR_HASH。采用一致性哈希而不是采用简单类似哈希求模映射的主要原因是当增加或减少节点时，不会产生由于重新匹配造成的 rehashing。一致性哈希只影响相邻节点 key 分配，影响量小。 为了避免一致性哈希只影响相邻节点造成节点分配压力，ShardedJedis 会对每个 Redis 节点根据名字(没有，Jedis 会赋予缺省名字)会虚拟化出 160 个虚拟节点进行散列。根据权重 weight，也可虚拟化出 160 倍数的虚拟节点。用虚拟节点做映射匹配，可以在增加或减少 Redis 节点时，key 在各 Redis 节点移动再分配更均匀，而不是只有相邻节点受影响。 ShardedJedis 支持 keyTagPattern 模式，即抽取 key 的一部分 keyTag 做 sharding，这样通过合理命名 key，可以将一组相关联的 key 放入同一个 Redis 节点，这在避免跨节点访问相关数据时很重要。 当然，Redis Sharding 这种轻量灵活方式必然在集群其它能力方面做出妥协。比如扩容，当想要增加 Redis 节点时，尽管采用一致性哈希，毕竟还是会有 key 匹配不到而丢失，这时需要键值迁移。作为轻量级客户端 sharding，处理 Redis 键值迁移是不现实的，这就要求应用层面允许 Redis 中数据丢失或从后端数据库重新加载数据。但有些时候，击穿缓存层，直接访问数据库层，会对系统访问造成很大压力。 利用中间件代理中间件的作用是将我们需要存入 redis 中的数据的 key 通过一套算法计算得出一个值。然后根据这个值找到对应的 redis 节点，将这些数据存在这个 redis 的节点中。 常用的中间件有这几种 Twemproxy Codis nginx 具体用法就不赘述了，可以自行百度。 总结 客户端分片（sharding）需要客户端维护分片算法，这是一种静态的分片方案，需要增加或者减少 Redis 实例的数量，需要手工调整分片的程序。 利用中间件的情况则会影响到 redis 的性能，具体看中间件而定，毕竟所有请求都要经过中间件一层过滤 官方提供方案 （Cluster），现时点成功案例不多。 Redis 分片Redis 的分片承担着两个主要目标： 允许使用很多电脑的内存总和来支持更大的数据库。没有分片，你就被局限于单机能支持的内存容量 允许伸缩计算能力到多核或多服务器，伸缩网络带宽到多服务器或多网络适配器范围分片的替代方案是哈希分片(hash partitioning)。这种模式适用于任何键哈希槽设置key–&gt;hashcode–&gt;16384 在 redis 的每一个节点上，都有这么两个东西 一个是插槽（slot）可以理解为是一个可以存储两个数值的一个变量这个变量的取值范围是：0-16383。 还有一个就是cluster我个人把这个cluster理解为是一个集群管理的插件。 当我们的存取的key到达的时候，redis会根据crc16的算法得出一个结果，然后把结果对 16384 求余数，这样每个 key 都会对应一个编号在 0-16383 之间的哈希槽，通过这个值，去找到对应的插槽所对应的节点，然后直接自动跳转到这个对应的节点上进行存取操作。 还有就是因为如果集群的话，是有好多个redis一起工作的，那么，就需要这个集群不是那么容易挂掉，所以呢，理论上就应该给集群中的每个节点至少一个备用的redis服务。这个备用的redis称为从节点（slave）。那么这个集群是如何判断是否有某个节点挂掉了呢？ 首先要说的是，每一个节点都存有这个集群所有主节点以及从节点的信息。 Redis 持久化RDB Redis Database，就是快照 snapshots。缺省情况情况下，Redis 把数据快照存放在磁盘上的二进制文件中，文件名为 dump.rdb。可以配置 Redis 的持久化策略，例如数据集中每 N 秒钟有超过 M 次更新，就将数据写入磁盘；或者你可以手工调用命令 SAVE 或 BGSAVE。 Redis 是使用 fork 函数复制一份当前进程(父进程)的副本(子进程) 子进程开始将数据写到临时 RDB 文件中 当子进程完成写 RDB 文件，用新文件替换老文件 这种方式可以使 Redis 使用 copy-on-write 技术。 AOF Append Only File。快照模式并不十分健壮，当系统停止或者无意中 Redis 被 kill 掉，最后写入 Redis 的数据就会丢失。这对某些应用也许不是大问题，但对于要求高可靠性的应用来说，Redis 就不是一个合适的选择。Append-only 文件模式是另一种选择。可以在配置文件中打开 AOF 模式Redis 中提供了 3 中同步策略，即每秒同步、每修改同步和不同步。事实上每秒同步也是异步完成的，其效率也是非常高的，所差的是一旦系统出现宕机现象，那么这一秒钟之内修改的数据将会丢失。 appendfsync always 每次有数据修改发生时都会写入 AOF 文件 appendfsync everysec 每秒钟同步一次，该策略为 AOF 的缺省策略。在性能和持久化方面作了很好的折中 appendfsync no 从不同步。高效但是数据不会被持久化。 虚拟内存方式 当 key 很小而 value 很大时，使用 VM 的效果会比较好，因为这样节约的内存比较大。当 key 不小时可以考虑使用一些非常方法将很大的 key 变成很大的 value，如可以考虑将 key&#x2F;value 组合成一个新 value.vm-max-threads 这个参数,可以设置访问 swap 文件的线程数,设置最好不要超过机器的核数,如果设置为 0,那么所有对 swap 文件的操作都是串行的.可能会造成比较长时间的延迟,但是对数据完整性有很好的保证.用虚拟内存性能也不错。如果数据量很大，可以考虑分布式或者其他数据库 redis.windows.confdaemonize no 默认情况下 redis 不是作为守护进程运行的，如果想让它在后台运行，就把它改成 yes。当 redis 作为守护进程运行的时候，它会写一个 pid 到&#x2F;var&#x2F;run&#x2F;redis.pid 文件里面 建议 更新频繁: 一致性要求比较高，AOF 策略为主 更新不频繁: 可以容忍少量数据丢失或错误，snapshot（快照）策略为主 Redis 事务redis 事务是通过 MULTI，EXEC，DISCARD 和 WATCH 四个原语实现的。 MULTI命令用于开启一个事务，它总是返回OK。 MULTI执行之后，客户端可以继续向服务器发送任意多条命令，这些命令不会立即被执行，而是被放到一个队列中，当EXEC命令被调用时，所有队列中的命令才会被执行。 另一方面，通过调用DISCARD，客户端可以清空事务队列，并放弃执行事务。 redis事务范围 从multi命令开始，到exec或者discard为止，整个操作过程是原子性的，不能打乱顺序，也不能插入操作 但是出错之前的操作会正常提交 WATCH 命令可以为 Redis 事务提供 check-and-set （CAS）行为。 被 WATCH 的键会被监视，并会发觉这些键是否被改动过了。 如果有至少一个被监视的键在 EXEC 执行之前 被修改了， 那么整个事务都会被取消， EXEC 返回空多条批量回复（null multi-bulk reply）来表示事务已 经失败。 使用 Redis 实现分布式锁1、向Redis中存放固定key的值，如果key不存在则实现存放并获取锁；如果key已经存在则不能获取锁 （依靠Redis中的原子操作进行CAS比对，实现锁的互斥） 2、获取key所对应的时间，时间是锁预期的实效时间，如果已经实效，则存储新值，并获取锁 3、否则获取锁失败 解锁： 删除指定key的redis列 抢购、秒杀是如今很常见的一个应用场景，主要需要解决的问题有两个： 高并发对数据库产生的压力 竞争状态下如何解决库存的正确减少（”超卖”问题） 对于第一个问题，已经很容易想到用缓存来处理抢购，避免直接操作数据库，例如使用 Redis。 Redis 使用 watch 完成秒杀抢购功能：使用 redis 中两个 key 完成秒杀抢购功能，mywatchkey 用于存储抢购数量和 mywatchlist 用户存储抢购列表。 优点： 1、首先选用内存数据库来抢购速度极快 2、速度快并发自然没不是问题 3、使用悲观锁，会迅速增加系统资源 4、比队列强的多，队列会使内存数据库资源瞬间爆棚 5、使用乐观锁，达到综合需求 与关系型数据库的区别数据 bai 存储方式不同。关系型和非关系型数据库的主要差异是数据存储的方式。关系型数据天然就是表格式的，因此存储在数据表的行和列中。数据表可以彼此关联协作存储，也很容易提取数据。 与其相反，非关系型数据不适合存储在数据表的行和列中，而是大块组合在一起。非关系型数据通常存储在数据集中，就像文档、键值对或者图结构。你的数据及其特性是选择数据存储和提取方式的首要影响因素。 扩展方式不同。SQL 和 NoSQL 数据库最大的差别可能是在扩展方式上，要支持日益增长的需求当然要扩展。 要支持更多并发量，SQL 数据库是纵向扩展，也就是说提高处理能力，使用速度更快速的计算机，这样处理相同的数据集就更快了。 因为数据存储在关系表中，操作的性能瓶颈可能涉及很多个表，这都需要通过提高计算机性能来客服。虽然 SQL 数据库有很大扩展空间，但最终肯定会达到纵向扩展的上限。而 NoSQL 数据库是横向扩展的。 而非关系型数据存储天然就是分布式的，NoSQL 数据库的扩展可以通过给资源池添加更多普通的数据库服务器(节点)来分担负载。 对事务性的支持不同。如果数据操作需要高事务性或者复杂数据查询需要控制执行计划，那么传统的 SQL 数据库从性能和稳定性方面考虑是你的最佳选择。SQL 数据库支持对事务原子性细粒度控制，并且易于回滚事务。 虽然 NoSQL 数据库也可以使用事务操作，但稳定性方面没法和关系型数据库比较，所以它们真正闪亮的价值是在操作的扩展性和大数据量处理方面。 关系型优点： 易于维护：都是使用表结构，格式一致； 使用方便：SQL 语言通用，可用于复杂查询； 复杂操作：支持 SQL，可用于一个表以及多个表之间非常复杂的查询。 缺点： 读写性能比较差，尤其是海量数据的高效率读写； 固定的表结构，灵活度稍欠； 高并发读写需求，传统关系型数据库来说，硬盘 I&#x2F;O 是一个很大的瓶颈。 非关系型非关系型数据库严格上不是一种数据库，应该是一种数据结构化存储方法的集合，可以是文档或者键值对等。 优点： 格式灵活：存储数据的格式可以是 key,value 形式、文档形式、图片形式等等，文档形式、图片形式等等，使用灵活，应用场景广泛，而关系型数据库则只支持基础类型。 速度快：nosql 可以使用硬盘或者随机存储器作为载体，而关系型数据库只能使用硬盘； 高扩展性； 成本低：nosql 数据库部署简单，基本都是开源软件。 缺点： 不提供 sql 支持，学习和使用成本较高； 无事务处理； 数据结构相对复杂，复杂查询方面稍欠。","categories":[{"name":"技术分享","slug":"技术分享","permalink":"https://docs.hehouhui.cn/categories/%E6%8A%80%E6%9C%AF%E5%88%86%E4%BA%AB/"}],"tags":[{"name":"微服务","slug":"微服务","permalink":"https://docs.hehouhui.cn/tags/%E5%BE%AE%E6%9C%8D%E5%8A%A1/"},{"name":"分布式","slug":"分布式","permalink":"https://docs.hehouhui.cn/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"},{"name":"Redis","slug":"Redis","permalink":"https://docs.hehouhui.cn/tags/Redis/"}]},{"title":"RestTemplate与OpenFeign","slug":"archives/RestTemplate与OpenFeign","date":"2021-08-27T00:00:00.000Z","updated":"2023-10-08T06:42:00.000Z","comments":true,"path":"archives/20.html","link":"","permalink":"https://docs.hehouhui.cn/archives/20.html","excerpt":"","text":"RestTemplate在SpringCloud体系中，我们知道服务之间的调用是通过http协议进行调用的。而注册中心的主要目的就是维护这些服务的服务列表。我们知道，在Spring中，提供了RestTemplate。RestTemplate是Spring提供的用于访问 Rest 服务的客户端。而在SpringCloud中也是使用此服务进行服务调用的。 同时在微服务中，一般上服务都不会进行单点部署的，都会至少部署 2 台及以上的。现在我们有了注册中心进行服务列表的维护，就需要一个客户端负载均衡来进行动态服务的调用。 所以开始示例前，我们先来大致了解下关于负载均衡和RestTemplate的相关知识点。其实后面实例的Ribbon和Feign最后的调用都是基于RestTemplate的。使用比较简单~ 何为负载均衡 负载均衡(Load Balance)是分布式系统架构设计中必须考虑的因素之一，它通常是指，将请求&#x2F;数据【均匀】分摊到多个操作单元上执行，负载均衡的关键在于【均匀】。 实现的方式实现负载均衡的方式有很多种，这里简单介绍下几种方式，并未过多深入。 注意：以下部分内容转至几种负载均衡技术的实现。 1.HTTP 重定向负载均衡 根据用户的 http 请求计算出一个真实的 web 服务器地址，并将该 web 服务器地址写入 http 重定向响应中返回给浏览器，由浏览器重新进行访问 优缺点：实现起来很简单，而缺点也显而易见了：请求两次才能完成一次访问；性能差;重定向服务器会成为瓶颈 2.DNS 域名解析负载均衡 在 DNS 服务器上配置多个域名对应 IP 的记录。例如一个域名www.baidu.com对应一组web服务器IP地址，域名解析时经过DNS服务器的算法将一个域名请求分配到合适的真实服务器上。 优缺点：加快访问速度,改善性能。同时由于 DNS 解析是多级解析，每一级 DNS 都可能化缓存记录 A，当某一服务器下线后，该服务器对应的 DNS 记录 A 可能仍然存在，导致分配到该服务器的用户访问失败，而且 DNS 负载均衡采用的是简单的轮询算法，不能区分服务器之间的差异，不能反映服务器当前运行状态。 3.反向代理负载均衡 反向代理处于 web 服务器这边，反向代理服务器提供负载均衡的功能，同时管理一组 web 服务器，它根据负载均衡算法将请求的浏览器访问转发到不同的 web 服务器处理，处理结果经过反向服务器返回给浏览器。 优缺点：实现简单，可利用反向代理缓存资源(这是最常用的了)及改善网站性能。同时因为是所有请求和响应的中转站，所以反向代理服务器可能成为瓶颈。 以上仅仅是部分实现方式，还有比如**IP负载均衡**、**数据链路层负载均衡**等等，这些可能涉及到相关网络方面的知识点了，不是很了解，大家有兴趣可以自行搜索下吧。 客户端和服务端的负载均衡实现负载均衡也又区分客户端和服务端之分，Ribbon就是基于客户端的负载均衡。客户端负载均衡： 服务端负载均衡： 服务端实现负载均衡方式有很多，比如：硬件F5、Nginx、HA Proxy等等，这些应该实施相关人员应该比较熟悉了，本人可能也就对Nginx了解下，⊙﹏⊙‖∣ RestTemplate 简单介绍 RestTemplate 是 Spring 提供的用于访问 Rest 服务的客户端，RestTemplate 提供了多种便捷访问远程 Http 服务的方法，能够大大提高客户端的编写效率。 简单来说，RestTemplate采用了模版设计的设计模式，将过程中与特定实现相关的部分委托给接口,而这个接口的不同实现定义了接口的不同行为,所以可以很容易的使用不同的第三方 http 服务，如okHttp、httpclient等。 RestTemplate定义了很多的与 REST 资源交互，这里简单介绍下一些常用的请求方式的使用。 exchange在 URL 上执行特定的 HTTP 方法，返回包含对象的ResponseEntity。其他的如GET、POST等方法底层都是基于此方法的。 如： get 请求 RequestEntity requestEntity = RequestEntity.get(new URI(uri)).build(); ResponseEntity&lt;User> responseEntity2 = this.restTemplate.exchange(requestEntity, User.class); post 请求 RequestEntity&lt;User> requestEntity = RequestEntity.post(new URI(uri)).body(user); ResponseEntity&lt;User> responseEntity2 = this.restTemplate.exchange(requestEntity, User.class); GET 请求 get 请求可以分为两类：getForEntity() 和 getForObject(). // 1-getForObject() User user1 = this.restTemplate.getForObject(uri, User.class); // 2-getForEntity() ResponseEntity&lt;User> responseEntity1 = this.restTemplate.getForEntity(uri, User.class); HttpStatus statusCode = responseEntity1.getStatusCode(); HttpHeaders header = responseEntity1.getHeaders(); User user2 = responseEntity1.getBody(); 其他的方法都大同小异了，可以根据实际的业务需求进行调用。 POST 请求 简单示例： // 1-postForObject() User user1 = this.restTemplate.postForObject(uri, user, User.class); // 2-postForEntity() ResponseEntity&lt;User> responseEntity1 = this.restTemplate.postForEntity(uri, user, User.class); 关于**postForLocation()**，用的比较少，作用是返回新创建资源的 URI，前面介绍的两者是返回资源本身，也就是结果集了。 关于其他的请求类型相关用法，这里就不详细阐述了，都是类似的。可以查看下此文章：详解 RestTemplate 操作，讲的蛮详细了。 *特别说明：系列教程为了方便，github 上分别创建了一个单体的Eureka注册中心和高可用的Eureka注册中心，无特殊说明，都是使用单体的Eureka注册中心进行服务注册与发现的，工程名为：spring-cloud-eureka-server，端口号为：1000。服务提供方工程名为：spring-cloud-eureka-client,应用名称为：eureka-client,端口号为：2000，提供了一个接口：http://127.0.0.1:2000/hello** spring-cloud-eureka-server 示例：spring-cloud-eureka-server spring-cloud-eureka-client 示例：spring-cloud-eureka-client LoadBalancerClient 实例此类是实现客户端负载均衡的关键。本身它是个接口类，位于spring-cloud-commons包下，此包包含了大量的服务治理相关的抽象接口，比如已经介绍过的DiscoveryClient、ServiceRegistry以及LoadBalancerClient实例等等。 首先，我们使用最原生的方式去获取调用服务接口。 创建个工程:**spring-cloud-eureka-consumer** 0.引入 pom 文件依赖。 &lt;!-- 客户端依赖 --> &lt;dependency> &lt;groupId>org.springframework.cloud&lt;/groupId> &lt;artifactId>spring-cloud-starter-netflix-eureka-client&lt;/artifactId> &lt;/dependency> &lt;dependency> &lt;groupId>org.springframework.boot&lt;/groupId> &lt;artifactId>spring-boot-starter-web&lt;/artifactId> &lt;/dependency> 1.配置文件添加相关注册中心等信息。 ## 服务名称 spring.application.name=eureka-consumer ## 端口号 server.port=8008 #指定注册中心地址 eureka.client.service-url.defaultZone=http://127.0.0.1:1000/eureka # 启用ip配置 这样在注册中心列表中看见的是以ip+端口呈现的 eureka.instance.prefer-ip-address=true # 实例名称 最后呈现地址：ip:2000 eureka.instance.instance-id=$&#123;spring.cloud.client.ip-address&#125;:$&#123;server.port&#125; 2.编写启动类，加入@EnableDiscoveryClient,申明为一个客户端应用,同时申明一个RestTemplate，最后是使用RestTemplate来完成 rest 服务调用的。 @SpringBootApplication @EnableDiscoveryClient @Slf4j public class EurekaConsumerApplication &#123; public static void main(String[] args) throws Exception &#123; SpringApplication.run(EurekaConsumerApplication.class, args); log.info(\"spring-cloud-eureka-consumer启动!\"); &#125; @Bean public RestTemplate restTemplate() &#123; return new RestTemplate(); &#125; &#125; 3.编写一个调用类，调用spring-cloud-eureka-client服务提供者提供的服务。 /** * 访问客户端示例 * @author Hehui * */ @RestController @Slf4j public class DemoController &#123; @Autowired LoadBalancerClient loadBalancerClient; @Autowired RestTemplate restTemplate; @GetMapping(\"/hello\") public String hello(String name) &#123; ServiceInstance serviceInstance = loadBalancerClient.choose(\"eureka-client\"); String url = \"http://\" + serviceInstance.getHost() + \":\" + serviceInstance.getPort() + \"/hello?name=\" + name; log.info(\"url地址为：&#123;&#125;\", url); return restTemplate.getForObject(url, String.class); &#125; &#125; 4.启动应用，访问：http://127.0.0.1:8008/hell0?name=oKong ，可以看见控制台输出了利用LoadBalancerClient的choose方法，获取到了对应eureka-client服务 ID 的服务地址。 最后通过范围对应的 http 地址进行服务请求： 最后浏览器上可以看见，进行了正确的访问了： 此时，切换到服务提供者 spring-cloud-eureka-client 控制台，可以看见日志输出： 此时我们已经调用成功了，通过LoadBalancerClient获取到了服务提供者实际服务地址，最后进行调用。 大家可以创建多个的spring-cloud-eureka-client服务提供者，再去调用下，可以看见会调用不同的服务地址的。 客户端负载均衡 Ribbon 实例 Spring Cloud Ribbon 是一个基于 Http 和 TCP 的客服端负载均衡工具，它是基于 Netflix Ribbon 实现的。与 Eureka 配合使用时，Ribbon 可自动从 Eureka Server (注册中心)获取服务提供者地址列表，并基于负载均衡算法，通过在客户端中配置 ribbonServerList 来设置服务端列表去轮询访问以达到均衡负载的作用。 上小节，简单的使用LoadBalancerClient进行了服务实例获取最后调用，也说了其实LoadBalancerClient是个接口类。而Ribbon实现了此接口，对应实现类为：RibbonLoadBalancerClient. Ribbon 实例现在我们来看下，使用Ribbon的方式如何进行更加优雅的方式进行服务调用。 创建一个工程：**spring-cloud-eureka-consumer-ribbon**(其实这个工程和spring-cloud-eureka-consumer是差不多的，只是有些许不同。) 0.加入 pom 依赖 &lt;!-- 客户端依赖 --> &lt;dependency> &lt;groupId>org.springframework.cloud&lt;/groupId> &lt;artifactId>spring-cloud-starter-netflix-eureka-client&lt;/artifactId> &lt;/dependency> &lt;dependency> &lt;groupId>org.springframework.boot&lt;/groupId> &lt;artifactId>spring-boot-starter-web&lt;/artifactId> &lt;/dependency> 1.配置文件修改，添加注册中心等相关信息。 spring.application.name=eureka-consumer-ribbon server.port=8018 #指定注册中心地址 eureka.client.service-url.defaultZone=http://127.0.0.1:1000/eureka # 启用ip配置 这样在注册中心列表中看见的是以ip+端口呈现的 eureka.instance.prefer-ip-address=true # 实例名称 最后呈现地址：ip:2000 eureka.instance.instance-id=$&#123;spring.cloud.client.ip-address&#125;:$&#123;server.port&#125; 2.编写启动类，加入@EnableDiscoveryClient，同时申明一个RestTemplate，这里和原先不同，就在于加入了**@LoadBalanced**注解进行修饰**RestTemplate**类，稍后会大致讲解下是如何进行实现的。 @SpringBootApplication @EnableDiscoveryClient @Slf4j public class EurekaConsumerRibbonApplication &#123; public static void main(String[] args) throws Exception &#123; SpringApplication.run(EurekaConsumerRibbonApplication.class, args); log.info(\"spring-cloud-eureka-consumer-ribbon启动!\"); &#125; //添加 @LoadBalanced 使其具备了使用LoadBalancerClient 进行负载均衡的能力 @Bean @LoadBalanced public RestTemplate restTemplage() &#123; return new RestTemplate(); &#125; &#125; 3.编写测试类，进行服务调用。 /** * ribbon访问客户端示例 * @author Hehui * */ @RestController @Slf4j public class DemoController &#123; @Autowired RestTemplate restTemplate; @GetMapping(\"/hello\") public String hello(String name) &#123; //直接使用服务名进行访问 log.info(\"请求参数name:&#123;&#125;\", name); return restTemplate.getForObject(\"&lt;http://eureka-client/hello?name=>\" + name, String.class); &#125; &#125; 可以看见，可以直接注入RestTemplate，通过服务名直接调用. 4.启动应用，访问:http://127.0.0.1:8018/hello?name=oKong ,可以看见调用成功： 控制台输出： 简单聊聊 LoadBalanced 注解 可以从以上示例中，可以看出，我们就加了一个@LoadBalanced 注解修饰 RestTemplatebean 类，就实现了服务的调用。现在来简单看看具体是如何实现的。 首先，我们看看此注解的代码说明： 从注释可以看出，该注解用来给 RestTemplate 做标记，以使用负载均衡的客户端LoadBalancerClient。 现在来看一眼相同包下的类的情况，可以看到有个LoadBalancerAutoConfiguration,字面意思可以知道这是一个自动配置类，此类就是我们要找的关键类了。 LoadBalancerAutoConfiguration,此类不长，一百来行，这里就不贴了。 简单说明下：首先，此类生效的条件是 @ConditionalOnClass(RestTemplate.class) @ConditionalOnBean(LoadBalancerClient.class) RestTemplate类必须存在于当前工程的环境中。 在 Spring 的 Bean 工程中有必须有LoadBalancerClient的实现 Bean。 该自动化配置类中，主要做了几件事情： 维护了一个被@LoadBalanced 注解修饰的 RestTemplate 对象列表 @LoadBalanced @Autowired(required = false) private List&lt;RestTemplate> restTemplates = Collections.emptyList(); 同时为其每个对象通过调用RestTemplateCustomizer添加了一个LoadBalancerInterceptor和RetryLoadBalancerInterceptor拦截器(有生效条件)，其为ClientHttpRequestInterceptor接口的实现类，ClientHttpRequestInterceptor是RestTemplate的请求拦截器 RetryLoadBalancerInterceptor 拦截器 LoadBalancerInterceptor 拦截器 我们主要看下LoadBalancerInterceptor： 可以看见，最后是实现了ClientHttpRequestInterceptor接口的实现类执行execute方法进行. 从继承关系里，此实现类就是RibbonLoadBalancerClient类了。 RibbonLoadBalancerClient 类： 简单来说：最后还是通过loadBalancerClient.choose()获取到服务实例，最通过拼凑 http 地址来进行最后的服务调用。 总体来说，就是通过为加入**@LoadBalanced**注解的**RestTemplate**添加一个请求拦截器，在请求前通过拦截器获取真正的请求地址，最后进行服务调用。 里面的细节就不阐述了，毕竟源码分析不是很在行呀，大家可以跟踪进去一探究竟吧。 友情提醒：若被**@LoadBalanced**注解的**RestTemplate**访问正常的服务地址，如**http://127.0.0.1:8080/hello**时，是会提示无法找到此服务的。 具体原因：serverid必须是我们访问的服务名称 ，当我们直接输入ip的时候获取的server是null，就会抛出异常。 此时，若是需要调用非注册中心的服务，可以创建一个不被**@LoadBalanced**注解的**RestTemplate**,同时指定 bean 的名称，使用时，使用**@Qualifier**指定 name 注入此**RestTemplate**。 @Bean(\"normalRestTemplage\") public RestTemplate normalRestTemplage() &#123; return new RestTemplate(); &#125; //使用 @Autowired @Qualifier(\"normalRestTemplage\") RestTemplate normalRestTemplate; @GetMapping(\"/ip\") public String ip(String name) &#123; //直接使用服务名进行访问 log.info(\"使用ip请求，请求参数name:&#123;&#125;\", name); return normalRestTemplate.getForObject(\"&lt;http://127.0.0.1:2000/hello?name=>\" + name, String.class); &#125; 负载均衡器目前还未进行过自定义负载均衡，这里就简单的举例下，上次整理 ppt 时有讲过一些，但未深入了解过 ⊙﹏⊙‖∣， 可以从继承关系看出，是通过继承IRule来实现的。 可继承 ClientConfigEnabledRoundRobinRule，来实现自己负载均衡策略。 声明式服务 Feign 实例从上一章节，我们知道，当我们要调用一个服务时，需要知道服务名和 api 地址，这样才能进行服务调用，服务少时，这样写觉得没有什么问题，但当服务一多，接口参数很多时，上面的写法就显得不够优雅了。所以，接下来，来说说一种更好更优雅的调用服务的方式：Feign。 Feign 是 Netflix 开发的声明式、模块化的 HTTP 客户端。Feign 可帮助我们更好更快的便捷、优雅地调用 HTTP API。 在Spring Cloud中，使用Feign非常简单——创建一个接口，并在接口上添加一些注解。Feign支持多种注释，例如 Feign 自带的注解或者 JAX-RS 注解等Spring Cloud 对 Feign 进行了增强，使 Feign 支持了 Spring MVC 注解，并整合了 Ribbon 和 Eureka,从而让 Feign 的使用更加方便。只需要通过创建接口并用注解来配置它既可完成对 Web 服务接口的绑定。 Feign 实例创建个**spring-cloud-eureka-consumer-ribbon**工程项目。 0.加入feigin依赖 &lt;!-- feign --> &lt;dependency> &lt;groupId>org.springframework.cloud&lt;/groupId> &lt;artifactId>spring-cloud-starter-openfeign&lt;/artifactId> &lt;/dependency> &lt;!-- eureka客户端依赖 --> &lt;dependency> &lt;groupId>org.springframework.cloud&lt;/groupId> &lt;artifactId>spring-cloud-starter-netflix-eureka-client&lt;/artifactId> &lt;/dependency> &lt;!-- rest api --> &lt;dependency> &lt;groupId>org.springframework.boot&lt;/groupId> &lt;artifactId>spring-boot-starter-web&lt;/artifactId> &lt;/dependency> 1.配置文件 spring.application.name=eureka-consumer-feign server.port=8028 #指定注册中心地址 eureka.client.service-url.defaultZone=http://127.0.0.1:1000/eureka # 启用ip配置 这样在注册中心列表中看见的是以ip+端口呈现的 eureka.instance.prefer-ip-address=true # 实例名称 最后呈现地址：ip:2000 eureka.instance.instance-id=$&#123;spring.cloud.client.ip-address&#125;:$&#123;server.port&#125; 2.创建启动类，加入注解@EnableFeignClients，开启feign支持。 @SpringBootApplication @EnableFeignClients @Slf4j public class EurekaConsumerFeignApplication &#123; public static void main(String[] args) throws Exception &#123; SpringApplication.run(EurekaConsumerFeignApplication.class, args); log.info(\"spring-cloud-eureka-consumer-feign启动\"); &#125; &#125; 3.创建一个接口类IHelloClient,加入注解@FeignClient来指定这个接口所要调用的服务名称。 @FeignClient(name=\"eureka-client\") public interface IHelloClient &#123; /** * 定义接口 * @param name * @return */ @RequestMapping(value=\"/hello\", method=RequestMethod.GET) public String hello(@RequestParam(\"name\") String name); &#125; 4.创建一个 demo 控制层，引入此接口类。 /** * feign 示例 * @author Hehui * */ @RestController @Slf4j public class DemoController &#123; @Autowired IHelloClient helloClient; @GetMapping(\"/hello\") public String hello(String name) &#123; log.info(\"使用feign调用服务，参数name:&#123;&#125;\", name); return helloClient.hello(name); &#125; &#125; 5.启动应用，访问：http://127.0.0.1:8028/hello?name=Hehui-feign 是不是很简单，和调用本地服务是一样的了！ Feign 继承特性 Feign 支持继承，但不支持多继承。使用继承，可将一些公共操作分组到一些父类接口中，从而简化 Feign 的开发。 所以在实际开发中，调用服务接口时，可直接按接口类和实现类进行编写，调用方引入接口依赖，继承一个本地接口，这样接口方法默认都是定义好的，也少了很多编码量。用起来就更爽了，就是有点依赖性，对方服务修改后需要同步更新下，但这个团队内部约定下问题不大的 这里简单实例下，创建一个spring-cloud-eureka-client-api工程。 0.加入依赖，注意此依赖的作用范围： &lt;!--api接口依赖--> &lt;dependency> &lt;groupId>org.springframework.boot&lt;/groupId> &lt;artifactId>spring-boot-starter-web&lt;/artifactId> &lt;scope>provided&lt;/scope> &lt;/dependency> 1.编写一个接口类IHellpApi： public interface IHelloApi &#123; //定义提供者服务名 public static final String SERVICE_NAME = \"eureka-client\"; /** * 定义接口 * @param name * @return */ @RequestMapping(value=\"/hello\", method=RequestMethod.GET) public String hello(@RequestParam(\"name\") String name); &#125; 修改**spring-cloud-eureka-client**工程 0.引入 api 依赖 &lt;!-- 导入接口依赖 --> &lt;dependency> &lt;groupId>cn.lqdev.learning&lt;/groupId> &lt;artifactId>spring-cloud-eureka-client-api&lt;/artifactId> &lt;version>0.0.1-SNAPSHOT&lt;/version> &lt;/dependency> 1.创建一个HelloApiImpl类，实现IHelloApi: /** * 使用接口方式进行接口编写 * @author Hehui * */ @RestController @Slf4j public class HelloApiImpl implements IHelloApi &#123; @Override public String helloApi(@RequestParam(\"name\") String name) &#123; log.info(\"[spring-cloud-eureka-client]服务[helloApi]被调用，参数name值为：&#123;&#125;\", name); return name + \",helloApi调用!\"; &#125; &#125; 此时，**HelloApiImpl**是个控制层也是个接口实现类了。 修改**spring-cloud-eureka-consumer-feign**工程。 0.引入 api 依赖 &lt;!-- 导入接口依赖 --> &lt;dependency> &lt;groupId>cn.lqdev.learning&lt;/groupId> &lt;artifactId>spring-cloud-eureka-client-api&lt;/artifactId> &lt;version>0.0.1-SNAPSHOT&lt;/version> &lt;/dependency> 1.同样创建一个接口，使其继承IHelloApi: /** * 直接继承接口 * @author Hehui * */ @FeignClient(name = IHelloApi.SERVICE_NAME) public interface HelloApi extends IHelloApi&#123; &#125; 小技巧：可以在**IHelloApi**定义一个服务名变量，如：SERVICE_NAME，这样让提供者进行变量的赋值，可以避免一些不必要的交流成本的，若有变化，服务调用方也无需关心的。一切都是约定编程！ 2.修改下DemoController类，注入HelloApi： @Autowired HelloApi helloApi; @GetMapping(\"hello2\") public String hello2(String name) &#123; log.info(\"使用feign继承方式调用服务，参数name:&#123;&#125;\", name); return helloApi.helloApi(name); &#125; 3.分别启动各服务，访问：http://127.0.0.1:8028/hello2?name=oKong-api 使用起来没啥差别的，一样的调用，但对于调用方而言，可以无需去理会具体细节了，照着接口方法去传参就好了。 这种方式，和原来的**dubbo**调用的方式是类似的，简单方便。大家可以把接口和实体放入一个包中，调用者和提供者都进行依赖即可。 注意事项在使用Feign时，会碰见一些问题，为了避免不必要的错误，以下这些需要额外注意下。 GET 请求多个参数时，需要使用@RequestParam GET 请求参数为实体时，会自动转换成 POST 请求 POST 请求使用@RequestBody 注解参数 不建议直接将@RequestMapping 注解在类上，直接写在方法上 参考资料 https://blog.csdn.net/mengdonghui123456/article/details/53981976 https://cloud.spring.io/spring-cloud-static/Finchley.SR1/single/spring-cloud.html#_spring_cloud_openfeign 总结 本章节主要讲解了下服务消费者如何利用原生、ribbon、fegin 三种方式进行服务调用的，其实每种调用方式都是使用 ribbon 来进行调用的，只是有些进行了增强，是的使用起来更简单高效而已。对于其原理的实现，本文未进行详细阐述，大家可以谷歌想相关知识，跟踪下源码了解下，本人也尚未深入研究过，还是停留在使用阶段，之后有时间了看一看，有啥心得再来分享吧。此时若服务上线下线，调用者调用可能会出现短暂的调用异常，最常见的就是找不到服务，此时服务容错保护就排上用场了，所以下一章节，就来说说关于服务容错保护相关知识点~ @LoadBalanced 注解与 RestTemplate在Spring Cloud微服务应用体系中，远程调用都应负载均衡。我们在使用RestTemplate作为远程调用客户端的时候，开启负载均衡极其简单：一个**@LoadBalanced**注解就搞定了。相信大家大都使用过Ribbon做Client 端的负载均衡，也许你有和我一样的感受：Ribbon 虽强大但不是特别的好用。我研究了一番，其实根源还是我们对它内部的原理不够了解，导致对一些现象无法给出合理解释，同时也影响了我们对它的定制和扩展。本文就针对此做出梳理，希望大家通过本文也能够对Ribbon有一个较为清晰的理解（本文只解释它@LoadBalanced这一小块内容）。 开启客户端负载均衡只需要一个注解即可，形如这样： @LoadBalanced // 标注此注解后，RestTemplate就具有了客户端负载均衡能力 @Bean public RestTemplate restTemplate()&#123; return new RestTemplate(); &#125; 说Spring是 Java 界最优秀、最杰出的重复发明轮子作品一点都不为过。本文就代领你一探究竟，为何开启RestTemplate的负载均衡如此简单。 说明：本文建立在你已经熟练使用 RestTemplate，并且了解 RestTemplate 它相关组件的原理的基础上分析。若对这部分还比较模糊，强行推荐你先参看我前面这篇文章：RestTemplate 的使用和原理你都烂熟于胸了吗？【享学 Spring MVC】 RibbonAutoConfiguration这是Spring Boot/Cloud启动Ribbon的入口自动配置类，需要先有个大概的了解： @Configuration // 类路径存在com.netflix.client.IClient、RestTemplate等时生效 @Conditional(RibbonAutoConfiguration.RibbonClassesConditions.class) // // 允许在单个类中使用多个@RibbonClient @RibbonClients // 若有Eureka，那就在Eureka配置好后再配置它~~~（如果是别的注册中心呢，ribbon还能玩吗？） @AutoConfigureAfter(name = \"org.springframework.cloud.netflix.eureka.EurekaClientAutoConfiguration\") @AutoConfigureBefore(&#123; LoadBalancerAutoConfiguration.class, AsyncLoadBalancerAutoConfiguration.class &#125;) // 加载配置：ribbon.eager-load --> true的话，那么项目启动的时候就会把Client初始化好，避免第一次惩罚 @EnableConfigurationProperties(&#123; RibbonEagerLoadProperties.class, ServerIntrospectorProperties.class &#125;) public class RibbonAutoConfiguration &#123; @Autowired private RibbonEagerLoadProperties ribbonEagerLoadProperties; // Ribbon的配置文件们~~~~~~~（复杂且重要） @Autowired(required = false) private List&lt;RibbonClientSpecification> configurations = new ArrayList&lt;>(); // 特征，FeaturesEndpoint这个端点(`/actuator/features`)会使用它org.springframework.cloud.client.actuator.HasFeatures @Bean public HasFeatures ribbonFeature() &#123; return HasFeatures.namedFeature(\"Ribbon\", Ribbon.class); &#125; // 它是最为重要的，是一个org.springframework.cloud.context.named.NamedContextFactory 此工厂用于创建命名的Spring容器 // 这里传入配置文件，每个不同命名空间就会创建一个新的容器（和Feign特别像） 设置当前容器为父容器 @Bean public SpringClientFactory springClientFactory() &#123; SpringClientFactory factory = new SpringClientFactory(); factory.setConfigurations(this.configurations); return factory; &#125; // 这个Bean是关键，若你没定义，就用系统默认提供的Client了~~~ // 内部使用和持有了SpringClientFactory。。。 @Bean @ConditionalOnMissingBean(LoadBalancerClient.class) public LoadBalancerClient loadBalancerClient() &#123; return new RibbonLoadBalancerClient(springClientFactory()); &#125; ... &#125; 这个配置类最重要的是完成了Ribbon相关组件的自动配置，有了LoadBalancerClient才能做负载均衡（这里使用的是它的唯一实现类RibbonLoadBalancerClient） @LoadBalanced注解本身及其简单（一个属性都木有）： // 所在包是org.springframework.cloud.client.loadbalancer // 能标注在字段、方法参数、方法上 // JavaDoc上说得很清楚：它只能标注在RestTemplate上才有效 @Target(&#123; ElementType.FIELD, ElementType.PARAMETER, ElementType.METHOD &#125;) @Retention(RetentionPolicy.RUNTIME) @Documented @Inherited @Qualifier public @interface LoadBalanced &#123; &#125; 它最大的特点：头上标注有@Qualifier注解，这是它生效的最重要因素之一，本文后半啦我花了大篇幅介绍它的生效时机。关于@LoadBalanced自动生效的配置，我们需要来到这个自动配置类：LoadBalancerAutoConfiguration LoadBalancerAutoConfiguration// Auto-configuration for Ribbon (client-side load balancing). // 它的负载均衡技术依赖于的是Ribbon组件~ // 它所在的包是：org.springframework.cloud.client.loadbalancer @Configuration @ConditionalOnClass(RestTemplate.class) //可见它只对RestTemplate生效 @ConditionalOnBean(LoadBalancerClient.class) // Spring容器内必须存在这个接口的Bean才会生效（参见：RibbonAutoConfiguration） @EnableConfigurationProperties(LoadBalancerRetryProperties.class) // retry的配置文件 public class LoadBalancerAutoConfiguration &#123; // 拿到容器内所有的标注有@LoadBalanced注解的Bean们 // 注意：必须标注有@LoadBalanced注解的才行 @LoadBalanced @Autowired(required = false) private List&lt;RestTemplate> restTemplates = Collections.emptyList(); // LoadBalancerRequestTransformer接口：允许使用者把request + ServiceInstance --> 改造一下 // Spring内部默认是没有提供任何实现类的（匿名的都木有） @Autowired(required = false) private List&lt;LoadBalancerRequestTransformer> transformers = Collections.emptyList(); // 配置一个匿名的SmartInitializingSingleton 此接口我们应该是熟悉的 // 它的afterSingletonsInstantiated()方法会在所有的单例Bean初始化完成之后，再调用一个一个的处理BeanName~ // 本处：使用配置好的所有的RestTemplateCustomizer定制器们，对所有的`RestTemplate`定制处理 // RestTemplateCustomizer下面有个lambda的实现。若调用者有需要可以书写然后扔进容器里既生效 // 这种定制器：若你项目中有多个RestTempalte，需要统一处理的话。写一个定制器是个不错的选择 // （比如统一要放置一个请求拦截器：输出日志之类的） @Bean public SmartInitializingSingleton loadBalancedRestTemplateInitializerDeprecated(final ObjectProvider&lt;List&lt;RestTemplateCustomizer>> restTemplateCustomizers) &#123; return () -> restTemplateCustomizers.ifAvailable(customizers -> &#123; for (RestTemplate restTemplate : LoadBalancerAutoConfiguration.this.restTemplates) &#123; for (RestTemplateCustomizer customizer : customizers) &#123; customizer.customize(restTemplate); &#125; &#125; &#125;); &#125; // 这个工厂用于createRequest()创建出一个LoadBalancerRequest // 这个请求里面是包含LoadBalancerClient以及HttpRequest request的 @Bean @ConditionalOnMissingBean public LoadBalancerRequestFactory loadBalancerRequestFactory(LoadBalancerClient loadBalancerClient) &#123; return new LoadBalancerRequestFactory(loadBalancerClient, this.transformers); &#125; // =========到目前为止还和负载均衡没啥关系========== // =========接下来的配置才和负载均衡有关（当然上面是基础项）========== // 若有Retry的包，就是另外一份配置，和这差不多~~ @Configuration @ConditionalOnMissingClass(\"org.springframework.retry.support.RetryTemplate\") static class LoadBalancerInterceptorConfig &#123;、 // 这个Bean的名称叫`loadBalancerClient`，我个人觉得叫`loadBalancerInterceptor`更合适吧（虽然ribbon是唯一实现） // 这里直接使用的是requestFactory和Client构建一个拦截器对象 // LoadBalancerInterceptor可是`ClientHttpRequestInterceptor`，它会介入到http.client里面去 // LoadBalancerInterceptor也是实现负载均衡的入口，下面详解 // Tips:这里可没有@ConditionalOnMissingBean哦~~~~ @Bean public LoadBalancerInterceptor ribbonInterceptor(LoadBalancerClient loadBalancerClient, LoadBalancerRequestFactory requestFactory) &#123; return new LoadBalancerInterceptor(loadBalancerClient, requestFactory); &#125; // 向容器内放入一个RestTemplateCustomizer 定制器 // 这个定制器的作用上面已经说了：在RestTemplate初始化完成后，应用此定制化器在**所有的实例上** // 这个匿名实现的逻辑超级简单：向所有的RestTemplate都塞入一个loadBalancerInterceptor 让其具备有负载均衡的能力 // Tips：此处有注解@ConditionalOnMissingBean。也就是说如果调用者自己定义过RestTemplateCustomizer类型的Bean，此处是不会执行的 // 请务必注意这点：容易让你的负载均衡不生效哦~~~~ @Bean @ConditionalOnMissingBean public RestTemplateCustomizer restTemplateCustomizer(final LoadBalancerInterceptor loadBalancerInterceptor) &#123; return restTemplate -> &#123; List&lt;ClientHttpRequestInterceptor> list = new ArrayList&lt;>(restTemplate.getInterceptors()); list.add(loadBalancerInterceptor); restTemplate.setInterceptors(list); &#125;; &#125; &#125; ... &#125; 这段配置代码稍微有点长，我把流程总结为如下几步： LoadBalancerAutoConfiguration要想生效类路径必须有RestTemplate，以及 Spring 容器内必须有LoadBalancerClient的实现 Bean\\1. LoadBalancerClient的唯一实现类是：org.springframework.cloud.netflix.ribbon.RibbonLoadBalancerClient LoadBalancerInterceptor是个ClientHttpRequestInterceptor客户端请求拦截器。它的作用是在客户端发起请求之前拦截，进而实现客户端的负载均衡 restTemplateCustomizer()返回的匿名定制器RestTemplateCustomizer它用来给所有的RestTemplate加上负载均衡拦截器（需要注意它的@ConditionalOnMissingBean注解~） 不难发现，负载均衡实现的核心就是一个拦截器，就是这个拦截器让一个普通的RestTemplate逆袭成为了一个具有负载均衡功能的请求器 LoadBalancerInterceptor该类唯一被使用的地方就是LoadBalancerAutoConfiguration里配置上去~ public class LoadBalancerInterceptor implements ClientHttpRequestInterceptor &#123; // 这个命名都不叫Client了，而叫loadBalancer~~~ private LoadBalancerClient loadBalancer; // 用于构建出一个Request private LoadBalancerRequestFactory requestFactory; ... // 省略构造函数（给这两个属性赋值） @Override public ClientHttpResponse intercept(final HttpRequest request, final byte[] body, final ClientHttpRequestExecution execution) throws IOException &#123; final URI originalUri = request.getURI(); String serviceName = originalUri.getHost(); Assert.state(serviceName != null, \"Request URI does not contain a valid hostname: \" + originalUri); return this.loadBalancer.execute(serviceName, this.requestFactory.createRequest(request, body, execution)); &#125; &#125; 此拦截器拦截请求后把它的serviceName委托给了LoadBalancerClient去执行，根据ServiceName可能对应 N 多个实际的Server，因此就可以从众多的 Server 中运用均衡算法，挑选出一个最为合适的Server做最终的请求（它持有真正的请求执行器ClientHttpRequestExecution）。 LoadBalancerClient请求被拦截后，最终都是委托给了LoadBalancerClient处理。 // 由使用负载平衡器选择要向其发送请求的服务器的类实现 public interface ServiceInstanceChooser &#123; // 从负载平衡器中为指定的服务选择Service服务实例。 // 也就是根据调用者传入的serviceId，负载均衡的选择出一个具体的实例出来 ServiceInstance choose(String serviceId); &#125; // 它自己定义了三个方法 public interface LoadBalancerClient extends ServiceInstanceChooser &#123; // 执行请求 &lt;T> T execute(String serviceId, LoadBalancerRequest&lt;T> request) throws IOException; &lt;T> T execute(String serviceId, ServiceInstance serviceInstance, LoadBalancerRequest&lt;T> request) throws IOException; // 重新构造url：把url中原来写的服务名 换掉 换成实际的 URI reconstructURI(ServiceInstance instance, URI original); &#125; 它只有一个实现类RibbonLoadBalancerClient（ServiceInstanceChooser是有多个实现类的~）。 RibbonLoadBalancerClient首先我们应当关注它的choose()方法： public class RibbonLoadBalancerClient implements LoadBalancerClient &#123; @Override public ServiceInstance choose(String serviceId) &#123; return choose(serviceId, null); &#125; // hint：你可以理解成分组。若指定了，只会在这个偏好的分组里面去均衡选择 // 得到一个Server后，使用RibbonServer把server适配起来~~~ // 这样一个实例就选好了~~~真正请求会落在这个实例上~ public ServiceInstance choose(String serviceId, Object hint) &#123; Server server = getServer(getLoadBalancer(serviceId), hint); if (server == null) &#123; return null; &#125; return new RibbonServer(serviceId, server, isSecure(server, serviceId), serverIntrospector(serviceId).getMetadata(server)); &#125; // 根据ServiceId去找到一个属于它的负载均衡器 protected ILoadBalancer getLoadBalancer(String serviceId) &#123; return this.clientFactory.getLoadBalancer(serviceId); &#125; &#125; choose方法：传入 serviceId，然后通过SpringClientFactory获取负载均衡器com.netflix.loadbalancer.ILoadBalancer，最终委托给它的chooseServer()方法选取到一个com.netflix.loadbalancer.Server实例，也就是说真正完成Server选取的是ILoadBalancer。 ILoadBalancer 以及它相关的类是一个较为庞大的体系，本文不做更多的展开，而是只聚焦在我们的流程上 LoadBalancerInterceptor执行的时候是直接委托执行的loadBalancer.execute()这个方法： RibbonLoadBalancerClient： // hint此处传值为null：一视同仁 // 说明：LoadBalancerRequest是通过LoadBalancerRequestFactory.createRequest(request, body, execution)创建出来的 // 它实现LoadBalancerRequest接口是用的一个匿名内部类，泛型类型是ClientHttpResponse // 因为最终执行的显然还是执行器：ClientHttpRequestExecution.execute() @Override public &lt;T> T execute(String serviceId, LoadBalancerRequest&lt;T> request) throws IOException &#123; return execute(serviceId, request, null); &#125; // public方法（非接口方法） public &lt;T> T execute(String serviceId, LoadBalancerRequest&lt;T> request, Object hint) throws IOException &#123; // 同上：拿到负载均衡器，然后拿到一个serverInstance实例 ILoadBalancer loadBalancer = getLoadBalancer(serviceId); Server server = getServer(loadBalancer, hint); if (server == null) &#123; // 若没找到就直接抛出异常。这里使用的是IllegalStateException这个异常 throw new IllegalStateException(\"No instances available for \" + serviceId); &#125; // 把Server适配为RibbonServer isSecure：客户端是否安全 // serverIntrospector内省 参考配置文件：ServerIntrospectorProperties RibbonServer ribbonServer = new RibbonServer(serviceId, server, isSecure(server, serviceId), serverIntrospector(serviceId).getMetadata(server)); //调用本类的重载接口方法~~~~~ return execute(serviceId, ribbonServer, request); &#125; // 接口方法：它的参数是ServiceInstance --> 已经确定了唯一的Server实例~~~ @Override public &lt;T> T execute(String serviceId, ServiceInstance serviceInstance, LoadBalancerRequest&lt;T> request) throws IOException &#123; // 拿到Server）（说白了，RibbonServer是execute时的唯一实现） Server server = null; if (serviceInstance instanceof RibbonServer) &#123; server = ((RibbonServer) serviceInstance).getServer(); &#125; if (server == null) &#123; throw new IllegalStateException(\"No instances available for \" + serviceId); &#125; // 说明：执行的上下文是和serviceId绑定的 RibbonLoadBalancerContext context = this.clientFactory.getLoadBalancerContext(serviceId); ... // 真正的向server发送请求，得到返回值 // 因为有拦截器，所以这里肯定说执行的是InterceptingRequestExecution.execute()方法 // so会调用ServiceRequestWrapper.getURI()，从而就会调用reconstructURI()方法 T returnVal = request.apply(serviceInstance); return returnVal; ... // 异常处理 &#125; returnVal是一个ClientHttpResponse，最后交给handleResponse()方法来处理异常情况（若存在的话），若无异常就交给提取器提值：responseExtractor.extractData(response)，这样整个请求就算全部完成了。 使用细节针对@LoadBalanced下的RestTemplate的使用，我总结如下细节供以参考： 传入的String类型的 url 必须是绝对路径（http://...），否则抛出异常：java.lang.IllegalArgumentException: URI is not absolute serviceId不区分大小写（http://user/...效果同http://USER/...） serviceId后请不要跟 port 端口号了~~~ 最后，需要特别指出的是：标注有@LoadBalanced的RestTemplate只能书写serviceId而不能再写IP地址/域名去发送请求了。若你的项目中两种 case 都有需要，请定义多个RestTemplate分别应对不同的使用场景~ 本地测试了解了它的执行流程后，若需要本地测试（不依赖于注册中心），可以这么来做： // 因为自动配置头上有@ConditionalOnMissingBean注解，所以自定义一个覆盖它的行为即可 // 此处复写它的getServer()方法，返回一个固定的（访问百度首页）即可，方便测试 @Bean public LoadBalancerClient loadBalancerClient(SpringClientFactory factory) &#123; return new RibbonLoadBalancerClient(factory) &#123; @Override protected Server getServer(ILoadBalancer loadBalancer, Object hint) &#123; return new Server(\"www.baidu.com\", 80); &#125; &#125;; &#125; 这么一来，下面这个访问结果就是百度首页的 html 内容喽。 @Test public void contextLoads() &#123; String obj = restTemplate.getForObject(\"&lt;http://my-serviceId>\", String.class); System.out.println(obj); &#125; 此处 my-serviceId 肯定是不存在的，但得益于我上面自定义配置的 LoadBalancerClient 什么，写死return一个Server实例不优雅？确实，总不能每次上线前还把这部分代码给注释掉吧，若有多个实例呢？还得自己写负载均衡算法吗？很显然Spring Cloud早早就为我们考虑到了这一点：脱离 Eureka 使用配置 listOfServers 进行客户端负载均衡调度（**&lt;clientName&gt;.&lt;nameSpace&gt;.listOfServers=&lt;comma delimited hostname:port strings&gt;**） 对于上例我只需要在主配置文件里这么配置一下： # ribbon.eureka.enabled=false # 若没用euraka，此配置可省略。否则不可以 my-serviceId.ribbon.listOfServers=www.baidu.com # 若有多个实例请用逗号分隔 效果完全同上。 Tips：这种配置法不需要是完整的绝对路径，http:&#x2F;&#x2F;是可以省略的（new Server()方式亦可） 自己添加一个记录请求日志的拦截器可行吗？显然是可行的，我给出示例如下： @LoadBalanced @Bean public RestTemplate restTemplate() &#123; RestTemplate restTemplate = new RestTemplate(); List&lt;ClientHttpRequestInterceptor> list = new ArrayList&lt;>(); list.add((request, body, execution) -> &#123; System.out.println(\"当前请求的URL是：\" + request.getURI().toString()); return execution.execute(request, body); &#125;); restTemplate.setInterceptors(list); return restTemplate; &#125; 这样每次客户端的请求都会打印这句话：当前请求的URI是：&lt;http://my-serviceId&gt;，一般情况（缺省情况）自定义的拦截器都会在负载均衡拦截器前面执行（因为它要执行最终的请求）。若你有必要定义多个拦截器且要控制顺序，可通过Ordered系列接口来实现~ 最后的最后，我抛出一个非常非常重要的问题： @LoadBalanced @Autowired(required = false) private List&lt;RestTemplate> restTemplates = Collections.emptyList(); @Autowired + @LoadBalanced能把你配置的RestTemplate自动注入进来拿来定制呢？？？核心原理是什么？ &gt; 提示：本原理内容属于Spring Framwork核心技术，建议深入思考而不囫囵吞枣。有疑问的可以给我留言，我也将会在下篇文章给出详细解答（建议先思考）推荐阅读RestTemplate 的使用和原理你都烂熟于胸了吗？【享学 Spring MVC】@Qualifier 高级应用—按类别批量依赖注入【享学 Spring】 -—————————————————– Ribbon 是如何通过一个@LoadBalanced 注解就实现负载均衡的原创绅士 jiejie 最后发布于 2019-11-08 15:09:04 阅读数 94 收藏 发布于 2019-11-06 16:14:45 分类专栏： Spring Cloud 版权声明：本文为博主原创文章，遵循 CC 4.0 BY-SA 版权协议，转载请附上原文出处链接和本声明。 本文链接：https://blog.csdn.net/weixin_38106322&#x2F;article&#x2F;details&#x2F;102937313 展开 一.介绍下测试用到的服务 从 Eureka 注册中心中可以可以看出有 EUREKA-CLIENT 和 RIBBON-CLIENT 的服务，其中 EUREKA-CLIENT 有两个节点作为服务提供者，而 RIBBON-CLIENT 则是服务消费者，通过 RestTemplate 来消费 EUREKA-CLIENT 的服务。 下面代码就是简单实现 Ribbon 负载均衡的配置类： @Configuration public class RibbonConfig &#123; @Bean @LoadBalanced RestTemplate getRestTemlate() &#123; return new RestTemplate(); &#125; &#125; 这样简单的通过一个@LoadBalanced 注解在 RestTemplate 上 ，在 RestTemplate 远程调用的时候，就会出现负载均衡的效果。 二.一步一步理清 Ribbon 负载均衡的逻辑 首先全局搜索@LoadBalanced 这个注解，发现在 LoadBalancerAutoConfiguration 类有用到该注解： @Configuration @ConditionalOnClass(RestTemplate.class) @ConditionalOnBean(LoadBalancerClient.class) @EnableConfigurationProperties(LoadBalancerRetryProperties.class) public class LoadBalancerAutoConfiguration &#123; /** * 这段代码的作用是将有用@LoadBalanced注解的RestTemplate注入 */ @LoadBalanced @Autowired(required = false) private List&lt;RestTemplate> restTemplates = Collections.emptyList(); &#125; 分析以上代码： 通过@Configuration 表明这是一个配置类 通过@ConditionalOnClass(RestTemplate.class)可以知道 RestTemplate 类要在类路径上存在才会实例化 LoadBalancerAutoConfiguration 通过@ConditionalOnBean(LoadBalancerClient.class)可以知道 LoadBalancerClient 类要存在才会实例化 LoadBalancerAutoConfiguration @EnableConfigurationProperties(LoadBalancerRetryProperties.class)是用来使用@ConfigurationProperties 注解的类 LoadBalancerRetryProperties 生效，贴上部分 LoadBalancerRetryProperties 类的代码，会更清晰： @ConfigurationProperties(\"spring.cloud.loadbalancer.retry\") public class LoadBalancerRetryProperties &#123; private boolean enabled = true; /** * Returns true if the load balancer should retry failed requests. * @return True if the load balancer should retry failed requests; false otherwise. */ public boolean isEnabled() &#123; return this.enabled; &#125; 所以重启下 RIBBON-CLIENT 服务，Debug 继续看 LoadBalancerAutoConfiguration 类的代码，发现在启动时会先进入 LoadBalancerAutoConfiguration 的 loadBalancerRequestFactory 方法，实例化出 LoadBalancerRequestFactory @Bean @ConditionalOnMissingBean public LoadBalancerRequestFactory loadBalancerRequestFactory( LoadBalancerClient loadBalancerClient) &#123; return new LoadBalancerRequestFactory(loadBalancerClient, this.transformers); &#125; 接下去断点进入 LoadBalancerAutoConfiguration 类中的静态内部类 LoadBalancerInterceptorConfig 的 ribbonInterceptor 方法，可以看出这是为了实例化出 LoadBalancerInterceptor 拦截器 @Configuration @ConditionalOnMissingClass(\"org.springframework.retry.support.RetryTemplate\") static class LoadBalancerInterceptorConfig &#123; @Bean public LoadBalancerInterceptor ribbonInterceptor( LoadBalancerClient loadBalancerClient, LoadBalancerRequestFactory requestFactory) &#123; return new LoadBalancerInterceptor(loadBalancerClient, requestFactory); &#125; 继续跟断点，进入了 loadBalancedRestTemplateInitializerDeprecated 方法，可以看出这个方法里主要的逻辑代码是 customizer.customize(restTemplate) @Bean public SmartInitializingSingleton loadBalancedRestTemplateInitializerDeprecated( final ObjectProvider&lt;List&lt;RestTemplateCustomizer>> restTemplateCustomizers) &#123; return () -> restTemplateCustomizers.ifAvailable(customizers -> &#123; for (RestTemplate restTemplate : LoadBalancerAutoConfiguration.this.restTemplates) &#123; for (RestTemplateCustomizer customizer : customizers) &#123; customizer.customize(restTemplate); &#125; &#125; &#125;); &#125; 继续 Debug,断点进入 LoadBalancerAutoConfiguration 类中的静态内部类 LoadBalancerInterceptorConfig： @Configuration @ConditionalOnMissingClass(\"org.springframework.retry.support.RetryTemplate\") static class LoadBalancerInterceptorConfig &#123; @Bean @ConditionalOnMissingBean public RestTemplateCustomizer restTemplateCustomizer( final LoadBalancerInterceptor loadBalancerInterceptor) &#123; return restTemplate -> &#123; List&lt;ClientHttpRequestInterceptor> list = new ArrayList&lt;>( restTemplate.getInterceptors()); list.add(loadBalancerInterceptor); restTemplate.setInterceptors(list); &#125;; &#125; &#125; 通过 list.add(loadBalancerInterceptor)和 restTemplate.setInterceptors(list)两段代码可以看出，这是要给 restTemplate 加上 loadBalancerInterceptor 拦截器。 那么接下来看看 loadBalancerInterceptor 拦截器里做了什么,通过页面发起一个 http 请求,断点进入到 LoadBalancerInterceptor 类的 intercept 方法， @Override public ClientHttpResponse intercept(final HttpRequest request, final byte[] body, final ClientHttpRequestExecution execution) throws IOException &#123; final URI originalUri = request.getURI(); String serviceName = originalUri.getHost(); Assert.state(serviceName != null, \"Request URI does not contain a valid hostname: \" + originalUri); return this.loadBalancer.execute(serviceName, this.requestFactory.createRequest(request, body, execution)); &#125; 截图看下信息： 可以看到该方法取得了 request 里的 url 和 servicName，然后将这些参数交给 loadBalancer.execute 去执行方法。而 loadBalancer 是 LoadBalancerClient 类的实例。看下 LoadBalancerClient 的类图，可以看到 LoadBalancerClient 继承了 ServiceInstanceChooser，LoadBalancerClient 的实现类是 RibbonLoadBalancerClient 逻辑继续，断点进入了 RibbonLoadBalancerClient 的 execute 方法 public &lt;T> T execute(String serviceId, LoadBalancerRequest&lt;T> request, Object hint) throws IOException &#123; ILoadBalancer loadBalancer = getLoadBalancer(serviceId); Server server = getServer(loadBalancer, hint); if (server == null) &#123; throw new IllegalStateException(\"No instances available for \" + serviceId); &#125; RibbonServer ribbonServer = new RibbonServer(serviceId, server, isSecure(server, serviceId), serverIntrospector(serviceId).getMetadata(server)); return execute(serviceId, ribbonServer, request); &#125; 跟着断点一步一步看方法： ILoadBalancer loadBalancer &#x3D; getLoadBalancer(serviceId); 经过这个方法，得到 loadBalancer，从截图里可以看到，loadBalancer 里有个 allServerList 集合，里面有两个对象，端口号分别是 8763 和 8762，这就是我们提供的服务节点。 ![20191107172116799.png](https://img-blog.csdnimg.cn/20191107172116799.png) Server server &#x3D; getServer(loadBalancer, hint) 从图里可以看出，通过这个 getServer 方法，会返回给我们一个当前可调用的服务节点，而至于怎么返回服务节点，会再写一篇分析，写完后会更新链接到该篇。 ![20191107172537248.png](https://img-blog.csdnimg.cn/20191107172537248.png) 生成 RibbonServer 作为参数传入 execute 方法 运行 execute 方法 接着跟进 execute 方法 可以看该方法里的关键执行方法是：T returnVal &#x3D; request.apply(serviceInstance);接着看 apply 方法，发现它是 LoadBalancerRequest 接口的方法，该接口却没有具体的实现类： public interface LoadBalancerRequest&lt;T> &#123; T apply(ServiceInstance instance) throws Exception; &#125; 思路回溯，是 request 对象调用的 apply 方法，而 request 其实是 execute 方法传进来的参数，追溯到源头，发现是 LoadBalancerInterceptor 类的 intercept 方法里 this.requestFactory.createRequest(request, body, execution)生成了 LoadBalancerRequest，然后作为参数传入，之后再调用了 apply 方法 @Override public ClientHttpResponse intercept(final HttpRequest request, final byte[] body, final ClientHttpRequestExecution execution) throws IOException &#123; final URI originalUri = request.getURI(); String serviceName = originalUri.getHost(); Assert.state(serviceName != null, \"Request URI does not contain a valid hostname: \" + originalUri); return this.loadBalancer.execute(serviceName, this.requestFactory.createRequest(request, body, execution)); &#125; 跟进 createRequest 方法里： 可以从图中看到，经过一些操作后，生成的 serviceRequest 对象里的 serviceId 是 eureka-client，也就是我们的服务节点名，而 server 是 localhost:8763，这是具体的服务节点 ip，之后作为参数调用 org.springframework.http.client 包下的 InterceptingClientHttpRequest 类中的 execute 方法断点进入该方法： 可以看出通过 requestFactory.createRequest(request.getURI(), method)方法生成了 ClientHttpRequest 类的实例 delegate，它的 url 就是我们最后真正要请求的，最后正常调用 delegate.execute()方法取得返回 ClientHttpResponse 就好了。 而这里产生了一个疑问，url 是怎么产生的？重新发起请求断点试下发现关键在 LoadBalancerRequestFactory 类中的 createRequest 方法中的这句： HttpRequest serviceRequest = new ServiceRequestWrapper(request, instance,his.loadBalancer); 跟进 ServiceRequestWrapper 类中，发现它继承了 HttpRequestWrapper 类，同时重写了 getURI 方法 public class ServiceRequestWrapper extends HttpRequestWrapper &#123; private final ServiceInstance instance; private final LoadBalancerClient loadBalancer; public ServiceRequestWrapper(HttpRequest request, ServiceInstance instance, LoadBalancerClient loadBalancer) &#123; super(request); this.instance = instance; this.loadBalancer = loadBalancer; &#125; @Override public URI getURI() &#123; URI uri = this.loadBalancer.reconstructURI(this.instance, getRequest().getURI()); return uri; &#125; &#125; 断点打在 getURI 方法里： 可以看到该方法返回了我们最后需要的 url。 最后，关于 Ribbon 是如何通过一个@LoadBalanced 注解就实现负载均衡的分析就到这了，还是有很多疏漏的地方，但是大致的逻辑就是这样的了，还有一些更深层的比如如何根据策略选出当前提供服务的节点等，留待后续补充，来日方长~","categories":[{"name":"技术分享","slug":"技术分享","permalink":"https://docs.hehouhui.cn/categories/%E6%8A%80%E6%9C%AF%E5%88%86%E4%BA%AB/"}],"tags":[{"name":"Spring","slug":"Spring","permalink":"https://docs.hehouhui.cn/tags/Spring/"},{"name":"微服务","slug":"微服务","permalink":"https://docs.hehouhui.cn/tags/%E5%BE%AE%E6%9C%8D%E5%8A%A1/"},{"name":"分布式","slug":"分布式","permalink":"https://docs.hehouhui.cn/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"},{"name":"Redis","slug":"Redis","permalink":"https://docs.hehouhui.cn/tags/Redis/"}]},{"title":"响应式开发之webFlux & Reactor","slug":"archives/响应式开发之webFlux & Reactor","date":"2021-04-10T00:00:00.000Z","updated":"2023-10-08T06:42:00.000Z","comments":true,"path":"archives/21.html","link":"","permalink":"https://docs.hehouhui.cn/archives/21.html","excerpt":"","text":"webFlux 初识LambdaLambda 表达式，有时候也称为匿名函数或箭头函数，几乎在当前的各种主流的编程语言中都有它的身影。Java8 中引入 Lambda 表达式，使原本需要用匿名类实现接口来传递行为，现在通过 Lambda 可以更直观的表达。 Lambda 表达式，也可称为闭包。闭包就是一个定义在函数内部的函数，闭包使得变量即使脱离了该函数的作用域范围也依然能被访问到。 Lambda 表达式的本质只是一个”语法糖”，由编译器推断并帮你转换包装为常规的代码,因此你可以使用更少的代码来实现同样的功能。 Lambda 表达式是一个匿名函数，即没有函数名的函数。有些函数如果只是临时一用，而且它的业务逻辑也很简单时，就没必要非给它取个名字不可。 Lambda 允许把函数作为一个方法的参数（函数作为参数传递进方法中）. Lambda 表达式语法如下：形参列表&#x3D;&gt;函数体（函数体多于一条语句的可用大括号括起）。在 Java 里就是**() -&gt; {}**： (参数) -> 表达式 或 (参数) ->&#123; 代码语句 &#125; Lambda 表达式的重要特征： Lambda 表达式主要用来定义行内执行的方法类型接口，例如，一个简单方法接口。 Lambda 表达式是通过函数式接口（必须有且仅有一个抽象方法声明）识别的 可选类型声明：不需要声明参数类型，编译器可以统一识别参数值。 可选的参数圆括号：一个参数无需定义圆括号，但多个参数需要定义圆括号。 可选的大括号：如果主体包含了一个语句，就不需要使用大括号。 可选的返回关键字：如果主体只有一个表达式返回值，则编译器会自动返回值，大括号需要指定表达式返回一个值。 Lambda 表达式中的变量作用域： 访问权限与匿名对象的方式非常类似。只能够访问局部对应的外部区域的局部 final 变量，以及成员变量和静态变量。 在 Lambda 表达式中能访问域外的局部非 final 变量、但不能修改 Lambda 域外的局部非 final 变量。因为在 Lambda 表达式中，Lambda 域外的局部非 final 变量会在编译的时候，会被隐式地当做 final 变量来处理。 Lambda 表达式内部无法访问接口默认（default）方法 例子：使用 Java 8 之前的方法来实现对一个列表进行排序： List&lt;String> names = Arrays.asList(\"aaa\", \"cccc\", \"ddd\", \"bbb\"); Collections.sort(names, new Comparator&lt;String>() &#123; @Override public int compare(String a, String b) &#123; return b.compareTo(a); &#125; &#125;); Java 8 Lambda 表达式： Collections.sort(names, (String a, String b) -> &#123; return b.compareTo(a); &#125;); // 只有一条逻辑语句，可以省略大括号 Collections.sort(names, (String a, String b) -> b.compareTo(a)); // 可以省略入参类型 Collections.sort(names, (a, b) -> b.compareTo(a)); 类型推断通常 Lambda 表达式的参数并不需要显示声明类型。那么对于给定的 Lambda 表达式，程序如何知道对应的是哪个函数接口以及参数的类型呢？编译器通过 Lambda 表达式所在的上下文来进行目标类型推断，通过检查 Lambda 表达式的入参类型及返回类型，和对应的目标类型的方法签名是否一致，推导出合适的函数接口。比如： Stream.of(\"我是字符串A\", \"我是字符串B\").map(s -> s.length()).filter(l -> l == 3); 在上面的例子中，对于传入 map 方法的 Lamda 表达式，从 Stream 的类型上下文可以推导出入参是 String 类型，从函数的返回值可以推导出出参是整形类型，因此可推导出对应的函数接口类型为 Function；对于传入 filter 方法的 Lamda 表达式，从 pipeline 的上下文可得知入参是整形类型，因此可推导出函数接口 Predicate。 方法引用Java 8 中还可以通过方法引用来表示 Lambda 表达式。方法引用是用来直接访问类或者实例的已经存在的方法或者构造方法。Java 8 允许你通过”::“关键字获取方法或者构造函数的引用。方法引用提供了一种引用而不执行方法的方式，它需要由兼容的函数式接口构成目标类型上下文。计算时，方法引用会创建函数式接口的一个实例。常用的方法引用有： 静态方法引用：ClassName::methodName 实例对象上的方法引用：instanceReference::methodName 类上的方法引用：ClassName::methodName 构造方法引用：Class::new 数组构造方法引用：TypeName[]::new 例子： // 静态方法引用 Stream.of(someStringArray).allMatch(StringUtils::isNotEmpty); // 实例对象上的方法引用 Stream.of(someStringArray).map(this::someTransform); // 类上的方法引用 Stream.of(someStringArray).mapToInt(String::length); // 构造方法引用 Stream.of(someStringArray).collect(Collectors.toCollection(LinkedList::new)); // 数组构造方法引用 Stream.of(someStringArray).toArray(String[]::new); 函数式接口Java 8 中采用函数式接口作为Lambda 表达式的目标类型。函数式接口(Functional Interface)**是一个**有且仅有一个抽象方法声明的接口。任意只包含一个抽象方法的接口，我们都可以用来做成 Lambda 表达式。每个与之对应的 lambda 表达式必须要与抽象方法的声明相匹配。函数式接口与其他普通接口的区别： 函数式接口中只能有一个抽象方法（这里不包括与 Object 的方法重名的方法） 接口中唯一抽象方法的命名并不重要，因为函数式接口就是对某一行为进行抽象，主要目的就是支持 Lambda 表达式 自定义函数式接口时，应当在接口前加上**@FunctionalInterface**标注（虽然不加也不会有错误）。编译器会注意到这个标注，如果你的接口中定义了第二个抽象方法的话，编译器会抛出异常。 函数式编程 Java 来讲，从命令式编程到函数式编程的关键转变是从 Java8 多了一个 funtcion 包开始，在此基础上的 stream 更好的诠释了这一点，而之后 java 9 的 reactor，再到 spring5 的 webflux 都是在其基础上一步步演变的 java.util.function Function&lt;T, R> stringIntegerFunction //输入T返回R的函数 Predicate&lt;T> predicate //输入T，返回boolean值，断言（谓词）函数 Consumer&lt;T> consumer; //消费者函数，消费一个数据 Supplier&lt;T> supplier; // 生产者函数，提供数据 Function /** * 将范型T对象应用到输入的参数上，然后返回计算结果 * * @param t the function argument * @return the function result */ R apply(T t); /** * 返回一个先执行before函数对象apply方法再执行当前函数对象apply方法的函数对象 * * @param &lt;V> 前置函数的的输入类型，以及函数的输入类型 由函数 * */ default &lt;V> Function&lt;V, R> compose(Function&lt;? super V, ? extends T> before) &#123; Objects.requireNonNull(before); return (V v) -> apply(before.apply(v)); &#125; /** * 返回一个先执行当前函数对象apply方法再执行after函数对象apply方法的函数对象。 * &lt;br> * compose 和 andThen 的不同之处是函数执行的顺序不同。compose 函数先执行参数， * 然后执行调用者，而 andThen 先执行调用者，然后再执行参数。 * &lt;/br> */ default &lt;V> Function&lt;T, V> andThen(Function&lt;? super R, ? extends V> after) &#123; Objects.requireNonNull(after); return (T t) -> after.apply(apply(t)); &#125; /** * 返回输入结果 */ static &lt;T> Function&lt;T, T> identity() &#123; return t -> t; &#125; 标注为 FunctionalInterface 的接口被称为函数式接口，该接口只能有一个自定义方法，但是可以包括从 object 类继承而来的方法。如果一个接口只有一个方法，则编译器会认为这就是一个函数式接口。 是否是一个函数式接口，需要注意的有以下几点： 该注解只能标记在”有且仅有一个抽象方法”的接口上。 JDK8 接口中的静态方法和默认方法，都不算是抽象方法。 接口默认继承 java.lang.Object，所以如果接口显示声明覆盖了 Object 中方法，那么也不算抽象方法。 该注解不是必须的，如果一个接口符合”函数式接口”定义，那么加不加该注解都没有影响。加上该注解能够更好地让编译器进行检查。如果编写的不是函数式接口，但是加上了@FunctionInterface，那么编译器会报错。 在一个接口中定义两个自定义的方法，就会产生 Invalid ‘@FunctionalInterface’ annotation; FunctionalInterfaceTest is not a functional interface 错误. 响应式 响应式流(Reactive Streams)通过定义一组实体，接口和互操作方法，给出了实现异步非阻塞背压的标准。第三方遵循这个标准来实现具体的解决方案，常见的有 Reactor，RxJava，Akka Streams，Ratpack 等。 响应式编程（reactive programming）是一种基于数据流（data stream）和变化传递（propagation of change）的声明式（declarative）的编程范式 一个通用的流处理架构一般会是这样的（生产者产生数据，对数据进行中间处理，消费者拿到数据消费) 数据来源，一般称为生产者（Producer） 数据的目的地，一般称为消费者(Consumer) 在处理时，对数据执行某些操作一个或多个处理阶段。（Processor) 规范定义了 4 个接口 在响应式流上提到了 back pressure（背压）这么一个概念。在响应式流实现异步非阻塞是基于生产者和消费者模式的，而生产者消费者很容易出现的一个问题就是：生产者生产数据多了，就把消费者给压垮了 通俗就是： 消费者能告诉生产者自己需要多少量的数据。这里就是Subscription接口所做的事 特质 原文:https://www.reactivemanifesto.org/zh-CN 即时响应性: ：只要有可能， 系统就会及时地做出响应。 即时响应是可用性和实用性的基石， 而更加重要的是，即时响应意味着可以快速地检测到问题并且有效地对其进行处理。 即时响应的系统专注于提供快速而一致的响应时间， 确立可靠的反馈上限， 以提供一致的服务质量。 这种一致的行为转而将简化错误处理、 建立最终用户的信任并促使用户与系统作进一步的互动。 *回弹性：**系统在出现失败时依然保持即时响应性。 这不仅适用于高可用的、 任务关键型系统——任何不具备回弹性的系统都将会在发生失败之后丢失即时响应性。 回弹性是通过复制、 遏制、 隔离以及委托来实现的。 失败的扩散被遏制在了每个组件内部， 与其他组件相互隔离， 从而确保系统某部分的失败不会危及整个系统，并能独立恢复。 每个组件的恢复都被委托给了另一个（外部的）组件， 此外，在必要时可以通过复制来保证高可用性。 （因此）组件的客户端不再承担组件失败的处理。 弹性： 系统在不断变化的工作负载之下依然保持即时响应性。 反应式系统可以对输入（负载）的速率变化做出反应，比如通过增加或者减少被分配用于服务这些输入（负载）的资源。 这意味着设计上并没有争用点和中央瓶颈， 得以进行组件的分片或者复制， 并在它们之间分布输入（负载）。 通过提供相关的实时性能指标， 反应式系统能支持预测式以及反应式的伸缩算法。 这些系统可以在常规的硬件以及软件平台上实现成本高效的弹性。 *消息驱动：**反应式系统依赖异步的消息传递，从而确保了松耦合、隔离、位置透明的组件之间有着明确边界。 这一边界还提供了将失败作为消息委托出去的手段。 使用显式的消息传递，可以通过在系统中塑造并监视消息流队列， 并在必要时应用回压， 从而实现负载管理、 弹性以及流量控制。 使用位置透明的消息传递作为通信的手段， 使得跨集群或者在单个主机中使用相同的结构成分和语义来管理失败成为了可能。 非阻塞的通信使得接收者可以只在活动时才消耗资源， 从而减少系统开销。 大型系统由多个较小型的系统所构成， 因此整体效用取决于它们的构成部分的反应式属性。 这意味着， 反应式系统应用着一些设计原则，使这些属性能在所有级别的规模上生效，而且可组合。 Reactive官网 https://projectreactor.io/docs/core/release/reference/index.html#which.windowprojectreactor.io 在 reactor 中有两个最基本的概念，发布者和订阅者，可以理解为生产者和消费者的概念。在 Reactor 中发布者有两个，一个是Flux，一个是Mono。 Flux 代表的是 0-N 个元素的响应式序列，而 Mono 代表的是 0-1 个的元素的结果。 在 Reactive 中 Publisher（发布者)相当于生产者(Producer) Subscriber(订阅者)相当于消费者(Consumer) Processor 就是在发布者与订阅者之间处理数据用的 // 发布者(生产者)public interface Publisher&lt;T> &#123; // 可以被订阅多次，每次生成新的Subscriber，每个消费者只能订阅一次Publisher，执行过程出错会直接报error public void subscribe(Subscriber&lt;? super T> s);&#125;// 订阅者(消费者)public interface Subscriber&lt;T> &#123; //该方法在订阅Publisher之后执行，在订阅之前不会有数据流的消费 public void onSubscribe(Subscription s); /** * 消费下一个消息，在执行request方法之后通知Publisher， *可被调用多次，有request（x），参数x决定执行几次 */ public void onNext(T t); //执行出错调用方法 public void onError(Throwable t); //执行完成之后调用方法 public void onComplete();&#125;// 用于发布者与订阅者之间的通信(实现背压：订阅者能够告诉生产者需要多少数据)public interface Subscription &#123; //消费请求 public void request(long n); //取消请求 public void cancel();&#125;// 用于处理发布者 发布消息后，对消息进行处理，再交由消费者消费public interface Processor&lt;T,R> extends Subscriber&lt;T>, Publisher&lt;R> &#123;&#125; Mono (返回 0 或 1 个元素)Mono 是响应流 Publisher 具有基础 rx 操作符。可以成功发布元素或者错误。如图所示： 常用方法Mono.create(); //：使用 MonoSink 来创建 MonoMono.justOrEmpty(); //：从一个 Optional 对象或 null 对象中创建 Mono。 只有 Optional 对象中包含值或对象不为 null 时，Mono 序列才产生对应的元素。Mono.error(); //：创建一个只包含错误消息的 MonoMono.never(); //：创建一个不包含任何消息通知的 MonoMono.delay(); //：在指定的延迟时间之后，创建一个 Mono，产生数字 0 作为唯一值Mono.just(); //创建一个不为null的数据流 声明的参数就是数据流的元素 创建出来的 Mono序列在发布这些元素之后会自动结束/**注释同下*/Mono.fromCallable(); // 从回调函数生产数据 CallableMono.fromCompletionStage(); //异步任务中 CompletionStage Mono.fromFuture(); //异步任务中 CompletableFutureMono.fromRunnable(); // 异步任务 RunnableMono.fromSupplier()：//Supplier 提供着 *Flux **(返回 0-n 个元素)Flux 是响应流 Publisher 具有基础 rx 操作符。可以成功发布 0 到 N 个元素或者错误。Flux 其实是 Mono 的一个补充。如图所示： 所以要注意：如果知道 Publisher 是 0 或 1 个，则用 Mono。 Flux 最值得一提的是 fromIterable 方法。 fromIterable(Iterable&lt;? extends T&gt; it) 可以发布 Iterable 类型的元素。 当调用 just 方法，查看源码可以得知，返回的是一个 Flux对象，当次数为 0 直接返回空，为 1 的时候单独处理，其余的通过onAssembly 方法处理返回 fluxarrary 对象。 /** * Create a &#123;@link Flux&#125; that emits the items contained in the provided array. * &lt;p> * &lt;img class=\"marble\" src=\"doc-files/marbles/fromArray.svg\" alt=\"\"> * * @param array the array to read data from * @param &lt;T> The type of values in the source array and resulting Flux * * @return a new &#123;@link Flux&#125; */ public static &lt;T> Flux&lt;T> fromArray(T[] array) &#123; if (array.length == 0) &#123; return empty(); &#125; if (array.length == 1) &#123; return just(array[0]); &#125; return onAssembly(new FluxArray&lt;>(array)); &#125;final class FluxArray&lt;T> extends Flux&lt;T> implements Fuseable, SourceProducer&lt;T> &#123; final T[] array; //存储数据 @SafeVarargs public FluxArray(T... array) &#123; this.array = Objects.requireNonNull(array, \"array\"); &#125; @SuppressWarnings(\"unchecked\") //订阅方法 public static &lt;T> void subscribe(CoreSubscriber&lt;? super T> s, T[] array) &#123; if (array.length == 0) &#123; Operators.complete(s); return; &#125; if (s instanceof ConditionalSubscriber) &#123; // 此处是个啥？ s.onSubscribe(new ArrayConditionalSubscription&lt;>((ConditionalSubscriber&lt;? super T>) s, array)); &#125; else &#123; s.onSubscribe(new ArraySubscription&lt;>(s, array)); &#125; &#125; // 正常消费者 static final class ArraySubscription&lt;T> implements InnerProducer&lt;T>, SynchronousSubscription&lt;T> &#123; final CoreSubscriber&lt;? super T> actual; final T[] array; //存储数据 int index; volatile boolean cancelled; //记录是否取消 volatile long requested; //记录请求多少次 @SuppressWarnings(\"rawtypes\") static final AtomicLongFieldUpdater&lt;ArraySubscription> REQUESTED = AtomicLongFieldUpdater.newUpdater(ArraySubscription.class, \"requested\"); ArraySubscription(CoreSubscriber&lt;? super T> actual, T[] array) &#123; this.actual = actual; this.array = array; &#125; @Override public void request(long n) &#123; if (Operators.validate(n)) &#123; if (Operators.addCap(REQUESTED, this, n) == 0) &#123; if (n == Long.MAX_VALUE) &#123; fastPath(); &#125; else &#123; slowPath(n); &#125; &#125; &#125; &#125; 流程 内置的 ProcessorProcessor既是一种特别的发布者（Publisher）又是一种订阅者（Subscriber）。 所以你能够订阅一个Processor，也可以调用它们提供的方法来手动插入数据到序列，或终止序列。 一直在聊响应式流的四个接口中的三个：Publisher、Subscriber、Subscription，唯独 Processor 迟迟没有提及。原因在于想用好它们不太容易，多数情况下，我们应该进行避免使用 Processor，通常来说仅用于一些特殊场景。 Reactor Core 内置多种 Processor。这些 processor 具有不同的语法，大概分为三类。 直接的（direct）（DirectProcessor 和 UnicastProcessor）：这些 processors 只能通过直接 调用 Sink 的方法来推送数据。 同步的（synchronous）（EmitterProcessor 和 ReplayProcessor）：这些 processors 既可以直接调用 Sink 方法来推送数据，也可以通过订阅到一个上游的发布者来同步地产生数据。 异步的（asynchronous）（WorkQueueProcessor 和 TopicProcessor）：这些 processors 可以将从多个上游发布者得到的数据推送下去。由于使用了 RingBuffer 的数据结构来缓存多个来自上游的数据，因此更加有健壮性。 异步的 processor 在实例化的时候最复杂，因为有许多不同的选项。因此它们暴露出一个 Builder 接口。 而简单的 processors 有静态的工厂方法。 DirectProcessor `DirectProcessor` 可以将信号分发给零到多个订阅者（Subscriber）。它是最容易实例化的，使用静态方法 create() 即可。另一方面，它的不足是无法处理背压。所以，当DirectProcessor推送的是 N 个元素，而至少有一个订阅者的请求个数少于 N 的时候，就会发出一个IllegalStateException。 一旦 Processor 结束（通常通过调用它的 Sink 的 error(Throwable) 或 complete() 方法）， 虽然它允许更多的订阅者订阅它，但是会立即向它们重新发送终止信号。 UnicastProcessor `UnicastProcessor`可以使用一个内置的缓存来处理背压。代价就是它最多只能有一个订阅者（上一节的例子通过publish转换成了ConnectableFlux，所以可以接入两个订阅者）。 UnicastProcessor 有多种选项，因此提供多种不同的 create 静态方法。例如，它默认是 无限的（unbounded） ：如果你在在订阅者还没有请求数据的情况下让它推送数据，它会缓存所有数据。 可以通过提供一个自定义的 Queue 的具体实现传递给 create 工厂方法来改变默认行为。如果给出的队列是有限的（bounded）， 并且缓存已满，而且未收到下游的请求，processor 会拒绝推送数据。 在“有限的”队列中，还可以在构造 processor 的时候提供一个回调方法，这个回调方法可以在每一个 被拒绝推送的元素上调用，从而让开发者有机会清理这些元素。 EmitterProcessor`EmitterProcessor`能够向多个订阅者发送数据，并且可以对每一个订阅者进行背压处理。它本身也可以订阅一个发布者并同步获得数据。 最初如果没有订阅者，它仍然允许推送一些数据到缓存，缓存大小由bufferSize定义。 之后如果仍然没有订阅者订阅它并消费数据，对onNext的调用会阻塞，直到有订阅者接入 （这时只能并发地订阅了）。 因此第一个订阅者会收到最多 bufferSize 个元素。然而之后，后续接入的订阅者只能获取到它们开始订阅之后推送的数据。这个内部的缓存会继续用于背压的目的。 默认情况下，如果所有的订阅者都取消了订阅，它会清空内部缓存，并且不再接受更多的订阅者。这一点可以通过 create 静态工厂方法的 autoCancel 参数来配置。 ReplayProcessorReplayProcessor会缓存直接通过自身的 Sink 推送的元素，以及来自上游发布者的元素， 并且后来的订阅者也会收到重发（replay）的这些元素。 可以通过多种配置方式创建它： 缓存一个元素（cacheLast）。 缓存一定个数的历史元素（create(int)），所有的历史元素（create()）。 缓存基于时间窗期间内的元素（createTimeout(Duration)）。 缓存基于历史个数和时间窗的元素（createSizeOrTimeout(int, Duration)）。 TopicProcessor TopicProcessor是一个异步的 processor，它能够重发来自多个上游发布者的元素， 这需要在创建它的时候配置shared（build() 的 share(boolean) 配置）。 如果你企图在并发环境下通过并发的上游发布者调用 TopicProcessor 的 onNext、 onComplete，或 onError 方法，就必须配置 shared。否则，并发调用就是非法的，从而 processor 是完全兼容响应式流规范的。 TopicProcessor 能够对多个订阅者发送数据。它通过对每一个订阅者关联一个线程来实现这一点， 这个线程会一直执行直到 processor 发出 onError 或 onComplete 信号，或关联的订阅者被取消。 最多可以接受的订阅者个数由构造者方法 executor 指定，通过提供一个有限线程数的 ExecutorService 来限制这一个数。 这个 processor 基于一个 RingBuffer 数据结构来存储已发送的数据。每一个订阅者线程 自行管理其相关的数据在 RingBuffer 中的索引。 这个 processor 也有一个 autoCancel 构造器方法：如果设置为 true（默认的），那么当 所有的订阅者取消之后，上游发布者也就被取消了。 WorkQueueProcessorWorkQueueProcessor也是一个异步的 processor，也能够重发来自多个上游发布者的元素， 同样在创建时需要配置shared（它多数构造器配置与TopicProcessor相同）。 它放松了对响应式流规范的兼容，但是好处就在于相对于 TopicProcessor 来说需要更少的资源。 它仍然基于 RingBuffer，但是不再要求每一个订阅者都关联一个线程，因此相对于 TopicProcessor 来说更具扩展性。 代价在于分发模式有些区别：来自订阅者的请求会汇总在一起，并且这个 processor 每次只对一个 订阅者发送数据，因此需要循环（round-robin）对订阅者发送数据，而不是一次全部发出的模式（无法保证完全公平的循环分发）。 WorkQueueProcessor 多数构造器方法与 TopicProcessor 相同，比如 autoCancel、share， 以及 waitStrategy。下游订阅者的最大数目同样由构造器 executor 配置的 ExecutorService 决定。 *注意：**最好不要有太多订阅者订阅 WorkQueueProcessor，因为这会锁住 processor。如果你需要限制订阅者数量，最好使用一个 ThreadPoolExecutor 或 ForkJoinPool。这个 processor 能够检测到（线程池）容量并在订阅者过多时抛出异常。 完成信号对于 Flux 和 Mono 来说,just 是数据完成的信号，那如果不是通过 just 声明的数据流，没有这种数据准备完成的信号，那么这个流就是一个无限流。除了我们手动声明数据准备的完成，错误信号也标志这整个流的完成。 Flux.error(new RuntimeException()); 还有一种情况就是当 Flux 和 Mono 没有发出任何一个元素，而是直接发出了完成信号，那么这个流就是一个空的流，像这样。 Flux.error(new RuntimeException()); Flux.just(); Flux.empty(); 还有很重要的一点就是 Flux.just(1，2，4) 只是定义了一个数据流而已，在subscribe() 之前的操作什么也不会发生，类似 Stream的惰性求值，在中止操作之前的操作都不会触发。 例如打印声明的数据流需要这样做 Flux.just(1, 2, 3).subscribe(System.out::println); 另外 subscribe 时，还可以指定错误的回调处理，以及数据处理完的完成回调 所以可以这样写 Flux.error(new Exception(\"error\")).subscribe( System.out::println, System.err::println, () -> System.out.println(\"Completed!\")); 流程： 流量控制（背压）上面提到了一个问题，当生产者生产的速度远远大于消费者消费的的速度的时候，会引发任务大量堆积的情况，最终压垮整个管道。 那么响应式是怎么解决这个问题的，通过背压（back pressure）的机制，如下图 这种下游可以向上游反馈自己消费能力的机制就叫做背压，具体背压的原理和运行机制会在后面的实战中带入，因为很多刚接触这种概念的同学只听理论的话会一时很难理解。 通过 Reactor 提供的 BaseSubscriber 来进行自定义我们自己流量控制的 subscriber Flux.just(1,2) .doOnRequest(s->System.out.println(\"no. \"+s)) .subscribe(new BaseSubscriber&lt;Integer>() &#123; @Override protected void hookOnSubscribe(Subscription subscription) &#123; System.out.println(\"订阅开始了，我要请求几个元素\"); request(1); &#125; @Override protected void hookOnNext(Integer value) &#123; System.out.println(\"收到一个元素，下一次请求几个元素\"); request(1); &#125; &#125;); Reactor 中的多线程在我们 java 的传统的编程中，对于线程之间的调度有封装好的线程池工具类供我们使用，或者我们可以通过线程池的构造函数定义自己的线程池，这一切都让多线程的调度都变得很容易，那么在 reactor 中怎么处理线程的调度 4.1 Schedulers 在 reactor 中处理线程调度的不叫 thread pool，而是 Schedulers（调度器），通过调度器就可以创建出供我们直接使用的多线程环境。 4.1.1 Schedulers.immediate() 在当前线程中使用 4.1.2 Schedulers.single() 创建一个可重用的单线程环境，该方法的所有调用者都会重复使用同一个线程。 4.1.3 Schedulers.elastic() 创建一个弹性线程池，会重用空闲线程，当线程池空闲时间过长就会自动废弃掉。通常使用的场景是给一个阻塞的任务分配自己的线程，从而不会影响到其他任务的执行。 4.1.4 Schedulers.parallel() 创建一个固定大小的线程池，线程池大小和 cpu 个数相等。 来看一个具体使用的实例，通过 Schedulers.elastic() 将一个同步阻塞的方法改写成异步的。 private Integer syncMethod()&#123; try &#123; TimeUnit.SECONDS.sleep(2); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; return 123456; &#125; @Test public void switchSyncToAsyncTest()&#123; CountDownLatch countDownLatch = new CountDownLatch(1); Mono.fromCallable(()->syncMethod()) .subscribeOn(Schedulers.elastic()) .subscribe(System.out::println,null,countDownLatch::countDown); try &#123; countDownLatch.await(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; 简单分析上述代码，通过 fromCallable 声明 一个 callable 的 mono，然后通过 subscribeOn 切换环境，调度任务到单独的弹性线程池工作。 错误处理在传统的编程中，我们处理单个接口错误的方式，可能是 try-catch-finally 的方式，也可能是 try-winth-resource 的语法糖，这些在 reactor 中变得不太一样。下面来说一说 reactor 中的几种错误处理方式。 5.1 onErrorReturn onErrorReturn 在发生错误的时候，会提供一个缺省值，类似于安全取值的问题，但是这个在响应式流里面会更加实用。 Flux.just(1,2,0) .map(v->2/v) .onErrorReturn(0) .map(v->v*2) .subscribe(System.out::println,System.err::println); 这样就可以在处理一些未知元素的时候，又不想让未知因素中止程序的继续运行，就可以采取这种方式。 5.2 onErrorResume 在发生错误的时候，提供一个新的流或者值返回，这样说可能不太清楚，看代码。 Flux.just(1,2,0) //调用redis服务获取数据 .flatMap(id->redisService.getUserByid(id)) //当发生异常的时候，从DB用户服务获取数据 .onErrorResume(v->userService.getUserByCache(id)); 类似于错误的一个 callback； 5.3 onErrorMap 上面的都是我们去提供缺省的方法或值处理错误，但是有的时候，我们需要抛出错误，但是需要将错误包装一下，可读性好一点，也就是抛出自定义异常。 Flux.just(1,2,0) .flatMap(id->getUserByid(id)) .onErrorMap(v->new CustomizeExcetion(\"服务器开小差了\",v)); 5.4 doOnError 记录错误日志 在发生错误的时候我们需要记录日志，在 reactor 里面专门独立出 api 记录错误日志 Flux.just(1,2,0) .flatMap(id->getUserByid(id)) .doOnError(e-> Log.error(\"this occur something error\")) .onErrorMap(v->new CustomizeExcetion(\"服务器开小差了\",v)); doOnError 对于流里面的元素只读，也就是他不会对流里面的任务元素操作，记录日志后，会讲错误信号继续抛到后面，让后面去处理。 5.5 finally 确保做一些事情 有的时候我们想要像传统的同步代码那样使用 finally 去做一些事情，比如关闭 http 连接，清理资源，那么在 reactor 中怎么去做 finally Flux.just(1,2,0) .flatMap(id->getUserByid(id)) .doOnError(e-> Log.error(\"this occur something error\")) .onErrorMap(v->new CustomizeExcetion(\"服务器开小差了\",v)) .doFinally(System.out.println(\"我会确保做一些事情\")) ; 或者当我们打开一个连接需要关闭资源的时候，还可以这样写 Flux.using( () -> createHttpClient(), client -> Flux.just(client.sendRequest()), createHttpClient::close ); 使用 using 函数的三个参数，获取 client，发送请求，关闭资源。 5.6 retry 重试机制 当遇到一些不可控因素导致的程序失败，但是代码逻辑确实是正确的，这个时候需要重试机制。 Flux.just(1,2,0) .map(v->2/v) .retry(1) .subscribe(System.out::println,System.err::println); 但是需要注意的是重试不是从错误的地方开始重试，相当于对 publisher 的重订阅，也就是从零开始重新执行一遍，所以无法达到类似于断点续传的功能，所以使用场景还是有限制。 如何调试 reactor在我们传统阻塞代码里面，调试（Debug）的时候是一件非常简单的事情，通过打断点，得到相关的 stack 的信息，就可以很清楚的知道错误信息（不过在多线程的环境下去打断点，需要切换线程环境，也有点麻烦）。 但是在 reactor 环境下去调试代码并不是一件简单的事情，最常见的就是 一个 Flux 流，怎么去得到每个元素信息，怎么去知道在管道里面下一个元素是什么，每个元素是否像期望的那样做了操作。所以这也是从传统编程切换到响应式编程的难点，开发人员需要花时间去学习这个操作，但是感觉难受总是好的，因为做什么都太容易的话，自己会长期止步于此，像早期的 EJB 到 j2ee，ssh -&gt; ssm -&gt; spring boot -&gt; spring cloud ，从微服务-&gt;service mesh -&gt; serve less ,到现在一些一线大厂盛行的中台。也许这一次就是改变自己的时候。 言归正传，关于比较复杂的调试后期再说，我们先从最基本的单元测试开始。官方推荐的工具是 StepVerifier @Test public void reactorTest()&#123; StepVerifier.create(Flux.just(1,2)) //1 .expectNext(1,2) //2 .expectComplete() //3 .verify(); //4 &#125; 创建测试的异步流 测试期望的值 测试是否完成 验证 我们通常使用 create 方法创建基于 Flux 或 Mono 的 StepVerifier，然后就可以进行以下测试： 测试期望发出的下一个信号。如果收到其他信号（或者信号与期望不匹配），整个测试就会 失败（AssertionError），如 expectNext(T…)或 expectNextCount(long)。&#96; 处理（consume）下一个信号。当你想要跳过部分序列或者当你想对信号内容进行自定义的校验的时候会用到它，可以使用 consumeNextWith(Consumer)。 其他操作，比如暂停或运行一段代码。比如，你想对测试状态或内容进行调整或处理， 你可能会用到 thenAwait(Duration)和 then(Runnable)。 对于终止事件，相应的期望方法（如expectComplete()、expectError()，及其所有的变体方法） 使用之后就不能再继续增加别的期望方法了。最后你只能对 StepVerifier 进行一些额外的配置并 触发校验（通常调用verify()及其变体方法）。 从StepVerifier内部实现来看，它订阅了待测试的 Flux 或 Mono，然后将序列中的每个信号与测试 场景的期望进行比对。如果匹配的话，测试成功。如果有不匹配的情况，则抛出AssertionError异常。 响应式流是一种基于时间的数据流。许多时候，待测试的数据流存在延迟，从而持续一段时间。如果这种场景比较多的话，那么会导致自动化测试运行时间较长。因此StepVerifier提供了可以操作“虚拟时间”的测试方式，这时候需要使用StepVerifier.withVirtualTime来构造。 为了提高 StepVerifier 正常起作用的概率，它一般不接收一个简单的 Flux 作为输入，而是接收 一个 Supplier，从而可以在配置好订阅者之后 “懒创建”待测试的 flux，如： StepVerifier.withVirtualTime(() -&gt; Mono.delay(Duration.ofDays(1))) &#x2F;&#x2F;… 继续追加期望方法 有两种处理时间的期望方法，无论是否配置虚拟时间都是可用的： thenAwait(Duration)会暂停校验步骤（允许信号延迟发出）。 expectNoEvent(Duration)同样让序列持续一定的时间，期间如果有任何信号发出则测试失败。在普通的测试中，两个方法都会基于给定的持续时间暂停线程的执行。而如果是在虚拟时间模式下就相应地使用虚拟时间。 StepVerifier.withVirtualTime(() -> Mono.delay(Duration.ofDays(1))) .expectSubscription() // 1 .expectNoEvent(Duration.ofDays(1)) // 2 .expectNext(0L) .verifyComplete(); // 3 expectNoEvent 将订阅（subscription）也认作一个事件。假设你用它作为第一步，如果检测 到有订阅信号，也会失败。这时候可以使用 expectSubscription().expectNoEvent(duration) 来代替； 期待“一天”内没有信号发生； verify 或变体方法最终会返回一个 Duration，这是实际的测试时长。 3.1 map 这里的 map 和 java 8 stream 的 map 是同一个意思，用于元素的转换，像这样 @Test public void reactorMapTest()&#123; StepVerifier.create(Flux.just(1,2) .map(v->v+1)) .expectNext(2,3) .expectComplete() .verify(); &#125; 还是之前的代码，只是对每一个元素都自增加一，这里就不多说了，对 lambada 熟悉的同学都了解。 3.2 flatmap flatmap 也是对元素的转换，但是不同的是 flatmap 是将元素转换为流，再将流合并为一个大的流。 @Test public void reactorFlatMapTest()&#123; StepVerifier.create(Flux.just(\"crabman\",\"is\",\"hero\") .flatMap(v->Flux.fromArray(v.split(\"\"))) .doOnNext(System.out::println)) .expectNextCount(13) .verifyComplete(); &#125; tips ：flatmap 和 map 的区别从源码上来看 map 就是一个 function 函数，输入一个输出一个，对于 flatmap 来讲它接受的是输出为 Publisher 的 function，也就是说对于 flatmap 来讲 输入一个值，输出的是一个 Publisher，所以 map 是一对一的关系，而 flatmap 是一对多或者多对多的关系，并且两者输出也不一样。那 flatmap 的应用场景在哪里，例如一个接口，入参是 List,用户 id 的集合，需求是返回每个 id 对应的具体信息，所以代码看起来就是这样 xx.flatmap(id-&gt;getUserInfoById(id)) 3.3 filter reactor 的 filter 和 java 8 stream 的 filter 是一样的，就不多说了，这里过滤掉值为 2 的 @Test public void reactorFilterTest()&#123; StepVerifier.create(Flux.just(1,2) .map(v->v+1) .filter(s->s!=2) .doOnNext(System.out::println)) .expectNext(3) .expectComplete() .verify(); &#125; 3.4 zip 这个是操作可能看起来比较陌生，顾名思义，“压缩”就是将多个流一对一的合并起来，还有一个原因，因为在每个 flux 流或者 mono 流里面，各个流的速度是不一样，zip 还有个作用就是将两个流进行同步对齐。例如我们这里在加入另一个流，这个流会不停的发出元素，为了让大家可以感受到同，这里限制另一个流的速度为没 1 秒发出一个元素，这样合并的流也会向另一个流对齐。 @Test public void reactorZipTest()&#123; //定义一个Flux流 Flux&lt;String> stringFlux = Flux.just(\"a\", \"b\", \"c\", \"d\"); //这里使用计时器，因为在单元测试里面，可能元素没执行完，他就会直接返回 CountDownLatch countDownLatch = new CountDownLatch(1); // 2 Flux.zip(stringFlux,Flux.interval(Duration.ofSeconds(1))) .subscribe(t->System.out.println(t.getT1()) ,System.err::println ,countDownLatch::countDown); try &#123; countDownLatch.await(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; 上面讲的这四个是比较常用的，还有很多。 jdk9 的响应式规范JDK 9 提供了对于 Reactive 的完整支持，JDK9 也定义了上述提到的四个接口，在java.util.concurrent包上 Flow 的源码 public final class Flow &#123; private Flow() &#123;&#125; // uninstantiable //发布者 @FunctionalInterface public static interface Publisher&lt;T> &#123; public void subscribe(Subscriber&lt;? super T> subscriber); &#125; //订阅者 public static interface Subscriber&lt;T> &#123; public void onNext(T item); public void onError(Throwable throwable); public void onComplete(); &#125; //订阅消费对象 public static interface Subscription &#123; public void request(long n); public void cancel(); &#125; //数据转换 public static interface Processor&lt;T,R> extends Subscriber&lt;T>, Publisher&lt;R> &#123; &#125; static final int DEFAULT_BUFFER_SIZE = 256; public static int defaultBufferSize() &#123; return DEFAULT_BUFFER_SIZE; &#125;&#125; webFlux WebFlux 是 Spring 推出响应式编程的一部分(web 端) 响应式编程是异步非阻塞的(是一种基于数据流（data stream）和变化传递（propagation of change）的声明式（declarative）的编程范式) 以往根据不同的应用场景选择不同的技术，有的场景适合用于同步阻塞的，有的场景适合用于异步非阻塞的。而 Spring5 提供了一整套响应式(非阻塞)的技术栈供我们使用(包括 Web 控制器、权限控制、数据访问层等等)。 响应式一般用 Netty 或者 Servlet 3.1 的容器(因为支持异步非阻塞)，而 Servlet 技术栈用的是 Servlet 容器 Spring 官方为了让我们更加快速&#x2F;平滑到 WebFlux 上，之前 SpringMVC 那套都是支持的。也就是说：我们可以像使用 SpringMVC 一样使用着 WebFlux。 WebFlux 使用的响应式流并不是用 JDK9 平台的，而是Reactor响应式流库为啥？因为人家是兄弟公司！ 两种路由方式 基于 Spring web 的注解声明 基于 routing function 的函数式开发 /** * 阻塞5秒钟 * @return */ private String createStr() &#123; try &#123; TimeUnit.SECONDS.sleep(5); &#125; catch (InterruptedException e) &#123; &#125; return \"……^ - ^\"; &#125; /** * 原mvc * * @return &#123;@link String&#125; */ @GetMapping(\"/mvc\") private String mvc() &#123; long millis = System.currentTimeMillis(); log.info(\"请求1:&#123;&#125;\",millis); String result = createStr(); log.info(\"结束1:&#123;&#125;\",System.currentTimeMillis() - millis); return result; &#125; /** * web flux * * @return &#123;@link Mono&lt;String>&#125; */ @GetMapping(\"/flux\") private Mono&lt;String> flux() &#123; long millis = System.currentTimeMillis(); log.info(\"请求2:&#123;&#125;\",millis); Mono&lt;String> result = Mono.fromSupplier(() -> createStr()); log.info(\"结束2:&#123;&#125;\",System.currentTimeMillis() - millis); return result; &#125; 从调用者(浏览器)的角度而言，是感知不到有什么变化的，因为都是得等待 5s 才返回数据。但是，从服务端的日志我们可以看出，WebFlux 是直接返回 Mono 对象的(而不是像 SpringMVC 一直同步阻塞 5s，线程才返回)。 这正是 WebFlux 的好处：能够以固定的线程来处理高并发（充分发挥机器的性能）。 WebFlux 还支持服务器推送(SSE - &gt;Server Send Event)，我们来看个例子： /** * * 定时 返回0-n个元素 * 注：需要指定MediaType * * @return &#123;@link Flux&lt;String>&#125; */ @GetMapping(value = \"/timing\", produces = MediaType.TEXT_EVENT_STREAM_VALUE) private Flux&lt;String> timing() &#123; Flux&lt;String> result = Flux .fromStream(IntStream.range(1, 5).mapToObj(i -> &#123; try &#123; TimeUnit.SECONDS.sleep(1); &#125; catch (InterruptedException e) &#123; &#125; return \"大内密探00\" + i; &#125;)); return result; &#125; 效果就是每秒会给浏览器推送数据： 核心组件1.HttpHandler 是一种带有处理 HTTP 请求和响应方法的简单契约。 2.WebHandler webHandler 显得有一些抽象，我们可以通过对比 SpringMVC 的一些组件帮助大家理解一下在 WebFlux 中各个组件的作用： 请求处理流程 RouterFunctionMapping 中有 private RouterFunction&lt;?&gt; routerFunction;这里面表面看起来只有一个 Bean，其实它里面组合了非常多的 RouterFunction，它是如何根据用户的请求找到对应的 Function 的呢？ // 查询处理器 @Override protected Mono&lt;?> getHandlerInternal(ServerWebExchange exchange) &#123; // 路由函数是否存在 if (this.routerFunction != null) &#123; // 创建请求，并绑定 ServerRequest request = ServerRequest.create(exchange, this.messageReaders); return this.routerFunction.route(request) .doOnNext(handler -> setAttributes(exchange.getAttributes(), request, handler)); &#125; else &#123; // 没有直接空 return Mono.empty(); &#125; &#125; 关键部分就是通过它的成员变量 routerFunction 的 route 方法来找，其实就是通过用户写的 predicate 来判断请求是否相符合，如果符合就返回一个 Mono&lt;HandlerFunction&gt; public Mono&lt;HandlerFunction&lt;T>> route(ServerRequest request) &#123; // routerFunction 中的路由谓词匹配 if (this.predicate.test(request)) &#123; if (logger.isTraceEnabled()) &#123; String logPrefix = request.exchange().getLogPrefix(); logger.trace(logPrefix + String.format(\"Matched %s\", this.predicate)); &#125; return Mono.just(this.handlerFunction); &#125; else &#123; return Mono.empty(); &#125; &#125; 总结反应式编程框架主要采用了观察者模式，而 Spring Reactor 的核心则是对观察者模式的一种衍伸。关于观察者模式的架构中被观察者(Observable)和观察者(Subscriber)处在不同的线程环境中时，由于者各自的工作量不一样，导致它们产生事件和处理事件的速度不一样，这时就出现了两种情况： 被观察者产生事件慢一些，观察者处理事件很快。那么观察者就会等着被观察者发送事件好比观察者在等米下锅，程序等待）。 被观察者产生事件的速度很快，而观察者处理很慢。那就出问题了，如果不作处理的话，事件会堆积起来，最终挤爆你的内存，导致程序崩溃。（好比被观察者生产的大米没人吃，堆积最后就会烂掉）。为了方便下面理解 Mono 和 Flux，也可以理解为 Publisher（发布者也可以理解为被观察者）主动推送数据给 Subscriber（订阅者也可以叫观察者），如果 Publisher 发布消息太快，超过了 Subscriber 的处理速度，如何处理。这时就出现了 Backpressure（背压—–指在异步场景中，被观察者发送事件速度远快于观察者的处理速度的情况下，一种告诉上游的被观察者降低发送速度的策略） WebFlux 提升的其实往往是系统的伸缩性，对于速度的提升没有太多的明显。 Reactive 编程尽管没有新增大量的代码，然而编码（和调试）却是变得更为复杂 现在面临的最大问题是缺少文档。在开发应用中给我们造成了最大障碍。且 Spring WebFlux 尚未证明自身明显地优于 Spring MVC","categories":[{"name":"技术分享","slug":"技术分享","permalink":"https://docs.hehouhui.cn/categories/%E6%8A%80%E6%9C%AF%E5%88%86%E4%BA%AB/"}],"tags":[{"name":"Spring","slug":"Spring","permalink":"https://docs.hehouhui.cn/tags/Spring/"},{"name":"Java","slug":"Java","permalink":"https://docs.hehouhui.cn/tags/Java/"},{"name":"异步编程","slug":"异步编程","permalink":"https://docs.hehouhui.cn/tags/%E5%BC%82%E6%AD%A5%E7%BC%96%E7%A8%8B/"},{"name":"响应式","slug":"响应式","permalink":"https://docs.hehouhui.cn/tags/%E5%93%8D%E5%BA%94%E5%BC%8F/"},{"name":"WebFlux","slug":"WebFlux","permalink":"https://docs.hehouhui.cn/tags/WebFlux/"}]},{"title":"RedisOperations scan 用法","slug":"archives/RedisOperations scan 用法","date":"2020-07-23T00:00:00.000Z","updated":"2023-10-08T06:42:00.000Z","comments":true,"path":"archives/9.html","link":"","permalink":"https://docs.hehouhui.cn/archives/9.html","excerpt":"","text":"记录一次 scan 和 keys 的使用,scan 和 key 都是 redis 搜索 key 的值函数,但实现却完全不同。生产环境用 key 的同学准备好跑路吧~ keys Warning: consider KEYS as a command that should only be used in production environments with extreme care. It may ruin performance when it is executed against large databases. This command is intended for debugging and special operations, such as changing your keyspace layout. Don’t use KEYS in your regular application code. If you’re looking for a way to find keys in a subset of your keyspace, consider using sets. 上面是官方文档声明，KEYS 命令不能用在生产的环境中，这个时候如果数量过大效率是十分低的。 /** * Find all keys matching the given &#123;@code pattern&#125;. * * @param pattern must not be &#123;@literal null&#125;. * @return &#123;@literal null&#125; when used in pipeline / transaction. * @see &lt;a href=\"&lt;https://redis.io/commands/keys>\">Redis Documentation: KEYS&lt;/a> */ @Nullable Set&lt;K> keys(K pattern); keys 函数支持传入一个正则字符串，遍历 redis 中所有匹配到的 key。命令会引起阻塞,性能随着数据库数据的增多而越来越慢。 SCAN Redis 从 2.8 版本开始支持 scan 命令，SCAN 命令的基本用法如下 复杂度虽然也是 O(n)，通过游标分步进行不会阻塞线程; 有限制参数 COUNT ； 同 keys 命令 一样提供模式匹配功能; 服务器不需要为游标保存状态，游标的唯一状态就是 scan 返回给客户端的游标整数; 但是需要注意返回结果可能重复，需要客户端去重。如果遍历过程中有数据修改，改动后的数据不保证同步。 scan 命令提供三个参数，第一个是 cursor，第二个是要匹配的正则，第三个是单次遍历的槽位 RedisOperations sacn spring 中 redisOperations 只有 HashOperations 类提供 scan 方法 /** * Use a &#123;@link Cursor&#125; to iterate over entries in hash at &#123;@code key&#125;. &lt;br /> * &lt;strong>Important:&lt;/strong> Call &#123;@link Cursor#close()&#125; when done to avoid resource leak. * * @param key must not be &#123;@literal null&#125;. * @param options * @return &#123;@literal null&#125; when used in pipeline / transaction. * @since 1.4 */ Cursor&lt;Map.Entry&lt;HK, HV>> scan(H key, ScanOptions options); 使用/** * 搜索 */ Cursor&lt;Map.Entry&lt;ID, Double>> cursor = hash.scan(key,new ScanOptions.ScanOptionsBuilder().match(sb.toString()).count(keys.size()).build()); Map&lt;ID, Double> map = new HashMap&lt;>(keys.size()); while (cursor.hasNext()) &#123; Map.Entry&lt;ID, Double> next = cursor.next(); map.put(next.getKey(), next.getValue()); &#125; 第一个遍历是 cursor 值为 0，然后将返回结果的第一个整数作为下一个遍历的游标，如果最后返回的到 cursor 的值为 0 就代表结束。","categories":[{"name":"技术分享","slug":"技术分享","permalink":"https://docs.hehouhui.cn/categories/%E6%8A%80%E6%9C%AF%E5%88%86%E4%BA%AB/"}],"tags":[{"name":"Spring","slug":"Spring","permalink":"https://docs.hehouhui.cn/tags/Spring/"},{"name":"Java","slug":"Java","permalink":"https://docs.hehouhui.cn/tags/Java/"},{"name":"Redis","slug":"Redis","permalink":"https://docs.hehouhui.cn/tags/Redis/"}]},{"title":"spring cache","slug":"archives/spring cache","date":"2020-06-02T00:00:00.000Z","updated":"2023-10-08T06:42:00.000Z","comments":true,"path":"archives/18.html","link":"","permalink":"https://docs.hehouhui.cn/archives/18.html","excerpt":"","text":"💡 Spring Cache 是一个强大的缓存框架，可用于提高应用程序的性能和可靠性。它是 Spring 框架的一部分，基于标准的缓存抽象，提供了一个统一的、可扩展的缓存解决方案。Spring Cache 支持多种缓存提供者，包括 Ehcache、Redis、Couchbase 等。它还提供了灵活的配置选项，使开发人员可以根据应用程序的需要选择最适合的缓存解决方案。 使用 Spring Cache 可以帮助应用程序减少对数据库和其他外部资源的访问次数，从而提高应用程序的响应速度和吞吐量。除了提高性能之外，Spring Cache 还可以用于添加应用程序级别的缓存逻辑，例如缓存用户会话、结果集等。 总之，Spring Cache 是一个十分强大的工具，可用于提高应用程序的性能和可靠性。它在 Spring 框架中的定位十分重要，是一个不可或缺的组件。如果您想要在应用程序中使用缓存，那么 Spring Cache 绝对是一个值得考虑的选择。 Spring Cache是一个强大的缓存框架，可用于提高应用程序的性能和可靠性。它是Spring框架的一部分，基于标准的缓存抽象，提供了一个统一的、可扩展的缓存解决方案。Spring Cache支持多种缓存提供者，包括Ehcache、Redis、Couchbase等。它还提供了灵活的配置选项，使开发人员可以根据应用程序的需要选择最适合的缓存解决方案。 使用Spring Cache可以帮助应用程序减少对数据库和其他外部资源的访问次数，从而提高应用程序的响应速度和吞吐量。除了提高性能之外，Spring Cache还可以用于添加应用程序级别的缓存逻辑，例如缓存用户会话、结果集等。 总之，Spring Cache是一个十分强大的工具，可用于提高应用程序的性能和可靠性。它在Spring框架中的定位十分重要，是一个不可或缺的组件。如果您想要在应用程序中使用缓存，那么Spring Cache绝对是一个值得考虑的选择。 @Cacheable@Cacheable 的作用 主要针对方法配置，能够根据方法的请求参数对其结果进行缓存 @Cacheable 作用和配置方法 参数 解释 example value 缓存的名称，在 spring 配置文件中定义，必须指定至少一个 例如: @Cacheable(value&#x3D;”mycache”) @Cacheable(value&#x3D;{”cache1”,”cache2”} key 缓存的 key，可以为空，如果指定要按照 SpEL 表达式编写，如果不指定，则缺省按照方法的所有参数进行组合 @Cacheable(value&#x3D;”testcache”,key&#x3D;”#userName”) condition 缓存的条件，可以为空，使用 SpEL 编写，返回 true 或者 false，只有为 true 才进行缓存 @Cacheable(value&#x3D;”testcache”,condition&#x3D;”#userName.length()&gt;2”) 实例 @Cacheable(value&#x3D;”accountCache”)，这个注释的意思是： 当调用这个方法的时候，会先从一个名叫 accountCache 的缓存中查询，如果没有，则执行实际的方法（即查询数据库），并将执行的结果存入缓存中，否则返回缓存中的对象。这里的缓存中的 key 就是参数 userName，value 就是 Account 对象。 **“accountCache”缓存是在 ****spring*.xml** 中定义的名称。 @Cacheable(value=\"accountCache\")// 使用了一个缓存名叫 accountCache public Account getAccountByName(String userName) &#123; // 方法内部实现不考虑缓存逻辑，直接实现业务 System.out.println(\"real query account.\"+userName); return getFromDB(userName); &#125; 而在 SpringBoot 中使用的话是存放在 resources 下的**ehcache.xml**。 &lt;?xml version=\"1.0\" encoding=\"UTF-8\"?> &lt;ehcache xmlns:xsi=\"&lt;http://www.w3.org/2001/XMLSchema-instance>\" xsi:noNamespaceSchemaLocation=\"&lt;http://ehcache.org/ehcache.xsd>\" updateCheck=\"false\"> &lt;!--账号缓存--> &lt;cache name=\"accountCache\" maxElementsInMemory=\"10000\" timeToLiveSeconds=\"300\" timeToIdleSeconds=\"310\" eternal=\"false\" overflowToDisk=\"true\"/> &lt;/ehcache> @CachePut @CachePut 作用和配置方法： @CachePut 的作用 主要针对方法配置，能够根据方法的请求参数对其结果进行缓存，和 @Cacheable 不同的是，它每次都会触发真实方法的调用 参数 解释 example value 缓存的名称，在 spring 配置文件中定义，必须指定至少一个 @CachePut(value&#x3D;”my cache”) key 缓存的 key，可以为空，如果指定要按照 SpEL 表达式编写，如果不指定，则缺省按照方法的所有参数进行组合 @CachePut(value&#x3D;”testcache”,key&#x3D;”#userName”) condition 缓存的条件，可以为空，使用 SpEL 编写，返回 true 或者 false，只有为 true 才进行缓存 @CachePut(value&#x3D;”testcache”,condition&#x3D;”#userName.length()&gt;2”) 实例 @CachePut 注释，这个注释可以确保方法被执行，同时方法的返回值也被记录到缓存中，实现缓存与数据库的同步更新。 @CachePut(value=\"accountCache\",key=\"#account.getName()\")// 更新accountCache 缓存 public Account updateAccount(Account account) &#123; return updateDB(account); &#125; @CacheEvict @CacheEvict 作用和配置方法 @CachEvict 的作用 主要针对方法配置，能够根据一定的条件对缓存进行清空 参数 解释 example value 缓存的名称，在 spring 配置文件中定义，必须指定至少一个 @CacheEvict(value&#x3D;”my cache”) key 缓存的 key，可以为空，如果指定要按照 SpEL 表达式编写，如果不指定，则缺省按照方法的所有参数进行组合 @CacheEvict(value&#x3D;”testcache”,key&#x3D;”#userName”) condition 缓存的条件，可以为空，使用 SpEL 编写，返回 true 或者 false，只有为 true 才进行缓存 @CacheEvict(value&#x3D;”testcache”,condition&#x3D;”#userName.length()&gt;2”) allEntries 是否清空所有缓存内容，缺省为 false，如果指定为 true，则方法调用后将立即清空所有缓存 @CachEvict(value&#x3D;”testcache”,allEntries&#x3D;true) beforeInvocation 是否在方法执行前就清空，缺省为 false，如果指定为 true，则在方法还没有执行的时候就清空缓存，缺省情况下，如果方法执行抛出异常，则不会清空缓存 @CachEvict(value&#x3D;”testcache”，beforeInvocation&#x3D;true) 实例 @CacheEvict(value=\"accountCache\",key=\"#account.getName()\")// 清空accountCache 缓存 public void updateAccount(Account account) &#123; updateDB(account); &#125; @CacheEvict(value=\"accountCache\",allEntries=true)// 清空accountCache 缓存 public void reload() &#123; reloadAll() &#125; @Cacheable(value=\"accountCache\",condition=\"#userName.length() &lt;=4\")// 缓存名叫 accountCache public Account getAccountByName(String userName) &#123; // 方法内部实现不考虑缓存逻辑，直接实现业务 return getFromDB(userName); &#125; @CacheConfig所有的@Cacheable（）里面都有一个 value ＝“xxx”的属性，这显然如果方法多了，写起来也是挺累的，如果可以一次性声明完 那就省事了， 所以，有了@CacheConfig 这个配置，@CacheConfig is a class-level annotation that allows to share the cache names，如果你在你的方法写别的名字，那么依然以方法的名字为准。 @CacheConfig(\"books\") public class BookRepositoryImpl implements BookRepository &#123; @Cacheable public Book findBook(ISBN isbn) &#123;...&#125; &#125; 条件缓存下面提供一些常用的条件缓存 //@Cacheable将在执行方法之前( #result还拿不到返回值)判断condition，如果返回true，则查缓存； @Cacheable(value = \"user\", key = \"#id\", condition = \"#id lt 10\") public User conditionFindById(final Long id) //@CachePut将在执行完方法后（#result就能拿到返回值了）判断condition，如果返回true，则放入缓存； @CachePut(value = \"user\", key = \"#id\", condition = \"#result.username ne 'zhang'\") public User conditionSave(final User user) //@CachePut将在执行完方法后（#result就能拿到返回值了）判断unless，如果返回false，则放入缓存；（即跟condition相反） @CachePut(value = \"user\", key = \"#user.id\", unless = \"#result.username eq 'zhang'\") public User conditionSave2(final User user) //@CacheEvict， beforeInvocation=false表示在方法执行之后调用（#result能拿到返回值了）；且判断condition，如果返回true，则移除缓存； @CacheEvict(value = \"user\", key = \"#user.id\", beforeInvocation = false, condition = \"#result.username ne 'zhang'\") public User conditionDelete(final User user) @Caching有时候我们可能组合多个 Cache 注解使用；比如用户新增成功后，我们要添加 id–&gt;user；username—&gt;user；email—&gt;user 的缓存；此时就需要@Caching 组合多个注解标签了。 @Caching(put = &#123; @CachePut(value = \"user\", key = \"#user.id\"), @CachePut(value = \"user\", key = \"#user.username\"), @CachePut(value = \"user\", key = \"#user.email\") &#125;) public User save(User user) &#123; 自定义缓存注解比如之前的那个@Caching 组合，会让方法上的注解显得整个代码比较乱，此时可以使用自定义注解把这些注解组合到一个注解中，如： @Caching(put = &#123; @CachePut(value = \"user\", key = \"#user.id\"), @CachePut(value = \"user\", key = \"#user.username\"), @CachePut(value = \"user\", key = \"#user.email\") &#125;) @Target(&#123;ElementType.METHOD, ElementType.TYPE&#125;) @Retention(RetentionPolicy.RUNTIME) @Inherited public @interface UserSaveCache &#123; &#125; 这样我们在方法上使用如下代码即可，整个代码显得比较干净。 @UserSaveCache public User save(User user) 扩展比如 findByUsername 时，不应该只放 username–&gt;user，应该连同 id—&gt;user 和 email—&gt;user 一起放入；这样下次如果按照 id 查找直接从缓存中就命中了 @Caching( cacheable = &#123; @Cacheable(value = \"user\", key = \"#username\") &#125;, put = &#123; @CachePut(value = \"user\", key = \"#result.id\", condition = \"#result != null\"), @CachePut(value = \"user\", key = \"#result.email\", condition = \"#result != null\") &#125; ) public User findByUsername(final String username) &#123; System.out.println(\"cache miss, invoke find by username, username:\" + username); for (User user : users) &#123; if (user.getUsername().equals(username)) &#123; return user; &#125; &#125; return null; &#125; 其实对于：id—&gt;user；username—-&gt;user；email—&gt;user；更好的方式可能是：id—&gt;user；username—&gt;id；email—&gt;id；保证 user 只存一份；如： @CachePut(value=\"cacheName\", key=\"#user.username\", cacheValue=\"#user.username\") public void save(User user) @Cacheable(value=\"cacheName\", key=\"#user.username\", cacheValue=\"#caches[0].get(#caches[0].get(#username).get())\") public User findByUsername(String username) SpEL 上下文数据 Spring Cache 提供了一些供我们使用的 SpEL 上下文数据，下表直接摘自 Spring 官方文档： 名称 位置 描述 示例 methodName root 对象 当前被调用的方法名 root.methodName method root 对象 当前被调用的方法 root.method.name target root 对象 当前被调用的目标对象 root.target targetClass root 对象 当前被调用的目标对象类 root.targetClass args root 对象 当前被调用的方法的参数列表 root.args[0] caches root 对象 当前方法调用使用的缓存列表（如@Cacheable(value&#x3D;{“cache1”, “cache2”})），则有两个 cache root.caches[0].name argument name 执行上下文 当前被调用的方法的参数，如 findById(Long id)，我们可以通过#id 拿到参数 user.id result 执行上下文 方法执行后的返回值（仅当方法执行之后的判断有效，如‘unless’，’cache evict’的 beforeInvocation&#x3D;false） result @CacheEvict(value = \"user\", key = \"#user.id\", condition = \"#root.target.canCache() and #root.caches[0].get(#user.id).get().username ne #user.username\", beforeInvocation = true) public void conditionUpdate(User user)","categories":[{"name":"技术分享","slug":"技术分享","permalink":"https://docs.hehouhui.cn/categories/%E6%8A%80%E6%9C%AF%E5%88%86%E4%BA%AB/"}],"tags":[{"name":"Spring","slug":"Spring","permalink":"https://docs.hehouhui.cn/tags/Spring/"},{"name":"缓存","slug":"缓存","permalink":"https://docs.hehouhui.cn/tags/%E7%BC%93%E5%AD%98/"}]},{"title":"Java基础-JVM","slug":"archives/Java基础-JVM","date":"2020-03-12T00:00:00.000Z","updated":"2023-10-08T06:42:00.000Z","comments":true,"path":"archives/42.html","link":"","permalink":"https://docs.hehouhui.cn/archives/42.html","excerpt":"","text":"JMM内存模型在 JDK1.2 之前，Java 的内存模型实现总是从主存（即共享内存）读取变量，是不需要进行特别的注意的。而在当前的 Java 内存模型下，线程可以把变量保存本地内存（比如机器的寄存器）中，而不是直接在主存中进行读写。这就可能造成一个线程在主存中修改了一个变量的值，而另外一个线程还继续使用它在寄存器中的变量值的拷贝，造成数据的不一致。 要解决这个问题，就需要把变量声明为**volatile**，这就指示 JVM，这个变量是共享且不稳定的，每次使用它都到主存中进行读取。 所以，**volatile** 关键字 除了防止 JVM 的指令重排 ，还有一个重要的作用就是保证变量的可见性。 JVM 对于 Java 程序员来说，在虚拟机自动内存管理机制下，不再需要像 C&#x2F;C++程序开发程序员这样为每一个 new 操作去写对应的 delete&#x2F;free 操作，不容易出现内存泄漏和内存溢出问题。正是因为 Java 程序员把内存控制权利交给 Java 虚拟机，一旦出现内存泄漏和溢出方面的问题，如果不了解虚拟机是怎样使用内存的，那么排查错误将会是一个非常艰巨的任务。 基本问题 介绍下 Java 内存区域（运行时数据区） Java 对象的创建过程（五步，建议能默写出来并且要知道每一步虚拟机做了什么） 对象的访问定位的两种方式（句柄和直接指针两种方式） 其他问题 String 类和常量池 8 种基本类型的包装类和常量池 运行时内存区ava 虚拟机在执行 Java 程序的过程中会把它管理的内存划分成若干个不同的数据区域。JDK. 1.8 和之前的版本略有不同，下面会介绍到。 JDK 1.8 之前： JDK 1.8 ： 线程私有的： 程序计数器 虚拟机栈 本地方法栈 线程共享的： 堆 方法区 直接内存 (非运行时数据区的一部分) 程序计数器程序计数器是一块较小的内存空间，可以看作是当前线程所执行的字节码的行号指示器。字节码解释器工作时通过改变这个计数器的值来选取下一条需要执行的字节码指令，分支、循环、跳转、异常处理、线程恢复等功能都需要依赖这个计数器来完成。 另外，为了线程切换后能恢复到正确的执行位置，每条线程都需要有一个独立的程序计数器，各线程之间计数器互不影响，独立存储，我们称这类内存区域为“线程私有”的内存。 从上面的介绍中我们知道程序计数器主要有两个作用： 字节码解释器通过改变程序计数器来依次读取指令，从而实现代码的流程控制，如：顺序执行、选择、循环、异常处理。 在多线程的情况下，程序计数器用于记录当前线程执行的位置，从而当线程被切换回来的时候能够知道该线程上次运行到哪儿了。 注意：程序计数器是唯一一个不会出现 OutOfMemoryError 的内存区域，它的生命周期随着线程的创建而创建，随着线程的结束而死亡。 虚拟机栈与程序计数器一样，Java 虚拟机栈也是线程私有的，它的生命周期和线程相同，描述的是 Java 方法执行的内存模型，每次方法调用的数据都是通过栈传递的。 Java 内存可以粗糙的区分为堆内存（Heap）和栈内存 (Stack),其中栈就是现在说的虚拟机栈，或者说是虚拟机栈中局部变量表部分。 （实际上，Java 虚拟机栈是由一个个栈帧组成，而每个栈帧中都拥有：局部变量表、操作数栈、动态链接、方法出口信息。） 局部变量表主要存放了编译期可知的各种数据类型（boolean、byte、char、short、int、float、long、double）、对象引用（reference 类型，它不同于对象本身，可能是一个指向对象起始地址的引用指针，也可能是指向一个代表对象的句柄或其他与此对象相关的位置）。 Java 虚拟机栈会出现两种错误：**StackOverFlowError** 和 **OutOfMemoryError****。** **StackOverFlowError****：** 若 Java 虚拟机栈的内存大小不允许动态扩展，那么当线程请求栈的深度超过当前 Java 虚拟机栈的最大深度的时候，就抛出 StackOverFlowError 错误。 **OutOfMemoryError****：** 若 Java 虚拟机堆中没有空闲内存，并且垃圾回收器也无法提供更多内存的话。就会抛出 OutOfMemoryError 错误。 Java 虚拟机栈也是线程私有的，每个线程都有各自的 Java 虚拟机栈，而且随着线程的创建而创建，随着线程的死亡而死亡。 扩展：那么方法&#x2F;函数如何调用？ Java 栈可用类比数据结构中栈，Java 栈中保存的主要内容是栈帧，每一次函数调用都会有一个对应的栈帧被压入 Java 栈，每一个函数调用结束后，都会有一个栈帧被弹出。 Java 方法有两种返回方式： return 语句。 抛出异常。 不管哪种返回方式都会导致栈帧被弹出。 本地方法栈和虚拟机栈所发挥的作用非常相似，区别是： 虚拟机栈为虚拟机执行 Java 方法 （也就是字节码）服务，而本地方法栈则为虚拟机使用到的 Native 方法服务。 在 HotSpot 虚拟机中和 Java 虚拟机栈合二为一。 本地方法被执行的时候，在本地方法栈也会创建一个栈帧，用于存放该本地方法的局部变量表、操作数栈、动态链接、出口信息。 方法执行完毕后相应的栈帧也会出栈并释放内存空间，也会出现 StackOverFlowError 和 OutOfMemoryError 两种错误。 堆Java 虚拟机所管理的内存中最大的一块，Java 堆是所有线程共享的一块内存区域，在虚拟机启动时创建。此内存区域的唯一目的就是存放对象实例，几乎所有的对象实例以及数组都在这里分配内存。 Java 世界中“几乎”所有的对象都在堆中分配，但是，随着 JIT 编译期的发展与逃逸分析技术逐渐成熟，栈上分配、标量替换优化技术将会导致一些微妙的变化，所有的对象都分配到堆上也渐渐变得不那么“绝对”了。从 jdk 1.7 开始已经默认开启逃逸分析，如果某些方法中的对象引用没有被返回或者未被外面使用（也就是未逃逸出去），那么对象可以直接在栈上分配内存。 Java 堆是垃圾收集器管理的主要区域，因此也被称作GC 堆（Garbage Collected Heap）.从垃圾回收的角度，由于现在收集器基本都采用分代垃圾收集算法，所以 Java 堆还可以细分为：新生代和老年代：再细致一点有：Eden 空间、From Survivor、To Survivor 空间等。进一步划分的目的是更好地回收内存，或者更快地分配内存。 在 JDK 7 版本及 JDK 7 版本之前，堆内存被通常被分为下面三部分： 新生代内存(Young Generation) 老生代(Old Generation) 永生代(Permanent Generation) JDK 8 版本之后方法区（HotSpot 的永久代）被彻底移除了（JDK1.7 就已经开始了），取而代之是元空间，元空间使用的是直接内存。 上图所示的 Eden 区、两个 Survivor 区都属于新生代（为了区分，这两个 Survivor 区域按照顺序被命名为 from 和 to），中间一层属于老年代。 大部分情况，对象都会首先在 Eden 区域分配，在一次新生代垃圾回收后，如果对象还存活，则会进入 s0 或者 s1，并且对象的年龄还会加 1(Eden 区-&gt;Survivor 区后对象的初始年龄变为 1)，当它的年龄增加到一定程度（默认为 15 岁），就会被晋升到老年代中。对象晋升到老年代的年龄阈值，可以通过参数 -XX:MaxTenuringThreshold 来设置。 修正（issue552）：“Hotspot 遍历所有对象时，按照年龄从小到大对其所占用的大小进行累积，当累积的某个年龄大小超过了 survivor 区的一半时，取这个年龄和 MaxTenuringThreshold 中更小的一个值，作为新的晋升年龄阈值”。 **动态年龄计算的代码如下** uint ageTable::compute_tenuring_threshold(size_t survivor_capacity) &#123; //survivor_capacity是survivor空间的大小 size_t desired_survivor_size = (size_t)((((double) survivor_capacity)*TargetSurvivorRatio)/100); size_t total = 0; uint age = 1; while (age &lt; table_size) &#123; total += sizes[age];//sizes数组是每个年龄段对象大小 if (total > desired_survivor_size) break; age++; &#125; uint result = age &lt; MaxTenuringThreshold ? age : MaxTenuringThreshold; ... &#125; Copy to clipboardErrorCopied 堆这里最容易出现的就是 OutOfMemoryError 错误，并且出现这种错误之后的表现形式还会有几种，比如： OutOfMemoryError: GC Overhead Limit Exceeded ： 当 JVM 花太多时间执行垃圾回收并且只能回收很少的堆空间时，就会发生此错误。 java.lang.OutOfMemoryError: Java heap space :假如在创建新的对象时, 堆内存中的空间不足以存放新创建的对象, 就会引发java.lang.OutOfMemoryError: Java heap space 错误。(和本机物理内存无关，和你配置的内存大小有关！) …… 方法区方法区与 Java 堆一样，是各个线程共享的内存区域，它用于存储已被虚拟机加载的类信息、常量、静态变量、即时编译器编译后的代码等数据。虽然 Java 虚拟机规范把方法区描述为堆的一个逻辑部分，但是它却有一个别名叫做 Non-Heap（非堆），目的应该是与 Java 堆区分开来。 方法区也被称为永久代。很多人都会分不清方法区和永久代的关系，为此我也查阅了文献。 方法区和永生代的关系 《Java 虚拟机规范》只是规定了有方法区这么个概念和它的作用，并没有规定如何去实现它。那么，在不同的 JVM 上方法区的实现肯定是不同的了。 方法区和永久代的关系很像 Java 中接口和类的关系，类实现了接口，而永久代就是 HotSpot 虚拟机对虚拟机规范中方法区的一种实现方式。 也就是说，永久代是 HotSpot 的概念，方法区是 Java 虚拟机规范中的定义，是一种规范，而永久代是一种实现，一个是标准一个是实现，其他的虚拟机实现并没有永久代这一说法。 参数 JDK 1.8 之前永久代还没被彻底移除的时候通常通过下面这些参数来调节方法区大小 -XX:PermSize=N //方法区 (永久代) 初始大小 -XX:MaxPermSize=N //方法区 (永久代) 最大大小,超过这个值将会抛出 OutOfMemoryError 异常:java.lang.OutOfMemoryError: PermGenCopy to clipboardErrorCopied 相对而言，垃圾收集行为在这个区域是比较少出现的，但并非数据进入方法区后就“永久存在”了。 JDK 1.8 的时候，方法区（HotSpot 的永久代）被彻底移除了（JDK1.7 就已经开始了），取而代之是元空间，元空间使用的是直接内存。 下面是一些常用参数： -XX:MetaspaceSize=N //设置 Metaspace 的初始（和最小大小） -XX:MaxMetaspaceSize=N //设置 Metaspace 的最大大小Copy to clipboardErrorCopied 与永久代很大的不同就是，如果不指定大小的话，随着更多类的创建，虚拟机会耗尽所有可用的系统内存。 元空间 整个永久代有一个 JVM 本身设置固定大小上限，无法进行调整，而元空间使用的是直接内存，受本机可用内存的限制，虽然元空间仍旧可能溢出，但是比原来出现的几率会更小。 当你元空间溢出时会得到如下错误： java.lang.OutOfMemoryError: MetaSpace 你可以使用 -XX：MaxMetaspaceSize 标志设置最大元空间大小，默认值为 unlimited，这意味着它只受系统内存的限制。-XX：MetaspaceSize 调整标志定义元空间的初始大小如果未指定此标志，则 Metaspace 将根据运行时的应用程序需求动态地重新调整大小。 元空间里面存放的是类的元数据，这样加载多少类的元数据就不由 MaxPermSize 控制了, 而由系统的实际可用空间来控制，这样能加载的类就更多了。 在 JDK8，合并 HotSpot 和 JRockit 的代码时, JRockit 从来没有一个叫永久代的东西, 合并之后就没有必要额外的设置这么一个永久代的地方了。 运行时常量池运行时常量池是方法区的一部分。Class 文件中除了有类的版本、字段、方法、接口等描述信息外，还有常量池表（用于存放编译期生成的各种字面量和符号引用） 既然运行时常量池是方法区的一部分，自然受到方法区内存的限制，当常量池无法再申请到内存时会抛出 OutOfMemoryError 错误。 JDK1.7 及之后版本的 JVM 已经将运行时常量池从方法区中移了出来，在 Java 堆（Heap）中开辟了一块区域存放运行时常量池。 修正(issue747，reference)： 1. **JDK1.7之前运行时常量池逻辑包含字符串常量池存放在方法区, 此时hotspot虚拟机对方法区的实现为永久代** 2. **JDK1.7 字符串常量池被从方法区拿到了堆中, 这里没有提到运行时常量池,也就是说字符串常量池被单独拿到堆,运行时常量池剩下的东西还在方法区, 也就是hotspot中的永久代** 。 3. **JDK1.8 hotspot移除了永久代用元空间(Metaspace)取而代之, 这时候字符串常量池还在堆, 运行时常量池还在方法区, 只不过方法区的实现从永久代变成了元空间(Metaspace)** 相关问题：JVM 常量池中存储的是对象还是引用呢？： https://www.zhihu.com/question/57109429/answer/151717241 by RednaxelaFX 特别注意 ⚠️ Java 基本类型的包装类的大部分都实现了常量池技术，即 Byte,Short,Integer,Long,Character,Boolean；前面 4 种包装类默认创建了数值[-128，127] 的相应类型的缓存数据，Character 创建了数值在[0,127]范围的缓存数据，Boolean 直接返回 True Or False。如果超出对应范围仍然会去创建新的对象。 为啥把缓存设置为[-128，127]区间？（参见 issue&#x2F;461）性能和资源之间的权衡。 public static Boolean valueOf(boolean b) &#123; return (b ? TRUE : FALSE); &#125;Copy to clipboardErrorCopied private static class CharacterCache &#123; private CharacterCache()&#123;&#125; static final Character cache[] = new Character[127 + 1]; static &#123; for (int i = 0; i &lt; cache.length; i++) cache[i] = new Character((char)i); &#125; &#125;Copy to clipboardErrorCopied 两种浮点数类型的包装类 Float,Double 并没有实现常量池技术。 Integer i1 = 33; Integer i2 = 33; System.out.println(i1 == i2);// 输出 true Integer i11 = 333; Integer i22 = 333; System.out.println(i11 == i22);// 输出 false Double i3 = 1.2; Double i4 = 1.2; System.out.println(i3 == i4);// 输出 falseCopy to clipboardErrorCopied Integer 缓存源代码： /** *此方法将始终缓存-128 到 127（包括端点）范围内的值，并可以缓存此范围之外的其他值。 */ public static Integer valueOf(int i) &#123; if (i >= IntegerCache.low &amp;&amp; i &lt;= IntegerCache.high) return IntegerCache.cache[i + (-IntegerCache.low)]; return new Integer(i); &#125; Copy to clipboardErrorCopied 应用场景： Integer i1&#x3D;40；Java 在编译的时候会直接将代码封装成 Integer i1&#x3D;Integer.valueOf(40);，从而使用常量池中的对象。 Integer i1 &#x3D; new Integer(40);这种情况下会创建新的对象。 Integer i1 = 40; Integer i2 = new Integer(40); System.out.println(i1==i2);//输出 falseCopy to clipboardErrorCopied Integer 比较更丰富的一个例子: Integer i1 = 40; Integer i2 = 40; Integer i3 = 0; Integer i4 = new Integer(40); Integer i5 = new Integer(40); Integer i6 = new Integer(0); System.out.println(\"i1=i2 \" + (i1 == i2)); System.out.println(\"i1=i2+i3 \" + (i1 == i2 + i3)); System.out.println(\"i1=i4 \" + (i1 == i4)); System.out.println(\"i4=i5 \" + (i4 == i5)); System.out.println(\"i4=i5+i6 \" + (i4 == i5 + i6)); System.out.println(\"40=i5+i6 \" + (40 == i5 + i6)); Copy to clipboardErrorCopied 结果： i1=i2 true i1=i2+i3 true i1=i4 false i4=i5 false i4=i5+i6 true 40=i5+i6 trueCopy to clipboardErrorCopied 解释： 语句 i4 &#x3D;&#x3D; i5 + i6，因为+这个操作符不适用于 Integer 对象，首先 i5 和 i6 进行自动拆箱操作，进行数值相加，即 i4 &#x3D;&#x3D; 40。然后 Integer 对象无法与数值进行直接比较，所以 i4 自动拆箱转为 int 值 40，最终这条语句转为 40 &#x3D;&#x3D; 40 进行数值比较。 直接内存直接内存并不是虚拟机运行时数据区的一部分，也不是虚拟机规范中定义的内存区域，但是这部分内存也被频繁地使用。而且也可能导致 OutOfMemoryError 错误出现。 JDK1.4 中新加入的 NIO(New Input&#x2F;Output) 类，引入了一种基于通道（Channel） 与缓存区（Buffer） 的 I&#x2F;O 方式，它可以直接使用 Native 函数库直接分配堆外内存，然后通过一个存储在 Java 堆中的 DirectByteBuffer 对象作为这块内存的引用进行操作。这样就能在一些场景中显著提高性能，因为避免了在 Java 堆和 Native 堆之间来回复制数据。 本机直接内存的分配不会受到 Java 堆的限制，但是，既然是内存就会受到本机总内存大小以及处理器寻址空间的限制。 垃圾回收Java 的自动内存管理主要是针对对象内存的回收和对象内存的分配。同时，Java 自动内存管理最核心的功能是 堆 内存中对象的分配与回收。 Java 堆是垃圾收集器管理的主要区域，因此也被称作GC 堆（Garbage Collected Heap）.从垃圾回收的角度，由于现在收集器基本都采用分代垃圾收集算法，所以 Java 堆还可以细分为：新生代和老年代：再细致一点有：Eden 空间、From Survivor、To Survivor 空间等。进一步划分的目的是更好地回收内存，或者更快地分配内存。 堆空间的基本结构： 上图所示的 Eden 区、From Survivor0(“From”) 区、To Survivor1(“To”) 区都属于新生代，Old Memory 区属于老年代。 大部分情况，对象都会首先在 Eden 区域分配，在一次新生代垃圾回收后，如果对象还存活，则会进入 s0 或者 s1，并且对象的年龄还会加 1(Eden 区-&gt;Survivor 区后对象的初始年龄变为 1)，当它的年龄增加到一定程度（默认为 15 岁），就会被晋升到老年代中。对象晋升到老年代的年龄阈值，可以通过参数 -XX:MaxTenuringThreshold 来设置。 修正（issue552）：“Hotspot 遍历所有对象时，按照年龄从小到大对其所占用的大小进行累积，当累积的某个年龄大小超过了 survivor 区的一半时，取这个年龄和 MaxTenuringThreshold 中更小的一个值，作为新的晋升年龄阈值”。 **动态年龄计算的代码如下** uint ageTable::compute_tenuring_threshold(size_t survivor_capacity) &#123; //survivor_capacity是survivor空间的大小 size_t desired_survivor_size = (size_t)((((double)survivor_capacity)*TargetSurvivorRatio)/100); size_t total = 0; uint age = 1; while (age &lt; table_size) &#123; //sizes数组是每个年龄段对象大小 total += sizes[age]; if (total > desired_survivor_size) &#123; break; &#125; age++; &#125; uint result = age &lt; MaxTenuringThreshold ? age : MaxTenuringThreshold; ... &#125; Copy to clipboardErrorCopied 经过这次 GC 后，Eden 区和”From”区已经被清空。这个时候，”From”和”To”会交换他们的角色，也就是新的”To”就是上次 GC 前的“From”，新的”From”就是上次 GC 前的”To”。不管怎样，都会保证名为 To 的 Survivor 区域是空的。Minor GC 会一直重复这样的过程，直到“To”区被填满，”To”区被填满之后，会将所有对象移动到老年代中。 当需要排查各种内存溢出问题、当垃圾收集成为系统达到更高并发的瓶颈时，我们就需要对这些“自动化”的技术实施必要的监控和调节。 问题答案在文中都有提到 如何判断对象是否死亡（两种方法）。 简单的介绍一下强引用、软引用、弱引用、虚引用（虚引用与软引用和弱引用的区别、使用软引用能带来的好处）。 如何判断一个常量是废弃常量 如何判断一个类是无用的类 垃圾收集有哪些算法，各自的特点？ HotSpot 为什么要分为新生代和老年代？ 常见的垃圾回收器有哪些？ 介绍一下 CMS,G1 收集器。 Minor Gc 和 Full GC 有什么不同呢？ 内存分配与回收机制对象优先在 eden 区分配目前主流的垃圾收集器都会采用分代回收算法，因此需要将堆内存分为新生代和老年代，这样我们就可以根据各个年代的特点选择合适的垃圾收集算法。 大多数情况下，对象在新生代中 eden 区分配。当 eden 区没有足够空间进行分配时，虚拟机将发起一次 Minor GC.下面我们来进行实际测试以下。 测试： public class GCTest &#123; public static void main(String[] args) &#123; byte[] allocation1, allocation2; allocation1 = new byte[30900*1024]; //allocation2 = new byte[900*1024]; &#125; &#125;Copy to clipboardErrorCopied 通过以下方式运行： 添加的参数： -XX:+PrintGCDetails 运行结果 (红色字体描述有误，应该是对应于 JDK1.7 的永久代)： 从上图我们可以看出 eden 区内存几乎已经被分配完全（即使程序什么也不做，新生代也会使用 2000 多 k 内存）。假如我们再为 allocation2 分配内存会出现什么情况呢？ allocation2 = new byte[900*1024];Copy to clipboardErrorCopied 简单解释一下为什么会出现这种情况： 因为给 allocation2 分配内存的时候 eden 区内存几乎已经被分配完了，我们刚刚讲了当 Eden 区没有足够空间进行分配时，虚拟机将发起一次 Minor GC.GC 期间虚拟机又发现 allocation1 无法存入 Survivor 空间，所以只好通过 分配担保机制 把新生代的对象提前转移到老年代中去，老年代上的空间足够存放 allocation1，所以不会出现 Full GC。执行 Minor GC 后，后面分配的对象如果能够存在 eden 区的话，还是会在 eden 区分配内存。可以执行如下代码验证： public class GCTest &#123; public static void main(String[] args) &#123; byte[] allocation1, allocation2,allocation3,allocation4,allocation5; allocation1 = new byte[32000*1024]; allocation2 = new byte[1000*1024]; allocation3 = new byte[1000*1024]; allocation4 = new byte[1000*1024]; allocation5 = new byte[1000*1024]; &#125; &#125; Copy to clipboardErrorCopied 大对象直接进入老年代 大对象就是需要大量连续内存空间的对象（比如：字符串、数组）。 为什么要这样呢？ 为了避免为大对象分配内存时由于分配担保机制带来的复制而降低效率 长期存活的对象进入老年代既然虚拟机采用了分代收集的思想来管理内存，那么内存回收时就必须能识别哪些对象应放在新生代，哪些对象应放在老年代中。为了做到这一点，虚拟机给每个对象一个对象年龄（Age）计数器。 如果对象在 Eden 出生并经过第一次 Minor GC 后仍然能够存活，并且能被 Survivor 容纳的话，将被移动到 Survivor 空间中，并将对象年龄设为 1.对象在 Survivor 中每熬过一次 MinorGC,年龄就增加 1 岁，当它的年龄增加到一定程度（默认为 15 岁），就会被晋升到老年代中。对象晋升到老年代的年龄阈值，可以通过参数 -XX:MaxTenuringThreshold 来设置。 动态判断对象年龄大部分情况，对象都会首先在 Eden 区域分配，在一次新生代垃圾回收后，如果对象还存活，则会进入 s0 或者 s1，并且对象的年龄还会加 1(Eden 区-&gt;Survivor 区后对象的初始年龄变为 1)，当它的年龄增加到一定程度（默认为 15 岁），就会被晋升到老年代中。对象晋升到老年代的年龄阈值，可以通过参数 -XX:MaxTenuringThreshold 来设置。 修正（issue552）：“Hotspot 遍历所有对象时，按照年龄从小到大对其所占用的大小进行累积，当累积的某个年龄大小超过了 survivor 区的一半时，取这个年龄和 MaxTenuringThreshold 中更小的一个值，作为新的晋升年龄阈值”。 **动态年龄计算的代码如下** uint ageTable::compute_tenuring_threshold(size_t survivor_capacity) &#123; //survivor_capacity是survivor空间的大小 size_t desired_survivor_size = (size_t)((((double)survivor_capacity)*TargetSurvivorRatio)/100); size_t total = 0; uint age = 1; while (age &lt; table_size) &#123; //sizes数组是每个年龄段对象大小 total += sizes[age]; if (total > desired_survivor_size) &#123; break; &#125; age++; &#125; uint result = age &lt; MaxTenuringThreshold ? age : MaxTenuringThreshold; ... &#125; Copy to clipboardErrorCopied 额外补充说明([issue672](https://github.com/Snailclimb/JavaGuide/issues/672))：**关于默认的晋升年龄是 15，这个说法的来源大部分都是《深入理解 Java 虚拟机》这本书。** 如果你去 Oracle 的官网阅读[相关的虚拟机参数](https://docs.oracle.com/javase/8/docs/technotes/tools/unix/java.html)，你会发现`-XX:MaxTenuringThreshold=threshold`这里有个说明 **Sets the maximum tenuring threshold for use in adaptive GC sizing. The largest value is 15. The default value is 15 for the parallel (throughput) collector, and 6 for the CMS collector.默认晋升年龄并不都是 15，这个是要区分垃圾收集器的，CMS 就是 6.** GC 区域周志明先生在《深入理解 Java 虚拟机》第二版中 P92 如是写道： “老年代 GC（Major GC&#x2F;Full GC），指发生在老年代的 GC……” 上面的说法已经在《深入理解 Java 虚拟机》第三版中被改正过来了。感谢 R 大的回答： 总结： 针对 HotSpot VM 的实现，它里面的 GC 其实准确分类只有两大种： 部分收集 (Partial GC)： 新生代收集（Minor GC &#x2F; Young GC）：只对新生代进行垃圾收集； 老年代收集（Major GC &#x2F; Old GC）：只对老年代进行垃圾收集。需要注意的是 Major GC 在有的语境中也用于指代整堆收集； 混合收集（Mixed GC）：对整个新生代和部分老年代进行垃圾收集。 整堆收集 (Full GC)：收集整个 Java 堆和方法区。 对象是否死亡堆中几乎放着所有的对象实例，对堆垃圾回收前的第一步就是要判断哪些对象已经死亡（即不能再被任何途径使用的对象）。 引用计数器给对象中添加一个引用计数器，每当有一个地方引用它，计数器就加 1；当引用失效，计数器就减 1；任何时候计数器为 0 的对象就是不可能再被使用的。 这个方法实现简单，效率高，但是目前主流的虚拟机中并没有选择这个算法来管理内存，其最主要的原因是它很难解决对象之间相互循环引用的问题。 所谓对象之间的相互引用问题，如下面代码所示：除了对象 objA 和 objB 相互引用着对方之外，这两个对象之间再无任何引用。但是他们因为互相引用对方，导致它们的引用计数器都不为 0，于是引用计数算法无法通知 GC 回收器回收他们。 public class ReferenceCountingGc &#123; Object instance = null; public static void main(String[] args) &#123; ReferenceCountingGc objA = new ReferenceCountingGc(); ReferenceCountingGc objB = new ReferenceCountingGc(); objA.instance = objB; objB.instance = objA; objA = null; objB = null; &#125; &#125;Copy to clipboardErrorCopied 可达性分析这个算法的基本思想就是通过一系列的称为 “GC Roots” 的对象作为起点，从这些节点开始向下搜索，节点所走过的路径称为引用链，当一个对象到 GC Roots 没有任何引用链相连的话，则证明此对象是不可用的。 可作为 GC Roots 的对象包括下面几种: 虚拟机栈(栈帧中的本地变量表)中引用的对象 本地方法栈(Native 方法)中引用的对象 方法区中类静态属性引用的对象 方法区中常量引用的对象 所有被同步锁持有的对象 引用无论是通过引用计数法判断对象引用数量，还是通过可达性分析法判断对象的引用链是否可达，判定对象的存活都与“引用”有关。 JDK1.2 之前，Java 中引用的定义很传统：如果 reference 类型的数据存储的数值代表的是另一块内存的起始地址，就称这块内存代表一个引用。 JDK1.2 以后，Java 对引用的概念进行了扩充，将引用分为强引用、软引用、弱引用、虚引用四种（引用强度逐渐减弱） 1．强引用（StrongReference） 以前我们使用的大部分引用实际上都是强引用，这是使用最普遍的引用。如果一个对象具有强引用，那就类似于必不可少的生活用品，垃圾回收器绝不会回收它。当内存空间不足，Java 虚拟机宁愿抛出 OutOfMemoryError 错误，使程序异常终止，也不会靠随意回收具有强引用的对象来解决内存不足问题。 2．软引用（SoftReference） 如果一个对象只具有软引用，那就类似于可有可无的生活用品。如果内存空间足够，垃圾回收器就不会回收它，如果内存空间不足了，就会回收这些对象的内存。只要垃圾回收器没有回收它，该对象就可以被程序使用。软引用可用来实现内存敏感的高速缓存。 软引用可以和一个引用队列（ReferenceQueue）联合使用，如果软引用所引用的对象被垃圾回收，JAVA 虚拟机就会把这个软引用加入到与之关联的引用队列中。 3．弱引用（WeakReference） 如果一个对象只具有弱引用，那就类似于可有可无的生活用品。弱引用与软引用的区别在于：只具有弱引用的对象拥有更短暂的生命周期。在垃圾回收器线程扫描它所管辖的内存区域的过程中，一旦发现了只具有弱引用的对象，不管当前内存空间足够与否，都会回收它的内存。不过，由于垃圾回收器是一个优先级很低的线程， 因此不一定会很快发现那些只具有弱引用的对象。 弱引用可以和一个引用队列（ReferenceQueue）联合使用，如果弱引用所引用的对象被垃圾回收，Java 虚拟机就会把这个弱引用加入到与之关联的引用队列中。 4．虚引用（PhantomReference） “虚引用”顾名思义，就是形同虚设，与其他几种引用都不同，虚引用并不会决定对象的生命周期。如果一个对象仅持有虚引用，那么它就和没有任何引用一样，在任何时候都可能被垃圾回收。 虚引用主要用来跟踪对象被垃圾回收的活动。 虚引用与软引用和弱引用的一个区别在于： 虚引用必须和引用队列（ReferenceQueue）联合使用。当垃圾回收器准备回收一个对象时，如果发现它还有虚引用，就会在回收对象的内存之前，把这个虚引用加入到与之关联的引用队列中。程序可以通过判断引用队列中是否已经加入了虚引用，来了解被引用的对象是否将要被垃圾回收。程序如果发现某个虚引用已经被加入到引用队列，那么就可以在所引用的对象的内存被回收之前采取必要的行动。 特别注意，在程序设计中一般很少使用弱引用与虚引用，使用软引用的情况较多，这是因为软引用可以加速 JVM 对垃圾内存的回收速度，可以维护系统的运行安全，防止内存溢出（OutOfMemory）等问题的产生。 不可达对象别非“非死不可“即使在可达性分析法中不可达的对象，也并非是“非死不可”的，这时候它们暂时处于“缓刑阶段”，要真正宣告一个对象死亡，至少要经历两次标记过程；可达性分析法中不可达的对象被第一次标记并且进行一次筛选，筛选的条件是此对象是否有必要执行 finalize 方法。当对象没有覆盖 finalize 方法，或 finalize 方法已经被虚拟机调用过时，虚拟机将这两种情况视为没有必要执行。 被判定为需要执行的对象将会被放在一个队列中进行第二次标记，除非这个对象与引用链上的任何一个对象建立关联，否则就会被真的回收。 如何判断常量是否废弃?运行时常量池主要回收的是废弃的常量。那么，我们如何判断一个常量是废弃常量呢？ JDK1.7 及之后版本的 JVM 已经将运行时常量池从方法区中移了出来，在 Java 堆（Heap）中开辟了一块区域存放运行时常量池。 修正(issue747，reference)： 1. **JDK1.7 之前运行时常量池逻辑包含字符串常量池存放在方法区, 此时 hotspot 虚拟机对方法区的实现为永久代** 2. **JDK1.7 字符串常量池被从方法区拿到了堆中, 这里没有提到运行时常量池,也就是说字符串常量池被单独拿到堆,运行时常量池剩下的东西还在方法区, 也就是 hotspot 中的永久代** 。 3. **JDK1.8 hotspot 移除了永久代用元空间(Metaspace)取而代之, 这时候字符串常量池还在堆, 运行时常量池还在方法区, 只不过方法区的实现从永久代变成了元空间(Metaspace)** 假如在字符串常量池中存在字符串 “abc”，如果当前没有任何 String 对象引用该字符串常量的话，就说明常量 “abc” 就是废弃常量，如果这时发生内存回收的话而且有必要的话，”abc” 就会被系统清理出常量池了。 如何判断类为无用类方法区主要回收的是无用的类，那么如何判断一个类是无用的类的呢？ 判定一个常量是否是“废弃常量”比较简单，而要判定一个类是否是“无用的类”的条件则相对苛刻许多。类需要同时满足下面 3 个条件才能算是 “无用的类” ： 该类所有的实例都已经被回收，也就是 Java 堆中不存在该类的任何实例。 加载该类的 ClassLoader 已经被回收。 该类对应的 java.lang.Class 对象没有在任何地方被引用，无法在任何地方通过反射访问该类的方法。 虚拟机可以对满足上述 3 个条件的无用类进行回收，这里说的仅仅是“可以”，而并不是和对象一样不使用了就会必然被回收。 垃圾收集算法 标记-清除术算法该算法分为“标记”和“清除”阶段：首先标记出所有不需要回收的对象，在标记完成后统一回收掉所有没有被标记的对象。它是最基础的收集算法，后续的算法都是对其不足进行改进得到。这种垃圾收集算法会带来两个明显的问题： 效率问题 空间问题（标记清除后会产生大量不连续的碎片） 标记-复制算法为了解决效率问题，“标记-复制”收集算法出现了。它可以将内存分为大小相同的两块，每次使用其中的一块。当这一块的内存使用完后，就将还存活的对象复制到另一块去，然后再把使用的空间一次清理掉。这样就使每次的内存回收都是对内存区间的一半进行回收。 标记-整理算法根据老年代的特点提出的一种标记算法，标记过程仍然与“标记-清除”算法一样，但后续步骤不是直接对可回收对象回收，而是让所有存活的对象向一端移动，然后直接清理掉端边界以外的内存。 分代收集算法当前虚拟机的垃圾收集都采用分代收集算法，这种算法没有什么新的思想，只是根据对象存活周期的不同将内存分为几块。一般将 java 堆分为新生代和老年代，这样我们就可以根据各个年代的特点选择合适的垃圾收集算法。 比如在新生代中，每次收集都会有大量对象死去，所以可以选择”标记-复制“算法，只需要付出少量对象的复制成本就可以完成每次垃圾收集。而老年代的对象存活几率是比较高的，而且没有额外的空间对它进行分配担保，所以我们必须选择“标记-清除”或“标记-整理”算法进行垃圾收集。 延伸面试问题： HotSpot 为什么要分为新生代和老年代？ 根据上面的对分代收集算法的介绍回答。 垃圾收集器 HotSpot VM 中的垃圾回收器，以及适用场景 到 jdk8 为止，默认的垃圾收集器是 Parallel Scavenge 和 Parallel Old 从 jdk9 开始，G1 收集器成为默认的垃圾收集器 目前来看，G1 回收器停顿时间最短而且没有明显缺点，非常适合 Web 应用。在 jdk8 中测试 Web 应用，堆内存 6G，新生代 4.5G 的情况下，Parallel Scavenge 回收新生代停顿长达 1.5 秒。G1 回收器回收同样大小的新生代只停顿 0.2 秒。 如果说收集算法是内存回收的方法论，那么垃圾收集器就是内存回收的具体实现。 虽然我们对各个收集器进行比较，但并非要挑选出一个最好的收集器。因为直到现在为止还没有最好的垃圾收集器出现，更加没有万能的垃圾收集器，我们能做的就是根据具体应用场景选择适合自己的垃圾收集器。试想一下：如果有一种四海之内、任何场景下都适用的完美收集器存在，那么我们的 HotSpot 虚拟机就不会实现那么多不同的垃圾收集器了。 Serial（串行）收集器Serial（串行）收集器是最基本、历史最悠久的垃圾收集器了。大家看名字就知道这个收集器是一个单线程收集器了。它的 “单线程” 的意义不仅仅意味着它只会使用一条垃圾收集线程去完成垃圾收集工作，更重要的是它在进行垃圾收集工作的时候必须暂停其他所有的工作线程（ “Stop The World” ），直到它收集结束。 新生代采用标记-复制算法，老年代采用标记-整理算法。 虚拟机的设计者们当然知道 Stop The World 带来的不良用户体验，所以在后续的垃圾收集器设计中停顿时间在不断缩短（仍然还有停顿，寻找最优秀的垃圾收集器的过程仍然在继续）。 但是 Serial 收集器有没有优于其他垃圾收集器的地方呢？当然有，它简单而高效（与其他收集器的单线程相比）。Serial 收集器由于没有线程交互的开销，自然可以获得很高的单线程收集效率。Serial 收集器对于运行在 Client 模式下的虚拟机来说是个不错的选择。 ParNew收集器ParNew 收集器其实就是 Serial 收集器的多线程版本，除了使用多线程进行垃圾收集外，其余行为（控制参数、收集算法、回收策略等等）和 Serial 收集器完全一样。 新生代采用标记-复制算法，老年代采用标记-整理算法。 它是许多运行在 Server 模式下的虚拟机的首要选择，除了 Serial 收集器外，只有它能与 CMS 收集器（真正意义上的并发收集器，后面会介绍到）配合工作。 并行和并发概念补充： 并行（Parallel） ：指多条垃圾收集线程并行工作，但此时用户线程仍然处于等待状态。 并发（Concurrent）：指用户线程与垃圾收集线程同时执行（但不一定是并行，可能会交替执行），用户程序在继续运行，而垃圾收集器运行在另一个 CPU 上。 Parallel Scavenge 收集器Parallel Scavenge 收集器也是使用标记-复制算法的多线程收集器，它看上去几乎和 ParNew 都一样。 那么它有什么特别之处呢？ -XX:+UseParallelGC 使用 Parallel 收集器+ 老年代串行 -XX:+UseParallelOldGC 使用 Parallel 收集器+ 老年代并行 Copy to clipboardErrorCopied Parallel Scavenge 收集器关注点是吞吐量（高效率的利用 CPU）。CMS 等垃圾收集器的关注点更多的是用户线程的停顿时间（提高用户体验）。所谓吞吐量就是 CPU 中用于运行用户代码的时间与 CPU 总消耗时间的比值。 Parallel Scavenge 收集器提供了很多参数供用户找到最合适的停顿时间或最大吞吐量，如果对于收集器运作不太了解，手工优化存在困难的时候，使用 Parallel Scavenge 收集器配合自适应调节策略，把内存管理优化交给虚拟机去完成也是一个不错的选择。 新生代采用标记-复制算法，老年代采用标记-整理算法。 这是 JDK1.8 默认收集器 使用 java -XX:+PrintCommandLineFlags -version 命令查看 -XX:InitialHeapSize=262921408 -XX:MaxHeapSize=4206742528 -XX:+PrintCommandLineFlags -XX:+UseCompressedClassPointers -XX:+UseCompressedOops -XX:+UseParallelGC java version \"1.8.0_211\" Java(TM) SE Runtime Environment (build 1.8.0_211-b12) Java HotSpot(TM) 64-Bit Server VM (build 25.211-b12, mixed mode)Copy to clipboardErrorCopied JDK1.8 默认使用的是 Parallel Scavenge + Parallel Old，如果指定了-XX:+UseParallelGC 参数，则默认指定了-XX:+UseParallelOldGC，可以使用-XX:-UseParallelOldGC 来禁用该功能 Serial Old 收集器Serial 收集器的老年代版本，它同样是一个单线程收集器。它主要有两大用途：一种用途是在 JDK1.5 以及以前的版本中与 Parallel Scavenge 收集器搭配使用，另一种用途是作为 CMS 收集器的后备方案。 Parallel Scavenge收集器Parallel Scavenge 收集器的老年代版本。使用多线程和“标记-整理”算法。在注重吞吐量以及 CPU 资源的场合，都可以优先考虑 Parallel Scavenge 收集器和 Parallel Old 收集器。 CMS 收集器CMS（Concurrent Mark Sweep）收集器是一种以获取最短回收停顿时间为目标的收集器。它非常符合在注重用户体验的应用上使用。 CMS（Concurrent Mark Sweep）收集器是 HotSpot 虚拟机第一款真正意义上的并发收集器，它第一次实现了让垃圾收集线程与用户线程（基本上）同时工作。 从名字中的Mark Sweep这两个词可以看出，CMS 收集器是一种 “标记-清除”算法实现的，它的运作过程相比于前面几种垃圾收集器来说更加复杂一些。整个过程分为四个步骤： 初始标记： 暂停所有的其他线程，并记录下直接与 root 相连的对象，速度很快 ； 并发标记： 同时开启 GC 和用户线程，用一个闭包结构去记录可达对象。但在这个阶段结束，这个闭包结构并不能保证包含当前所有的可达对象。因为用户线程可能会不断的更新引用域，所以 GC 线程无法保证可达性分析的实时性。所以这个算法里会跟踪记录这些发生引用更新的地方。 重新标记： 重新标记阶段就是为了修正并发标记期间因为用户程序继续运行而导致标记产生变动的那一部分对象的标记记录，这个阶段的停顿时间一般会比初始标记阶段的时间稍长，远远比并发标记阶段时间短 并发清除： 开启用户线程，同时 GC 线程开始对未标记的区域做清扫。 从它的名字就可以看出它是一款优秀的垃圾收集器，主要优点：并发收集、低停顿。但是它有下面三个明显的缺点： 对 CPU 资源敏感； 无法处理浮动垃圾； 它使用的回收算法-“标记-清除”算法会导致收集结束时会有大量空间碎片产生。 G1 收集器G1 (Garbage-First) 是一款面向服务器的垃圾收集器,主要针对配备多颗处理器及大容量内存的机器. 以极高概率满足 GC 停顿时间要求的同时,还具备高吞吐量性能特征. 被视为 JDK1.7 中 HotSpot 虚拟机的一个重要进化特征。它具备一下特点： 并行与并发：G1 能充分利用 CPU、多核环境下的硬件优势，使用多个 CPU（CPU 或者 CPU 核心）来缩短 Stop-The-World 停顿时间。部分其他收集器原本需要停顿 Java 线程执行的 GC 动作，G1 收集器仍然可以通过并发的方式让 java 程序继续执行。 分代收集：虽然 G1 可以不需要其他收集器配合就能独立管理整个 GC 堆，但是还是保留了分代的概念。 空间整合：与 CMS 的“标记-清理”算法不同，G1 从整体来看是基于“标记-整理”算法实现的收集器；从局部上来看是基于“标记-复制”算法实现的。 可预测的停顿：这是 G1 相对于 CMS 的另一个大优势，降低停顿时间是 G1 和 CMS 共同的关注点，但 G1 除了追求低停顿外，还能建立可预测的停顿时间模型，能让使用者明确指定在一个长度为 M 毫秒的时间片段内。 G1 收集器的运作大致分为以下几个步骤： 初始标记 并发标记 最终标记 筛选回收 G1 收集器在后台维护了一个优先列表，每次根据允许的收集时间，优先选择回收价值最大的 Region(这也就是它的名字 Garbage-First 的由来) 。这种使用 Region 划分内存空间以及有优先级的区域回收方式，保证了 G1 收集器在有限时间内可以尽可能高的收集效率（把内存化整为零）。 ZGC 收集器与 CMS 中的 ParNew 和 G1 类似，ZGC 也采用标记-复制算法，不过 ZGC 对该算法做了重大改进。 在 ZGC 中出现 Stop The World 的情况会更少！ 详情可以看 ： 《新一代垃圾回收器 ZGC 的探索与实践》 参考 《深入理解 Java 虚拟机：JVM 高级特性与最佳实践（第二版》 https://my.oschina.net/hosee/blog/644618 https://docs.oracle.com/javase/specs/jvms/se8/html/index.html","categories":[{"name":"技术分享","slug":"技术分享","permalink":"https://docs.hehouhui.cn/categories/%E6%8A%80%E6%9C%AF%E5%88%86%E4%BA%AB/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://docs.hehouhui.cn/tags/Java/"},{"name":"Jvm","slug":"Jvm","permalink":"https://docs.hehouhui.cn/tags/Jvm/"}]},{"title":"Java基础-集合","slug":"archives/Java基础-集合","date":"2020-03-08T00:00:00.000Z","updated":"2023-10-08T06:42:00.000Z","comments":true,"path":"archives/44.html","link":"","permalink":"https://docs.hehouhui.cn/archives/44.html","excerpt":"","text":"ListArrayListArrayList 的底层是数组队列，相当于动态数组。与 Java 中的数组相比，它的容量能动态增长。在添加大量元素前，应用程序可以使用ensureCapacity操作来增加 ArrayList 实例的容量。这可以减少递增式再分配的数量。 ArrayList继承于 AbstractList ，实现了 List, RandomAccess, Cloneable, java.io.Serializable 这些接口。 public class ArrayList&lt;E> extends AbstractList&lt;E> implements List&lt;E>, RandomAccess, Cloneable, java.io.Serializable&#123; &#125; RandomAccess 是一个标志接口，表明实现这个这个接口的 List 集合是支持快速随机访问的。在 ArrayList 中，我们即可以通过元素的序号快速获取元素对象，这就是快速随机访问。 ArrayList 实现了 Cloneable 接口 ，即覆盖了函数clone()，能被克隆。 ArrayList 实现了 java.io.Serializable 接口，这意味着ArrayList支持序列化，能通过序列化去传输。 与 Vector 的区别 ArrayList 是 List 的主要实现类，底层使用 Object[ ]存储，适用于频繁的查找工作，线程不安全 ； Vector 是 List 的古老实现类，底层使用 Object[ ]存储，线程安全的。 与 LinkedList 区别 是否保证线程安全： ArrayList 和 LinkedList 都是不同步的，也就是不保证线程安全； 底层数据结构： Arraylist 底层使用的是 Object 数组；LinkedList 底层使用的是 双向链表 数据结构（JDK1.6 之前为循环链表，JDK1.7 取消了循环。注意双向链表和双向循环链表的区别，下面有介绍到！） 插入和删除是否受元素位置的影响： ① ArrayList 采用数组存储，所以插入和删除元素的时间复杂度受元素位置的影响。 比如：执行add(E e)方法的时候， ArrayList 会默认在将指定的元素追加到此列表的末尾，这种情况时间复杂度就是 O(1)。但是如果要在指定位置 i 插入和删除元素的话（add(int index, E element)）时间复杂度就为 O(n-i)。因为在进行上述操作的时候集合中第 i 和第 i 个元素之后的(n-i)个元素都要执行向后位&#x2F;向前移一位的操作。 ② LinkedList 采用链表存储，所以对于**add(E e)**方法的插入，删除元素时间复杂度不受元素位置的影响，近似 O(1)，如果是要在指定位置**i**插入和删除元素的话（**(add(int index, E element)**） 时间复杂度近似为**o(n))**因为需要先移动到指定位置再插入。 是否支持快速随机访问： LinkedList 不支持高效的随机元素访问，而 ArrayList 支持。快速随机访问就是通过元素的序号快速获取元素对象(对应于get(int index)方法)。 内存空间占用： ArrayList 的空 间浪费主要体现在在 list 列表的结尾会预留一定的容量空间，而 LinkedList 的空间花费则体现在它的每一个元素都需要消耗比 ArrayList 更多的空间（因为要存放直接后继和直接前驱以及数据）。 扩容机制JDK8）ArrayList 有三种方式来初始化，构造方法源码如下： /** * 默认初始容量大小 */ private static final int DEFAULT_CAPACITY = 10; private static final Object[] DEFAULTCAPACITY_EMPTY_ELEMENTDATA = &#123;&#125;; /** *默认构造函数，使用初始容量10构造一个空列表(无参数构造) */ public ArrayList() &#123; this.elementData = DEFAULTCAPACITY_EMPTY_ELEMENTDATA; &#125; /** * 带初始容量参数的构造函数。（用户自己指定容量） */ public ArrayList(int initialCapacity) &#123; if (initialCapacity > 0) &#123;//初始容量大于0 //创建initialCapacity大小的数组 this.elementData = new Object[initialCapacity]; &#125; else if (initialCapacity == 0) &#123;//初始容量等于0 //创建空数组 this.elementData = EMPTY_ELEMENTDATA; &#125; else &#123;//初始容量小于0，抛出异常 throw new IllegalArgumentException(\"Illegal Capacity: \"+ initialCapacity); &#125; &#125; /** *构造包含指定collection元素的列表，这些元素利用该集合的迭代器按顺序返回 *如果指定的集合为null，throws NullPointerException。 */ public ArrayList(Collection&lt;? extends E> c) &#123; elementData = c.toArray(); if ((size = elementData.length) != 0) &#123; // c.toArray might (incorrectly) not return Object[] (see 6260652) if (elementData.getClass() != Object[].class) elementData = Arrays.copyOf(elementData, size, Object[].class); &#125; else &#123; // replace with empty array. this.elementData = EMPTY_ELEMENTDATA; &#125; &#125; 细心的同学一定会发现 ：以无参数构造方法创建 ArrayList 时，实际上初始化赋值的是一个空数组。当真正对数组进行添加元素操作时，才真正分配容量。即向数组中添加第一个元素时，数组容量扩为 10。 下面在我们分析 ArrayList 扩容时会讲到这一点内容！ 补充：JDK7 new 无参构造的 ArrayList 对象时，直接创建了长度是 10 的 Object[]数组 elementData 。jdk7 中的 ArrayList 的对象的创建类似于单例的饿汉式，而 jdk8 中的 ArrayList 的对象的创建类似于单例的懒汉式。JDK8 的内存优化也值得我们在平时开发中学习。 当我们要 add 进第 1 个元素到 ArrayList 时，elementData.length 为 0 （因为还是一个空的 list），因为执行了 ensureCapacityInternal() 方法 ，所以 minCapacity 此时为 10。此时，minCapacity - elementData.length &gt; 0成立，所以会进入 grow(minCapacity) 方法。 当 add 第 2 个元素时，minCapacity 为 2，此时 e lementData.length(容量)在添加第一个元素后扩容成 10 了。此时，minCapacity - elementData.length &gt; 0 不成立，所以不会进入 （执行）grow(minCapacity) 方法。 添加第 3、4···到第 10 个元素时，依然不会执行 grow 方法，数组容量都为 10。 直到添加第 11 个元素，minCapacity(为 11)比 elementData.length（为 10）要大。进入 grow 方法进行扩容。 grow()方法 int newCapacity &#x3D; oldCapacity + (oldCapacity &gt;&gt; 1),所以 ArrayList 每次扩容之后容量都会变为原来的 1.5 倍左右（oldCapacity 为偶数就是 1.5 倍，否则是 1.5 倍左右）！ 奇偶不同，比如 ：10+10&#x2F;2 &#x3D; 15, 33+33&#x2F;2&#x3D;49。如果是奇数的话会丢掉小数. “&gt;&gt;”（移位运算符）：&gt;&gt;1 右移一位相当于除 2，右移 n 位相当于除以 2 的 n 次方。这里 oldCapacity 明显右移了 1 位所以相当于 oldCapacity &#x2F;2。对于大数据的 2 进制运算,位移运算符比那些普通运算符的运算要快很多,因为程序仅仅移动一下而已,不去计算,这样提高了效率,节省了资源 我们再来通过例子探究一下**grow()** 方法 ： 当 add 第 1 个元素时，oldCapacity 为 0，经比较后第一个 if 判断成立，newCapacity &#x3D; minCapacity(为 10)。但是第二个 if 判断不会成立，即 newCapacity 不比 MAX_ARRAY_SIZE 大，则不会进入 hugeCapacity 方法。数组容量为 10，add 方法中 return true,size 增为 1。 当 add 第 11 个元素进入 grow 方法时，newCapacity 为 15，比 minCapacity（为 11）大，第一个 if 判断不成立。新容量没有大于数组最大 size，不会进入 hugeCapacity 方法。数组容量扩为 15，add 方法中 return true,size 增为 11。 以此类推······ 这里补充一点比较重要，但是容易被忽视掉的知识点： java 中的 length属性是针对数组说的,比如说你声明了一个数组,想知道这个数组的长度则用到了 length 这个属性. java 中的 length() 方法是针对字符串说的,如果想看这个字符串的长度则用到 length() 这个方法. java 中的 size() 方法是针对泛型集合说的,如果想看这个泛型有多少个元素,就调用此方法来查看! LinkedList LinkedList 是一个实现了 List 接口和 Deque 接口的双端链表。 LinkedList 底层的链表结构使它支持高效的插入和删除操作，另外它实现了 Deque 接口，使得 LinkedList 类也具有队列的特性; LinkedList 不是线程安全的，如果想使 LinkedList 变成线程安全的，可以调用静态类 Collections 类中的 synchronizedList 方法： 如下图所示： 看完了图之后，我们再看 LinkedList 类中的一个 内部私有类 Node 就很好理解了： private static class Node&lt;E> &#123; E item;//节点值 Node&lt;E> next;//后继节点 Node&lt;E> prev;//前驱节点 Node(Node&lt;E> prev, E element, Node&lt;E> next) &#123; this.item = element; this.next = next; this.prev = prev; &#125; &#125; 这个类就代表双端链表的节点 Node。这个类有三个属性，分别是前驱节点，本节点的值，后继结点。 HashMapHashMap 主要用来存放键值对，它基于哈希表的 Map 接口实现，是常用的 Java 集合之一。 JDK1.8 之前 HashMap 由 数组+链表 组成的，数组是 HashMap 的主体，链表则是主要为了解决哈希冲突而存在的（“拉链法”解决冲突）.JDK1.8 以后在解决哈希冲突时有了较大的变化，当链表长度大于阈值（默认为 8）时，将链表转化为红黑树（将链表转换成红黑树前会判断，如果当前数组的长度小于 64，那么会选择先进行数组扩容，而不是转换为红黑树），以减少搜索时间，具体可以参考 treeifyBin方法。 底层结构1.8 前JDK1.8 之前 HashMap 底层是 数组和链表 结合在一起使用也就是 链表散列。HashMap 通过 key 的 hashCode 经过扰动函数处理过后得到 hash 值，然后通过 (n - 1) &amp; hash 判断当前元素存放的位置（这里的 n 指的是数组的长度），如果当前位置存在元素的话，就判断该元素与要存入的元素的 hash 值以及 key 是否相同，如果相同的话，直接覆盖，不相同就通过拉链法解决冲突。 所谓扰动函数指的就是 HashMap 的 hash 方法。使用 hash 方法也就是扰动函数是为了防止一些实现比较差的 hashCode() 方法 换句话说使用扰动函数之后可以减少碰撞。 JDK 1.8 的 hash 方法 相比于 JDK 1.7 hash 方法更加简化，但是原理不变。 static final int hash(Object key) &#123; int h; // key.hashCode()：返回散列值也就是hashcode // ^ ：按位异或 // >>>:无符号右移，忽略符号位，空位都以0补齐 return (key == null) ? 0 : (h = key.hashCode()) ^ (h >>> 16); &#125; 对比一下 JDK1.7 的 HashMap 的 hash 方法源码. static int hash(int h) &#123; // This function ensures that hashCodes that differ only by // constant multiples at each bit position have a bounded // number of collisions (approximately 8 at default load factor). h ^= (h >>> 20) ^ (h >>> 12); return h ^ (h >>> 7) ^ (h >>> 4); &#125; 相比于 JDK1.8 的 hash 方法 ，JDK 1.7 的 hash 方法的性能会稍差一点点，因为毕竟扰动了 4 次。 所谓 “拉链法” 就是：将链表和数组相结合。也就是说创建一个链表数组，数组中每一格就是一个链表。若遇到哈希冲突，则将冲突的值加到链表中即可。 1.8 后相比于之前的版本，jdk1.8 在解决哈希冲突时有了较大的变化，当链表长度大于阈值（默认为 8）时，将链表转化为红黑树，以减少搜索时间。 类的属性： public class HashMap&lt;K,V> extends AbstractMap&lt;K,V> implements Map&lt;K,V>, Cloneable, Serializable &#123; // 序列号 private static final long serialVersionUID = 362498820763181265L; // 默认的初始容量是16 static final int DEFAULT_INITIAL_CAPACITY = 1 &lt;&lt; 4; // 最大容量 static final int MAXIMUM_CAPACITY = 1 &lt;&lt; 30; // 默认的填充因子 static final float DEFAULT_LOAD_FACTOR = 0.75f; // 当桶(bucket)上的结点数大于这个值时会转成红黑树 static final int TREEIFY_THRESHOLD = 8; // 当桶(bucket)上的结点数小于这个值时树转链表 static final int UNTREEIFY_THRESHOLD = 6; // 桶中结构转化为红黑树对应的table的最小大小 static final int MIN_TREEIFY_CAPACITY = 64; // 存储元素的数组，总是2的幂次倍 transient Node&lt;k,v>[] table; // 存放具体元素的集 transient Set&lt;map.entry&lt;k,v>> entrySet; // 存放元素的个数，注意这个不等于数组的长度。 transient int size; // 每次扩容和更改map结构的计数器 transient int modCount; // 临界值 当实际大小(容量*填充因子)超过临界值时，会进行扩容 int threshold; // 加载因子 final float loadFactor; &#125; loadFactor 加载因子 loadFactor 加载因子是控制数组存放数据的疏密程度，loadFactor 越趋近于 1，那么 数组中存放的数据(entry)也就越多，也就越密，也就是会让链表的长度增加，loadFactor 越小，也就是趋近于 0，数组中存放的数据(entry)也就越少，也就越稀疏。 loadFactor 太大导致查找元素效率低，太小导致数组的利用率低，存放的数据会很分散。loadFactor 的默认值为 0.75f 是官方给出的一个比较好的临界值。 给定的默认容量为 16，负载因子为 0.75。Map 在使用过程中不断的往里面存放数据，当数量达到了 16 * 0.75 &#x3D; 12 就需要将当前 16 的容量进行扩容，而扩容这个过程涉及到 rehash、复制数据等操作，所以非常消耗性能。 threshold threshold &#x3D; capacity _ _loadFactor*，当 Size&gt;&#x3D;threshold的时候，那么就要考虑对数组的扩增了，也就是说，这个的意思就是 衡量数组是否需要扩增的一个标准。 put 方法（扩容）HashMap 只提供了 put 用于添加元素，putVal 方法只是给 put 方法调用的一个方法，并没有提供给用户使用。 对 putVal 方法添加元素的分析如下： ① 如果定位到的数组位置没有元素 就直接插入。 ② 如果定位到的数组位置有元素就和要插入的 key 比较，如果 key 相同就直接覆盖，如果 key 不相同，就判断 p 是否是一个树节点，如果是就调用e = ((TreeNode)p).putTreeVal(this, tab, hash, key, value)将元素添加进入。如果不是就遍历链表插入(插入的是链表尾部)。 ps:下图有一个小问题，来自 issue#608指出：直接覆盖之后应该就会 return，不会有后续操作。参考 JDK8 HashMap.java 658 行。 我们再来对比一下 JDK1.7 put 方法的代码 对于 put 方法的分析如下： ① 如果定位到的数组位置没有元素 就直接插入。 ② 如果定位到的数组位置有元素，遍历以这个元素为头结点的链表，依次和插入的 key 比较，如果 key 相同就直接覆盖，不同就采用头插法插入元素。 resize 方法进行扩容，会伴随着一次重新 hash 分配，并且会遍历 hash 表中所有的元素，是非常耗时的。在编写程序中，要尽量避免 resize。 LinkedList 的底层结构是双端链表，支持高效的插入和删除操作，同时实现了 Deque 接口，也具有队列的特性。 HashMap 主要用来存放键值对，基于哈希表的 Map 接口实现，是常用的 Java 集合之一。JDK1.8 之前的 HashMap 由数组+链表组成，数组是 HashMap 的主体，链表则是主要为了解决哈希冲突而存在的。JDK1.8 以后，当链表长度大于阈值（默认为 8）时，将链表转化为红黑树，以减少搜索时间。 在进行扩容时，会伴随着一次重新 hash 分配，并且会遍历 hash 表中所有的元素，是非常耗时的，要尽量避免 resize。","categories":[{"name":"技术分享","slug":"技术分享","permalink":"https://docs.hehouhui.cn/categories/%E6%8A%80%E6%9C%AF%E5%88%86%E4%BA%AB/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://docs.hehouhui.cn/tags/Java/"},{"name":"数据结构","slug":"数据结构","permalink":"https://docs.hehouhui.cn/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}]},{"title":"Java基础-线程&并发","slug":"archives/Java基础-线程&并发","date":"2020-02-25T00:00:00.000Z","updated":"2023-10-08T06:42:00.000Z","comments":true,"path":"archives/22.html","link":"","permalink":"https://docs.hehouhui.cn/archives/22.html","excerpt":"","text":"线程线程与进程相似，但线程是一个比进程更小的执行单位。一个进程在其执行的过程中可以产生多个线程。与进程不同的是同类的多个线程共享同一块内存空间和一组系统资源，所以系统在产生一个线程，或是在各个线程之间作切换工作时，负担要比进程小得多，也正因为如此，线程也被称为轻量级进程。 程序是含有指令和数据的文件，被存储在磁盘或其他的数据存储设备中，也就是说程序是静态的代码。 进程是程序的一次执行过程，是系统运行程序的基本单位，因此进程是动态的。系统运行一个程序即是一个进程从创建，运行到消亡的过程。简单来说，一个进程就是一个执行中的程序，它在计算机中一个指令接着一个指令地执行着，同时，每个进程还占有某些系统资源如 CPU 时间，内存空间，文件，输入输出设备的使用权等等。换句话说，当程序在执行时，将会被操作系统载入内存中。 线程是进程划分成的更小的运行单位。线程和进程最大的不同在于基本上各进程是独立的，而各线程则不一定，因为同一进程中的线程极有可能会相互影响。从另一角度来说，进程属于操作系统的范畴，主要是同一段时间内，可以同时执行一个以上的程序，而线程则是在同一程序内几乎同时执行一个以上的程序段。 线程的生命周期Java 线程在运行的生命周期中的指定时刻只可能处于下面 6 种不同状态的其中一个状态（图源《Java 并发编程艺术》4.1.4 节）。 线程在生命周期中并不是固定处于某一个状态而是随着代码的执行在不同状态之间切换。Java 线程状态变迁如下图所示（图源《Java 并发编程艺术》4.1.4 节）： 订正(来自 issue736)：原图中 wait 到 runnable 状态的转换中，join 实际上是 Thread 类的方法，但这里写成了 Object。 由上图可以看出：线程创建之后它将处于 NEW（新建） 状态，调用 start() 方法后开始运行，线程这时候处于 READY（可运行） 状态。可运行状态的线程获得了 CPU 时间片（timeslice）后就处于 RUNNING（运行） 状态。 操作系统隐藏 Java 虚拟机（JVM）中的 READY 和 RUNNING 状态，它只能看到 RUNNABLE 状态（图源：HowToDoInJava：Java Thread Life Cycle and Thread States），所以 Java 系统一般将这两个状态统称为 RUNNABLE（运行中） 状态 。 当线程执行 wait()方法之后，线程进入 WAITING（等待） 状态。进入等待状态的线程需要依靠其他线程的通知才能够返回到运行状态，而 TIME_WAITING(超时等待) 状态相当于在等待状态的基础上增加了超时限制，比如通过 sleep（long millis）方法或 wait（long millis）方法可以将 Java 线程置于 TIMED WAITING 状态。当超时时间到达后 Java 线程将会返回到 RUNNABLE 状态。当线程调用同步方法时，在没有获取到锁的情况下，线程将会进入到 BLOCKED（阻塞） 状态。线程在执行 Runnable 的run()方法之后将会进入到 TERMINATED（终止） 状态。 反射AVA 反射机制是在运行状态中，对于任意一个类，都能够知道这个类的所有属性和方法；对于任意一个对象，都能够调用它的任意一个方法和属性；这种动态获取的信息以及动态调用对象的方法的功能称为 java 语言的反射机制。 静态编译和动态编译 静态编译： 在编译时确定类型，绑定对象 动态编译： 运行时确定类型，绑定对象 优缺点 优点： 运行期类型的判断，动态加载类，提高代码灵活度。 缺点： 1,性能瓶颈：反射相当于一系列解释操作，通知 JVM 要做的事情，性能比直接的 java 代码要慢很多。2,安全问题，让我们可以动态操作改变类的属性同时也增加了类的安全隐患。 应用场景反射是框架设计的灵魂。 在我们平时的项目开发过程中，基本上很少会直接使用到反射机制，但这不能说明反射机制没有用，实际上有很多设计、开发都与反射机制有关，例如模块化的开发，通过反射去调用对应的字节码；动态代理设计模式也采用了反射机制，还有我们日常使用的 Spring／Hibernate 等框架也大量使用到了反射机制。 举例： 我们在使用 JDBC 连接数据库时使用 Class.forName()通过反射加载数据库的驱动程序； Spring 框架的 IOC（动态加载管理 Bean）创建对象以及 AOP（动态代理）功能都和反射有联系； 动态配置实例的属性； …… 异常结构图 在 Java 中，所有的异常都有一个共同的祖先 java.lang 包中的 Throwable 类。Throwable 类有两个重要的子类 Exception（异常）和 Error（错误）。Exception 能被程序本身处理(try-catch)， Error 是无法处理的(只能尽量避免)。 Exception 和 Error 二者都是 Java 异常处理的重要子类，各自都包含大量子类。 Exception :程序本身可以处理的异常，可以通过 catch 来进行捕获。Exception 又可以分为 受检查异常(必须处理) 和 不受检查异常(可以不处理)。 Error ：Error 属于程序无法处理的错误 ，我们没办法通过 catch 来进行捕获 。例如，Java 虚拟机运行错误（Virtual MachineError）、虚拟机内存不够错误(OutOfMemoryError)、类定义错误（NoClassDefFoundError）等 。这些异常发生时，Java 虚拟机（JVM）一般会选择线程终止。 受检查异常Java 代码在编译过程中，如果受检查异常没有被 catch&#x2F;throw 处理的话，就没办法通过编译 。比如下面这段 IO 操作的代码。 除了RuntimeException及其子类以外，其他的Exception类及其子类都属于受检查异常 。常见的受检查异常有： IO 相关的异常、ClassNotFoundException 、SQLException…。 不受检查异常Java 代码在编译过程中 ，我们即使不处理不受检查异常也可以正常通过编译。 RuntimeException 及其子类都统称为非受检查异常，例如：NullPointExecrption、NumberFormatException（字符串转换为数字）、ArrayIndexOutOfBoundsException（数组越界）、ClassCastException（类型转换错误）、ArithmeticException（算术错误）等。 IOIO 流分为几种 按照流的流向分，可以分为输入流和输出流； 按照操作单元划分，可以划分为字节流和字符流； 按照流的角色划分为节点流和处理流。 Java Io 流共涉及 40 多个类，这些类看上去很杂乱，但实际上很有规则，而且彼此之间存在非常紧密的联系， Java I0 流的 40 多个类都是从如下 4 个抽象类基类中派生出来的。 InputStream&#x2F;Reader: 所有的输入流的基类，前者是字节输入流，后者是字符输入流。 OutputStream&#x2F;Writer: 所有输出流的基类，前者是字节输出流，后者是字符输出流。 按操作方式分类结构图： 按操作对象分类结构图： 为什么要字符流问题本质想问：不管是文件读写还是网络发送接收，信息的最小存储单元都是字节，那为什么 I&#x2F;O 流操作要分为字节流操作和字符流操作呢？ 回答：字符流是由 Java 虚拟机将字节转换得到的，问题就出在这个过程还算是非常耗时，并且，如果我们不知道编码类型就很容易出现乱码问题。所以， I&#x2F;O 流就干脆提供了一个直接操作字符的接口，方便我们平时对字符进行流操作。如果音频文件、图片等媒体文件用字节流比较好，如果涉及到字符的话使用字符流比较好。 BIO,NIO,AIO BIO (Blocking I&#x2F;O): 同步阻塞 I&#x2F;O 模式，数据的读取写入必须阻塞在一个线程内等待其完成。在活动连接数不是特别高（小于单机 1000）的情况下，这种模型是比较不错的，可以让每一个连接专注于自己的 I&#x2F;O 并且编程模型简单，也不用过多考虑系统的过载、限流等问题。线程池本身就是一个天然的漏斗，可以缓冲一些系统处理不了的连接或请求。但是，当面对十万甚至百万级连接的时候，传统的 BIO 模型是无能为力的。因此，我们需要一种更高效的 I&#x2F;O 处理模型来应对更高的并发量。 NIO (Non-blocking&#x2F;New I&#x2F;O): NIO 是一种同步非阻塞的 I&#x2F;O 模型，在 Java 1.4 中引入了 NIO 框架，对应 java.nio 包，提供了 Channel , Selector，Buffer 等抽象。NIO 中的 N 可以理解为 Non-blocking，不单纯是 New。它支持面向缓冲的，基于通道的 I&#x2F;O 操作方法。 NIO 提供了与传统 BIO 模型中的 Socket 和 ServerSocket 相对应的 SocketChannel 和 ServerSocketChannel 两种不同的套接字通道实现,两种通道都支持阻塞和非阻塞两种模式。阻塞模式使用就像传统中的支持一样，比较简单，但是性能和可靠性都不好；非阻塞模式正好与之相反。对于低负载、低并发的应用程序，可以使用同步阻塞 I&#x2F;O 来提升开发速率和更好的维护性；对于高负载、高并发的（网络）应用，应使用 NIO 的非阻塞模式来开发 AIO (Asynchronous I&#x2F;O): AIO 也就是 NIO 2。在 Java 7 中引入了 NIO 的改进版 NIO 2,它是异步非阻塞的 IO 模型。异步 IO 是基于事件和回调机制实现的，也就是应用操作之后会直接返回，不会堵塞在那里，当后台处理完成，操作系统会通知相应的线程进行后续的操作。AIO 是异步 IO 的缩写，虽然 NIO 在网络操作中，提供了非阻塞的方法，但是 NIO 的 IO 行为还是同步的。对于 NIO 来说，我们的业务线程是在 IO 操作准备好时，得到通知，接着就由这个线程自行进行 IO 操作，IO 操作本身是同步的。查阅网上相关资料，我发现就目前来说 AIO 的应用还不是很广泛，Netty 之前也尝试使用过 AIO，不过又放弃了。 并发synchronizedsynchronized 关键字解决的是多个线程之间访问资源的同步性，**synchronized**关键字可以保证被它修饰的方法或者代码块在任意时刻只能有一个线程执行。 另外，在 Java 早期版本中，synchronized 属于 重量级锁，效率低下。 为什么呢？ 因为监视器锁（monitor）是依赖于底层的操作系统的 Mutex Lock 来实现的，Java 的线程是映射到操作系统的原生线程之上的。如果要挂起或者唤醒一个线程，都需要操作系统帮忙完成，而操作系统实现线程之间的切换时需要从用户态转换到内核态，这个状态之间的转换需要相对比较长的时间，时间成本相对较高。 庆幸的是在 Java 6 之后 Java 官方对从 JVM 层面对 synchronized 较大优化，所以现在的 synchronized 锁效率也优化得很不错了。JDK1.6 对锁的实现引入了大量的优化，如自旋锁、适应性自旋锁、锁消除、锁粗化、偏向锁、轻量级锁等技术来减少锁操作的开销。 所以，你会发现目前的话，不论是各种开源框架还是 JDK 源码都大量使用了 synchronized 关键字。 使用方式synchronized 关键字最主要的三种使用方式： 1.修饰实例方法: 作用于当前对象实例加锁，进入同步代码前要获得 当前对象实例的锁 synchronized void method() &#123; //业务代码 &#125;Copy to clipboardErrorCopied 2.修饰静态方法: 也就是给当前类加锁，会作用于类的所有对象实例 ，进入同步代码前要获得 当前 class 的锁。因为静态成员不属于任何一个实例对象，是类成员（ _static 表明这是该类的一个静态资源，不管 new 了多少个对象，只有一份_）。所以，如果一个线程 A 调用一个实例对象的非静态 synchronized 方法，而线程 B 需要调用这个实例对象所属类的静态 synchronized 方法，是允许的，不会发生互斥现象，因为访问静态 synchronized 方法占用的锁是当前类的锁，而访问非静态 synchronized 方法占用的锁是当前实例对象锁。 synchronized void staic method() &#123; //业务代码 &#125;Copy to clipboardErrorCopied 3.修饰代码块 ：指定加锁对象，对给定对象&#x2F;类加锁。synchronized(this|object) 表示进入同步代码库前要获得给定对象的锁。synchronized(类.class) 表示进入同步代码前要获得 当前 class 的锁 synchronized(this) &#123; //业务代码 &#125;Copy to clipboardErrorCopied 总结： synchronized 关键字加到 static 静态方法和 synchronized(class) 代码块上都是是给 Class 类上锁。 synchronized 关键字加到实例方法上是给对象实例上锁。 尽量不要使用 synchronized(String a) 因为 JVM 中，字符串常量池具有缓存功能！ 下面我以一个常见的面试题为例讲解一下 synchronized 关键字的具体使用。 面试中面试官经常会说：“单例模式了解吗？来给我手写一下！给我解释一下双重检验锁方式实现单例模式的原理呗！” 双重校验锁实现对象单例（线程安全） public class Singleton &#123; private volatile static Singleton uniqueInstance; private Singleton() &#123; &#125; public static Singleton getUniqueInstance() &#123; //先判断对象是否已经实例过，没有实例化过才进入加锁代码 if (uniqueInstance == null) &#123; //类对象加锁 synchronized (Singleton.class) &#123; if (uniqueInstance == null) &#123; uniqueInstance = new Singleton(); &#125; &#125; &#125; return uniqueInstance; &#125; &#125;Copy to clipboardErrorCopied 另外，需要注意 uniqueInstance 采用 volatile 关键字修饰也是很有必要。 uniqueInstance 采用 volatile 关键字修饰也是很有必要的， uniqueInstance = new Singleton(); 这段代码其实是分为三步执行： 为 uniqueInstance 分配内存空间 初始化 uniqueInstance 将 uniqueInstance 指向分配的内存地址 但是由于 JVM 具有指令重排的特性，执行顺序有可能变成 1-&gt;3-&gt;2。指令重排在单线程环境下不会出现问题，但是在多线程环境下会导致一个线程获得还没有初始化的实例。例如，线程 T1 执行了 1 和 3，此时 T2 调用 getUniqueInstance() 后发现 uniqueInstance 不为空，因此返回 uniqueInstance，但此时 uniqueInstance 还未被初始化。 使用 volatile 可以禁止 JVM 的指令重排，保证在多线程环境下也能正常运行。 底层原理synchronized 关键字底层原理属于 JVM 层面。 代码块 public class SynchronizedDemo &#123; public void method() &#123; synchronized (this) &#123; System.out.println(\"synchronized 代码块\"); &#125; &#125; &#125; 通过 JDK 自带的 javap 命令查看 SynchronizedDemo 类的相关字节码信息：首先切换到类的对应目录执行 javac SynchronizedDemo.java 命令生成编译后的 .class 文件，然后执行javap -c -s -v -l SynchronizedDemo.class。 从上面我们可以看出： synchronized 同步语句块的实现使用的是 monitorenter 和 monitorexit 指令，其中 monitorenter 指令指向同步代码块的开始位置，**monitorexit** 指令则指明同步代码块的结束位置。 当执行 monitorenter 指令时，线程试图获取锁也就是获取 对象监视器 monitor 的持有权。 在 Java 虚拟机(HotSpot)中，Monitor 是基于 C++实现的，由 ObjectMonitor 实现的。每个对象中都内置了一个 ObjectMonitor 对象。 另外，**`wait/notify`****等方法也依赖于****`monitor`****对象，这就是为什么只有在同步的块或者方法中才能调用****`wait/notify`****等方法，否则会抛出****`java.lang.IllegalMonitorStateException`****的异常的原因。** 在执行monitorenter时，会尝试获取对象的锁，如果锁的计数器为 0 则表示锁可以被获取，获取后将锁计数器设为 1 也就是加 1。 在执行 monitorexit 指令后，将锁计数器设为 0，表明锁被释放。如果获取对象锁失败，那当前线程就要阻塞等待，直到锁被另外一个线程释放为止。 修饰方法 public class SynchronizedDemo2 &#123; public synchronized void method() &#123; System.out.println(\"synchronized 方法\"); &#125; &#125; Copy to clipboardErrorCopied synchronized 修饰的方法并没有 monitorenter 指令和 monitorexit 指令，取得代之的确实是 ACC_SYNCHRONIZED 标识，该标识指明了该方法是一个同步方法。JVM 通过该 ACC_SYNCHRONIZED 访问标志来辨别一个方法是否声明为同步方法，从而执行相应的同步调用。 锁乐观锁和悲观锁 乐观锁对应于生活中乐观的人总是想着事情往好的方向发展，悲观锁对应于生活中悲观的人总是想着事情往坏的方向发展。这两种人各有优缺点，不能不以场景而定说一种人好于另外一种人。 悲观锁 总是假设最坏的情况，每次去拿数据的时候都认为别人会修改，所以每次在拿数据的时候都会上锁，这样别人想拿这个数据就会阻塞直到它拿到锁（共享资源每次只给一个线程使用，其它线程阻塞，用完后再把资源转让给其它线程）。传统的关系型数据库里边就用到了很多这种锁机制，比如行锁，表锁等，读锁，写锁等，都是在做操作之前先上锁。Java 中 synchronized 和 ReentrantLock 等独占锁就是悲观锁思想的实现。 乐观锁 总是假设最好的情况，每次去拿数据的时候都认为别人不会修改，所以不会上锁，但是在更新的时候会判断一下在此期间别人有没有去更新这个数据，可以使用版本号机制和 CAS 算法实现。乐观锁适用于多读的应用类型，这样可以提高吞吐量，像数据库提供的类似于 write_condition 机制，其实都是提供的乐观锁。在 Java 中 java.util.concurrent.atomic 包下面的原子变量类就是使用了乐观锁的一种实现方式 CAS 实现的。 使用场景从上面对两种锁的介绍，我们知道两种锁各有优缺点，不可认为一种好于另一种，像乐观锁适用于写比较少的情况下（多读场景），即冲突真的很少发生的时候，这样可以省去了锁的开销，加大了系统的整个吞吐量。但如果是多写的情况，一般会经常产生冲突，这就会导致上层应用会不断的进行 retry，这样反倒是降低了性能，所以一般多写的场景下用悲观锁就比较合适。 乐观锁版本号机制一般是在数据表中加上一个数据版本号 version 字段，表示数据被修改的次数，当数据被修改时，version 值会加一。当线程 A 要更新数据值时，在读取数据的同时也会读取 version 值，在提交更新时，若刚才读取到的 version 值为当前数据库中的 version 值相等时才更新，否则重试更新操作，直到更新成功。 举一个简单的例子： 假设数据库中帐户信息表中有一个 version 字段，当前值为 1 ；而当前帐户余额字段（ balance ）为 $100 。 操作员 A 此时将其读出（ version&#x3D;1 ），并从其帐户余额中扣除 $50（ $100-$50 ）。 在操作员 A 操作的过程中，操作员 B 也读入此用户信息（ version&#x3D;1 ），并从其帐户余额中扣除 $20 （ $100-$20 ）。 操作员 A 完成了修改工作，将数据版本号（ version&#x3D;1 ），连同帐户扣除后余额（ balance&#x3D;$50 ），提交至数据库更新，此时由于提交数据版本等于数据库记录当前版本，数据被更新，数据库记录 version 更新为 2 。 操作员 B 完成了操作，也将版本号（ version&#x3D;1 ）试图向数据库提交数据（ balance&#x3D;$80 ），但此时比对数据库记录版本时发现，操作员 B 提交的数据版本号为 1 ，数据库记录当前版本也为 2 ，不满足 “ 提交版本必须等于当前版本才能执行更新 “ 的乐观锁策略，因此，操作员 B 的提交被驳回。 这样，就避免了操作员 B 用基于 version&#x3D;1 的旧数据修改的结果覆盖操作员 A 的操作结果的可能 CAS 算法即compare and swap（比较与交换），是一种有名的无锁算法。无锁编程，即不使用锁的情况下实现多线程之间的变量同步，也就是在没有线程被阻塞的情况下实现变量的同步，所以也叫非阻塞同步（Non-blocking Synchronization）。CAS 算法涉及到三个操作数 需要读写的内存值 V 进行比较的值 A 拟写入的新值 B 当且仅当 V 的值等于 A 时，CAS 通过原子方式用新值 B 来更新 V 的值，否则不会执行任何操作（比较和替换是一个原子操作）。一般情况下是一个自旋操作，即不断的重试。 关于自旋锁，大家可以看一下这篇文章，非常不错：《 面试必备之深入理解自旋锁》 缺点 ABA 问题是乐观锁一个常见的问题 ABA 问题如果一个变量 V 初次读取的时候是 A 值，并且在准备赋值的时候检查到它仍然是 A 值，那我们就能说明它的值没有被其他线程修改过了吗？很明显是不能的，因为在这段时间它的值可能被改为其他值，然后又改回 A，那 CAS 操作就会误认为它从来没有被修改过。这个问题被称为 CAS 操作的 “ABA”问题。 JDK 1.5 以后的 AtomicStampedReference 类就提供了此种能力，其中的 compareAndSet 方法就是首先检查当前引用是否等于预期引用，并且当前标志是否等于预期标志，如果全部相等，则以原子方式将该引用和该标志的值设置为给定的更新值。 开销大自旋 CAS（也就是不成功就一直循环执行直到成功）如果长时间不成功，会给 CPU 带来非常大的执行开销。 如果 JVM 能支持处理器提供的 pause 指令那么效率会有一定的提升，pause 指令有两个作用，第一它可以延迟流水线执行指令（de-pipeline）,使 CPU 不会消耗过多的执行资源，延迟的时间取决于具体实现的版本，在一些处理器上延迟时间是零。第二它可以避免在退出循环的时候因内存顺序冲突（memory order violation）而引起 CPU 流水线被清空（CPU pipeline flush），从而提高 CPU 的执行效率。 只能一个变量原子操作CAS 只对单个共享变量有效，当操作涉及跨多个共享变量时 CAS 无效。但是从 JDK 1.5 开始，提供了AtomicReference类来保证引用对象之间的原子性，你可以把多个变量放在一个对象里来进行 CAS 操作.所以我们可以使用锁或者利用AtomicReference类把多个共享变量合并成一个共享变量来操作。 CAS 与synchronized的使用情景 简单的来说 CAS 适用于写比较少的情况下（多读场景，冲突一般较少），synchronized 适用于写比较多的情况下（多写场景，冲突一般较多） 对于资源竞争较少（线程冲突较轻）的情况，使用synchronized同步锁进行线程阻塞和唤醒切换以及用户态内核态间的切换操作额外浪费消耗 cpu 资源；而 CAS 基于硬件实现，不需要进入内核，不需要切换线程，操作自旋几率较少，因此可以获得更高的性能。 对于资源竞争严重（线程冲突严重）的情况，CAS 自旋的概率会比较大，从而浪费更多的 CPU 资源，效率低于 synchronized。 补充： Java 并发编程这个领域中synchronized关键字一直都是元老级的角色，很久之前很多人都会称它为 “重量级锁” 。但是，在 JavaSE 1.6 之后进行了主要包括为了减少获得锁和释放锁带来的性能消耗而引入的 偏向锁 和 轻量级锁 以及其它各种优化之后变得在某些情况下并不是那么重了。synchronized的底层实现主要依靠 Lock-Free 的队列，基本思路是 自旋后阻塞，竞争切换后继续竞争锁，稍微牺牲了公平性，但获得了高吞吐量。在线程冲突较少的情况下，可以获得和 CAS 类似的性能；而线程冲突严重的情况下，性能远高于 CAS。 死锁线程死锁描述的是这样一种情况：多个线程同时被阻塞，它们中的一个或者全部都在等待某个资源被释放。由于线程被无限期地阻塞，因此程序不可能正常终止。 如下图所示，线程 A 持有资源 2，线程 B 持有资源 1，他们同时都想申请对方的资源，所以这两个线程就会互相等待而进入死锁状态。 线程 A 通过 synchronized (resource1) 获得 resource1 的监视器锁，然后通过Thread.sleep(1000);让线程 A 休眠 1s 为的是让线程 B 得到执行然后获取到 resource2 的监视器锁。线程 A 和线程 B 休眠结束了都开始企图请求获取对方的资源，然后这两个线程就会陷入互相等待的状态，这也就产生了死锁。上面的例子符合产生死锁的四个必要条件。 学过操作系统的朋友都知道产生死锁必须具备以下四个条件： 互斥条件：该资源任意一个时刻只由一个线程占用。 请求与保持条件：一个进程因请求资源而阻塞时，对已获得的资源保持不放。 不剥夺条件:线程已获得的资源在未使用完之前不能被其他线程强行剥夺，只有自己使用完毕后才释放资源。 循环等待条件:若干进程之间形成一种头尾相接的循环等待资源关系。 如何避免死锁我上面说了产生死锁的四个必要条件，为了避免死锁，我们只要破坏产生死锁的四个条件中的其中一个就可以了。现在我们来挨个分析一下： 破坏互斥条件 ：这个条件我们没有办法破坏，因为我们用锁本来就是想让他们互斥的（临界资源需要互斥访问）。 破坏请求与保持条件 ：一次性申请所有的资源。 破坏不剥夺条件 ：占用部分资源的线程进一步申请其他资源时，如果申请不到，可以主动释放它占有的资源。 破坏循环等待条件 ：靠按序申请资源来预防。按某一顺序申请资源，释放资源则反序释放。破坏循环等待条件。","categories":[{"name":"技术分享","slug":"技术分享","permalink":"https://docs.hehouhui.cn/categories/%E6%8A%80%E6%9C%AF%E5%88%86%E4%BA%AB/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://docs.hehouhui.cn/tags/Java/"},{"name":"异步编程","slug":"异步编程","permalink":"https://docs.hehouhui.cn/tags/%E5%BC%82%E6%AD%A5%E7%BC%96%E7%A8%8B/"},{"name":"多线程","slug":"多线程","permalink":"https://docs.hehouhui.cn/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"}]},{"title":"Java基础-class","slug":"archives/Java基础-class","date":"2020-01-26T00:00:00.000Z","updated":"2023-10-08T06:42:00.000Z","comments":true,"path":"archives/43.html","link":"","permalink":"https://docs.hehouhui.cn/archives/43.html","excerpt":"","text":"对象，类类加载一个类的完整生命周期如下： Class 文件需要加载到虚拟机中之后才能运行和使用，那么虚拟机是如何加载这些 Class 文件呢？ 系统加载 Class 类型的文件主要三步:加载-&gt;连接-&gt;初始化。连接过程又可分为三步:验证-&gt;准备-&gt;解析。 加载类加载过程的第一步，主要完成下面 3 件事情： 通过全类名获取定义此类的二进制字节流 将字节流所代表的静态存储结构转换为方法区的运行时数据结构 在内存中生成一个代表该类的 Class 对象,作为方法区这些数据的访问入口 虚拟机规范上面这 3 点并不具体，因此是非常灵活的。比如：”通过全类名获取定义此类的二进制字节流” 并没有指明具体从哪里获取、怎样获取。比如：比较常见的就是从 ZIP 包中读取（日后出现的 JAR、EAR、WAR 格式的基础）、其他文件生成（典型应用就是 JSP）等等。 一个非数组类的加载阶段（加载阶段获取类的二进制字节流的动作）是可控性最强的阶段，这一步我们可以去完成还可以自定义类加载器去控制字节流的获取方式（重写一个类加载器的 loadClass() 方法）。数组类型不通过类加载器创建，它由 Java 虚拟机直接创建。 类加载器、双亲委派模型也是非常重要的知识点，这部分内容会在后面的文章中单独介绍到。 加载阶段和连接阶段的部分内容是交叉进行的，加载阶段尚未结束，连接阶段可能就已经开始了。 验证 准备准备阶段是正式为类变量分配内存并设置类变量初始值的阶段，这些内存都将在方法区中分配。对于该阶段有以下几点需要注意： 这时候进行内存分配的仅包括类变量（static），而不包括实例变量，实例变量会在对象实例化时随着对象一块分配在 Java 堆中。 这里所设置的初始值”通常情况”下是数据类型默认的零值（如 0、0L、null、false 等），比如我们定义了public static int value=111 ，那么 value 变量在准备阶段的初始值就是 0 而不是 111（初始化阶段才会赋值）。特殊情况：比如给 value 变量加上了 fianl 关键字public static final int value=111 ，那么准备阶段 value 的值就被赋值为 111。 基本数据类型的零值： 解析解析阶段是虚拟机将常量池内的符号引用替换为直接引用的过程。解析动作主要针对类或接口、字段、类方法、接口方法、方法类型、方法句柄和调用限定符 7 类符号引用进行。 符号引用就是一组符号来描述目标，可以是任何字面量。直接引用就是直接指向目标的指针、相对偏移量或一个间接定位到目标的句柄。在程序实际运行时，只有符号引用是不够的，举个例子：在程序执行方法时，系统需要明确知道这个方法所在的位置。Java 虚拟机为每个类都准备了一张方法表来存放类中所有的方法。当需要调用一个类的方法的时候，只要知道这个方法在方发表中的偏移量就可以直接调用该方法了。通过解析操作符号引用就可以直接转变为目标方法在类中方法表的位置，从而使得方法可以被调用。 综上，解析阶段是虚拟机将常量池内的符号引用替换为直接引用的过程，也就是得到类或者字段、方法在内存中的指针或者偏移量。 初始化初始化是类加载的最后一步，也是真正执行类中定义的 Java 程序代码(字节码)，初始化阶段是执行初始化方法 ()方法的过程。 对于（） 方法的调用，虚拟机会自己确保其在多线程环境中的安全性。因为 （） 方法是带锁线程安全，所以在多线程环境下进行类初始化的话可能会引起死锁，并且这种死锁很难被发现。 对于初始化阶段，虚拟机严格规范了有且只有 5 种情况下，必须对类进行初始化(只有主动去使用类才会初始化类)： 当遇到 new 、 getstatic、putstatic 或 invokestatic 这 4 条直接码指令时，比如 new 一个类，读取一个静态字段(未被 final 修饰)、或调用一个类的静态方法时。 当 jvm 执行 new 指令时会初始化类。即当程序创建一个类的实例对象。 当 jvm 执行 getstatic 指令时会初始化类。即程序访问类的静态变量(不是静态常量，常量会被加载到运行时常量池)。 当 jvm 执行 putstatic 指令时会初始化类。即程序给类的静态变量赋值。 当 jvm 执行 invokestatic 指令时会初始化类。即程序调用类的静态方法。 使用 java.lang.reflect 包的方法对类进行反射调用时如 Class.forname(“…”),newInstance()等等。 ，如果类没初始化，需要触发其初始化。 初始化一个类，如果其父类还未初始化，则先触发该父类的初始化。 当虚拟机启动时，用户需要定义一个要执行的主类 (包含 main 方法的那个类)，虚拟机会先初始化这个类。 MethodHandle 和 VarHandle 可以看作是轻量级的反射调用机制，而要想使用这 2 个调用， 就必须先使用 findStaticVarHandle 来初始化要调用的类。 「补充，来自issue745」 当一个接口中定义了 JDK8 新加入的默认方法（被 default 关键字修饰的接口方法）时，如果有这个接口的实现类发生了初始化，那该接口要在其之前被初始化。 卸载 卸载这部分内容来自 issue#662 由 guang19 补充完善。 卸载类即该类的 Class 对象被 GC。 卸载类需要满足 3 个要求: 该类的所有的实例对象都已被 GC，也就是说堆不存在该类的实例对象。 该类没有在其他任何地方被引用 该类的类加载器的实例已被 GC 所以，在 JVM 生命周期类，由 jvm 自带的类加载器加载的类是不会被卸载的。但是由我们自定义的类加载器加载的类是可能被卸载的。 只要想通一点就好了，jdk 自带的 BootstrapClassLoader,ExtClassLoader,AppClassLoader 负责加载 jdk 提供的类，所以它们(类加载器的实例)肯定不会被回收。而我们自定义的类加载器的实例是可以被回收的，所以使用我们自定义加载器加载的类是可以被卸载掉的。 对象的创建下图便是 Java 对象的创建过程，我建议最好是能默写出来，并且要掌握每一步在做什么。 类加载检查 虚拟机遇到一条 new 指令时，首先将去检查这个指令的参数是否能在常量池中定位到这个类的符号引用，并且检查这个符号引用代表的类是否已被加载过、解析和初始化过。如果没有，那必须先执行相应的类加载过程。 分配内存 在类加载检查通过后，接下来虚拟机将为新生对象分配内存。对象所需的内存大小在类加载完成后便可确定，为对象分配空间的任务等同于把一块确定大小的内存从 Java 堆中划分出来。分配方式有 “指针碰撞” 和 “空闲列表” 两种，选择哪种分配方式由 Java 堆是否规整决定，而 Java 堆是否规整又由所采用的垃圾收集器是否带有压缩整理功能决定。 内存分配的两种方式：（补充内容，需要掌握） 选择以上两种方式中的哪一种，取决于 Java 堆内存是否规整。而 Java 堆内存是否规整，取决于 GC 收集器的算法是”标记-清除”，还是”标记-整理”（也称作”标记-压缩”），值得注意的是，复制算法内存也是规整的 内存分配并发问题（补充内容，需要掌握） 在创建对象的时候有一个很重要的问题，就是线程安全，因为在实际开发过程中，创建对象是很频繁的事情，作为虚拟机来说，必须要保证线程是安全的，通常来讲，虚拟机采用两种方式来保证线程安全： CAS+失败重试： CAS 是乐观锁的一种实现方式。所谓乐观锁就是，每次不加锁而是假设没有冲突而去完成某项操作，如果因为冲突失败就重试，直到成功为止。虚拟机采用 CAS 配上失败重试的方式保证更新操作的原子性。 TLAB： 为每一个线程预先在 Eden 区分配一块儿内存，JVM 在给线程中的对象分配内存时，首先在 TLAB 分配，当对象大于 TLAB 中的剩余内存或 TLAB 的内存已用尽时，再采用上述的 CAS 进行内存分配 初始化零值 内存分配完成后，虚拟机需要将分配到的内存空间都初始化为零值（不包括对象头），这一步操作保证了对象的实例字段在 Java 代码中可以不赋初始值就直接使用，程序能访问到这些字段的数据类型所对应的零值。 设置对象头 初始化零值完成之后，虚拟机要对对象进行必要的设置，例如这个对象是哪个类的实例、如何才能找到类的元数据信息、对象的哈希码、对象的 GC 分代年龄等信息。 这些信息存放在对象头中。 另外，根据虚拟机当前运行状态的不同，如是否启用偏向锁等，对象头会有不同的设置方式。 执行 init 方法在上面工作都完成之后，从虚拟机的视角来看，一个新的对象已经产生了，但从 Java 程序的视角来看，对象创建才刚开始，方法还没有执行，所有的字段都还为零。所以一般来说，执行 new 指令之后会接着执行 方法，把对象按照程序员的意愿进行初始化，这样一个真正可用的对象才算完全产生出来。 对象的内存区域在 Hotspot 虚拟机中，对象在内存中的布局可以分为 3 块区域：对象头、实例数据和对齐填充。 Hotspot 虚拟机的对象头包括两部分信息，第一部分用于存储对象自身的运行时数据（哈希码、GC 分代年龄、锁状态标志等等），另一部分是类型指针，即对象指向它的类元数据的指针，虚拟机通过这个指针来确定这个对象是那个类的实例。 实例数据部分是对象真正存储的有效信息，也是在程序中所定义的各种类型的字段内容。 对齐填充部分不是必然存在的，也没有什么特别的含义，仅仅起占位作用。 因为 Hotspot 虚拟机的自动内存管理系统要求对象起始地址必须是 8 字节的整数倍，换句话说就是对象的大小必须是 8 字节的整数倍。而对象头部分正好是 8 字节的倍数（1 倍或 2 倍），因此，当对象实例数据部分没有对齐时，就需要通过对齐填充来补全。 对象的访问定位建立对象就是为了使用对象，我们的 Java 程序通过栈上的 reference 数据来操作堆上的具体对象。对象的访问方式由虚拟机实现而定，目前主流的访问方式有① 使用句柄和② 直接指针两种： 句柄： 如果使用句柄的话，那么 Java 堆中将会划分出一块内存来作为句柄池，reference 中存储的就是对象的句柄地址，而句柄中包含了对象实例数据与类型数据各自的具体地址信息； 直接指针： 如果使用直接指针访问，那么 Java 堆对象的布局中就必须考虑如何放置访问类型数据的相关信息，而 reference 中存储的直接就是对象的地址。 这两种对象访问方式各有优势。使用句柄来访问的最大好处是 reference 中存储的是稳定的句柄地址，在对象被移动时只会改变句柄中的实例数据指针，而 reference 本身不需要修改。使用直接指针访问方式最大的好处就是速度快，它节省了一次指针定位的时间开销。","categories":[{"name":"技术分享","slug":"技术分享","permalink":"https://docs.hehouhui.cn/categories/%E6%8A%80%E6%9C%AF%E5%88%86%E4%BA%AB/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://docs.hehouhui.cn/tags/Java/"}]},{"title":"Java基础-String","slug":"archives/Java基础-String","date":"2020-01-22T00:00:00.000Z","updated":"2023-10-08T06:42:00.000Z","comments":true,"path":"archives/45.html","link":"","permalink":"https://docs.hehouhui.cn/archives/45.html","excerpt":"","text":"String 对象常量池String 对象的两种创建方式： String str1 = \"abcd\";//先检查字符串常量池中有没有\"abcd\"，如果字符串常量池中没有，则创建一个，然后 str1 指向字符串常量池中的对象，如果有，则直接将 str1 指向\"abcd\"\"； String str2 = new String(\"abcd\");//堆中创建一个新的对象 String str3 = new String(\"abcd\");//堆中创建一个新的对象 System.out.println(str1==str2);//false System.out.println(str2==str3);//falseCopy to clipboardErrorCopied 这两种不同的创建方法是有差别的。 第一种方式是在常量池中拿对象； 第二种方式是直接在堆内存空间创建一个新的对象。 记住一点：只要使用 new 方法，便需要创建新的对象。 再给大家一个图应该更容易理解，图片来源：https://www.journaldev.com/797/what-is-java-string-pool： String 类型的常量池比较特殊。它的主要使用方法有两种： 直接使用双引号声明出来的 String 对象会直接存储在常量池中。 如果不是用双引号声明的 String 对象，可以使用 String 提供的 intern 方法。String.intern() 是一个 Native 方法，它的作用是：如果运行时常量池中已经包含一个等于此 String 对象内容的字符串，则返回常量池中该字符串的引用；如果没有，JDK1.7 之前（不包含 1.7）的处理方式是在常量池中创建与此 String 内容相同的字符串，并返回常量池中创建的字符串的引用，JDK1.7 以及之后的处理方式是在常量池中记录此字符串的引用，并返回该引用。 String s1 = new String(\"计算机\"); String s2 = s1.intern(); String s3 = \"计算机\"; System.out.println(s2);//计算机 System.out.println(s1 == s2);//false，因为一个是堆内存中的 String 对象一个是常量池中的 String 对象， System.out.println(s3 == s2);//true，因为两个都是常量池中的 String 对象Copy to clipboardErrorCopied 字符串拼接: String str1 = \"str\"; String str2 = \"ing\"; String str3 = \"str\" + \"ing\";//常量池中的对象 String str4 = str1 + str2; //在堆上创建的新的对象 String str5 = \"string\";//常量池中的对象 System.out.println(str3 == str4);//false System.out.println(str3 == str5);//true System.out.println(str4 == str5);//falseCopy to clipboardErrorCopied 尽量避免多个字符串拼接，因为这样会重新创建对象。如果需要改变字符串的话，可以使用 StringBuilder 或者 StringBuffer。 new String(“test”); 创建几个对象?. 将创建 1 或 2 个字符串。如果池中已存在字符串常量“abc”，则只会在堆空间创建一个字符串常量“abc”。如果池中没有字符串常量“abc”，那么它将首先在池中创建，然后在堆空间中创建，因此将创建总共 2 个字符串对象。 验证： String s1 = new String(\"abc\");// 堆内存的地址值 String s2 = \"abc\"; System.out.println(s1 == s2);// 输出 false,因为一个是堆内存，一个是常量池的内存，故两者是不同的。 System.out.println(s1.equals(s2));// 输出 trueCopy to clipboardErrorCopied 结果： false true","categories":[{"name":"技术分享","slug":"技术分享","permalink":"https://docs.hehouhui.cn/categories/%E6%8A%80%E6%9C%AF%E5%88%86%E4%BA%AB/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://docs.hehouhui.cn/tags/Java/"}]}],"categories":[{"name":"技术分享","slug":"技术分享","permalink":"https://docs.hehouhui.cn/categories/%E6%8A%80%E6%9C%AF%E5%88%86%E4%BA%AB/"},{"name":"碎片杂文","slug":"碎片杂文","permalink":"https://docs.hehouhui.cn/categories/%E7%A2%8E%E7%89%87%E6%9D%82%E6%96%87/"},{"name":"学习思考","slug":"学习思考","permalink":"https://docs.hehouhui.cn/categories/%E5%AD%A6%E4%B9%A0%E6%80%9D%E8%80%83/"},{"name":"创作分享","slug":"创作分享","permalink":"https://docs.hehouhui.cn/categories/%E5%88%9B%E4%BD%9C%E5%88%86%E4%BA%AB/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://docs.hehouhui.cn/tags/Java/"},{"name":"异步编程","slug":"异步编程","permalink":"https://docs.hehouhui.cn/tags/%E5%BC%82%E6%AD%A5%E7%BC%96%E7%A8%8B/"},{"name":"多线程","slug":"多线程","permalink":"https://docs.hehouhui.cn/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"},{"name":"Spring","slug":"Spring","permalink":"https://docs.hehouhui.cn/tags/Spring/"},{"name":"微服务","slug":"微服务","permalink":"https://docs.hehouhui.cn/tags/%E5%BE%AE%E6%9C%8D%E5%8A%A1/"},{"name":"SAAS","slug":"SAAS","permalink":"https://docs.hehouhui.cn/tags/SAAS/"},{"name":"Redis","slug":"Redis","permalink":"https://docs.hehouhui.cn/tags/Redis/"},{"name":"咖啡","slug":"咖啡","permalink":"https://docs.hehouhui.cn/tags/%E5%92%96%E5%95%A1/"},{"name":"整活","slug":"整活","permalink":"https://docs.hehouhui.cn/tags/%E6%95%B4%E6%B4%BB/"},{"name":"技术流","slug":"技术流","permalink":"https://docs.hehouhui.cn/tags/%E6%8A%80%E6%9C%AF%E6%B5%81/"},{"name":"BUG","slug":"BUG","permalink":"https://docs.hehouhui.cn/tags/BUG/"},{"name":"hutool","slug":"hutool","permalink":"https://docs.hehouhui.cn/tags/hutool/"},{"name":"建站","slug":"建站","permalink":"https://docs.hehouhui.cn/tags/%E5%BB%BA%E7%AB%99/"},{"name":"keycloak","slug":"keycloak","permalink":"https://docs.hehouhui.cn/tags/keycloak/"},{"name":"oauth","slug":"oauth","permalink":"https://docs.hehouhui.cn/tags/oauth/"},{"name":"开发","slug":"开发","permalink":"https://docs.hehouhui.cn/tags/%E5%BC%80%E5%8F%91/"},{"name":"思考","slug":"思考","permalink":"https://docs.hehouhui.cn/tags/%E6%80%9D%E8%80%83/"},{"name":"物联网","slug":"物联网","permalink":"https://docs.hehouhui.cn/tags/%E7%89%A9%E8%81%94%E7%BD%91/"},{"name":"文字","slug":"文字","permalink":"https://docs.hehouhui.cn/tags/%E6%96%87%E5%AD%97/"},{"name":"学习","slug":"学习","permalink":"https://docs.hehouhui.cn/tags/%E5%AD%A6%E4%B9%A0/"},{"name":"工具","slug":"工具","permalink":"https://docs.hehouhui.cn/tags/%E5%B7%A5%E5%85%B7/"},{"name":"chatgpt","slug":"chatgpt","permalink":"https://docs.hehouhui.cn/tags/chatgpt/"},{"name":"notion","slug":"notion","permalink":"https://docs.hehouhui.cn/tags/notion/"},{"name":"分布式","slug":"分布式","permalink":"https://docs.hehouhui.cn/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"},{"name":"缓存","slug":"缓存","permalink":"https://docs.hehouhui.cn/tags/%E7%BC%93%E5%AD%98/"},{"name":"Python","slug":"Python","permalink":"https://docs.hehouhui.cn/tags/Python/"},{"name":"健康","slug":"健康","permalink":"https://docs.hehouhui.cn/tags/%E5%81%A5%E5%BA%B7/"},{"name":"图像处理","slug":"图像处理","permalink":"https://docs.hehouhui.cn/tags/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/"},{"name":"mysql","slug":"mysql","permalink":"https://docs.hehouhui.cn/tags/mysql/"},{"name":"响应式","slug":"响应式","permalink":"https://docs.hehouhui.cn/tags/%E5%93%8D%E5%BA%94%E5%BC%8F/"},{"name":"WebFlux","slug":"WebFlux","permalink":"https://docs.hehouhui.cn/tags/WebFlux/"},{"name":"Jvm","slug":"Jvm","permalink":"https://docs.hehouhui.cn/tags/Jvm/"},{"name":"数据结构","slug":"数据结构","permalink":"https://docs.hehouhui.cn/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}]}