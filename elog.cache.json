{
  "docs": [
    {
      "id": "3a5d9238-0fa8-4154-a6d5-92340f2ad733",
      "doc_id": "3a5d9238-0fa8-4154-a6d5-92340f2ad733",
      "title": "3a5d9238-0fa8-4154-a6d5-92340f2ad733",
      "updated": 1696747320000,
      "body_original": "\n微服务，顾名思义，就是将我们程序拆分为最小化单元来提供服务。在一体化系统中，各个微服务也是不可能独立存在的，那么微服务之间涉及到的数据依赖问题，应该怎么处理呢？我们从场景入手来分析考虑此类问题。\n\n\n## **一、场景**\n\n\n在一个供应链系统中，存在商品、销售订单、采购三个微服务，他们的主数据部分数据结构如下：\n\n\n商品：\n\n\n![](https://cdn.jsdelivr.net/gh/listener-He/images@default/202309151508807.png)\n\n\n订单和子订单：\n\n\n![](https://cdn.jsdelivr.net/gh/listener-He/images@default/202309151508648.png)\n\n\n采购单和子订单：\n\n\n![](https://cdn.jsdelivr.net/gh/listener-He/images@default/202309151508948.png)\n\n\n在设计这个供应链系统时，我们需要满足以下两个需求：\n\n- 根据商品的型号/分类/生成年份/编码等查找订单；\n- 根据商品的型号/分类/生成年份/编码等查找采购订单。\n\n初期我们的方案是这样设计的：严格按照的微服务划分原则将商品相关的职责存放在商品系统中。因此，在查询订单与采购单时，如果查询字段包含商品字段，我们需要按照如下顺序进行查询：\n\n- 先根据商品字段调用商品的服务，然后返回匹配的商品信息；\n- 在订单或采购单中，通过 IN 语句匹配商品 ID，再关联查询对应的单据。\n\n为了方便理解这个过程，订单查询流程图如下图所示：\n\n\n![](https://mmbiz.qpic.cn/sz_mmbiz_png/knmrNHnmCLEDxC5VyDG939eNmSKG8Ss1JefXwrKlDMa75ZnvAGdkGSGibc4FP547PlYCvtlX8hw3tlomQ6LYKdg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)\n\n\n图片\n\n\n图片\n\n\n初期方案设计完后，很快我们就遇到了一系列问题：\n\n- 随着商品数量的增多，匹配的商品越来越多，于是订单服务中包含 IN 语句的查询效率越来越慢；\n- 商品作为一个核心服务，依赖它的服务越来越多，同时随着商品数据量的增长，商品服务已不堪重负，响应速度也变慢，还存在请求超时的情况；\n- 由于商品服务超时，相关服务处理请求经常失败。\n\n结果就是业务方每次查询订单或采购单时，只要带上了商品这个关键字，查询效率就会很慢而且老是失败。于是，我们重新想了一个新方案——数据冗余，下面我们一起来看下。\n\n\n欢迎关注SpringForAll社区（http://spring4all.com），专注分享关于Spring的一切，每周还有2-3次的福利赠书活动，一起来学习、分享、交流吧。\n\n\n## **二、数据冗余的方案**\n\n\n数据冗余说白了就是在订单、采购单中保存一些商品字段信息。\n\n\n为了方便理解，我们借助上面实际业务场景具体说明下，看看两者的区别。\n\n\n商品：\n\n\n![](https://cdn.jsdelivr.net/gh/listener-He/images@default/202309151511076.png)\n\n\n订单和子订单：\n\n\n![](https://cdn.jsdelivr.net/gh/listener-He/images@default/202309151512290.png)\n\n\n采购单和子订单：\n\n\n![](https://cdn.jsdelivr.net/gh/listener-He/images@default/202309151512847.png)\n\n\n调整架构方案后，每次查询时，我们就可以不再依赖商品服务了。\n\n\n但是，如果商品进行了更新，我们如何同步冗余的数据呢？在此分享2种解决办法。\n\n- 每次更新商品时，先调用订单与采购服务，再更新商品的冗余数据。\n- 每次更新商品时，先发布一条消息，订单与采购服务各自订阅这条消息后，再各自更新商品冗余数据。\n\n看到这里是不是觉得很眼熟了呢？没错，这就是我们上一篇提到过的数据一致性问题。那么这2种方案会出现哪些问题呢？\n\n\n**如果商品服务每次更新商品都要调用订单与采购服务，然后再更新冗余数据，则会出现以下两种问题。**\n\n- **数据一致性问题**：如果订单与采购的冗余数据更新失败了，整个操作都需要回滚。这时商品服务的开发人员肯定不乐意，因为冗余数据不是商品服务的核心需求，不能因为边缘流程阻断了自身的核心流程。\n- **依赖问题**：从职责来说，商品服务应该只关注商品本身，但是现在商品还需要调用订单与采购服务。而且，依赖商品这个核心服务的服务实在是太多了，也就导致后续商品服务每次更新商品时，都需要调用更新订单冗余数据、更新采购冗余数据、更新门店库存冗余数据、更新运营冗余数据等一大堆服务。那么商品到底是下游服务还是上游服务？还能不能安心当底层核心服务？\n\n因此，第一个解决办法直接被我们否决了，即我们采取的第二个解决办法——通过消息发布订阅的方案，因为它存在如下 2 点优势。\n\n- 商品无须调用其他服务，它只需要关注自身逻辑即可，顶多多生成一条消息送到 MQ。\n- 如果订单、采购等服务的更新冗余数据失败了，我们使用消息重试机制就可以了，最终能保证数据的一致性。\n\n此时，我们的架构方案如下图所示：\n\n\n![](https://cdn.jsdelivr.net/gh/listener-He/images@default/202309151215375.png)\n\n\n这个方案看起来已经挺完美了，而且市面上基本也是这么做的，不过该方案存在如下几个问题。\n\n\n1.在这个方案中，仅仅保存冗余数据还远远不够，我们还需要将商品分类与生产批号的清单进行关联查询。也就是说，每个服务不只是订阅商品变更这一种消息，还需要订阅商品分类、商品生产批号变更等消息。下面请注意查看订单表结构的加粗部分内容。\n\n\n![](https://cdn.jsdelivr.net/gh/listener-He/images@default/202309151512057.png)\n\n\n以上只是列举了一部分的结构，事实上，商品表中还有很多字段存在冗余，比如保修类型、包换类型等。为了更新这些冗余数据，采购服务与订单服务往往需要订阅近十种消息，因此，我们基本上需要把商品的一小半逻辑复制过来。\n\n\n2.每个依赖的服务需要重复实现冗余数据更新同步的逻辑。前面我们讲了采购、订单及其他服务都需要依赖商品数据，因此每个服务需要将冗余数据的订阅、更新逻辑做一遍，最终重复的代码就会很多。\n\n\n3.MQ 消息类型太多了：联调时最麻烦的是 MQ 之间的联动，如果是接口联调还好说，因为调用哪个服务器的接口相对可控而且比较好追溯；如果是消息联调就比较麻烦，因为我们常常不知道某条消息被哪台服务节点消费了，为了让特定的服务器消费特定的消息，我们就需要临时改动双方的代码。不过联调完成后，我们经常忘了改回原代码。\n\n\n为此，我们不希望针对冗余数据这种非核心需求出现如此多的问题，最终决定使用一个特别的同步冗余数据方案，接下来我们进一步说明。\n\n\n## **三、解耦业务逻辑的数据同步方案**\n\n\n解耦业务逻辑的数据同步方案的设计思路是这样的：\n\n- 将商品及商品相关的一些表（比如分类表、生产批号表、保修类型、包换类型等）实时同步到需要依赖使用它们的服务的数据库，并且保持表结构不变；\n- 在查询采购、订单等服务时，直接关联同步过来的商品相关表；\n- 不允许采购、订单等服务修改商品相关表。\n\n此时，整个方案的架构如下图所示：\n\n\n![](https://cdn.jsdelivr.net/gh/listener-He/images@default/202309151216480.png)\n\n\n以上方案就能轻松解决如下两个问题：\n\n- 商品无须依赖其他服务，如果其他服务的冗余数据同步失败，它也不需要回滚自身的流程；\n- 采购、订单等服务无须关注冗余数据的同步。\n\n不过，该方案的“缺点”是增加了订单、采购等数据库的存储空间（因为增加了商品相关表）。\n\n\n仔细计算后，我们发现之前数据冗余的方案中每个订单都需要保存一份商品的冗余数据，假设订单总数是 N，商品总数是 M，而 N 一般远远大于 M。因此，在之前数据冗余的方案中，N 条订单就会产生 N 条商品的冗余数据。相比之下，解耦业务逻辑的数据同步方案更省空间，因为只增加了 M 条商品的数据。\n\n\n此时问题又来了，如何实时同步相关表的数据呢？我们直接找一个现成的开源中间件就可以了，不过它需要满足支持实时同步、支持增量同步、不用写业务逻辑、支持 MySQL 之间同步、活跃度高这五点要求。\n\n\n根据这五点要求，我们在市面上找了一圈，发现了 Canal、Debezium、DataX、Databus、Flinkx、Bifrost 这几款开源中间件，它们之间的区别如下表所示：\n\n\n![](https://cdn.jsdelivr.net/gh/listener-He/images@default/202309151216798.png)\n\n\n从对比表中来看，比较贴近我们需求的开源中间件是 Bifrost，原因如下：\n\n- 它的界面管理不错；\n- 它的架构比较简单，出现问题后，我们可以自行调查，之后就算作者不维护了也可以自我维护，相对比较可控。\n- 作者更新活跃；\n- 自带监控报警功能。\n\n因此，最终我们使用了 Bifrost 开源中间件，此时整个方案的架构如下图所示：\n\n\n![](https://cdn.jsdelivr.net/gh/listener-He/images@default/202309151216381.png)\n\n\n## **四、上线效果**\n\n\n整个架构方案上线后，商品数据的同步还算比较稳定，此时商品服务的开发人员只需要关注自身逻辑，无须再关注使用数据的人。如果需要关联使用商品数据的订单，采购服务的开发人员也无须关注商品数据的同步问题，只需要在查询时加上关联语句即可，实现了双赢。\n\n\n然而，唯一让我们担心的是 Bifrost 不支持集群，没法保障高可用性。不过，到目前为止，它还没有出现宕机的情况，反而是那些部署多台节点负载均衡的后台服务常常会出现宕机。\n\n\n最终，我们总算解决了服务之间数据依赖的问题。\n\n\n## **五、总结**\n\n\n这里我们探讨了服务间的数据依赖问题，并给出了目前较为合适的解决方案。其实这里提到的方案不是一个很大众的方案，肯定会存在一些遗漏的问题没考虑，如果你有更好的方案，欢迎留言讨论。\n\n",
      "properties": {
        "password": "",
        "icon": "",
        "date": "2023-09-15",
        "type": "Post",
        "category": "技术分享",
        "urlname": "springcloud-data-202309",
        "catalog": [
          "archives"
        ],
        "tags": [
          "Spring",
          "微服务"
        ],
        "summary": "微服务，顾名思义，就是将我们程序拆分为最小化单元来提供服务。在一体化系统中，各个微服务也是不可能独立存在的，那么微服务之间涉及到的数据依赖问题，应该怎么处理呢？我们从场景入手来分析考虑此类问题。",
        "sort": "",
        "title": "微服务之间的数据依赖问题，该如何解决？",
        "status": "Published",
        "updated": "2023-10-08 14:42:00"
      },
      "catalog": [
        {
          "title": "archives",
          "doc_id": "3a5d9238-0fa8-4154-a6d5-92340f2ad733"
        }
      ],
      "body": "",
      "realName": "微服务之间的数据依赖问题，该如何解决？",
      "relativePath": "/archives/微服务之间的数据依赖问题，该如何解决？.md"
    },
    {
      "id": "f8b19b63-1f95-492f-afb0-70b7d69ca735",
      "doc_id": "f8b19b63-1f95-492f-afb0-70b7d69ca735",
      "title": "f8b19b63-1f95-492f-afb0-70b7d69ca735",
      "updated": 1696747200000,
      "body_original": "\n### 1 什么是多租户架构？\n\n\n多租户架构是指在一个应用中支持多个租户（Tenant）同时访问，每个租户拥有独立的资源和数据，并且彼此之间完全隔离。通俗来说，多租户就是把一个应用按照客户的需求“分割”成多个独立的实例，每个实例互不干扰。\n\n\n### 2 多租户架构的优势\n\n- 更好地满足不同租户的个性化需求。\n- 可以降低运维成本，减少硬件、网络等基础设施的投入。\n- 节约开发成本，通过复用代码，快速上线新的租户实例。\n- 增强了系统的可扩展性和可伸缩性，支持水平扩展，每个租户的数据和资源均可管理和控制。\n\n### 3 实现多租户架构的技术选择\n\n\n对于实现多租户架构技术不是最重要的最重要的是正确的架构思路。但是选择正确的技术可以更快地实现多租户架构。\n\n\n### 1 架构选型\n\n\n基于Java开发多租户应用推荐使用Spring Boot和Spring Cloud。Spring Boot能快速搭建应用并提供许多成熟的插件。Spring Cloud则提供了许多实现微服务架构的工具和组件。\n\n\n### 1.1 Spring Boot\n\n\n使用Spring Boot可以简化项目的搭建过程自动配置许多常见的第三方库和组件，减少了开发人员的工作量。\n\n\n```java\n@RestController\npublic class TenantController {\n\n\t\t@GetMapping(\"/hello\")\n\t\tpublic String hello(@RequestHeader(\"tenant-id\") String tenantId) {\n\t\t\treturn \"Hello, \" + tenantId;\n\t\t}\n}\n```\n\n\n### 1.2 Spring Cloud\n\n\n在架构多租户的系统时Spring Cloud会更加有用。Spring Cloud提供了一些成熟的解决方案，如Eureka、Zookeeper、Consul等，以实现服务发现、负载均衡等微服务功能。\n\n\n### 2 数据库设计\n\n\n在多租户环境中数据库必须为每个租户分别存储数据并确保数据隔离。我们通常使用以下两种方式实现：\n\n- 多个租户共享相同的数据库，每个表中都包含tenant_id这一列，用于区分不同租户的数据。\n- 为每个租户创建单独的数据库，每个数据库内的表结构相同，但数据相互隔离。\n\n### 3 应用多租户部署\n\n\n为了实现多租户在应用部署时我们需要考虑以下两个问题。\n\n\n### 3.1 应用隔离\n\n\n在多租户环境中不同租户需要访问不同的资源，因此需要进行应用隔离。可以通过构建独立的容器或虚拟机、使用命名空间等方式实现。Docker就是一种非常流行的隔离容器技术。\n\n\n### 3.2 应用配置\n\n\n由于每个租户都有自己的配置需求因此需要为每个租户分别设置应用配置信息，例如端口号、SSL证书等等。这些配置可以存储在数据库中，也可以存储在云配置中心中。\n\n\n### 4 租户管理\n\n\n在多租户系统中需要能够管理不同租户的数据和资源，同时需要为每个租户分配相应的权限。解决方案通常包括以下两部分。\n\n\n### 4.1 租户信息维护\n\n\n租户信息的维护包括添加、修改、删除、查询等操作，要求能够根据租户名称或租户ID快速查找对应的租户信息。\n\n\n```sql\nCREATE TABLE tenant (    \n\tid BIGINT AUTO_INCREMENT PRIMARY KEY,\n    name VARCHAR(50) NOT NULL UNIQUE,\n    description VARCHAR(255),\n    created_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n    updated_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP\n);\n```\n\n\n### 4.2 租户权限控制\n\n\n在多租户应用中必须为每个租户分别设置对系统资源的访问权限。例如，A租户和B租户不能访问彼此的数据。\n\n\n```java\n@EnableGlobalMethodSecurity(prePostEnabled = true)\n@Configuration \npublic class SecurityConfig extends WebSecurityConfigurerAdapter {\n\t@Override\n\tprotected void configure(HttpSecurity http) throws Exception {\n\t    http.authorizeRequests()\n\t        .antMatchers(\"/api/tenant/**\").hasRole(\"ADMIN\")\n\t        .anyRequest().authenticated()\n\t        .and()\n\t        .formLogin();\n\t}\n\t@Autowired\n\tpublic void configureGlobal(AuthenticationManagerBuilder auth) throws Exception {\n\t    auth.userDetailsService(userDetailsService())\n\t        .passwordEncoder(new BCryptPasswordEncoder())\n\t        .and()\n\t        .inMemoryAuthentication()\n\t        .withUser(\"admin\")\n\t        .password(new BCryptPasswordEncoder().encode(\"123456\"))\n\t        .roles(\"ADMIN\");\n\t}\n}\n```\n\n\n### 1 Spring Boot中的多租户实现\n\n\n在Spring Boot中可以通过多数据源和动态路由来实现多租户机制。\n\n\n### 1.1 多数据源实现\n\n\n多数据源是指为不同的租户配置不同的数据源，使得每个租户都可以访问自己的独立数据。具体实现方法如下：\n\n\n```java\n@Configuration \npublic class DataSourceConfig {\n\t\t@Bean(name = \"dataSourceA\")\n\t\t@ConfigurationProperties(prefix = \"spring.datasource.a\")\n\t\tpublic DataSource dataSourceA() {\n\t\t    return DataSourceBuilder.create().build();\n\t\t}\n\t\t\n\t\t@Bean(name = \"dataSourceB\")\n\t\t@ConfigurationProperties(prefix = \"spring.datasource.b\")\n\t\tpublic DataSource dataSourceB() {\n\t\t    return DataSourceBuilder.create().build();\n\t\t}\n\t\t\n\t\t@Bean(name = \"dataSourceC\")\n\t\t@ConfigurationProperties(prefix = \"spring.datasource.c\")\n\t\tpublic DataSource dataSourceC() {\n\t\t    return DataSourceBuilder.create().build();\n\t\t}\n}\n```\n\n\n以上代码是配置了三个数据源分别对应三个租户。然后在使用时，可以使用注解标记需要连接的数据源。\n\n\n```java\n@Service\npublic class ProductService {\n\t@Autowired\n\t@Qualifier(“dataSourceA”)\n\tprivate DataSource dataSource;\n\t// …\n}\n```\n\n\n### 1.2 动态路由实现\n\n\n动态路由是指根据请求的URL或参数动态地切换到对应租户的数据源。具体实现如下：\n\n\n```java\npublic class DynamicDataSource extends AbstractRoutingDataSource {\n\t@Override\n\tprotected Object determineCurrentLookupKey() {\n\treturn TenantContextHolder.getTenantId();\n\t}\n}\n\n@Configuration\npublic class DataSourceConfig {\n\n\t@Bean(name = \"dataSource\")\n\t@ConfigurationProperties(prefix = \"spring.datasource\")\n\tpublic DataSource dataSource() {\n\t\treturn DataSourceBuilder.create().type(DynamicDataSource.class).build();\n\t}\n}\n```\n\n\n以上是动态路由的核心代码`DynamicDataSource`继承自`AbstractRoutingDataSource`，通过`determineCurrentLookupKey()`方法动态获得租户ID，然后切换到对应的数据源。\n\n\n### 2 Spring Cloud中的多租户实现\n\n\n在Spring Cloud中可以通过服务注册与发现、配置中心、负载均衡等方式实现多租户机制。\n\n\n```java\n/**\n * 多租户配置类\n */\n@Configuration\npublic class TenantConfig {\n\n    /**\n     * 注册多租户拦截器\n     */\n    @Bean\n    public TenantInterceptor tenantInterceptor() {\n        return new TenantInterceptor();\n    }\n\n    /**\n     * 注册多租户数据源\n     */\n    @Bean\n    @ConfigurationProperties(prefix = \"spring.datasource\")\n    public DataSource dataSource() {\n        return DataSourceBuilder.create().build();\n    }\n\n    /**\n     * 配置多租户Mybatis插件\n     */\n    @Bean\n    public MybatisPlusInterceptor mybatisPlusInterceptor() {\n        MybatisPlusInterceptor interceptor = new MybatisPlusInterceptor();\n        interceptor.addInnerInterceptor(new TenantLineInnerInterceptor(new TenantHandler()));\n        return interceptor;\n    }\n\n    /**\n     * 配置多租户Mybatis拦截器\n     */\n    @Bean\n    public MybatisInterceptor mybatisInterceptor() {\n        MybatisInterceptor interceptor = new MybatisInterceptor();\n        interceptor.addInnerInterceptor(new TenantLineInnerInterceptor(new TenantHandler()));\n        return interceptor;\n    }\n\n    /**\n     * 配置多租户Feign拦截器\n     */\n    @Bean\n    public RequestInterceptor feignInterceptor() {\n        return new TenantFeignInterceptor();\n    }\n\n    /**\n     * 配置多租户Ribbon拦截器\n     */\n    @Bean\n    public LoadBalancerInterceptor ribbonInterceptor() {\n        return new TenantLoadBalancerInterceptor();\n    }\n\n    /**\n     * 注册多租户Redis配置\n     */\n    @Bean\n    public TenantRedisConfig tenantRedisConfig() {\n        return new TenantRedisConfig();\n    }\n\n}\n```\n\n\n### 2.1 服务注册与发现\n\n\n使用Spring Cloud中的Eureka实现服务注册与发现。每个租户的服务都在注册中心以不同的应用名称进行注册，客户端可以通过服务名称来访问对应租户的服务。\n\n\n### 2.2 配置中心\n\n\n使用Spring Cloud Config作为配置中心。配置文件以租户ID进行区分，客户端通过读取对应租户的配置文件来获取配置信息。\n\n\n### 2.3 负载均衡\n\n\n使用Spring Cloud Ribbon作为负载均衡器。根据请求的URL或参数选择对应租户的服务实例进行请求转发。\n\n\n### 2.4 API\n\n\n在API网关层面实现多租户机制根据请求的URL或参数判断所属租户，并转发到对应租户的服务实例。\n\n\n### 1 私有云环境\n\n\n私有云环境指的是由企业自行搭建的云环境，不对外提供服务，主要应用于企业内部的数据存储、管理、共享和安全控制。相较于公有云，私有云的优点在于可以更好地保护企业核心数据，同时也能够满足企业对于数据安全性和可控性的要求。\n\n\n### 2 公有云环境\n\n\n公有云环境指的是由云服务商搭建并对外提供服务的云环境，用户可以根据需要购买相应的云服务，如云存储、云计算、云数据库等。相较于私有云，公有云的优点在于具有成本低廉、弹性伸缩、全球化部署等特点，能够更好地满足企业快速发展的需求。\n\n\n### 3 企业级应用\n\n\n企业级应用是指面向企业客户的应用程序，主要包括ERP、CRM、OA等一系列应用系统。这类应用的特点在于功能强大、流程复杂、数据量大，需要满足企业的高效率、高可靠性、高安全性和易维护性等要求。在云计算环境下，企业可以将这些应用部署在私有云或公有云上，减少了硬件设备的投入和维护成本，提高了管理效率。\n\n\n### 1 搭建Spring Boot和Spring Cloud环境\n\n\n首先需要在Maven项目中引入以下依赖：\n\n\n```xml\n<dependency>\n    <groupId>org.springframework.boot</groupId>\n    <artifactId>spring-boot-starter-web</artifactId>\n</dependency>\n<dependencyManagement>\n    <dependencies>\n        <dependency>\n            <groupId>org.springframework.cloud</groupId>\n            <artifactId>spring-cloud-dependencies</artifactId>\n            <version>2020.0.3</version>\n            <type>pom</type>\n            <scope>import</scope>\n        </dependency>\n    </dependencies>\n</dependencyManagement>\n```\n\n\n然后需要在application.yml中配置相应的参数，如下所示：\n\n\n```yaml\nspring:\n  datasource:\n    url: jdbc:mysql://localhost:3306/appdb?useUnicode=true&characterEncoding=utf-8&serverTimezone=Asia/Shanghai\n    username: root\n    password: 123456\n  mybatis:\n    type-aliases-package: com.example.demo.model\n    mapper-locations: classpath:mapper/*.xml\nserver:\n  port: 8080\neureka:\n  client:\n    serviceUrl:\n      defaultZone: http://localhost:8761/eureka/\n  management:\n    endpoints:\n      web:\n        exposure:\n          include: \"*\"\n```\n\n\n其中`datasource.url`为数据库连接的URL，username和password为数据库连接的账号和密码；`server.port`为Spring Boot应用启动的端口；`eureka.client.serviceUrl.defaultZone`为Eureka服务注册中心的URL。\n\n\n### 2 修改数据库设计\n\n\n接下来需要对数据库进行相应的修改，以支持多租户部署。具体来说，我们需要在数据库中添加一个与租户相关的字段，以便在应用中区分不同的租户。\n\n\n### 3 实现应用多租户部署\n\n\n接着需要在代码中实现应用的多租户部署功能。具体来说，我们需要为每个租户实例化对应的Spring Bean，并根据租户ID将请求路由到相应的Bean中去处理。\n\n\n以下是一个简单的实现示例：\n\n\n```java\n@Configuration\npublic class MultiTenantConfig {\n    // 提供对应租户的数据源\n    @Bean\n    public DataSource dataSource(TenantRegistry tenantRegistry) {\n        return new TenantAwareDataSource(tenantRegistry);\n    }\n\n    // 多租户Session工厂\n    @Bean(name = \"sqlSessionFactory\")\n    public SqlSessionFactory sqlSessionFactory(DataSource dataSource) throws Exception {\n        SqlSessionFactoryBean sessionFactory = new SqlSessionFactoryBean();\n        sessionFactory.setDataSource(dataSource);\n        return sessionFactory.getObject();\n    }\n\n    // 动态切换租户\n    @Bean\n    public MultiTenantInterceptor multiTenantInterceptor(TenantResolver tenantResolver) {\n        MultiTenantInterceptor interceptor = new MultiTenantInterceptor();\n        interceptor.setTenantResolver(tenantResolver);\n        return interceptor;\n    }\n\n    // 注册拦截器\n    @Override\n    public void addInterceptors(InterceptorRegistry registry) {\n        registry.addInterceptor(multiTenantInterceptor());\n    }\n\n    // 注册租户信息\n    @Bean\n    public TenantRegistry tenantRegistry() {\n        return new TenantRegistryImpl();\n    }\n\n    // 解析租户ID\n    @Bean\n    public TenantResolver tenantResolver() {\n        return new HeaderTenantResolver();\n    }\n}\n```\n\n\n其中`MultiTenantConfig`是多租户部署的核心配置类，它提供了对应租户数据源、多租户Session工厂、动态切换租户等功能。\n\n\n### 4 实现租户管理\n\n\n最后需要实现一个租户管理的功能，以便在系统中管理不同的租户。具体来说，我们可以使用Spring Cloud的服务注册与发现组件Eureka来注册每个租户的实例，并在管理界面中进行相应的操作。另外，我们还需要为每个租户提供一个独立的数据库，以保证数据隔离性。\n\n\n本文详细介绍了如何使用Spring Boot和Spring Cloud实现一个支持多租户部署的应用。主要包括搭建Spring Boot和Spring Cloud环境、修改数据库设计、实现应用多租户部署、实现租户管理等方面。\n\n\n应用场景主要包括SaaS应用、多租户云服务等。优劣势主要体现在提升了应用的可扩展性和可维护性，但也增加了部署和管理的复杂度。未来的改进方向可以考虑进一步提升多租户管理的自动化程度，减少人工干预和错误率。\n\n\n添加Eureka依赖\n\n\n```xml\n\n<dependency>\n    <groupId>org.springframework.cloud</groupId>\n    <artifactId>spring-cloud-starter-netflix-eureka-server</artifactId>\n</dependency>\n// 创建租户服务\n// 添加Eureka依赖\n<dependency>\n    <groupId>org.springframework.cloud</groupId>\n    <artifactId>spring-cloud-starter-netflix-eureka-client</artifactId>\n</dependency>\n// 创建租户管理模块\n// 添加Spring Data JPA依赖\n<dependency>\n    <groupId>org.springframework.boot</groupId>\n    <artifactId>spring-boot-starter-data-jpa</artifactId>\n</dependency>\n```\n\n\n```java\n// 在Spring Boot和Spring Cloud环境下搭建多租户应用\n// 修改数据库设计，实现应用多租户部署，实现租户管理\n// 使用Eureka服务注册与发现组件注册每个租户的实例\n\n\n\n// 启用Eureka\n@EnableEurekaServer\n@SpringBootApplication\npublic class Application {\n    public static void main(String[] args) {\n        SpringApplication.run(Application.class, args);\n    }\n}\n\n\n// 注册租户服务\n@EnableDiscoveryClient\n@SpringBootApplication\npublic class TenantServiceApplication {\n    public static void main(String[] args) {\n        SpringApplication.run(TenantServiceApplication.class, args);\n    }\n}\n\n// 配置文件\nspring:\n  application:\n    name: tenant-service\neureka:\n  client:\n    service-url:\n      defaultZone: http://localhost:8761/eureka/\n\n\n\n// 创建Tenant实体类\n@Entity\n@Table(name = \"tenant\")\npublic class Tenant {\n    @Id\n    @GeneratedValue(strategy = GenerationType.IDENTITY)\n    private Long id;\n    private String name;\n    private String databaseName;\n    private String databaseUsername;\n    private String databasePassword;\n    // getter和setter\n}\n\n// 创建TenantRepository接口\npublic interface TenantRepository extends JpaRepository<Tenant, Long> {\n}\n\n// 创建TenantService类\n@Service\npublic class TenantService {\n    @Autowired\n    private TenantRepository tenantRepository;\n    // 添加租户\n    public Tenant addTenant(Tenant tenant) {\n        return tenantRepository.save(tenant);\n    }\n    // 获取所有租户\n    public List<Tenant> getAllTenants() {\n        return tenantRepository.findAll();\n    }\n    // 根据id获取租户\n    public Tenant getTenantById(Long id) {\n        return tenantRepository.findById(id).orElse(null);\n    }\n    // 根据id删除租户\n    public void deleteTenantById(Long id) {\n        tenantRepository.deleteById(id);\n    }\n}\n\n// 创建TenantController类\n@RestController\n@RequestMapping(\"/tenants\")\npublic class TenantController {\n    @Autowired\n    private TenantService tenantService;\n    // 添加租户\n    @PostMapping(\"\")\n    public Tenant addTenant(@RequestBody Tenant tenant) {\n        return tenantService.addTenant(tenant);\n    }\n    // 获取所有租户\n    @GetMapping(\"\")\n    public List<Tenant> getAllTenants() {\n        return tenantService.getAllTenants();\n    }\n    // 根据id获取租户\n    @GetMapping(\"/{id}\")\n    public Tenant getTenantById(@PathVariable Long id) {\n        return tenantService.getTenantById(id);\n    }\n    // 根据id删除租户\n    @DeleteMapping(\"/{id}\")\n    public void deleteTenantById(@PathVariable Long id) {\n        tenantService.deleteTenantById(id);\n    }\n}\n\n// 创建多租户数据库\npublic class MultiTenantConnectionProviderImpl implements MultiTenantConnectionProvider {\n    private Map<String, DataSource> dataSourceMap = new HashMap<>();\n    @Override\n    public Connection getAnyConnection() throws SQLException {\n        return dataSourceMap.values().iterator().next().getConnection();\n    }\n    @Override\n    public Connection getConnection(String tenantIdentifier) throws SQLException {\n        DataSource dataSource = dataSourceMap.get(tenantIdentifier);\n        if (dataSource == null) {\n            throw new SQLException(\"No dataSource found for tenantIdentifier: \" + tenantIdentifier);\n        }\n        return dataSource.getConnection();\n    }\n    @Override\n    public void releaseAnyConnection(Connection connection) throws SQLException {\n        connection.close();\n    }\n    @Override\n    public void releaseConnection(String tenantIdentifier, Connection connection) throws SQLException {\n        connection.close();\n    }\n    @Override\n    public boolean supportsAggressiveRelease() {\n        return true;\n    }\n}\n\n// 创建多租户实体类\n@Entity\n@Table(name = \"customer\", schema = \"tenant1\")\npublic class Customer {\n    @Id\n    @GeneratedValue(strategy = GenerationType.IDENTITY)\n    private Long id;\n    private String name;\n    private String email;\n    // getter和setter\n}\n\n// 创建多租户Repository接口\npublic interface CustomerRepository extends JpaRepository<Customer, Long> {\n    List<Customer> findByTenantId(Long tenantId);\n}\n\n// 创建多租户Service类\n@Service\npublic class CustomerService {\n    @Autowired\n    private CustomerRepository customerRepository;\n    // 添加客户\n    public Customer addCustomer(Customer customer) {\n        return customerRepository.save(customer);\n    }\n    // 获取所有客户\n    public List<Customer> getAllCustomers(Long tenantId) {\n        return customerRepository.findByTenantId(tenantId);\n    }\n    // 根据id获取客户\n    public Customer getCustomerById(Long id) {\n        return customerRepository.findById(id).orElse(null);\n    }\n    // 根据id删除客户\n    public void deleteCustomerById(Long id) {\n        customerRepository.deleteById(id);\n    }\n}\n\n// 创建多租户Controller类\n@RestController\n@RequestMapping(\"/{tenantId}/customers\")\npublic class CustomerController {\n    @Autowired\n    private CustomerService customerService;\n    // 添加客户\n    @PostMapping(\"\")\n    public Customer addCustomer(@RequestBody Customer customer) {\n        return customerService.addCustomer(customer);\n    }\n    // 获取所有客户\n    @GetMapping(\"\")\n    public List<Customer> getAllCustomers(@PathVariable Long tenantId) {\n        return customerService.getAllCustomers(tenantId);\n    }\n    // 根据id获取客户\n    @GetMapping(\"/{id}\")\n    public Customer getCustomerById(@PathVariable Long id) {\n        return customerService.getCustomerById(id);\n    }\n    // 根据id删除客户\n    @DeleteMapping(\"/{id}\")\n    public void deleteCustomerById(@PathVariable Long id) {\n        customerService.deleteCustomerById(id);\n    }\n}\n```\n\n\n```yaml\n// 配置文件\nspring:\n  application:\n    name: eureka-server\nserver:\n  port: 8761\n```\n\n",
      "properties": {
        "password": "",
        "icon": "",
        "date": "2023-09-15",
        "type": "Post",
        "category": "技术分享",
        "urlname": "spring-boot-tenant-202309",
        "catalog": [
          "archives"
        ],
        "tags": [
          "Spring",
          "微服务",
          "SAAS"
        ],
        "summary": "常见的技术选择包括：\n• 数据库级多租户：使用多个数据库实例分别存储各个租户的数据，每个租户对应一个独立的数据库。\n• 模式级多租户：使用同一个数据库实例，但是为每个租户创建独立的数据库模式，实现数据隔离。\n• 表级多租户：在同一个数据库模式下，使用不同的表来存储各个租户的数据，实现数据隔离。\n• 应用级多租户：在应用程序中实现租户隔离，例如使用Spring Cloud等微服务框架。\n• 容器级多租户：使用容器技术，为每个租户创建独立的容器，实现资源隔离和控制。\n4 实现多租户架构的注意事项\n• 保证租户数据的隔离性和安全性。\n• 设计合理的租户数据结构和关系模型。\n• 统一管理租户的配置和权限。\n• 保证系统的可扩展性和可伸缩性，支持水平扩展。\n• 保证系统的高可用性和容错性，避免单点故障。\n• 对租户数据进行备份和恢复，保证数据的可靠性和完整性。",
        "sort": "",
        "title": "Spring Boot 实现多租户架构：支持应用多租户部署和管理",
        "status": "Published",
        "updated": "2023-10-08 14:40:00"
      },
      "catalog": [
        {
          "title": "archives",
          "doc_id": "f8b19b63-1f95-492f-afb0-70b7d69ca735"
        }
      ],
      "body": "",
      "realName": "Spring Boot 实现多租户架构：支持应用多租户部署和管理",
      "relativePath": "/archives/Spring Boot 实现多租户架构：支持应用多租户部署和管理.md"
    },
    {
      "id": "28f5e6a0-2d06-439d-b947-b7121480c146",
      "doc_id": "28f5e6a0-2d06-439d-b947-b7121480c146",
      "title": "28f5e6a0-2d06-439d-b947-b7121480c146",
      "updated": 1696747320000,
      "body_original": "\n做一些C端业务，不可避免的要引入一级缓存来代替数据库的压力并且减少业务响应时间，其实每次引入一个中间件来解决问题的同时，必然会带来很多新的问题需要注意，比如缓存一致性问题。\n\n\n那么其实还会有一些其他问题比如使用Redis作为一级缓存时可能带来的热key、大key等问题，本文我们就热key（hot key）问题来讨论，如何合理的解决热key问题。\n\n\n## 背景\n\n\n> 热key是什么问题，如何导致的？\n\n\n一般来说，我们使用的缓存Redis都是多节点的集群版，对某个key进行读写时，会根据该key的hash计算出对应的slot，根据这个slot就能找到与之对应的分片(一个master和多个slave组成的一组redis集群)来存取该K-V。但是在实际应用过程中，对于某些特定业务或者一些特定的时段（比如电商业务的商品秒杀活动），可能会发生大量的请求访问同一个key。\n\n\n所有的请求（且这类请求读写比例非常高）都会落到同一个redis server上，该redis的负载就会严重加剧，此时整个系统增加新redis实例也没有任何用处，因为根据hash算法，同一个key的请求还是会落到同一台新机器上，该机器依然会成为系统瓶颈2，甚至造成整个集群宕掉，若此热点key的value 也比较大，也会造成网卡达到瓶颈，这种问题称为 “热key” 问题。\n\n\n如下图1、2所示，分别是正常redis cluster集群和使用一层proxy代理的redis 集群key访问。\n\n\n![](https://cdn.jsdelivr.net/gh/listener-He/images@default/202309121604507.png)\n\n\n![](https://cdn.jsdelivr.net/gh/listener-He/images@default/202309121605080.png)\n\n\n如上所说，热key会给集群中的少部分节点带来超高的负载压力，如果不正确处理，那么这些节点宕机都有可能，从而会影响整个缓存集群的运作，因此我们必须及时发现热key、解决热key问题。\n\n\n## 1.热key探测\n\n\n热key探测，看到由于redis集群的分散性以及热点key带来的一些显著影响，我们可以通过由粗及细的思考流程来做热点key探测的方案。\n\n\n### 1.1 集群中每个slot的qps监控\n\n\n热key最明显的影响是整个redis集群中的qps并没有那么大的前提下，流量分布在集群中slot不均的问题，那么我们可以最先想到的就是对于每个slot中的流量做监控，上报之后做每个slot的流量对比，就能在热key出现时发现影响到的具体slot。虽然这个监控最为方便，但是粒度过于粗了，仅适用于前期集群监控方案，并不适用于精准探测到热key的场景。\n\n\n### 1.2 proxy的代理机制作为整个流量入口统计\n\n\n如果我们使用的是图2的redis集群proxy代理模式，由于所有的请求都会先到proxy再到具体的slot节点，那么这个热点key的探测统计就可以放在proxy中做，在proxy中基于时间滑动窗口，对每个key做计数，然后统计出超出对应阈值的key。为了防止过多冗余的统计，还可以设定一些规则，仅统计对应前缀和类型的key。这种方式需要至少有proxy的代理机制，对于redis架构有要求。\n\n\n### 1.3 redis基于LFU的热点key发现机制\n\n\nredis 4.0以上的版本支持了每个节点上的基于LFU的热点key发现机制，使用redis-cli –hotkeys即可，执行redis-cli时加上–hotkeys选项。可以定时在节点中使用该命令来发现对应热点key。\n\n\n![](https://cdn.jsdelivr.net/gh/listener-He/images@default/202309121606497.png)\n\n\n如下所示，可以看到redis-cli –hotkeys的执行结果，热key的统计信息，这个命令的执行时间较长，可以设置定时执行来统计。\n\n\n### 1.4 基于Redis客户端做探测\n\n\n由于redis的命令每次都是从客户端发出，基于此我们可以在redis client的一些代码处进行统计计数，每个client做基于时间滑动窗口的统计，超过一定的阈值之后上报至server，然后统一由server下发至各个client，并且配置对应的过期时间。\n\n\n这个方式看起来更优美，其实在一些应用场景中并不是那么合适，因为在client端这一侧的改造，会给运行的进程带来更大的内存开销，更直接的来说，对于Java和goLang这种自动内存管理的语言，会更加频繁的创建对象，从而触发gc导致接口响应耗时增加的问题，这个反而是不太容易预料到的事情。\n\n\n最终可以通过各个公司的基建，做出对应的选择。\n\n\n## 2.热key解决\n\n\n通过上述几种方式我们探测到了对应热key或者热slot，那么我们就要解决对应的热key问题。解决热key也有好几种思路可以参考，我们一个一个捋一下。\n\n\n### 2.1 对特定key或slot做限流\n\n\n一种最简单粗暴的方式，对于特定的slot或者热key做限流，这个方案明显对于业务来说是有损的，所以建议只用在出现线上问题，需要止损的时候进行特定的限流。\n\n\n### 2.2 使用二级（本地）缓存\n\n\n本地缓存也是一个最常用的解决方案，既然我们的一级缓存扛不住这么大的压力，就再加一个二级缓存吧。由于每个请求都是由service发出的，这个二级缓存加在service端是再合适不过了，因此可以在服务端每次获取到对应热key时，使用本地缓存存储一份，等本地缓存过期后再重新请求，降低redis集群压力。以java为例，guavaCache就是现成的工具。以下示例：\n\n\n```java\n//本地缓存初始化以及构造\n\nprivate static LoadingCache<String, List> configCache\n        = CacheBuilder.newBuilder()\n        .concurrencyLevel(8)  //并发读写的级别，建议设置cpu核数\n        .expireAfterWrite(10, TimeUnit.SECONDS)  //写入数据后多久过期\n        .initialCapacity(10) //初始化cache的容器大小\n        .maximumSize(10)//cache的容器最大\n        .recordStats()\n        // build方法中可以指定CacheLoader，在缓存不存在时通过CacheLoader的实现自动加载缓存\n        .build(new CacheLoader<String, List>() {\n            @Override\n            public List load(String hotKey) throws Exception {\n\n}\n\n});\n\n//本地缓存获取\n\nObject result = configCache.get(key);\n```\n\n\n本地缓存对于我们的最大的影响就是数据不一致的问题，我们设置多长的缓存过期时间，就会导致最长有多久的线上数据不一致问题，这个缓存时间需要衡量自身的集群压力以及业务接受的最大不一致时间。\n\n\n### 2.3 拆key\n\n\n如何既能保证不出现热key问题，又能尽量的保证数据一致性呢？拆key也是一个好的解决方案。\n\n\n我们在放入缓存时就将对应业务的缓存key拆分成多个不同的key。如下图所示，我们首先在更新缓存的一侧，将key拆成N份，比如一个key名字叫做“good_100”，那我们就可以把它拆成四份，“`good_100_copy1`”、“`good_100_copy2`”、“`good_100_copy3`”、“`good_100_copy4`”，每次更新和新增时都需要去改动这N个key，这一步就是拆key。\n\n\n![](https://cdn.jsdelivr.net/gh/listener-He/images@default/202309121606262.png)\n\n\n对于service端来讲，我们就需要想办法尽量将自己访问的流量足够的均匀，如何给自己即将访问的热key上加入后缀。几种办法，根据本机的ip或mac地址做hash，之后的值与拆key的数量做取余，最终决定拼接成什么样的key后缀，从而打到哪台机器上；服务启动时的一个随机数对拆key的数量做取余。\n\n\n### 2.4 本地缓存的另外一种思路 配置中心\n\n\n对于熟悉微服务配置中心的伙伴来讲，我们的思路可以向配置中心的一致性转变一下。拿nacos来举例，它是如何做到分布式的配置一致性的，并且相应速度很快？那我们可以将缓存类比配置，这样去做。\n\n\n长轮询+本地化的配置。首先服务启动时会初始化全部的配置，然后定时启动长轮询去查询当前服务监听的配置有没有变更，如果有变更，长轮询的请求便会立刻返回，更新本地配置；如果没有变更，对于所有的业务代码都是使用本地的内存缓存配置。这样就能保证分布式的缓存配置时效性与一致性。\n\n\n### 2.5 其他可以提前做的预案\n\n\n上面的每一个方案都相对独立的去解决热key问题，那么如果我们真的在面临业务诉求时，其实会有很长的时间来考虑整体的方案设计。一些极端的秒杀场景带来的热key问题，如果我们预算充足，可以直接做服务的业务隔离、redis缓存集群的隔离，避免影响到正常业务的同时，也会可以临时采取更好的容灾、限流措施。\n\n\n## 一些整合的方案\n\n\n目前市面上已经有了不少关于hotKey相对完整的应用级解决方案，其中京东在这方面有开源的hotkey工具，原理就是在client端做洞察，然后上报对应hotkey，server端检测到后，将对应hotkey下发到对应服务端做本地缓存，并且这个本地缓存在远程对应的key更新后，会同步更新，已经是目前较为成熟的自动探测热key、分布式一致性缓存解决方案\n\n\n![](https://cdn.jsdelivr.net/gh/listener-He/images@default/202309121607163.png)\n\n\n## 总结\n\n\n以上就是笔者大概了解或实践过的的如何应对热key的一些方案，从发现热key到解决热key的两个关键问题的应对。每一个方案都有优缺点，比如会带来业务的不一致性，实施起来较为困难等等，可以根据目前自身业务的特点、以及目前公司的基建去做对应的调整和改变。\n\n",
      "properties": {
        "password": "",
        "icon": "",
        "date": "2023-09-12",
        "type": "Post",
        "category": "技术分享",
        "urlname": "redis-key-202309",
        "catalog": [
          "archives"
        ],
        "tags": [
          "Redis"
        ],
        "summary": "做一些C端业务，不可避免的要引入一级缓存来代替数据库的压力并且减少业务响应时间，其实每次引入一个中间件来解决问题的同时，必然会带来很多新的问题需要注意，比如缓存一致性问题。\n那么其实还会有一些其他问题比如使用Redis作为一级缓存时可能带来的热key、大key等问题，本文我们就热key（hot key）问题来讨论，如何合理的解决热key问题。",
        "sort": "",
        "title": "Redis 热key是什么问题，如何导致的？有什么解决方案？",
        "status": "Published",
        "updated": "2023-10-08 14:42:00"
      },
      "catalog": [
        {
          "title": "archives",
          "doc_id": "28f5e6a0-2d06-439d-b947-b7121480c146"
        }
      ],
      "body": "",
      "realName": "Redis 热key是什么问题，如何导致的？有什么解决方案？",
      "relativePath": "/archives/Redis 热key是什么问题，如何导致的？有什么解决方案？.md"
    },
    {
      "id": "613b560e-75d7-4f45-88e9-b487310cfe82",
      "doc_id": "613b560e-75d7-4f45-88e9-b487310cfe82",
      "title": "613b560e-75d7-4f45-88e9-b487310cfe82",
      "updated": 1696747320000,
      "body_original": "\n> 首先声明：本人没有恶意利用这个漏洞去谋利，只是测试用了那么一次。不提倡大家去做这种违法行为，本文仅供学习\n\n\n> 💡 最近瑞幸在搞活动，每天免费送10000份咖啡，我是个狂热喝咖啡的人儿，今天最后一天来整个活儿，点开瑞幸咖啡小程序主页，banner 栏轮播图中有一张海报入口，操作一通下来，果然，没抢到。  \n> 手速不够快不是主要原因，手指操作延迟 + 系统页面跳转耗时加起来到 http 发出就已经耽误了1 -2 秒钟了，这个时间才是关键，本文从技术角度探讨下怎么在最小成本比如几分钟内，实现一个小工具，来解决这个问题。\n\n\n![](https://cdn.jsdelivr.net/gh/listener-He/images@default/202306021513432.png)\n\n\n首先需要一个抓包工具，iphone 手机可以用 stream, 有几个注意点：\n\n\n1、默认安装后是无法抓取 https 类型的，需要在设置里进行相关配置：\n\n\n> 如果您要抓取 HTTPS 的请求，需要先启动抓包，然后安装 CA 证书后，去设置-通用-关于-证书信任设置 里信任 CA，才可以查看请求的内容。\n\n\n![](https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/923893c1d0a9421b9f0a1bc63cf321a7~tplv-k3u1fbpfcp-zoom-in-crop-mark:1512:0:0:0.awebp?)\n\n\n2、注意小程序里面哦（原生的可能抓不到），拿到的接口名如下：\n\n\n`https://mkt.lkcoffee.com/ladder/capi/resource/m/promo/activity/send`\n\n\nstream 提供了 curl 的拷贝，将其复制并导入到 postman 中。\n\n\n![](https://cdn.jsdelivr.net/gh/listener-He/images@default/202306021523307.png)\n\n\n## postman 导入&复现\n\n\n点击 `import` 按钮，在弹窗中选择 `raw text` 将复制的 curl 字符串粘贴进去，点击确认，就成功的将 这个 http 接口导入到了 postman 中，尝试点击 send 按钮，发现拿到了正确的响应，验证了该接口已经可以正常使用。\n\n\n![](https://cdn.jsdelivr.net/gh/listener-He/images@default/202306021522744.png)\n\n\n![](https://p1-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/f96e756ed7ad4a6f83baac571d9e00e7~tplv-k3u1fbpfcp-zoom-in-crop-mark:1512:0:0:0.awebp?)\n\n\n## 自动化脚本？\n\n\n其实到这一步，已经实现了目标，点击 `send` 直接发送请求，大大提升了抢到的概率，如果你还想更进一步，那么可以尝试将其封装成 自动化脚本，来实现定时、自动、重复发送；\n\n\n点开右侧代码块，选择语言，假设选择 `python`（也可以任意选择你擅长的语言），然后就自动生成 python 版本的可执行代码片段，我们就在这个基础上拓展功能；\n\n\n![](https://cdn.jsdelivr.net/gh/listener-He/images@default/202306021521797.png)\n\n\n示例代码如下：\n\n\n```python\n\nimport requests\nimport time\n\nurl = \"http://example.com\"  # 将此处的 URL 替换为你要请求的地址payload = {}\nheaders = {\n\t#将 postman 中的headers 复制过来}\n\nstart_time = \"09:59:55\"  # 设置开始请求的时间end_time = \"10:00:30\"  # 设置结束请求的时间def make_request():\n    response = requests.get(url, headers=headers, data=payload)\n    if \"成功\" in response.text:\n        print(\"响应内容:\", response.text)\n        raise SystemExit  # 中断程序while True:\n    current_time = time.strftime(\"%H:%M:%S\", time.localtime())\n    if current_time >= start_time and current_time <= end_time:\n        make_request()\n    time.sleep(1)  # 每秒检查一次当前时间\n```\n\n\n将其保存到本地并通过 python 指令来执行，就可以运行了。\n\n\n## 总结\n\n\n用今天的午睡时间，写了这篇文，以瑞幸的营销活动为例子，带你感受了下技术的魅力，其中涉及到了抓包、自动化脚本、定时任务、请求策略、stream 和 postman 等知识；\n\n",
      "properties": {
        "password": "",
        "icon": "",
        "date": "2023-06-02",
        "type": "Post",
        "category": "碎片杂文",
        "urlname": "lkcoffee-shanghai-coffee-activities-20230602",
        "catalog": [
          "archives"
        ],
        "tags": [
          "咖啡",
          "整活",
          "技术流"
        ],
        "summary": "最近瑞幸在搞活动，每天免费送10000份咖啡，我是个狂热喝咖啡的人儿，今天最后一天来整个活儿，点开瑞幸咖啡小程序主页，banner 栏轮播图中有一张海报入口，操作一通下来，果然，没抢到。\n手速不够快不是主要原因，手指操作延迟 + 系统页面跳转耗时加起来到 http 发出就已经耽误了1 -2 秒钟了，这个时间才是关键，本文从技术角度探讨下怎么在最小成本比如几分钟内，实现一个小工具，来解决这个问题。",
        "sort": "",
        "title": "上海咖啡文化周之薅瑞幸羊毛",
        "status": "Published",
        "updated": "2023-10-08 14:42:00"
      },
      "catalog": [
        {
          "title": "archives",
          "doc_id": "613b560e-75d7-4f45-88e9-b487310cfe82"
        }
      ],
      "body": "",
      "realName": "上海咖啡文化周之薅瑞幸羊毛",
      "relativePath": "/archives/上海咖啡文化周之薅瑞幸羊毛.md"
    },
    {
      "id": "121dcfb7-7028-42d1-998e-b81fe6e0d870",
      "doc_id": "121dcfb7-7028-42d1-998e-b81fe6e0d870",
      "title": "121dcfb7-7028-42d1-998e-b81fe6e0d870",
      "updated": 1696747320000,
      "body_original": "\n## 介绍\n\n\nSpring 框架一直提供了两种不同的客户端来执行 http 请求:\n\n- RestTemplate: 它在 Spring 3 中被引入，提供同步的阻塞式通信。\n- WebClient: 它在 Spring 5 的 Spring WebFlux 库中作为一部分被发布。它提供了流式 API,遵循响应式模型。\n\nRestTemplate 的方法暴露了太多的 HTTP 特性,导致了大量重载的方法，使用成本较高。WebClient 是 RestTemplate 的替代品,支持同步和异步调用。它是 Spring Web Reactive 项目的一部分。\n\n\n现在 Spring 6.1 M1 版本引入了 RestClient。一个新的同步 http 客户端,其工作方式与 WebClient 类似,使用与 RestTemplate 相同的基础设施。\n\n\n## 准备项目\n\n\n### jar 依赖\n\n\n```xml\n<dependency>  \n\t<groupId>org.springframework.boot</groupId>\n  <artifactId>spring-boot-starter-web</artifactId>\n</dependency>\n```\n\n\n![](https://cdn.jsdelivr.net/gh/listener-He/images@default/202309061708385.png)\n\n\n## 创建全局 RestClient\n\n\n创建 RestClient 实例有可用的静态方法:\n\n- create(): 委托给默认的 rest 客户端。\n- create(String url): 接受一个默认的基础 url。\n- create(RestTemplate restTemplate): 基于给定 rest 模板的配置初始化一个新的 RestClient。\n- builder(): 允许使用 headers、错误处理程序、拦截器等选项自定义一个 RestClient。\n- builder(RestTemplate restTemplate): 基于给定 RestTemplate 的配置获取一个 RestClient builder。\n\n让我们使用 builder 方法调用客户 API 来编写一个 RestClient:\n\n\n```java\nRestClient restClient = RestClient.builder()\n    .baseUrl(properties.getUrl())\n    .defaultHeader(HttpHeaders.AUTHORIZATION, encodeBasic(\"pig\", \"pig\"))\n    .build();\n```\n\n\n参数说明:\n\n- baseUrl 设置基础 url\n- defaultHeader 允许设置一个默认 http 请求头\n\n## 接收数据 retrieve\n\n\n下一步是使用客户端发送 http 请求并接收响应。RestClient 为每种 HTTP 方法都提供了方法。例如,要搜索所有活动客户,必须执行 GET 请求。retrieve 方法获取响应并声明如何提取它。\n\n\n让我们从使用完整正文作为 String 的简单情况开始。\n\n\n```java\nString data = restClient.get()\n    .uri(\"?name={name}&type={type}\", \"lengleng\", \"1\")\n    .accept(MediaType.APPLICATION_JSON)\n    .retrieve()\n    .body(String.class);\n```\n\n\nuri 方法可以设置 http 参数第一个参数(一个字符串模板)是附加到 RestClient 中定义的 base url 的查询字符串。第二个参数是模板的 uri 变量(varargs)。\n\n\n我们还指定媒体类型为 JSON。输出显示在控制台中:\n\n\n```json\n[\n  {\n    \"id\": 1,\n    \"name\": \"lengleng\",\n    \"type\": \"1\"\n  }\n]\n```\n\n\n如果需要检查响应状态码或响应头怎么办?别担心,toEntity 方法会返回一个 **ResponseEntity**。\n\n\n```java\nResponseEntity response = restClient\n    .get()\n    .uri(\"?name={name}&type={type}\", \"lengleng\", \"1\")\n    .accept(MediaType.APPLICATION_JSON)\n    .retrieve()\n    .toEntity(String.class);\n\nlogger.info(\"Status \" + response.getStatusCode());\nlogger.info(\"Headers \" + response.getHeaders());\n```\n\n\n## 结果转换 Bean\n\n\nRestClient 还可以将响应主体转换为 JSON 格式。Spring 将自动默认注册 MappingJackson2HttpMessageConverter 或 MappingJacksonHttpMessageConverter,如果在类路径中检测到 Jackson 2 库或 Jackson 库。但是你可以注册自己的消息转换器并覆盖默认设置。\n\n\n在我们的例子中,响应可以直接转换为记录。例如,检索特定客户的 API:\n\n\n```java\nReqUserResponse customer = restClient.get()\n    .uri(\"/{name}\", \"lengleng\")\n    .accept(MediaType.APPLICATION_JSON)\n    .retrieve()\n    .body(ReqUserResponse.class);\nlogger.info(\"res name: \" + customer.personInfo().name());\n```\n\n\n要搜索客户,我们只需要使用 List 类,如下所示:\n\n\n```java\nList<ReqUserResponse> customers = restClient.get()\n    .uri(\"?type={type}\", \"1\")\n    .accept(MediaType.APPLICATION_JSON)\n    .retrieve()\n    .body(List.class);\n    \nlogger.info(\"res size \" + customers.size());\n```\n\n\n## 发布数据\n\n\n要发送 post 请求,只需调用 post 方法。下一段代码片段创建一个新客户。\n\n\n```java\nReqUserResponse customer = new ReqUserResponse(\"lengleng-plus\", \"1\");\nResponseEntity<Void> response = restClient\n    .post()\n    .accept(MediaType.APPLICATION_JSON)\n    .body(customer)\n    .retrieve()\n    .toBodilessEntity();\n\nif (response.getStatusCode().is2xxSuccessful()) {\n    logger.info(\"Created \" + response.getStatusCode());\n    logger.info(\"New URL \" + response.getHeaders().getLocation());\n}\n```\n\n\n响应代码确认客户已成功创建:\n\n\n```text\nCreated 201 CREATEDNew URL http://localhost:8080/api/v1/customers/11\n```\n\n\n要验证客户是否已添加,可以通过 postman 检索以上 URL:\n\n\n```text\n{  \"id\": 2,  \"name\": \"lengleng-plus\",  \"type\": \"1\"}\n```\n\n\n当然,可以使用与前一节类似的代码通过 RestClient 获取它。\n\n\n## 删除数据\n\n\n调用 delete 方法发出 HTTP delete 请求尝试删除资源非常简单。\n\n\n```java\nResponseEntity<Void> response = restClient.delete()\n    .uri(\"/{id}\", 2)\n    .accept(MediaType.APPLICATION_JSON)\n    .retrieve()\n    .toBodilessEntity();\nlogger.info(\"Deleted with status \" + response.getStatusCode());\n```\n\n\n值得一提的是,如果操作成功,响应主体将为空。对于这种情况,toBodilessEntity 方法非常方便。要删除的客户 ID 作为 uri 变量传递。\n\n\n```text\nDeleted with status 204 NO_CONTENT\n```\n\n\n## 处理错误\n\n\n如果我们尝试删除或查询一个不存在的客户会发生什么?客户端点将返回一个 404 错误代码以及消息详细信息。然而,每当接收到客户端错误状态码(400-499)或服务器错误状态码(500-599)时,RestClient 将抛出 RestClientException 的子类。\n\n\n要定义自定义异常处理程序,有两种选项适用于不同的级别:\n\n- 在 RestClient 中使用 defaultStatusHandler 方法(对其发送的所有 http 请求)\n\n```java\nRestClient restClient = RestClient.builder()\n    .baseUrl(properties.getUrl())\n    .defaultHeader(HttpHeaders.AUTHORIZATION, encodeBasic(\"pig\", \"pig\"))\n    .defaultStatusHandler(\n        HttpStatusCode::is4xxClientError,\n        (request, response) -> {\n            logger.error(\"Client Error Status \" + response.getStatusCode());\n            logger.error(\"Client Error Body \" + new String(response.getBody().readAllBytes()));\n        }\n    )\n    .build();\n```\n\n\n在运行删除命令行运行程序后,控制台的输出如下:\n\n\n```text\nClient Error Status 404 NOT_FOUNDClient Error Body {\"status\":404,\"message\":\"Entity Customer for id 2 was not found.\",\"timestamp\":\"2023-07-23T09:24:55.4088208\"}\n```\n\n- 另一种选择是为删除操作实现 **onstatus** 方法。它优先于 RestClient 默认处理程序行为。\n\n```java\nResponseEntity response = restClient.delete()\n    .uri(\"/{id}\", 2)\n    .accept(MediaType.APPLICATION_JSON)\n    .retrieve()\n    .onStatus(HttpStatusCode::is4xxClientError, (req, res) -> logger.error(\"Couldn't delete \" + res.getStatusText()))\n    .toBodilessEntity();\nif (response.getStatusCode().is2xxSuccessful())\n    logger.info(\"Deleted with status \" + response.getStatusCode());\n```\n\n\n现在控制台中的消息将是:\n\n\n```text\nCouldn't delete Not Found\n```\n\n\n## Exchange 方法\n\n\n当响应必须根据响应状态进行不同解码时,exchange 方法很有用。使用 exchange 方法时,状态处理程序将被忽略。\n\n\n在这个虚构的示例代码中,响应基于状态映射到实体:\n\n\n```java\nSimpleResponse simpleResponse = restClient.get()\n    .uri(\"/{id}\", 4)\n    .accept(MediaType.APPLICATION_JSON)\n    .exchange((req, res) -> switch (res.getStatusCode().value()) {\n        case 200 -> SimpleResponse.FOUND;\n        case 404 -> SimpleResponse.NOT_FOUND;\n        default -> SimpleResponse.ERROR;\n    });\n```\n\n\n## 总结\n\n\n与旧的 RestTemplate 相比,这个新 API 更易于管理，官方重构的目的绝非是造轮子，而是目标去实现基于 Loom 标准的的 Http 客户端,更好的适配 JDK21 的虚拟线程，提供更高性能的客户端实现。\n\n",
      "properties": {
        "password": "",
        "icon": "",
        "date": "2023-09-06",
        "type": "Post",
        "category": "技术分享",
        "urlname": "spring-restclient-2023",
        "catalog": [
          "archives"
        ],
        "tags": [
          "Spring",
          "Java"
        ],
        "summary": "Spring 框架一直提供了两种不同的客户端来执行 http 请求:\n• RestTemplate: 它在 Spring 3 中被引入，提供同步的阻塞式通信。\n• WebClient: 它在 Spring 5 的 Spring WebFlux 库中作为一部分被发布。它提供了流式 API,遵循响应式模型。\nRestTemplate 的方法暴露了太多的 HTTP 特性,导致了大量重载的方法，使用成本较高。WebClient 是 RestTemplate 的替代品,支持同步和异步调用。它是 Spring Web Reactive 项目的一部分。\n现在 Spring 6.1 M1 版本引入了 RestClient。一个新的同步 http 客户端,其工作方式与 WebClient 类似,使用与 RestTemplate 相同的基础设施。",
        "sort": "",
        "title": "HttpClient? RestTemplate？WebClient? 不~是 RestClient",
        "status": "Published",
        "updated": "2023-10-08 14:42:00"
      },
      "catalog": [
        {
          "title": "archives",
          "doc_id": "121dcfb7-7028-42d1-998e-b81fe6e0d870"
        }
      ],
      "body": "",
      "realName": "HttpClient! RestTemplate？WebClient! 不~是 RestClient",
      "relativePath": "/archives/HttpClient! RestTemplate？WebClient! 不~是 RestClient.md"
    },
    {
      "id": "d8051be9-7bd7-4b59-a482-f05b9fb15699",
      "doc_id": "d8051be9-7bd7-4b59-a482-f05b9fb15699",
      "title": "d8051be9-7bd7-4b59-a482-f05b9fb15699",
      "updated": 1696747320000,
      "body_original": "\n## 前言\n\n\n最近项目上要求升级一个工具包`hutool`的版本，以解决安全漏洞问题，这不升级还好，一升级反而捅出了更大的篓子，究竟是怎么回事呢？\n\n\n## 事件回顾\n\n\n我们项目原先使用的`hutool`版本是5.7.2，在代码中，我们的数据传输对象DTO和数据实体对象中大量使用了工具包中的`BeanUtil.copyProperties()`, 大体代码如下：\n\n1. 数据传输对象\n\n```java\n@Data\n@ToString\npublic class DiagramDTO { \n   // 前端生产的字符串id  \n   private String id;  \n   private String code;\n   private String name;\n}\n```\n\n1. 数据实体对象\n\n```java\n@Data\n@ToString\npublic class Diagram {  \n  private Integer id;   \n  private String code; \n  private String name;\n}\n```\n\n1. 业务逻辑\n\n```java\npublic class BeanCopyTest {   \n\t public static void main(String[] args) { \n       // 前端传输的对象       \n\t\t\t DiagramDTO diagramDTO = new DiagramDTO();  \n      // 如果前端传入的id事包含e的，升级后就会报错     \n\t\t   diagramDTO.setId(\"3em3dgqsgmn0\");     \n\t\t   diagramDTO.setCode(\"d1\");       \n\t\t\t diagramDTO.setName(\"图表\");   \n\t     Diagram diagram = new Diagram();  \n      // 关键点，数据拷贝        \n\t\t\tBeanUtil.copyProperties(diagramDTO, diagram);   \n      System.out.println(\"数据实体对象：\" + diagram);     \n\t   //设置id为空，自增        \n\t\t\tdiagram.setId(null); \n      //保存到数据库中 TODO    \n\t    //diagramMapper.save(diagram); \n   }}\n```\n\n\n升级前，`hutool`是5.7.2版本下，执行结果如下图。\n\n\n![](https://mmbiz.qpic.cn/mmbiz_png/oH5VKTC5sLgticcibSXbXUSYLrUMNovDaTjjibKRMn2hre1iczL2YJiaKBfP6nMRibht3Aj5rABic23bXfGCiaAW0E9slQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)\n\n- `BeanUtil.copyProperties`虽然字段类型不一样，但是做了兼容处理，所以业务没有影响业务逻辑。\n\n升级后，`hutool`是5.8.8版本，执行结果如下图所示：\n\n\n![](https://mmbiz.qpic.cn/mmbiz_png/oH5VKTC5sLgticcibSXbXUSYLrUMNovDaTU3PxCE8ZdB1RbGTZeOico3N8u9KV7okT3zWGN5kV5les3lYcX5SAXjg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)\n\n- 执行报错，因为升级后的版本修改了实现，增加了下面的逻辑，如果包含E, 就会抛错，从而影响了业务逻辑，同时这个id是否包含e又是随机因素，到了生产才发现，就悲剧了。\n\n![](https://mmbiz.qpic.cn/mmbiz_png/oH5VKTC5sLgticcibSXbXUSYLrUMNovDaTG9TzjlbbByPMyMa7cAztWZJ3hBTRiatE1I4pLotd84fNgwkRIibyKAVw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)\n\n\n## 分析探讨\n\n\n我发现大部分人写代码都喜欢偷懒，在上面的场景中，虽然`BeanUtil.copyProperties`用的一时爽，但有时候带来的后果是很严重的，所以很不推荐这种方式。为什么这么说呢？\n\n\n比如团队中的某些人偷偷改了数据传输对象DTO，比如修改了类型、删去了某个字段。用`BeanUtil.copyProperties`的方式压根无法在编译阶段发现，更别提修改的影响范围了，这就只能把风险暴露到生产上去了。那有什么更好的方法呢？\n\n\n## 推荐方案\n\n1. 原始的`get`、`set`方式\n\n我是比较推崇这种做法的，比如现在`DiagramDTO`删去某个字段，编译器就会报错，就会引起你的注意了，让问题提前暴露，无处遁形。\n\n\n![](https://mmbiz.qpic.cn/mmbiz_png/oH5VKTC5sLgticcibSXbXUSYLrUMNovDaTliaL28982IYstMia7EjVia0KhJvWAwSkD4bjUwWFBnsAXGGvtbQfoOK0Q/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)\n\n\n你可能觉得站着说话不腰疼，字段少好，如果字段很多还不得写死啊，我这里推荐一个IDEA的插件，可以帮你智能生成这样的代码。\n\n\n![](https://mmbiz.qpic.cn/mmbiz_png/oH5VKTC5sLgticcibSXbXUSYLrUMNovDaT3mFLBMO8icetJcjicn4RqCwMuaeqibKoTNxTaJRuZOicXn4OdnYP9TPibsw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)\n\n\n![](https://mmbiz.qpic.cn/mmbiz_png/oH5VKTC5sLgticcibSXbXUSYLrUMNovDaTSGKwbEQxgZgfbZQumibSH0yavicrPa9TJB2vxz6UCXN4hHBGv0j5flFA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)\n\n\n话不多说，自己玩儿去~~\n\n1. 使用开源库`ModelMapper`\n\n`ModelMapper`是一个开源库，可以很方便、简单地将对象从一种类型映射到另一种类型，底层是通过反射来自动确定对象之间的映射，还可以自定义映射规则。\n\n\n```java\n private static void testModelMapper() {  \n      ModelMapper modelMapper = new ModelMapper(); \n      DiagramDTO diagramDTO = new DiagramDTO(); \n       diagramDTO.setId(\"3em3dgqsgmn0\");    \n\t     diagramDTO.setCode(\"d1\");    \n\t     diagramDTO.setName(\"图表\");   \n     Diagram diagram = modelMapper.map(diagramDTO, Diagram.class);   \n }\n```\n\n1. 使用开源库`MapStruct`\n\n`MapStruct`也是Java中另外一个用于映射对象很流行的开源工具。它是在编译阶段生成对应的映射代码，相对于`ModelMapper`底层放射的方案，性能更好。\n\n\n```java\n@Mapperpublic \ninterface DiagramMapper {  \n  DiagramMapper INSTANCE = Mappers.getMapper(DiagramMapper.class);   \n DiagramDTO toDTO(Diagram diagram); \n Diagram toEntity(DiagramDTO diagram);\n}\n\nprivate static void testMapStruct() {  \n  DiagramDTO diagramDTO = new DiagramDTO();\n    diagramDTO.setId(\"3em3dgqsgmn0\");  \n    diagramDTO.setCode(\"d1\");   \n    diagramDTO.setName(\"图表\"); \n  Diagram diagram = DiagramMapper.INSTANCE.toEntity(diagramDTO);\n}\n```\n\n- `DiagramMapper`接口使用了`@Mapper`注解，用来表明使用`MapStruct`处理\n- `MapStruct`中更多高级特性大家自己探索一下。\n\n## 总结\n\n\n小结一下，对象在不同层之间进行转换映射，很不建议使用`BeanUtil.copyProperties`这种方式，更加推荐使用原生的`set`, `get`方式，不容易出错。当然这不是将`BeanUtil.copyProperties`一棒子打死，毫无用武之地，在特定场景，比如方法内部对象的转换等影响小的范围还是很方便的。\n\n\n原作者： [https://blog.csdn.net/weiioy](https://blog.csdn.net/weiioy)\n\n",
      "properties": {
        "password": "",
        "icon": "",
        "date": "2023-05-23",
        "type": "Post",
        "category": "技术分享",
        "urlname": "hutool-beanutil-error",
        "catalog": [
          "archives"
        ],
        "tags": [
          "BUG",
          "Java",
          "hutool"
        ],
        "summary": "Hutool是一个小而全的Java工具类库，通过静态方法封装，降低相关API的学习成本，提高工作效率，使Java拥有函数式语言般的优雅，让Java语言也可以“甜甜的”。 JavaBean是一个拥有对属性进行set和get方法的类。它可以被简单地定义为包含setXXX和getXXX方法的对象。在Hutool中，判定Bean的方法为：是否存在只有一个参数的setXXX方法。Bean工具类主要操作setXXX和getXXX方法，如将Bean对象转为Map等。",
        "sort": "",
        "title": "Hutool 5.8.8  BeanUtil.copyProperties 致命异常",
        "status": "Published",
        "updated": "2023-10-08 14:42:00"
      },
      "catalog": [
        {
          "title": "archives",
          "doc_id": "d8051be9-7bd7-4b59-a482-f05b9fb15699"
        }
      ],
      "body": "",
      "realName": "Hutool 5.8.8  BeanUtil.copyProperties 致命异常",
      "relativePath": "/archives/Hutool 5.8.8  BeanUtil.copyProperties 致命异常.md"
    },
    {
      "id": "50c530fc-8d3c-4f6b-9956-3be2602e1be8",
      "doc_id": "50c530fc-8d3c-4f6b-9956-3be2602e1be8",
      "title": "50c530fc-8d3c-4f6b-9956-3be2602e1be8",
      "updated": 1696747320000,
      "body_original": "\n在 21 世纪的今天，我们每天都要面临大量的知识和信息。我们每个人都需要不断学习新知识、新思想和进行新的实践。\n\n\n这意味着，学习不再是学生阶段才需要做的事情，终身教育应该成为我们工作和生活的有机组成部分。\n\n\n在自我学习的过程中，很多人都会尝试寻找高效的学习方法，以便提升自己的学习能力和学习效率。\n\n\n接下来我分享的学习方法，不仅是我自己切身实践有效，也有科学依据作为支撑。\n\n\n## PQ4R 阅读法\n\n\nPQ4R 是英文首字母缩写，代表了阅读理解流程中的六个阶段。\n\n- **预览 Preview**：在开始阅读之前预览信息，以了解主题是什么。略读材料并查看文章标题、副标题和一些加粗重点文本。\n- **问题 Question**：问自己与该主题相关的问题，例如，我希望学到什么？关于这个主题，我已经知道什么？\n- **阅读 Read**：每次阅读一条信息，并尝试找出问题的答案。\n- **反思 Reflect**：你回答所有问题了吗？如果没有，请返回原文查看，看看是否可以找到答案。\n- **背诵 Recite**：用你自己的话，说出或写下你刚刚阅读内容的简要摘要。\n- **复习 Review**：再看一遍材料，回答哪些尚未回答的问题。\n\n![](https://static.effie.co/blog/2021/07/20210709020705553.png?x-oss-process=image/auto-orient,1/quality,q_90/format,webp)\n\n\n## 间隔练习 Spaced Practice\n\n\n间隔练习，英文为 Spaced Practice, 也被称为分散式练习 distributed practice. 这是一种被科学证明富有效率的记忆方法。\n\n\n近年来，随着 Anki 等软件的普及，间隔练习的理念也逐步为学习者所熟悉。\n\n\n间隔练习反对学习者对于知识进行突击式的死记硬背，而是鼓励学习者在更长的时间段内进行学习。当我们的大脑几乎忘记某事时，大脑会更加努力地回忆起这些信息。\n\n\n间隔学习根据记忆规律，将学习按照一定的时间进行间隔，这样将会促使我们的大脑在知识和想法之间建立联系，并且轻松回忆知识内容。\n\n\n间隔练习要求学习者以下列时间表的间隔时间回顾我们的学习内容：第 1 天：学习所学知识。随后第 2 天、第 3 天、一周后、两周后各进行一次回顾与复习。\n\n\n![](https://static.effie.co/blog/2021/07/20210709020804102-1024x428.png?x-oss-process=image/auto-orient,1/quality,q_90/format,webp)\n\n\n## 使用费曼技巧\n\n\n术语或者概念是知识的基本单位，相当于构建成知识大厦的砖瓦。如果对于一个术语或概念一知半解，那最后肯定会影响我们对于知识掌握的深度和广度。\n\n\n而费曼技巧是一种学习各种术语和概念的有效方法。\n\n\n费曼技巧的**基本思想**：如果我们想很好地理解某事，那么试着简单地解释它。这意味着，我们可以尝试通过自己的话解释某个概念，这样我们可能会更为快速更加深刻地理解这个概念。\n\n\n费曼技巧的**基本流程**是：\n\n\n首先在一张纸的顶部写下你正在研究的主题/概念。\n\n\n然后，用你自己的话解释这个主题和概念，就像你在正在教别人一样。\n\n\n随后，查看你所写的内容并找出任何错误的地方，并且回到笔记或阅读材料中找出正确的答案。\n\n\n最后，如果你在写作中有任何使用专业术语的地方，而你的读者不是专业人士的话，最好使用一些更为简单的术语重新表达你的写作内容。\n\n\n## 善用思维导图\n\n\n使用思维导图记录和整理笔记，不仅可以提高我们对于所学习内容的理解能力，并且通过思维导图能够有效地了解各种想法之间的层次结构关系。\n\n\n如果你是视觉型学习者，那强烈建议你使用思维导图。打开思维导图工具，新建文稿，在中心写下你的学习主题，然后建立子节点。使用思维导图，最好选择合适的思维导图工具。\n\n\n在我看来，使用传统的思维导图工具，往往还需要导出为图片等文件，再根据思维导图的内容记录笔记或者进行写作。\n\n\n在这样的场景中，思维导图和文本编辑器是割裂的，不利于在记笔记或者写作时随时查看、编辑思维导图。\n\n\n因此，我选择了专业写作工具 [Effie](https://www.effie.co/), 其内置了大纲一键转化为思维导图的功能。如此，我便可以在学习过程中随时使用思维导图了。\n\n\n![](https://static.effie.co/blog/2021/05/20210524053542357-1024x796.png?x-oss-process=image/auto-orient,1/quality,q_90/format,webp)\n\n\n## 睡眠与学习：保障睡眠，睡前学习\n\n\n高效的学习方法，不是无限延长学习时间，而是学会劳逸结合。如果你想最大限度地利用时间，那也要保障每天睡眠 6-8 小时。这是大量科学研究证明的结果。\n\n\n不要再去检索如何每天只睡 4 个小时又同时保障精力充沛这样的问题了。\n\n\n每天只睡如此少的时间，不适用于绝大多数人。并且，每天只睡眠 4 个小时是成功人的特质，这样的说法基本上属于身居高位者对于打工人的心灵鸡汤。\n\n\n足够的睡眠除了对身体健康有重要影响外，睡眠对大脑功能、记忆形成和学习也至关重要。睡眠会使得大脑进行一定程度的休息，并且会优化和完善在清醒状态时所获取的知识和信息。\n\n\n此外，睡前学习是一种良好的学习习惯。无论是复习笔记，还是复习抽认卡，这都有助于提高我们的记忆。与其熬夜，不如在睡前几个小时学习，然后在早上查看回顾知识。\n\n\n## 运动与学习\n\n\n在学习方法的分享中，很少有人会讨论运动和学习之间的关系。究其原因，可能很多人潜意识里认为运动会占用学习时间。然而，事实上，运动与学习之间关系密切。\n\n\n很多人在进行长时段学习后，经常会出现精力不济、学习效率下降的现象。\n\n\n如果每天保持适当的锻炼，不仅可以对抗疲劳，提升我们的记忆力和认知能力，而且会促使大脑释放脑内啡等化合物，改善我们的情绪并且降低我们的精神压力水平。\n\n\n## 注释\n\n- 文章**转载自 Effie Blog**, 已经获得授权。\n- **Effie 官方介绍**：无论是**严肃写作**，**随手记录**，亦或是把逻辑完善成思维导图，Effie 都是您明智的选择。\n- **Effie 简要介绍**：一款**思写合一**的**专业写作软件**，拥有 Markdown 语法支持、全平台、极简沉浸设计等基本要求，同时内置了思维导图，允许用户将大纲列表**一键转化为思维导图模式**。\n- [Effie 官网| 把思想变为价值](https://www.effie.co/)\n- [Effie 海外英文官网](https://www.effie.pro/)\n- [Effie Blog](https://blog.effie.co/)\n- [Effie 新手入门完全指南](https://blog.effie.co/effie-%e6%96%b0%e6%89%8b%e5%85%a5%e9%97%a8%e5%ae%8c%e5%85%a8%e6%8c%87%e5%8d%97-%e4%b8%89%e5%88%86%e9%92%9f%e6%8a%8a%e4%bd%a0%e7%9a%84%e6%80%9d%e6%83%b3%e5%8f%98%e4%b8%ba%e4%bb%b7%e5%80%bc/)\n- [Effie：为深度创作，也为记录灵感](https://blog.effie.co/effie%ef%bc%9a%e4%b8%ba%e6%b7%b1%e5%ba%a6%e5%88%9b%e4%bd%9c%ef%bc%8c%e4%b9%9f%e4%b8%ba%e8%ae%b0%e5%bd%95%e7%81%b5%e6%84%9f/)\n",
      "properties": {
        "password": "",
        "icon": "",
        "date": "2023-04-21",
        "type": "Post",
        "category": "学习思考",
        "urlname": "efficient-learning-methods-pq4r",
        "catalog": [
          "archives"
        ],
        "tags": [
          "文字",
          "思考",
          "学习"
        ],
        "summary": "在 21 世纪的今天，我们每天都要面临大量的知识和信息。我们每个人都需要不断学习新知识、新思想和进行新的实践。\n这意味着，学习不再是学生阶段才需要做的事情，终身教育应该成为我们工作和生活的有机组成部分。\n在自我学习的过程中，很多人都会尝试寻找高效的学习方法，以便提升自己的学习能力和学习效率。\n接下来我分享的学习方法，不仅是我自己切身实践有效，也有科学依据作为支撑。",
        "sort": "",
        "title": "你有什么值得分享的高效学习方法？",
        "status": "Published",
        "updated": "2023-10-08 14:42:00"
      },
      "catalog": [
        {
          "title": "archives",
          "doc_id": "50c530fc-8d3c-4f6b-9956-3be2602e1be8"
        }
      ],
      "body": "",
      "realName": "你有什么值得分享的高效学习方法？",
      "relativePath": "/archives/你有什么值得分享的高效学习方法？.md"
    },
    {
      "id": "8900fb84-5f49-4f66-a5fa-fd34b7fd832a",
      "doc_id": "8900fb84-5f49-4f66-a5fa-fd34b7fd832a",
      "title": "8900fb84-5f49-4f66-a5fa-fd34b7fd832a",
      "updated": 1696747320000,
      "body_original": "\n## 什么是主动阅读？\n\n\n**被动阅读**的典型代表便是应试教育之下的各种填鸭式阅读。\n\n\n如果阅读者在阅读过程中的阅读心理和阅读行为都是被动的，那么对于知识和信息的掌握必然是低效的。\n\n\n高效阅读者都会使用一系列被称为**主动阅读**的阅读策略。通过主动阅读，我们更加全面、更加深入地理解和掌握知识。\n\n\n主动阅读意味着阅读者在阅读某些内容时，积极地与文本或者作者进行对话，尝试去理解、评估、记忆、反思所读内容。\n\n\n![](https://static.effie.co/blog/2021/08/20210816143032448-1024x512.png?x-oss-process=image/auto-orient,1/quality,q_90/format,webp)\n\n\n在《如何阅读一本书》这本经典著作中，作者**区分了基础阅读、检视阅读、分析阅读、主题阅读四种阅读层次。**\n\n\n其中，基础阅读以外的其他三种阅读层次都属于主动阅读。在作者看来，一个好的读者自然应该是主动阅读者。阅读的艺术便是需要通过批判性思维以适当的顺序提出适当的问题。\n\n\n具体而言，主动阅读的基础包含四个基础问题：\n\n1. **整体而言，这本书在谈论什么？（关于主题与关键议题）**\n2. **作者具体说了什么，怎么说的？（关于作者的想法、声明与论点）**\n3. **这本书说的是否有道理？全部有道理，还是部分有道理？（对书进行批判性评价）**\n4. **这本书跟你有什么关系？为你提供了资讯？启发了你？**\n\n如果你想成为主动阅读者，那基本要求便是勇于承担阅读者的责任，在阅读过程中利用批判性思维对这四个基础问题进行自我提问并认真回答。\n\n\n其中，关于批判性思维与阅读方法，具体可以移步阅读我以前的博文《[批判性阅读和批判性写作](https://blog.effie.co/%e6%89%b9%e5%88%a4%e6%80%a7%e9%98%85%e8%af%bb%e5%92%8c%e6%89%b9%e5%88%a4%e6%80%a7%e5%86%99%e4%bd%9c/)》。\n\n\n![](https://static.effie.co/blog/2021/08/20210816143128331-1024x537.png?x-oss-process=image/auto-orient,1/quality,q_90/format,webp)\n\n\n## 14 种主动阅读策略\n\n\n### 理解作者的意图\n\n\n作者的目标是告知、论证某个理论和观点，还是想要宣传他们的产品或服务吗？花几分钟认真阅读文本资料的内容介绍（前言、摘要、说明文档等），以了解作者写作的原因和意图。\n\n\n### 调整阅读速度\n\n\n不要使用固定速度阅读文本内容，而是应该让自己的阅读速度适应当前正在阅读的内容。\n\n\n这意味着在阅读一些你已经熟悉的内容时加快速度，而遇到一些新的信息、知识时降低速度进行细读。\n\n\n### 标记文本\n\n\n必要时给段落进行编号。圈出阅读过程中比较重要的单词、短语、名称、日期等重要信息。在作者的观点、声明等重要信息下面划线。\n\n\n### 注释内容\n\n\n记笔记是与阅读内容保持联系、进行对话的好方法。请在纸质页面空白处或者编辑器页面随时写下你在阅读某些内容时突然想到的各种想法。\n\n\n**标记文本只是辅助，重要的是注释和评论。**\n\n\n在阅读过程中，可以尝试总结文本、提出问题、对某个观点表示同意或者质疑。当然，你也可以写下一些关键词帮助你讨论或者后面回忆重点内容。\n\n\n或者，你可以将文章的标题、副标题、章节和段落标题更改为问题，引导自己进行积极思考。上面的这些要求都是要求主动阅读者尽量与作者进行对话。\n\n\n### 释义\n\n\n当在阅读过程中，遇到一个你感觉比较难以掌握的新概念时，建议停下来尝试使用自己的话对这个概念进行解释。\n\n\n### 总结段落思想\n\n\n仔细阅读每个段落，使用你自己的话表达段落的主要思想。然后分析文章的主要观点是什么，以及具体是如何论证的。\n\n\n### 书写章节摘要\n\n\n使用你自己的话写出一篇文章或某个章节的摘要。在摘要中使用凝练的语言概括阅读内容的基本思想，并罗列出关键词。这是一种能检验自己是否真正理解所读内容的有效方法。\n\n\n### 建立信息或者知识块\n\n\n将信息或者知识分解成更小的块，以便更为轻松地保留。比如，以概念或者理论为单位进行分解成知识块，后期可以较为方便地对这些知识块进行排列组合。\n\n\n### 连接节点\n\n\n当遇到新的知识，尝试将文本内容与个人经验或者其他知识内容进行联系。事实上，这是在释义、信息/知识块的基础上，连接节点可以建立知识网络。\n\n\n### 直观地组织信息\n\n\n将文本内容可视化，以便更好地查看、新建和发现知识之间的联系。换而言之，知识内容图谱化。\n\n\n目前，已经有一些编辑器提供了知识图谱辅助功能。不过，任何知识图谱只有阅读者的主动建构才有意义和价值。\n\n\n利用大纲列表、思维导图、流程图、数字白板、图表等工具，我们也可以直观地梳理我们的想法。\n\n\n### 获取参考信息\n\n\n每当你有疑问时，请使用字典或其他参考资料工具，以此确保你能准确理解所读汉字、单词、专业术语的含义，以及掌握相关的背景知识信息。\n\n\n### 总结内容\n\n\n一旦你读完一本书，那最好坐下来写你自己的总结。如果你在网上发布你的阅读体验，你还可以获取其他读者的反馈。\n\n\n通过与他人观点的交汇和碰撞，我们对于阅读内容的理解和反思会更加深入。\n\n\n### 评价内容\n\n\n每隔一段时间，后退几步——从阅读内容中跳出来，然后批判性地思考你正在阅读的内容。思考一下，阅读资料的结构是否合理？论点是否存在漏洞？作者是否有明显或者潜在的偏见？\n\n\n### 通过输出增强理解\n\n\n尝试将自己的所掌握的新知教给别人（输出）。大量研究明确表明，教学是最有效的学习方式之一。\n\n\n如果你无法表达已读内容，那说明你对这些知识掌握并不牢靠。给别人讲解所读内容，是一种非常好的检验以及增强自己对于已读内容的自我学习方式。\n\n\n甚至在这个过程中，还可能激发你对这些已读内容产生新的理解。\n\n\n![](https://static.effie.co/blog/2021/08/20210817021017232-1024x663.png?x-oss-process=image/auto-orient,1/quality,q_90/format,webp)\n\n\n学习金字塔：主动学习和消极学习所获得知识的内容留存率差别悬殊\n\n\n上述主动阅读策略基本上展示了主动阅读的学习流程。需要说明的是，这 14 种阅读策略里面有一些是为学生等群体进行准备的。\n\n\n因此，在进行阅读时，并没有必要使用所有的主动阅读策略。当然，如果你想对某个文本进行认真研读，那这些阅读策略基本上都是适用的。\n\n\n## 注释\n\n- 文章**转载自 Effie Blog**, 已经获得授权。\n- **Effie 官方介绍**：无论是**严肃写作**，**随手记录**，亦或是把逻辑完善成思维导图，Effie 都是您明智的选择。\n- **Effie 简要介绍**：一款**思写合一**的**专业写作软件**，拥有 Markdown 语法支持、全平台、极简沉浸设计等基本要求，同时内置了思维导图，允许用户将大纲列表**一键转化为思维导图模式**。\n- [Effie 官网| 把思想变为价值](https://www.effie.co/)\n- [Effie 海外英文官网](https://www.effie.pro/)\n- [Effie Blog](https://blog.effie.co/)\n- [Effie 新手入门完全指南](https://blog.effie.co/effie-%e6%96%b0%e6%89%8b%e5%85%a5%e9%97%a8%e5%ae%8c%e5%85%a8%e6%8c%87%e5%8d%97-%e4%b8%89%e5%88%86%e9%92%9f%e6%8a%8a%e4%bd%a0%e7%9a%84%e6%80%9d%e6%83%b3%e5%8f%98%e4%b8%ba%e4%bb%b7%e5%80%bc/)\n- [Effie：为深度创作，也为记录灵感](https://blog.effie.co/effie%ef%bc%9a%e4%b8%ba%e6%b7%b1%e5%ba%a6%e5%88%9b%e4%bd%9c%ef%bc%8c%e4%b9%9f%e4%b8%ba%e8%ae%b0%e5%bd%95%e7%81%b5%e6%84%9f/)\n",
      "properties": {
        "password": "",
        "icon": "",
        "date": "2023-04-25",
        "type": "Post",
        "category": "学习思考",
        "urlname": "active-reading-becoming-better-reader",
        "catalog": [
          "archives"
        ],
        "tags": [
          "文字",
          "思考",
          "学习"
        ],
        "summary": "主动阅读是一种积极、深入的阅读方式，它需要读者在阅读过程中利用多种策略，如提问、概括、笔记、推理等，不仅理解文字表意，还要加深对其背后涵义的理解和记忆，同时，将自己的经验和知识运用到阅读中去，以达到更好的阅读效果。主动阅读能够提高我们的阅读能力和思维水平，培养我们批判性地思考和分析问题的能力，帮助我们更好地掌握知识和信息。",
        "sort": "",
        "title": "主动阅读：成为更好的阅读者",
        "status": "Published",
        "updated": "2023-10-08 14:42:00"
      },
      "catalog": [
        {
          "title": "archives",
          "doc_id": "8900fb84-5f49-4f66-a5fa-fd34b7fd832a"
        }
      ],
      "body": "",
      "realName": "主动阅读：成为更好的阅读者",
      "relativePath": "/archives/主动阅读：成为更好的阅读者.md"
    },
    {
      "id": "3a77d2b4-f1df-48cc-bdc1-3192e46e674c",
      "doc_id": "3a77d2b4-f1df-48cc-bdc1-3192e46e674c",
      "title": "3a77d2b4-f1df-48cc-bdc1-3192e46e674c",
      "updated": 1696747320000,
      "body_original": "\n[Red Hat 单点登录](https://www.redhat.com/en/products/middleware)(SSO) — 或其开源版本 Keycloak — 是 Web SSO 功能的领先产品之一，它基于流行的标准，例如安全断言标记语言 (SAML) 2.0、OpenID Connect 和OAuth 2.0。Red Hat SSO 最强大的功能之一是我们可以通过多种方式直接访问 Keycloak，无论是通过简单的 HTML 登录表单，还是通过 API 调用。在以下场景中，我们将生成一个 JWT 令牌，然后对其进行验证。一切都将使用 API 调用来完成，因此 Keycloak 的 UI 不会直接暴露给公众。\n\n\n设置用户\n\n\n首先，我们将在Keycloak中创建一个简单的用户，如图1所示。\n\n\n![](https://developers.redhat.com/sites/default/files/styles/article_floated/public/blog/2019/12/keycloak01-1.png?itok=WywnX4-4)\n\n\n图 1：在 Keycloak 中创建用户。\">\n\n\n填写所有必填字段，例如**Username**、**First Name**和**Last Name**，如图 2 所示。\n\n\n![](https://developers.redhat.com/sites/default/files/styles/article_floated/public/blog/2019/12/keycloak02-1.png?itok=6nuYjWAS)\n\n\n图 2：输入用户信息。\">\n\n\n设置用户密码，如图3所示。\n\n\n![](https://developers.redhat.com/sites/default/files/styles/article_floated/public/blog/2019/12/keycloak03-1.png?itok=xhzQFM8t)\n\n\n图 3：设置用户密码。\">\n\n\n# 设置客户端\n\n\n下一步是在我们的领域中创建一个特定的_客户端_，如图 4 所示。Keycloak 中的客户端代表特定用户可以访问的资源，无论是用于验证用户身份、请求身份信息还是验证访问令牌。\n\n\n![](https://developers.redhat.com/sites/default/files/styles/article_floated/public/blog/2019/12/keycloak04-1.png?itok=cKzhT2We)\n\n\n图 4：查看您现有的客户。\">\n\n\n单击**“创建”** ，打开**“添加客户端”**对话框，如图5所示。\n\n\n![](https://developers.redhat.com/sites/default/files/styles/article_floated/public/blog/2019/12/keycloak05-1.png?itok=GQ60hiap)\n\n\n图 5：创建新客户端。\">\n\n\n填写客户表格中的所有必填字段。请特别注意**Direct Grant Flow**（如图 6 所示）并将其值设置为**direct grant**。此外，将**访问类型**更改为**机密**。\n\n\n![](https://developers.redhat.com/sites/default/files/styles/article_floated/public/blog/2019/12/keycloak06-1.png?itok=azkV9Cm1)\n\n\n图 6：覆盖客户端的身份验证流程。\">\n\n\n**最后，将Client Authenticator**字段中的客户端凭证更改为**Client Id 和 Secret**，如图 7 所示。\n\n\n![](https://developers.redhat.com/sites/default/files/styles/article_floated/public/blog/2019/12/keycloak07-1.png?itok=3soIf6MC)\n\n\n图 7：设置新客户的凭据。\">\n\n\n# 测试你的新客户\n\n\n现在我们可以通过 REST API 来测试我们新创建的客户端来模拟一个简单的登录。我们的身份验证 URL 是\n\n\n\n填写参数并使用我们的用户名和密码设置client_id和client_secret：\n\n\n\n\n```shell\ncurl -L -X POST 'http://localhost:8080/auth/realms/whatever-realm/protocol/openid-connect/token' \\\n-H 'Content-Type: application/x-www-form-urlencoded' \\\n--data-urlencode 'client_id=clientid-03' \\\n--data-urlencode 'grant_type=password' \\\n--data-urlencode 'client_secret=ec78c6bb-8339-4bed-9b1b-e973d27107dc' \\\n--data-urlencode 'scope=openid' \\\n--data-urlencode 'username=emuhamma' \\\n--data-urlencode 'password=1'\n```\n\n\n或者，我们可以使用 Postman 等 REST API 工具来模拟 HTTP POST 请求，如图 8 所示。\n\n\n![](https://developers.redhat.com/sites/default/files/styles/article_floated/public/blog/2019/12/keycloak08-1.png?itok=lXBjdXCy)\n\n\n图 8：我们模拟的 HTTP POST 请求。\">\n\n\n结果将是一个有效的 JWT 令牌：\n\n\n```json\n{ \n    \"access_token\": \"eyJhbGciOiJSUzI1NiIsInR5cCIgOiAiSldUIiwia2lkIiAwNjEwLCJpc3MiOiJodHRwO.......wKRTus6PAoHMFlIlYQ75dYiLzzuRMvdXkHl6naLNQ8wYDv4gi7A3eJ163YzXSJf5PmQ\", \n    \"expires_in\": 600, \n    \"refresh_expires_in\": 1800, \n    \"refresh_token\": \"eyJhbGciOiJIUzI1NiIsInR5cC.......IsInZpZXctcHJvZmlsZSJdfX0sInNjb3BlIjoib3BlbmlkIGVtYWlsIHByb2ZpbGUifQ.ePV2aqeDjlg6ih6SA7_x77gT4JYyv7HvK7PLQW-X1mM\", \n    \" token_type\": \"bearer\", \n    \"id_token\": \"eyJhbGciOiJSUz......A_d_LV96VCLBeTJSpqeqpMJYlh4AMJqN6kddtrI4ixZLfwAIj-Qwqn9kzGe-v1-oe80wQXrXzVBG7TJbKm4x5bgCO_B9lnDMrey9 0rvaKKr48K697ug\", \" \n    not-before-policy\": 0, \n    \"session_state\": \"22c8278b-3346-468e-9533-f41f22ed264f\", \n    \"scope\": \"openid email profile\" \n}\n```\n\n\n错误的用户名和密码组合会导致 HTTP 401 响应代码和如下响应正文：\n\n\n```json\n{\n    “error”：“invalid_grant”，\n    “error_description”：“无效的用户凭证”\n}\n```\n\n\n给你。现在您已经配置了一个登录 API，可以很好地与 Keycloak 配合使用。玩得开心！\n\n",
      "properties": {
        "password": "",
        "icon": "",
        "date": "2023-04-28",
        "type": "Post",
        "category": "技术分享",
        "urlname": "api-login-and-jwt-token-generation-using-keycloak",
        "catalog": [
          "archives"
        ],
        "tags": [
          "开发",
          "建站",
          "Java",
          "keycloak",
          "oauth"
        ],
        "summary": "Red Hat SSO (或Keycloak)是领先的Web SSO产品之一，支持SAML 2.0、OpenID Connect和OAuth 2.0等标准，强大之处在于可通过多种方式直接访问Keycloak，包括API调用生成和验证JWT令牌。操作仅限API调用，无需暴露Keycloak的UI给公众。",
        "sort": "",
        "title": "使用 Keycloak 的 API 登录和 JWT 令牌生成",
        "status": "Published",
        "updated": "2023-10-08 14:42:00"
      },
      "catalog": [
        {
          "title": "archives",
          "doc_id": "3a77d2b4-f1df-48cc-bdc1-3192e46e674c"
        }
      ],
      "body": "",
      "realName": "使用 Keycloak 的 API 登录和 JWT 令牌生成",
      "relativePath": "/archives/使用 Keycloak 的 API 登录和 JWT 令牌生成.md"
    },
    {
      "id": "e454567c-abfe-4f35-9826-05e8eff97c1f",
      "doc_id": "e454567c-abfe-4f35-9826-05e8eff97c1f",
      "title": "e454567c-abfe-4f35-9826-05e8eff97c1f",
      "updated": 1696747320000,
      "body_original": "\n物联网的未来发展将会越来越广泛和深入，涉及到包括家庭、工业、医疗、交通等各个方面。随着物联网技术的不断发展，将会给人们带来更加便利和舒适的生活方式。\n\n\n> 🥅 物联网的应用场景非常广泛，例如智能家居、智慧城市、智能医疗、智能制造等。智能家居可以通过物联网技术实现家电之间的互联互通，例如智能门锁、智能音响、智能家电等，用户可以通过手机APP或者语音控制完成对家中设备的操作。智慧城市可以通过物联网技术实现城市基础设施之间的互联互通和数据共享，例如智能交通、智能停车、智能照明等，实现城市的智能化管理和优化。智能医疗可以通过物联网技术实现医疗设备之间的互联互通和数据共享，例如智能健康监测设备、智能手环、智能康复仪等，可以有效地提高医疗设备的使用效率和医疗服务的质量。智能制造可以通过物联网技术实现工业设备之间的互联互通和数据共享，例如智能机床、智能物流、智能质检等，可以提高生产线的自动化程度和生产效率。\n\n\n总之，物联网技术的未来将会更加广阔和深入，它将会引领人们进入一个智能化的世界，为人们带来更加便利和舒适的生活方式，也将会对社会经济发展和生产力的提升起到重要的推动作用。\n\n\n### 物联网开发\n\n\n   物联网开发板块包括：硬件开发、通讯协议、后台平台、前端应用等。硬件开发包括传感器、嵌入式系统、无线通信模块等；通讯协议包括蓝牙、Wi-Fi、ZigBee、LoRa等；后台平台包括云平台、数据中心等；前端应用包括手机APP、智能家居设备、物流追踪等。\n\n\nJava是目前应用比较广泛的开发语言之一，它不仅可以在传统的软件开发领域中发挥重要作用，同时还能在物联网领域中有所作为。Java所提供的多样化的技术可以实现物联网技术的全面应用与发展。\n\n\n例如，在物联网应用领域中，Java EE技术可以为传感器和设备的数据传输、处理、存储及应用提供服务器端的支持，以保证物联网系统的稳定性和效率。Java ME技术则可以专门针对嵌入式设备进行优化，尤其在轻量级设备领域中有着广泛的应用。而Java SE技术则可以用于物联网领域中的通信管理，使得数据的传输变得更为安全与高效。\n\n\n同时，Java还可以通过容器化部署、服务治理等技术来为物联网提供更完善的支持。容器化部署可以将物联网系统进行模块化处理，从而保证系统的灵活性和扩展性。而服务治理则可以为物联网系统提供更加规范化的服务管理，从而能够更好地保证系统的稳定性和安全性。\n\n\n接入第三方云平台，如AWS、Azure等，则可以让Java更好地实现物联网中的云端计算、数据存储和分析等功能，并且还能够提供更加高效和可靠的服务。同时，Java的多线程技术也可以实现物联网中的即时通信和实时数据传输，从而能够更加快速地进行数据同步和处理。\n\n\n总之，Java中众多的技术可以使物联网系统更加智能化、高效化、可扩展，并且还可以实现更好的数据管理、存储和分析等功能。因此，Java在物联网领域中具有广泛的应用前景和发展空间。\n\n\nJava在iot中技术中常用MQTT协议进行交互，除了MQTT之外，Java在IoT中还有许多其他知名的技术栈和框架。其中一个重要的技术是CoAP（Constrained Application Protocol），它是一种轻量级的Web协议，用于将资源暴露给IoT设备，并且可以负责设备的发现和管理。此外，Java还有很多基于RESTful架构的框架，如Jersey和Spring，可以用于快速构建可扩展的Web应用程序，同时提供与IoT设备交互的API。还有一个名为Apache Camel的框架，它提供了一种用于数据传输和转换的规则引擎，可以很好地用于IoT设备之间的消息传递和数据交换。总之，Java在IoT中技术栈非常丰富，包括不同的框架和协议，可以帮助开发人员构建最适合他们需求的应用程序。\n\n",
      "properties": {
        "password": "",
        "icon": "",
        "date": "2023-04-27",
        "type": "Post",
        "category": "学习思考",
        "urlname": "iot-prospect-for-java",
        "catalog": [
          "archives"
        ],
        "tags": [
          "思考",
          "物联网"
        ],
        "summary": "物联网的应用场景非常广泛，例如智能家居、智慧城市、智能医疗、智能制造等。智能家居可以通过物联网技术实现家电之间的互联互通，例如智能门锁、智能音响、智能家电等，用户可以通过手机APP或者语音控制完成对家中设备的操作。智慧城市可以通过物联网技术实现城市基础设施之间的互联互通和数据共享，例如智能交通、智能停车、智能照明等，实现城市的智能化管理和优化。智能医疗可以通过物联网技术实现医疗设备之间的互联互通和数据共享，例如智能健康监测设备、智能手环、智能康复仪等，可以有效地提高医疗设备的使用效率和医疗服务的质量。智能制造可以通过物联网技术实现工业设备之间的互联互通和数据共享，例如智能机床、智能物流、智能质检等，可以提高生产线的自动化程度和生产效率。",
        "sort": "",
        "title": "物联网发展及前景如何？物联网开发有哪些板块？java 如何驱动万物互联？",
        "status": "Published",
        "updated": "2023-10-08 14:42:00"
      },
      "catalog": [
        {
          "title": "archives",
          "doc_id": "e454567c-abfe-4f35-9826-05e8eff97c1f"
        }
      ],
      "body": "",
      "realName": "物联网发展及前景如何？物联网开发有哪些板块？java 如何驱动万物互联？",
      "relativePath": "/archives/物联网发展及前景如何？物联网开发有哪些板块？java 如何驱动万物互联？.md"
    },
    {
      "id": "4f23b21c-584b-4922-a7dc-1fa3e68e609f",
      "doc_id": "4f23b21c-584b-4922-a7dc-1fa3e68e609f",
      "title": "4f23b21c-584b-4922-a7dc-1fa3e68e609f",
      "updated": 1696747320000,
      "body_original": "\nAI 技术的迅速发展和普及，为人类的生活带来了很多便利。在写作领域，也出现了不少基于人工智能技术的写作工具。其中最近推出的一款基于chatGPT技术的写作神器Writely，引起了广泛关注。\n\n\nWritely可以说是一款非常优秀的写作工具，它可以自动生成文章内容，并进行语法纠错。而且，它提供的多种模板和主题，使得写作更加高效实用。\n\n\n这款工具的优势不仅仅在于自动生成文章内容，而且在于生成的内容质量非常高。它可以根据你的写作目的和文章类型，去自动匹配适合的模板和主题，从而为你提供更加具有针对性的文章内容。同时，这款工具还可以进行语法纠错，并提供优化建议，使文章的表达更加准确清晰。\n\n\nWritely的问世，既是技术的一次升级，也是写作工具创新的一次尝试。它为初学者和专业写作者都提供了一种全新的写作方式，使得写作变得更加轻松高效。尤其是对于一些需要大量写作的职业人士，使用Writely工具可以大大提高效率，降低工作量。\n\n\n除了对写作领域的影响外，Writely也预示着未来AI技术将在更多领域有所应用。可以预见，在未来，人工智能将越来越多地渗透到我们的生活和工作中，为我们带来更多便利和可能性。\n\n\n总之，Writely是一款非常优秀的基于chatGPT技术的写作工具，它为写作人士提供了一种全新的写作方式，同时也预示着未来AI技术的不断发展和普及。\n\n\n> 💡 以上内容使用：Writely自动生成\n\n\n## 特性\n\n\n1.🔥 基于 Open AI GPT 模型，带来了全新的智能写作体验。\n\n\n2.✍️ 支持在互联网上的任何编辑器网页上进行写作辅助，有效提高用户的写作效率和质量。\n\n\n3.📖 该产品可以执行查询翻译和阅读辅助功能，大大减少用户的阅读时间并提高理解能力。\n\n\n## 使用方法\n\n\n### 安装\n\n\nChrome 插件：[](https://chrome.google.com/webstore/detail/writely/eocenplmfgoaibmmohkhhocnlkpaecgn)\n\n\n![](https://camo.githubusercontent.com/8a0f1cbc977222a795118f3a124cc31fbadb75ba71c444ae8c9851e98e436d0f/68747470733a2f2f696d672e736869656c64732e696f2f6368726f6d652d7765622d73746f72652f762f656f63656e706c6d66676f6169626d6d6f686b68686f636e6c6b70616563676e)\n\n\nFirefox 扩展：\n\n\n![](https://camo.githubusercontent.com/267163d02f6bd288e0175f2c81f50e8f015f9f982ea1cf8b19d25419ded08dac/68747470733a2f2f696d672e736869656c64732e696f2f616d6f2f762f77726974656c79)\n\n\n### 配置\n\n1. 获取 Open AI API Key。 如果您没有，请在 [https://platform.openai.com/account/api-keys](https://platform.openai.com/account/api-keys) 上进行申请。\n2. 单击插件图标，然后单击“设置”图标。\n\n![](https://user-images.githubusercontent.com/13167934/223933756-b001d01a-899c-42e5-be14-753357a1bba5.png)\n\n1. 进行配置。\n\n![](https://user-images.githubusercontent.com/13167934/224465348-f2e0aaf8-ce7b-48d2-9637-be2a205f317f.png)\n\n1. 将鼠标滑动到任何网页上的单词上，一个“W”图标将出现在鼠标附近，单击以使用。\n\n![](https://user-images.githubusercontent.com/13167934/224236822-eb1cc963-77e5-4820-aa6d-63088989c0cf.gif)\n\n\nGitHub地址 : \n\n\n[bookmark](https://github.com/listener-He/writely)\n\n\n原作者开源地址：\n\n\n[bookmark](https://github.com/anc95/writely)\n\n\n## 如浏览器查询无法安装从这下载\n\n\n[谷歌浏览器插件下载地址](https://ghproxy.com/https://github.com/anc95/writely/releases/download/v0.0.16/writely-chrome-0.0.16.zip)\n\n",
      "properties": {
        "password": "",
        "icon": "",
        "date": "2023-04-24T00:00:00.000+08:00",
        "type": "Post",
        "category": "创作分享",
        "urlname": "notion-ai-writely",
        "catalog": [
          "archives"
        ],
        "tags": [
          "文字",
          "工具",
          "chatgpt",
          "notion"
        ],
        "summary": "作为NiotionAi的重度依赖者，最近观察到一个比NotionAI更强大而且还免费的插件。一款基于chatGPT技术的写作神器Writely。借助自然语言处理技术，Writely可以自动生成、推荐内容，并进行语法纠错优化。同时提供多种模板和主题，让写作更加高效实用。该工具适合初学者和专业写作者使用",
        "sort": "",
        "title": "Notion AI平替 Writely 基于chatGPT免费实现的写作神器",
        "status": "Published",
        "updated": "2023-10-08 14:42:00"
      },
      "catalog": [
        {
          "title": "archives",
          "doc_id": "4f23b21c-584b-4922-a7dc-1fa3e68e609f"
        }
      ],
      "body": "",
      "realName": "Notion AI平替 Writely 基于chatGPT免费实现的写作神器",
      "relativePath": "/archives/Notion AI平替 Writely 基于chatGPT免费实现的写作神器.md"
    },
    {
      "id": "921e862a-78e5-4248-97a1-0fc04c2c558b",
      "doc_id": "921e862a-78e5-4248-97a1-0fc04c2c558b",
      "title": "921e862a-78e5-4248-97a1-0fc04c2c558b",
      "updated": 1696747320000,
      "body_original": "\n## String对象\n\n\n### 常量池\n\n\n**String 对象的两种创建方式：**\n\n\n```java\nString str1 = \"abcd\";//先检查字符串常量池中有没有\"abcd\"，如果字符串常量池中没有，则创建一个，然后 str1 指向字符串常量池中的对象，如果有，则直接将 str1 指向\"abcd\"\"；\nString str2 = new String(\"abcd\");//堆中创建一个新的对象\nString str3 = new String(\"abcd\");//堆中创建一个新的对象\nSystem.out.println(str1==str2);//false\nSystem.out.println(str2==str3);//falseCopy to clipboardErrorCopied\n\n```\n\n\n这两种不同的创建方法是有差别的。\n\n- 第一种方式是在常量池中拿对象；\n- 第二种方式是直接在堆内存空间创建一个新的对象。\n\n记住一点：**只要使用 new 方法，便需要创建新的对象。**\n\n\n再给大家一个图应该更容易理解，图片来源：[https://www.journaldev.com/797/what-is-java-string-pool：](https://www.journaldev.com/797/what-is-java-string-pool%EF%BC%9A)\n\n\n![](https://blog-file.hehouhui.cn/202203222203380.png)\n\n\n**String 类型的常量池比较特殊。它的主要使用方法有两种：**\n\n- 直接使用双引号声明出来的 String 对象会直接存储在常量池中。\n- 如果不是用双引号声明的 String 对象，可以使用 String 提供的 intern 方法。String.intern() 是一个 Native 方法，它的作用是：如果运行时常量池中已经包含一个等于此 String 对象内容的字符串，则返回常量池中该字符串的引用；如果没有，JDK1.7之前（不包含1.7）的处理方式是在常量池中创建与此 String 内容相同的字符串，并返回常量池中创建的字符串的引用，JDK1.7以及之后的处理方式是在常量池中记录此字符串的引用，并返回该引用。\n\n```java\n          String s1 = new String(\"计算机\");\n          String s2 = s1.intern();\n          String s3 = \"计算机\";\n          System.out.println(s2);//计算机\n          System.out.println(s1 == s2);//false，因为一个是堆内存中的 String 对象一个是常量池中的 String 对象，\n          System.out.println(s3 == s2);//true，因为两个都是常量池中的 String 对象Copy to clipboardErrorCopied\n\n```\n\n\n**字符串拼接:**\n\n\n```java\n          String str1 = \"str\";\n          String str2 = \"ing\";\n\n          String str3 = \"str\" + \"ing\";//常量池中的对象\n          String str4 = str1 + str2; //在堆上创建的新的对象\n          String str5 = \"string\";//常量池中的对象\n          System.out.println(str3 == str4);//false\n          System.out.println(str3 == str5);//true\n          System.out.println(str4 == str5);//falseCopy to clipboardErrorCopied\n\n```\n\n\n![](https://blog-file.hehouhui.cn/202203222203646.png)\n\n\n尽量避免多个字符串拼接，因为这样会重新创建对象。如果需要改变字符串的话，可以使用 StringBuilder 或者 StringBuffer。\n\n\n### new String(\"test\"); 创建几个对象?\n\n\n**. 将创建 1 或 2 个字符串。如果池中已存在字符串常量“abc”，则只会在堆空间创建一个字符串常量“abc”。如果池中没有字符串常量“abc”，那么它将首先在池中创建，然后在堆空间中创建，因此将创建总共 2 个字符串对象。**\n\n\n**验证：**\n\n\n```java\n        String s1 = new String(\"abc\");// 堆内存的地址值\n        String s2 = \"abc\";\n        System.out.println(s1 == s2);// 输出 false,因为一个是堆内存，一个是常量池的内存，故两者是不同的。\n        System.out.println(s1.equals(s2));// 输出 trueCopy to clipboardErrorCopied\n\n```\n\n\n**结果：**\n\n\n```text\nfalse\ntrue\n\n```\n\n",
      "properties": {
        "password": "",
        "icon": "",
        "date": "2020-01-22",
        "type": "Post",
        "category": "技术分享",
        "urlname": "45",
        "catalog": [
          "archives"
        ],
        "tags": [
          "Java"
        ],
        "summary": "String对象常量池String 对象的两种创建方式：String str1 = \"abcd\";//先检查字符串常量池中有没有\"abcd\"，如果字符串常量池中",
        "sort": "",
        "title": "Java基础-String",
        "status": "Published",
        "updated": "2023-10-08 14:42:00"
      },
      "catalog": [
        {
          "title": "archives",
          "doc_id": "921e862a-78e5-4248-97a1-0fc04c2c558b"
        }
      ],
      "body": "",
      "realName": "Java基础-String",
      "relativePath": "/archives/Java基础-String.md"
    },
    {
      "id": "f9af0c25-a672-4cac-bc8e-43bb190216fb",
      "doc_id": "f9af0c25-a672-4cac-bc8e-43bb190216fb",
      "title": "f9af0c25-a672-4cac-bc8e-43bb190216fb",
      "updated": 1696747320000,
      "body_original": "\n## 对象，类\n\n\n### 类加载\n\n\n一个类的完整生命周期如下：\n\n\n![](https://blog-file.hehouhui.cn/202203222121164.png)\n\n\nClass 文件需要加载到虚拟机中之后才能运行和使用，那么虚拟机是如何加载这些 Class 文件呢？\n\n\n系统加载 Class 类型的文件主要三步:**加载->连接->初始化**。连接过程又可分为三步:**验证->准备->解析**。\n\n\n![](https://blog-file.hehouhui.cn/202203222203871.png)\n\n\n### 加载\n\n\n类加载过程的第一步，主要完成下面3件事情：\n\n1. 通过全类名获取定义此类的二进制字节流\n2. 将字节流所代表的静态存储结构转换为方法区的运行时数据结构\n3. 在内存中生成一个代表该类的 Class 对象,作为方法区这些数据的访问入口\n\n虚拟机规范上面这3点并不具体，因此是非常灵活的。比如：\"通过全类名获取定义此类的二进制字节流\" 并没有指明具体从哪里获取、怎样获取。比如：比较常见的就是从 ZIP 包中读取（日后出现的JAR、EAR、WAR格式的基础）、其他文件生成（典型应用就是JSP）等等。\n\n\n**一个非数组类的加载阶段（加载阶段获取类的二进制字节流的动作）是可控性最强的阶段，这一步我们可以去完成还可以自定义类加载器去控制字节流的获取方式（重写一个类加载器的** **`loadClass()`** **方法）。数组类型不通过类加载器创建，它由 Java 虚拟机直接创建。**\n\n\n类加载器、双亲委派模型也是非常重要的知识点，这部分内容会在后面的文章中单独介绍到。\n\n\n加载阶段和连接阶段的部分内容是交叉进行的，加载阶段尚未结束，连接阶段可能就已经开始了。\n\n\n### 验证\n\n\n![](https://blog-file.hehouhui.cn/202203222203084.png)\n\n\n### 准备\n\n\n**准备阶段是正式为类变量分配内存并设置类变量初始值的阶段**，这些内存都将在方法区中分配。对于该阶段有以下几点需要注意：\n\n1. 这时候进行内存分配的仅包括类变量（static），而不包括实例变量，实例变量会在对象实例化时随着对象一块分配在 Java 堆中。\n2. 这里所设置的初始值\"通常情况\"下是数据类型默认的零值（如0、0L、null、false等），比如我们定义了`public static int value=111` ，那么 value 变量在准备阶段的初始值就是 0 而不是111（初始化阶段才会赋值）。特殊情况：比如给 value 变量加上了 fianl 关键字`public static final int value=111` ，那么准备阶段 value 的值就被赋值为 111。\n\n**基本数据类型的零值：**\n\n\n![](https://blog-file.hehouhui.cn/202203222203304.png)\n\n\n### 解析\n\n\n解析阶段是虚拟机将常量池内的符号引用替换为直接引用的过程。解析动作主要针对类或接口、字段、类方法、接口方法、方法类型、方法句柄和调用限定符7类符号引用进行。\n\n\n符号引用就是一组符号来描述目标，可以是任何字面量。**直接引用**就是直接指向目标的指针、相对偏移量或一个间接定位到目标的句柄。在程序实际运行时，只有符号引用是不够的，举个例子：在程序执行方法时，系统需要明确知道这个方法所在的位置。Java 虚拟机为每个类都准备了一张方法表来存放类中所有的方法。当需要调用一个类的方法的时候，只要知道这个方法在方发表中的偏移量就可以直接调用该方法了。通过解析操作符号引用就可以直接转变为目标方法在类中方法表的位置，从而使得方法可以被调用。\n\n\n综上，解析阶段是虚拟机将常量池内的符号引用替换为直接引用的过程，也就是得到类或者字段、方法在内存中的指针或者偏移量。\n\n\n### 初始化\n\n\n初始化是类加载的最后一步，也是真正执行类中定义的 Java 程序代码(字节码)，初始化阶段是执行初始化方法  `()`方法的过程。\n\n\n对于`（）` 方法的调用，虚拟机会自己确保其在多线程环境中的安全性。因为 `（）` 方法是带锁线程安全，所以在多线程环境下进行类初始化的话可能会引起死锁，并且这种死锁很难被发现。\n\n\n对于初始化阶段，虚拟机严格规范了有且只有5种情况下，必须对类进行初始化(只有主动去使用类才会初始化类)：\n\n1. 当遇到 new 、 getstatic、putstatic或invokestatic 这4条直接码指令时，比如 new 一个类，读取一个静态字段(未被 final 修饰)、或调用一个类的静态方法时。\n\t- 当jvm执行new指令时会初始化类。即当程序创建一个类的实例对象。\n\t- 当jvm执行getstatic指令时会初始化类。即程序访问类的静态变量(不是静态常量，常量会被加载到运行时常量池)。\n\t- 当jvm执行putstatic指令时会初始化类。即程序给类的静态变量赋值。\n\t- 当jvm执行invokestatic指令时会初始化类。即程序调用类的静态方法。\n2. 使用 `java.lang.reflect` 包的方法对类进行反射调用时如Class.forname(\"...\"),newInstance()等等。 ，如果类没初始化，需要触发其初始化。\n3. 初始化一个类，如果其父类还未初始化，则先触发该父类的初始化。\n4. 当虚拟机启动时，用户需要定义一个要执行的主类 (包含 main 方法的那个类)，虚拟机会先初始化这个类。\n5. MethodHandle和VarHandle可以看作是轻量级的反射调用机制，而要想使用这2个调用， 就必须先使用findStaticVarHandle来初始化要调用的类。\n6. **「补充，来自**[**issue745**](https://github.com/Snailclimb/JavaGuide/issues/745)**」** 当一个接口中定义了JDK8新加入的默认方法（被default关键字修饰的接口方法）时，如果有这个接口的实现类发生了初始化，那该接口要在其之前被初始化。\n\n### 卸载\n\n\n> 卸载这部分内容来自 issue#662由 guang19 补充完善。\n\n\n卸载类即该类的Class对象被GC。\n\n\n卸载类需要满足3个要求:\n\n1. 该类的所有的实例对象都已被GC，也就是说堆不存在该类的实例对象。\n2. 该类没有在其他任何地方被引用\n3. 该类的类加载器的实例已被GC\n\n所以，在JVM生命周期类，由jvm自带的类加载器加载的类是不会被卸载的。但是由我们自定义的类加载器加载的类是可能被卸载的。\n\n\n只要想通一点就好了，jdk自带的BootstrapClassLoader,ExtClassLoader,AppClassLoader负责加载jdk提供的类，所以它们(类加载器的实例)肯定不会被回收。而我们自定义的类加载器的实例是可以被回收的，所以使用我们自定义加载器加载的类是可以被卸载掉的。\n\n\n### 对象的创建\n\n\n下图便是 Java 对象的创建过程，我建议最好是能默写出来，并且要掌握每一步在做什么。\n\n\n![](https://blog-file.hehouhui.cn/202203222203447.png)\n\n\n### 类加载检查\n\n\n> 虚拟机遇到一条 new 指令时，首先将去检查这个指令的参数是否能在常量池中定位到这个类的符号引用，并且检查这个符号引用代表的类是否已被加载过、解析和初始化过。如果没有，那必须先执行相应的类加载过程。\n\n\n### 分配内存\n\n\n> 在类加载检查通过后，接下来虚拟机将为新生对象分配内存。对象所需的内存大小在类加载完成后便可确定，为对象分配空间的任务等同于把一块确定大小的内存从 Java 堆中划分出来。分配方式有 “指针碰撞” 和 “空闲列表” 两种，选择哪种分配方式由 Java 堆是否规整决定，而 Java 堆是否规整又由所采用的垃圾收集器是否带有压缩整理功能决定。\n\n\n**内存分配的两种方式：（补充内容，需要掌握）**\n\n\n选择以上两种方式中的哪一种，取决于 Java 堆内存是否规整。而 Java 堆内存是否规整，取决于 GC 收集器的算法是\"标记-清除\"，还是\"标记-整理\"（也称作\"标记-压缩\"），值得注意的是，复制算法内存也是规整的\n\n\n![](https://blog-file.hehouhui.cn/202203222203619.png)\n\n\n**内存分配并发问题（补充内容，需要掌握）**\n\n\n在创建对象的时候有一个很重要的问题，就是线程安全，因为在实际开发过程中，创建对象是很频繁的事情，作为虚拟机来说，必须要保证线程是安全的，通常来讲，虚拟机采用两种方式来保证线程安全：\n\n- **CAS+失败重试：** CAS 是乐观锁的一种实现方式。所谓乐观锁就是，每次不加锁而是假设没有冲突而去完成某项操作，如果因为冲突失败就重试，直到成功为止。**虚拟机采用 CAS 配上失败重试的方式保证更新操作的原子性。**\n- **TLAB：** 为每一个线程预先在 Eden 区分配一块儿内存，JVM 在给线程中的对象分配内存时，首先在 TLAB 分配，当对象大于 TLAB 中的剩余内存或 TLAB 的内存已用尽时，再采用上述的 CAS 进行内存分配\n\n### 初始化零值\n\n\n> 内存分配完成后，虚拟机需要将分配到的内存空间都初始化为零值（不包括对象头），这一步操作保证了对象的实例字段在 Java 代码中可以不赋初始值就直接使用，程序能访问到这些字段的数据类型所对应的零值。\n\n\n### 设置对象头\n\n\n> 初始化零值完成之后，虚拟机要对对象进行必要的设置，例如这个对象是哪个类的实例、如何才能找到类的元数据信息、对象的哈希码、对象的 GC 分代年龄等信息。 这些信息存放在对象头中。 另外，根据虚拟机当前运行状态的不同，如是否启用偏向锁等，对象头会有不同的设置方式。\n\n\n### 执行init方法\n\n\n在上面工作都完成之后，从虚拟机的视角来看，一个新的对象已经产生了，但从 Java 程序的视角来看，对象创建才刚开始，`方法还没有执行，所有的字段都还为零。所以一般来说，执行 new 指令之后会接着执行` 方法，把对象按照程序员的意愿进行初始化，这样一个真正可用的对象才算完全产生出来。\n\n\n### 对象的内存区域\n\n\n在 Hotspot 虚拟机中，对象在内存中的布局可以分为 3 块区域：**对象头**、**实例数据**和**对齐填充**。\n\n\n**Hotspot 虚拟机的对象头包括两部分信息**，**第一部分用于存储对象自身的运行时数据**（哈希码、GC 分代年龄、锁状态标志等等），**另一部分是类型指针**，即对象指向它的类元数据的指针，虚拟机通过这个指针来确定这个对象是那个类的实例。\n\n\n**实例数据部分是对象真正存储的有效信息**，也是在程序中所定义的各种类型的字段内容。\n\n\n**对齐填充部分不是必然存在的，也没有什么特别的含义，仅仅起占位作用。** 因为 Hotspot 虚拟机的自动内存管理系统要求对象起始地址必须是 8 字节的整数倍，换句话说就是对象的大小必须是 8 字节的整数倍。而对象头部分正好是 8 字节的倍数（1 倍或 2 倍），因此，当对象实例数据部分没有对齐时，就需要通过对齐填充来补全。\n\n\n### 对象的访问定位\n\n\n建立对象就是为了使用对象，我们的 Java 程序通过栈上的 reference 数据来操作堆上的具体对象。对象的访问方式由虚拟机实现而定，目前主流的访问方式有**①使用句柄**和**②直接指针**两种：\n\n1. **句柄：** 如果使用句柄的话，那么 Java 堆中将会划分出一块内存来作为句柄池，reference 中存储的就是对象的句柄地址，而句柄中包含了对象实例数据与类型数据各自的具体地址信息；\n\n\t![](https://blog-file.hehouhui.cn/202203222203076.png)\n\n2. **直接指针：** 如果使用直接指针访问，那么 Java 堆对象的布局中就必须考虑如何放置访问类型数据的相关信息，而 reference 中存储的直接就是对象的地址。\n\n![](https://blog-file.hehouhui.cn/202203222155695.png)\n\n\n**这两种对象访问方式各有优势。使用句柄来访问的最大好处是 reference 中存储的是稳定的句柄地址，在对象被移动时只会改变句柄中的实例数据指针，而 reference 本身不需要修改。使用直接指针访问方式最大的好处就是速度快，它节省了一次指针定位的时间开销。**\n\n",
      "properties": {
        "password": "",
        "icon": "",
        "date": "2020-01-26",
        "type": "Post",
        "category": "技术分享",
        "urlname": "43",
        "catalog": [
          "archives"
        ],
        "tags": [
          "Java"
        ],
        "summary": "对象，类类加载一个类的完整生命周期如下：Class 文件需要加载到虚拟机中之后才能运行和使用，那么虚拟机是如何加载这些 Class 文件呢？系统加载 Class 类型的文件主要三步:加载->连接",
        "sort": "",
        "title": "Java基础-class",
        "status": "Published",
        "updated": "2023-10-08 14:42:00"
      },
      "catalog": [
        {
          "title": "archives",
          "doc_id": "f9af0c25-a672-4cac-bc8e-43bb190216fb"
        }
      ],
      "body": "",
      "realName": "Java基础-class",
      "relativePath": "/archives/Java基础-class.md"
    },
    {
      "id": "54c4aad1-0a7a-4a4b-9906-1f54a3810362",
      "doc_id": "54c4aad1-0a7a-4a4b-9906-1f54a3810362",
      "title": "54c4aad1-0a7a-4a4b-9906-1f54a3810362",
      "updated": 1696747320000,
      "body_original": "\n## 线程\n\n\n**线程**与进程相似，但线程是一个比进程更小的执行单位。一个进程在其执行的过程中可以产生多个线程。与进程不同的是同类的多个线程共享同一块内存空间和一组系统资源，所以系统在产生一个线程，或是在各个线程之间作切换工作时，负担要比进程小得多，也正因为如此，线程也被称为轻量级进程。\n\n\n**程序**是含有指令和数据的文件，被存储在磁盘或其他的数据存储设备中，也就是说程序是静态的代码。\n\n\n**进程**是程序的一次执行过程，是系统运行程序的基本单位，因此进程是动态的。系统运行一个程序即是一个进程从创建，运行到消亡的过程。简单来说，一个进程就是一个执行中的程序，它在计算机中一个指令接着一个指令地执行着，同时，每个进程还占有某些系统资源如 CPU 时间，内存空间，文件，输入输出设备的使用权等等。换句话说，当程序在执行时，将会被操作系统载入内存中。 线程是进程划分成的更小的运行单位。线程和进程最大的不同在于基本上各进程是独立的，而各线程则不一定，因为同一进程中的线程极有可能会相互影响。从另一角度来说，进程属于操作系统的范畴，主要是同一段时间内，可以同时执行一个以上的程序，而线程则是在同一程序内几乎同时执行一个以上的程序段。\n\n\n### 线程的生命周期\n\n\nJava 线程在运行的生命周期中的指定时刻只可能处于下面 6 种不同状态的其中一个状态（图源《Java 并发编程艺术》4.1.4 节）。\n\n\n![](https://blog-file.hehouhui.cn/202203222203748.png)\n\n\n线程在生命周期中并不是固定处于某一个状态而是随着代码的执行在不同状态之间切换。Java 线程状态变迁如下图所示（图源《Java 并发编程艺术》4.1.4 节）：\n\n\n![](https://blog-file.hehouhui.cn/202203222203017.png)\n\n\n> 订正(来自issue736)：原图中 wait到 runnable状态的转换中，join实际上是Thread类的方法，但这里写成了Object。\n\n\n由上图可以看出：线程创建之后它将处于 **NEW（新建）** 状态，调用 `start()` 方法后开始运行，线程这时候处于 **READY（可运行）** 状态。可运行状态的线程获得了 CPU 时间片（timeslice）后就处于 **RUNNING（运行）** 状态。\n\n\n> 操作系统隐藏 Java 虚拟机（JVM）中的 READY 和 RUNNING 状态，它只能看到 RUNNABLE 状态（图源：HowToDoInJava：Java Thread Life Cycle and Thread States），所以 Java 系统一般将这两个状态统称为 RUNNABLE（运行中） 状态 。\n\n\n![](https://blog-file.hehouhui.cn/202203222203261.png)\n\n\n当线程执行 `wait()`方法之后，线程进入 **WAITING（等待）** 状态。进入等待状态的线程需要依靠其他线程的通知才能够返回到运行状态，而 **TIME_WAITING(超时等待)** 状态相当于在等待状态的基础上增加了超时限制，比如通过 `sleep（long millis）`方法或 `wait（long millis）`方法可以将 Java 线程置于 TIMED WAITING 状态。当超时时间到达后 Java 线程将会返回到 RUNNABLE 状态。当线程调用同步方法时，在没有获取到锁的情况下，线程将会进入到 **BLOCKED（阻塞）** 状态。线程在执行 Runnable 的`run()`方法之后将会进入到 **TERMINATED（终止）** 状态。\n\n\n## 反射\n\n\nAVA 反射机制是在运行状态中，对于任意一个类，都能够知道这个类的所有属性和方法；对于任意一个对象，都能够调用它的任意一个方法和属性；这种动态获取的信息以及动态调用对象的方法的功能称为 java 语言的反射机制。\n\n\n### 静态编译和动态编译\n\n- **静态编译：** 在编译时确定类型，绑定对象\n- **动态编译：** 运行时确定类型，绑定对象\n\n### 优缺点\n\n- **优点：** 运行期类型的判断，动态加载类，提高代码灵活度。\n- **缺点：** 1,性能瓶颈：反射相当于一系列解释操作，通知 JVM 要做的事情，性能比直接的 java 代码要慢很多。2,安全问题，让我们可以动态操作改变类的属性同时也增加了类的安全隐患。\n\n### 应用场景\n\n\n**反射是框架设计的灵魂。**\n\n\n在我们平时的项目开发过程中，基本上很少会直接使用到反射机制，但这不能说明反射机制没有用，实际上有很多设计、开发都与反射机制有关，例如模块化的开发，通过反射去调用对应的字节码；动态代理设计模式也采用了反射机制，还有我们日常使用的 Spring／Hibernate 等框架也大量使用到了反射机制。\n\n\n举例：\n\n1. 我们在使用 JDBC 连接数据库时使用 `Class.forName()`通过反射加载数据库的驱动程序；\n2. Spring 框架的 IOC（动态加载管理 Bean）创建对象以及 AOP（动态代理）功能都和反射有联系；\n3. 动态配置实例的属性；\n4. ......\n\n## 异常\n\n\n**结构图**\n\n\n![](https://blog-file.hehouhui.cn/202203222203465.png)\n\n\n在 Java 中，所有的异常都有一个共同的祖先 `java.lang` 包中的 `Throwable` 类。`Throwable` 类有两个重要的子类 `Exception`（异常）和 `Error`（错误）。`Exception` 能被程序本身处理(`try-catch`)， `Error` 是无法处理的(只能尽量避免)。\n\n\n`Exception` 和 `Error` 二者都是 Java 异常处理的重要子类，各自都包含大量子类。\n\n- **`Exception`** :程序本身可以处理的异常，可以通过 `catch` 来进行捕获。`Exception` 又可以分为 受检查异常(必须处理) 和 不受检查异常(可以不处理)。\n- **`Error`** ：`Error` 属于程序无法处理的错误 ，我们没办法通过 `catch` 来进行捕获 。例如，Java 虚拟机运行错误（`Virtual MachineError`）、虚拟机内存不够错误(`OutOfMemoryError`)、类定义错误（`NoClassDefFoundError`）等 。这些异常发生时，Java 虚拟机（JVM）一般会选择线程终止。\n\n### 受检查异常\n\n\nJava 代码在编译过程中，如果受检查异常没有被 `catch`/`throw` 处理的话，就没办法通过编译 。比如下面这段 IO 操作的代码。\n\n\n![](https://blog-file.hehouhui.cn/202203222203783.png)\n\n\n除了`RuntimeException`及其子类以外，其他的`Exception`类及其子类都属于受检查异常 。常见的受检查异常有： IO 相关的异常、`ClassNotFoundException` 、`SQLException`...。\n\n\n### 不受检查异常\n\n\nJava 代码在编译过程中 ，我们即使不处理不受检查异常也可以正常通过编译。\n\n\n`RuntimeException` 及其子类都统称为非受检查异常，例如：`NullPointExecrption`、`NumberFormatException`（字符串转换为数字）、`ArrayIndexOutOfBoundsException`（数组越界）、`ClassCastException`（类型转换错误）、`ArithmeticException`（算术错误）等。\n\n\n## IO\n\n\n### IO 流分为几种\n\n- 按照流的流向分，可以分为输入流和输出流；\n- 按照操作单元划分，可以划分为字节流和字符流；\n- 按照流的角色划分为节点流和处理流。\n\nJava Io 流共涉及 40 多个类，这些类看上去很杂乱，但实际上很有规则，而且彼此之间存在非常紧密的联系， Java I0 流的 40 多个类都是从如下 4 个抽象类基类中派生出来的。\n\n- InputStream/Reader: 所有的输入流的基类，前者是字节输入流，后者是字符输入流。\n- OutputStream/Writer: 所有输出流的基类，前者是字节输出流，后者是字符输出流。\n\n按操作方式分类结构图：\n\n\n![](https://blog-file.hehouhui.cn/202203222203257.jpeg)\n\n\n按操作对象分类结构图：\n\n\n![](https://blog-file.hehouhui.cn/202203222203094.jpeg)\n\n\n### 为什么要字符流\n\n\n问题本质想问：**不管是文件读写还是网络发送接收，信息的最小存储单元都是字节，那为什么 I/O 流操作要分为字节流操作和字符流操作呢？**\n\n\n回答：字符流是由 Java 虚拟机将字节转换得到的，问题就出在这个过程还算是非常耗时，并且，如果我们不知道编码类型就很容易出现乱码问题。所以， I/O 流就干脆提供了一个直接操作字符的接口，方便我们平时对字符进行流操作。如果音频文件、图片等媒体文件用字节流比较好，如果涉及到字符的话使用字符流比较好。\n\n\n### BIO,NIO,AIO\n\n- **BIO (Blocking I/O):** 同步阻塞 I/O 模式，数据的读取写入必须阻塞在一个线程内等待其完成。在活动连接数不是特别高（小于单机 1000）的情况下，这种模型是比较不错的，可以让每一个连接专注于自己的 I/O 并且编程模型简单，也不用过多考虑系统的过载、限流等问题。线程池本身就是一个天然的漏斗，可以缓冲一些系统处理不了的连接或请求。但是，当面对十万甚至百万级连接的时候，传统的 BIO 模型是无能为力的。因此，我们需要一种更高效的 I/O 处理模型来应对更高的并发量。\n- **NIO (Non-blocking/New I/O):** NIO 是一种同步非阻塞的 I/O 模型，在 Java 1.4 中引入了 NIO 框架，对应 java.nio 包，提供了 Channel , Selector，Buffer 等抽象。NIO 中的 N 可以理解为 Non-blocking，不单纯是 New。它支持面向缓冲的，基于通道的 I/O 操作方法。 NIO 提供了与传统 BIO 模型中的 `Socket` 和 `ServerSocket` 相对应的 `SocketChannel` 和 `ServerSocketChannel` 两种不同的套接字通道实现,两种通道都支持阻塞和非阻塞两种模式。阻塞模式使用就像传统中的支持一样，比较简单，但是性能和可靠性都不好；非阻塞模式正好与之相反。对于低负载、低并发的应用程序，可以使用同步阻塞 I/O 来提升开发速率和更好的维护性；对于高负载、高并发的（网络）应用，应使用 NIO 的非阻塞模式来开发\n- **AIO (Asynchronous I/O):** AIO 也就是 NIO 2。在 Java 7 中引入了 NIO 的改进版 NIO 2,它是异步非阻塞的 IO 模型。异步 IO 是基于事件和回调机制实现的，也就是应用操作之后会直接返回，不会堵塞在那里，当后台处理完成，操作系统会通知相应的线程进行后续的操作。AIO 是异步 IO 的缩写，虽然 NIO 在网络操作中，提供了非阻塞的方法，但是 NIO 的 IO 行为还是同步的。对于 NIO 来说，我们的业务线程是在 IO 操作准备好时，得到通知，接着就由这个线程自行进行 IO 操作，IO 操作本身是同步的。查阅网上相关资料，我发现就目前来说 AIO 的应用还不是很广泛，Netty 之前也尝试使用过 AIO，不过又放弃了。\n\n## 并发\n\n\n### synchronized\n\n\n### \n\n\n**`synchronized`** **关键字解决的是多个线程之间访问资源的同步性，****`synchronized`****关键字可以保证被它修饰的方法或者代码块在任意时刻只能有一个线程执行。**\n\n\n另外，在 Java 早期版本中，`synchronized` 属于 **重量级锁**，效率低下。\n\n\n**为什么呢？**\n\n\n因为监视器锁（monitor）是依赖于底层的操作系统的 `Mutex Lock` 来实现的，Java 的线程是映射到操作系统的原生线程之上的。如果要挂起或者唤醒一个线程，都需要操作系统帮忙完成，而操作系统实现线程之间的切换时需要从用户态转换到内核态，这个状态之间的转换需要相对比较长的时间，时间成本相对较高。\n\n\n庆幸的是在 Java 6 之后 Java 官方对从 JVM 层面对 `synchronized` 较大优化，所以现在的 `synchronized` 锁效率也优化得很不错了。JDK1.6 对锁的实现引入了大量的优化，如自旋锁、适应性自旋锁、锁消除、锁粗化、偏向锁、轻量级锁等技术来减少锁操作的开销。\n\n\n所以，你会发现目前的话，不论是各种开源框架还是 JDK 源码都大量使用了 `synchronized` 关键字。\n\n\n### 使用方式\n\n\n**synchronized 关键字最主要的三种使用方式：**\n\n\n**1.修饰实例方法:** 作用于当前对象实例加锁，进入同步代码前要获得 **当前对象实例的锁**\n\n\n```text\nsynchronized void method() {\n  //业务代码\n}Copy to clipboardErrorCopied\n\n```\n\n\n**2.修饰静态方法:** 也就是给当前类加锁，会作用于类的所有对象实例 ，进入同步代码前要获得 **当前 class 的锁**。因为静态成员不属于任何一个实例对象，是类成员（ _static 表明这是该类的一个静态资源，不管 new 了多少个对象，只有一份_）。所以，如果一个线程 A 调用一个实例对象的非静态 `synchronized` 方法，而线程 B 需要调用这个实例对象所属类的静态 `synchronized` 方法，是允许的，不会发生互斥现象，**因为访问静态** **`synchronized`** **方法占用的锁是当前类的锁，而访问非静态** **`synchronized`** **方法占用的锁是当前实例对象锁**。\n\n\n```text\nsynchronized void staic method() {\n  //业务代码\n}Copy to clipboardErrorCopied\n\n```\n\n\n**3.修饰代码块** ：指定加锁对象，对给定对象/类加锁。`synchronized(this|object)` 表示进入同步代码库前要获得**给定对象的锁**。`synchronized(类.class)` 表示进入同步代码前要获得 **当前 class 的锁**\n\n\n```text\nsynchronized(this) {\n  //业务代码\n}Copy to clipboardErrorCopied\n\n```\n\n\n**总结：**\n\n- `synchronized` 关键字加到 `static` 静态方法和 `synchronized(class)` 代码块上都是是给 Class 类上锁。\n- `synchronized` 关键字加到实例方法上是给对象实例上锁。\n- 尽量不要使用 `synchronized(String a)` 因为 JVM 中，字符串常量池具有缓存功能！\n\n下面我以一个常见的面试题为例讲解一下 `synchronized` 关键字的具体使用。\n\n\n面试中面试官经常会说：“单例模式了解吗？来给我手写一下！给我解释一下双重检验锁方式实现单例模式的原理呗！”\n\n\n**双重校验锁实现对象单例（线程安全）**\n\n\n```text\npublic class Singleton {\n\n    private volatile static Singleton uniqueInstance;\n\n    private Singleton() {\n    }\n\n    public  static Singleton getUniqueInstance() {\n       //先判断对象是否已经实例过，没有实例化过才进入加锁代码\n        if (uniqueInstance == null) {\n            //类对象加锁\n            synchronized (Singleton.class) {\n                if (uniqueInstance == null) {\n                    uniqueInstance = new Singleton();\n                }\n            }\n        }\n        return uniqueInstance;\n    }\n}Copy to clipboardErrorCopied\n\n```\n\n\n另外，需要注意 `uniqueInstance` 采用 `volatile` 关键字修饰也是很有必要。\n\n\n`uniqueInstance` 采用 `volatile` 关键字修饰也是很有必要的， `uniqueInstance = new Singleton();` 这段代码其实是分为三步执行：\n\n1. 为 `uniqueInstance` 分配内存空间\n2. 初始化 `uniqueInstance`\n3. 将 `uniqueInstance` 指向分配的内存地址\n\n但是由于 JVM 具有指令重排的特性，执行顺序有可能变成 1->3->2。指令重排在单线程环境下不会出现问题，但是在多线程环境下会导致一个线程获得还没有初始化的实例。例如，线程 T1 执行了 1 和 3，此时 T2 调用 `getUniqueInstance`() 后发现 `uniqueInstance` 不为空，因此返回 `uniqueInstance`，但此时 `uniqueInstance` 还未被初始化。\n\n\n使用 `volatile` 可以禁止 JVM 的指令重排，保证在多线程环境下也能正常运行。\n\n\n### 底层原理\n\n\n**synchronized 关键字底层原理属于 JVM 层面。**\n\n\n**`代码块`**\n\n\n```text\npublic class SynchronizedDemo {\n    public void method() {\n        synchronized (this) {\n            System.out.println(\"synchronized 代码块\");\n        }\n    }\n}\n\n```\n\n\n通过 JDK 自带的 `javap` 命令查看 `SynchronizedDemo` 类的相关字节码信息：首先切换到类的对应目录执行 `javac SynchronizedDemo.java` 命令生成编译后的 .class 文件，然后执行`javap -c -s -v -l SynchronizedDemo.class`。\n\n\n![](https://blog-file.hehouhui.cn/202203222203508.png)\n\n\n从上面我们可以看出：\n\n\n**`synchronized`** **同步语句块的实现使用的是** **`monitorenter`** **和** **`monitorexit`** **指令，其中** **`monitorenter`** **指令指向同步代码块的开始位置，****`monitorexit`** **指令则指明同步代码块的结束位置。**\n\n\n当执行 `monitorenter` 指令时，线程试图获取锁也就是获取 **对象监视器** **`monitor`** 的持有权。\n\n\n> 在 Java 虚拟机(HotSpot)中，Monitor 是基于 C++实现的，由ObjectMonitor实现的。每个对象中都内置了一个 ObjectMonitor对象。\n\n\n\t另外，**`wait/notify`****等方法也依赖于****`monitor`****对象，这就是为什么只有在同步的块或者方法中才能调用****`wait/notify`****等方法，否则会抛出****`java.lang.IllegalMonitorStateException`****的异常的原因。**\n\n\n在执行`monitorenter`时，会尝试获取对象的锁，如果锁的计数器为 0 则表示锁可以被获取，获取后将锁计数器设为 1 也就是加 1。\n\n\n在执行 `monitorexit` 指令后，将锁计数器设为 0，表明锁被释放。如果获取对象锁失败，那当前线程就要阻塞等待，直到锁被另外一个线程释放为止。\n\n\n**修饰方法**\n\n\n```text\npublic class SynchronizedDemo2 {\n    public synchronized void method() {\n        System.out.println(\"synchronized 方法\");\n    }\n}\nCopy to clipboardErrorCopied\n\n```\n\n\n![](https://blog-file.hehouhui.cn/202203222128189.png)\n\n\n`synchronized` 修饰的方法并没有 `monitorenter` 指令和 `monitorexit` 指令，取得代之的确实是 `ACC_SYNCHRONIZED` 标识，该标识指明了该方法是一个同步方法。JVM 通过该 `ACC_SYNCHRONIZED` 访问标志来辨别一个方法是否声明为同步方法，从而执行相应的同步调用。\n\n\n### 锁\n\n\n### 乐观锁和悲观锁\n\n\n> 乐观锁对应于生活中乐观的人总是想着事情往好的方向发展，悲观锁对应于生活中悲观的人总是想着事情往坏的方向发展。这两种人各有优缺点，不能不以场景而定说一种人好于另外一种人。\n\n\n**悲观锁**\n\n\n> 总是假设最坏的情况，每次去拿数据的时候都认为别人会修改，所以每次在拿数据的时候都会上锁，这样别人想拿这个数据就会阻塞直到它拿到锁（共享资源每次只给一个线程使用，其它线程阻塞，用完后再把资源转让给其它线程）。传统的关系型数据库里边就用到了很多这种锁机制，比如行锁，表锁等，读锁，写锁等，都是在做操作之前先上锁。Java中synchronized和ReentrantLock等独占锁就是悲观锁思想的实现。\n\n\n**乐观锁**\n\n\n> 总是假设最好的情况，每次去拿数据的时候都认为别人不会修改，所以不会上锁，但是在更新的时候会判断一下在此期间别人有没有去更新这个数据，可以使用版本号机制和CAS算法实现。乐观锁适用于多读的应用类型，这样可以提高吞吐量，像数据库提供的类似于write_condition机制，其实都是提供的乐观锁。在Java中java.util.concurrent.atomic包下面的原子变量类就是使用了乐观锁的一种实现方式CAS实现的。\n\n\n### 使用场景\n\n\n从上面对两种锁的介绍，我们知道两种锁各有优缺点，不可认为一种好于另一种，像**乐观锁适用于写比较少的情况下（多读场景）**，即冲突真的很少发生的时候，这样可以省去了锁的开销，加大了系统的整个吞吐量。但如果是多写的情况，一般会经常产生冲突，这就会导致上层应用会不断的进行retry，这样反倒是降低了性能，所以**一般多写的场景下用悲观锁就比较合适。**\n\n\n### 乐观锁\n\n\n### 版本号机制\n\n\n一般是在数据表中加上一个数据版本号version字段，表示数据被修改的次数，当数据被修改时，version值会加一。当线程A要更新数据值时，在读取数据的同时也会读取version值，在提交更新时，若刚才读取到的version值为当前数据库中的version值相等时才更新，否则重试更新操作，直到更新成功。\n\n\n**举一个简单的例子：** 假设数据库中帐户信息表中有一个 version 字段，当前值为 1 ；而当前帐户余额字段（ balance ）为 $100 。\n\n1. 操作员 A 此时将其读出（ version=1 ），并从其帐户余额中扣除 $50（ $100-$50 ）。\n2. 在操作员 A 操作的过程中，操作员B 也读入此用户信息（ version=1 ），并从其帐户余额中扣除 $20 （ $100-$20 ）。\n3. 操作员 A 完成了修改工作，将数据版本号（ version=1 ），连同帐户扣除后余额（ balance=$50 ），提交至数据库更新，此时由于提交数据版本等于数据库记录当前版本，数据被更新，数据库记录 version 更新为 2 。\n4. 操作员 B 完成了操作，也将版本号（ version=1 ）试图向数据库提交数据（ balance=$80 ），但此时比对数据库记录版本时发现，操作员 B 提交的数据版本号为 1 ，数据库记录当前版本也为 2 ，不满足 “ 提交版本必须等于当前版本才能执行更新 “ 的乐观锁策略，因此，操作员 B 的提交被驳回。\n\n这样，就避免了操作员 B 用基于 version=1 的旧数据修改的结果覆盖操作员A 的操作结果的可能\n\n\n### CAS算法\n\n\n即**compare and swap（比较与交换）**，是一种有名的**无锁算法**。无锁编程，即不使用锁的情况下实现多线程之间的变量同步，也就是在没有线程被阻塞的情况下实现变量的同步，所以也叫非阻塞同步（Non-blocking Synchronization）。**CAS算法**涉及到三个操作数\n\n- 需要读写的内存值 V\n- 进行比较的值 A\n- 拟写入的新值 B\n\n当且仅当 V 的值等于 A时，CAS通过原子方式用新值B来更新V的值，否则不会执行任何操作（比较和替换是一个原子操作）。一般情况下是一个**自旋操作**，即**不断的重试**。\n\n\n关于自旋锁，大家可以看一下这篇文章，非常不错：[《 面试必备之深入理解自旋锁》](https://blog.csdn.net/qq_34337272/article/details/81252853)\n\n\n### 缺点\n\n\n> ABA 问题是乐观锁一个常见的问题\n\n\n### ABA 问题\n\n\n如果一个变量V初次读取的时候是A值，并且在准备赋值的时候检查到它仍然是A值，那我们就能说明它的值没有被其他线程修改过了吗？很明显是不能的，因为在这段时间它的值可能被改为其他值，然后又改回A，那CAS操作就会误认为它从来没有被修改过。这个问题被称为CAS操作的 **\"ABA\"问题。**\n\n\nJDK 1.5 以后的 `AtomicStampedReference 类`就提供了此种能力，其中的 `compareAndSet 方法`就是首先检查当前引用是否等于预期引用，并且当前标志是否等于预期标志，如果全部相等，则以原子方式将该引用和该标志的值设置为给定的更新值。\n\n\n### 开销大\n\n\n**自旋CAS（也就是不成功就一直循环执行直到成功）如果长时间不成功，会给CPU带来非常大的执行开销。** 如果JVM能支持处理器提供的pause指令那么效率会有一定的提升，pause指令有两个作用，第一它可以延迟流水线执行指令（de-pipeline）,使CPU不会消耗过多的执行资源，延迟的时间取决于具体实现的版本，在一些处理器上延迟时间是零。第二它可以避免在退出循环的时候因内存顺序冲突（memory order violation）而引起CPU流水线被清空（CPU pipeline flush），从而提高CPU的执行效率。\n\n\n### 只能一个变量原子操作\n\n\nCAS 只对单个共享变量有效，当操作涉及跨多个共享变量时 CAS 无效。但是从 JDK 1.5开始，提供了`AtomicReference类`来保证引用对象之间的原子性，你可以把多个变量放在一个对象里来进行 CAS 操作.所以我们可以使用锁或者利用`AtomicReference类`把多个共享变量合并成一个共享变量来操作。\n\n\nCAS与`synchronized`的使用情景\n\n\n> 简单的来说CAS适用于写比较少的情况下（多读场景，冲突一般较少），synchronized适用于写比较多的情况下（多写场景，冲突一般较多）\n\n1. 对于资源竞争较少（线程冲突较轻）的情况，使用`synchronized`同步锁进行线程阻塞和唤醒切换以及用户态内核态间的切换操作额外浪费消耗cpu资源；而CAS基于硬件实现，不需要进入内核，不需要切换线程，操作自旋几率较少，因此可以获得更高的性能。\n2. 对于资源竞争严重（线程冲突严重）的情况，CAS自旋的概率会比较大，从而浪费更多的CPU资源，效率低于synchronized。\n\n补充： Java并发编程这个领域中`synchronized`关键字一直都是元老级的角色，很久之前很多人都会称它为 **“重量级锁”** 。但是，在JavaSE 1.6之后进行了主要包括为了减少获得锁和释放锁带来的性能消耗而引入的 **偏向锁** 和 **轻量级锁** 以及其它**各种优化**之后变得在某些情况下并不是那么重了。`synchronized`的底层实现主要依靠 **Lock-Free** 的队列，基本思路是 **自旋后阻塞**，**竞争切换后继续竞争锁**，**稍微牺牲了公平性，但获得了高吞吐量**。在线程冲突较少的情况下，可以获得和CAS类似的性能；而线程冲突严重的情况下，性能远高于CAS。\n\n\n### 死锁\n\n\n线程死锁描述的是这样一种情况：多个线程同时被阻塞，它们中的一个或者全部都在等待某个资源被释放。由于线程被无限期地阻塞，因此程序不可能正常终止。\n\n\n如下图所示，线程 A 持有资源 2，线程 B 持有资源 1，他们同时都想申请对方的资源，所以这两个线程就会互相等待而进入死锁状态。\n\n\n![](https://blog-file.hehouhui.cn/202203222203695.png)\n\n\n线程 A 通过 synchronized (resource1) 获得 resource1 的监视器锁，然后通过`Thread.sleep(1000);`让线程 A 休眠 1s 为的是让线程 B 得到执行然后获取到 resource2 的监视器锁。线程 A 和线程 B 休眠结束了都开始企图请求获取对方的资源，然后这两个线程就会陷入互相等待的状态，这也就产生了死锁。上面的例子符合产生死锁的四个必要条件。\n\n\n学过操作系统的朋友都知道产生死锁必须具备以下`四个条件`：\n\n1. 互斥条件：该资源任意一个时刻只由一个线程占用。\n2. 请求与保持条件：一个进程因请求资源而阻塞时，对已获得的资源保持不放。\n3. 不剥夺条件:线程已获得的资源在未使用完之前不能被其他线程强行剥夺，只有自己使用完毕后才释放资源。\n4. 循环等待条件:若干进程之间形成一种头尾相接的循环等待资源关系。\n\n### 如何避免死锁\n\n\n我上面说了产生死锁的四个必要条件，为了避免死锁，我们只要破坏产生死锁的四个条件中的其中一个就可以了。现在我们来挨个分析一下：\n\n1. **破坏互斥条件** ：这个条件我们没有办法破坏，因为我们用锁本来就是想让他们互斥的（临界资源需要互斥访问）。\n2. **破坏请求与保持条件** ：一次性申请所有的资源。\n3. **破坏不剥夺条件** ：占用部分资源的线程进一步申请其他资源时，如果申请不到，可以主动释放它占有的资源。\n4. **破坏循环等待条件** ：靠按序申请资源来预防。按某一顺序申请资源，释放资源则反序释放。破坏循环等待条件。\n",
      "properties": {
        "password": "",
        "icon": "",
        "date": "2020-02-25",
        "type": "Post",
        "category": "技术分享",
        "urlname": "22",
        "catalog": [
          "archives"
        ],
        "tags": [
          "Java",
          "异步编程",
          "多线程"
        ],
        "summary": "线程与进程相似，但线程是一个比进程更小的执行单位。一个进程在其执行的过程中可以产生多个线程。与进程不同的是同类的多个线程共享同一块内存空间和一组系统资源，所以系统在产生一个线程，或是在各个线程之间作切换工作时，负担要比进程小得多，也正因为如此，线程也被称为轻量级进程。",
        "sort": "",
        "title": "Java基础-线程&并发",
        "status": "Published",
        "updated": "2023-10-08 14:42:00"
      },
      "catalog": [
        {
          "title": "archives",
          "doc_id": "54c4aad1-0a7a-4a4b-9906-1f54a3810362"
        }
      ],
      "body": "",
      "realName": "Java基础-线程&并发",
      "relativePath": "/archives/Java基础-线程&并发.md"
    },
    {
      "id": "ba68c9c4-7d5d-4640-80d9-c123616ea287",
      "doc_id": "ba68c9c4-7d5d-4640-80d9-c123616ea287",
      "title": "ba68c9c4-7d5d-4640-80d9-c123616ea287",
      "updated": 1696747320000,
      "body_original": "\n## List\n\n\n### ArrayList\n\n\n`ArrayList` 的底层是数组队列，相当于动态数组。与 Java 中的数组相比，它的容量能动态增长。在添加大量元素前，应用程序可以使用`ensureCapacity`操作来增加 `ArrayList` 实例的容量。这可以减少递增式再分配的数量。\n\n\n`ArrayList`继承于 **`AbstractList`** ，实现了 **`List`**, **`RandomAccess`**, **`Cloneable`**, **`java.io.Serializable`** 这些接口。\n\n\n```java\npublic class ArrayList<E> extends AbstractList<E>\n        implements List<E>, RandomAccess, Cloneable, java.io.Serializable{\n\n  }\n\n```\n\n- `RandomAccess` 是一个标志接口，表明实现这个这个接口的 List 集合是支持**快速随机访问**的。在 `ArrayList` 中，我们即可以通过元素的序号快速获取元素对象，这就是快速随机访问。\n- `ArrayList` 实现了 **`Cloneable`** **接口** ，即覆盖了函数`clone()`，能被克隆。\n- `ArrayList` 实现了 `java.io.Serializable` 接口，这意味着`ArrayList`支持序列化，能通过序列化去传输。\n\n### 与Vector的区别\n\n1. `ArrayList` 是 `List` 的主要实现类，底层使用 `Object[ ]`存储，适用于频繁的查找工作，线程不安全 ；\n2. `Vector` 是 `List` 的古老实现类，底层使用 `Object[ ]`存储，线程安全的。\n\n### 与LinkedList区别\n\n1. **是否保证线程安全：** `ArrayList` 和 `LinkedList` 都是不同步的，也就是不保证线程安全；\n2. **底层数据结构：** `Arraylist` 底层使用的是 **`Object`** **数组**；`LinkedList` 底层使用的是 **双向链表** 数据结构（JDK1.6 之前为循环链表，JDK1.7 取消了循环。注意双向链表和双向循环链表的区别，下面有介绍到！）\n3. **插入和删除是否受元素位置的影响：** ① **`ArrayList`** **采用数组存储，所以插入和删除元素的时间复杂度受元素位置的影响。** 比如：执行`add(E e)`方法的时候， `ArrayList` 会默认在将指定的元素追加到此列表的末尾，这种情况时间复杂度就是 O(1)。但是如果要在指定位置 i 插入和删除元素的话（`add(int index, E element)`）时间复杂度就为 O(n-i)。因为在进行上述操作的时候集合中第 i 和第 i 个元素之后的(n-i)个元素都要执行向后位/向前移一位的操作。 ② **`LinkedList`** **采用链表存储，所以对于****`add(E e)`****方法的插入，删除元素时间复杂度不受元素位置的影响，近似 O(1)，如果是要在指定位置****`i`****插入和删除元素的话（****`(add(int index, E element)`****） 时间复杂度近似为****`o(n))`****因为需要先移动到指定位置再插入。**\n4. **是否支持快速随机访问：** `LinkedList` 不支持高效的随机元素访问，而 `ArrayList` 支持。快速随机访问就是通过元素的序号快速获取元素对象(对应于`get(int index)`方法)。\n5. **内存空间占用：** `ArrayList` 的空 间浪费主要体现在在 list 列表的结尾会预留一定的容量空间，而 `LinkedList` 的空间花费则体现在它的每一个元素都需要消耗比 `ArrayList` 更多的空间（因为要存放直接后继和直接前驱以及数据）。\n\n### 扩容机制\n\n\n**JDK8）ArrayList 有三种方式来初始化，构造方法源码如下：**\n\n\n```java\n   /**\n     * 默认初始容量大小\n     */\n    private static final int DEFAULT_CAPACITY = 10;\n\n\n    private static final Object[] DEFAULTCAPACITY_EMPTY_ELEMENTDATA = {};\n\n    /**\n     *默认构造函数，使用初始容量10构造一个空列表(无参数构造)\n     */\n    public ArrayList() {\n        this.elementData = DEFAULTCAPACITY_EMPTY_ELEMENTDATA;\n    }\n\n    /**\n     * 带初始容量参数的构造函数。（用户自己指定容量）\n     */\n    public ArrayList(int initialCapacity) {\n        if (initialCapacity > 0) {//初始容量大于0\n            //创建initialCapacity大小的数组\n            this.elementData = new Object[initialCapacity];\n        } else if (initialCapacity == 0) {//初始容量等于0\n            //创建空数组\n            this.elementData = EMPTY_ELEMENTDATA;\n        } else {//初始容量小于0，抛出异常\n            throw new IllegalArgumentException(\"Illegal Capacity: \"+\n                                               initialCapacity);\n        }\n    }\n\n\n   /**\n    *构造包含指定collection元素的列表，这些元素利用该集合的迭代器按顺序返回\n    *如果指定的集合为null，throws NullPointerException。\n    */\n     public ArrayList(Collection<? extends E> c) {\n        elementData = c.toArray();\n        if ((size = elementData.length) != 0) {\n            // c.toArray might (incorrectly) not return Object[] (see 6260652)\n            if (elementData.getClass() != Object[].class)\n                elementData = Arrays.copyOf(elementData, size, Object[].class);\n        } else {\n            // replace with empty array.\n            this.elementData = EMPTY_ELEMENTDATA;\n        }\n    }\n```\n\n\n细心的同学一定会发现 ：**以无参数构造方法创建 ArrayList 时，实际上初始化赋值的是一个空数组。当真正对数组进行添加元素操作时，才真正分配容量。即向数组中添加第一个元素时，数组容量扩为 10。** 下面在我们分析 ArrayList 扩容时会讲到这一点内容！\n\n\n> 补充：JDK7 new无参构造的ArrayList对象时，直接创建了长度是10的Object[]数组elementData 。jdk7中的ArrayList的对象的创建类似于单例的饿汉式，而jdk8中的ArrayList的对象的创建类似于单例的懒汉式。JDK8的内存优化也值得我们在平时开发中学习。\n\n- 当我们要 add 进第 1 个元素到 ArrayList 时，elementData.length 为 0 （因为还是一个空的 list），因为执行了 `ensureCapacityInternal()` 方法 ，所以 minCapacity 此时为 10。此时，`minCapacity - elementData.length > 0`成立，所以会进入 `grow(minCapacity)` 方法。\n- 当 add 第 2 个元素时，minCapacity 为 2，此时 e lementData.length(容量)在添加第一个元素后扩容成 10 了。此时，`minCapacity - elementData.length > 0` 不成立，所以不会进入 （执行）`grow(minCapacity)` 方法。\n- 添加第 3、4···到第 10 个元素时，依然不会执行 grow 方法，数组容量都为 10。\n\n直到添加第 11 个元素，minCapacity(为 11)比 elementData.length（为 10）要大。进入 grow 方法进行扩容。\n\n\n**grow()方法**\n\n\n**int newCapacity = oldCapacity + (oldCapacity >> 1),所以 ArrayList 每次扩容之后容量都会变为原来的 1.5 倍左右（oldCapacity 为偶数就是 1.5 倍，否则是 1.5 倍左右）！** 奇偶不同，比如 ：10+10/2 = 15, 33+33/2=49。如果是奇数的话会丢掉小数.\n\n\n> \">>\"（移位运算符）：>>1 右移一位相当于除 2，右移 n 位相当于除以 2 的 n 次方。这里 oldCapacity 明显右移了 1 位所以相当于 oldCapacity /2。对于大数据的 2 进制运算,位移运算符比那些普通运算符的运算要快很多,因为程序仅仅移动一下而已,不去计算,这样提高了效率,节省了资源\n\n\n**我们再来通过例子探究一下****`grow()`** **方法 ：**\n\n- 当 add 第 1 个元素时，oldCapacity 为 0，经比较后第一个 if 判断成立，newCapacity = minCapacity(为 10)。但是第二个 if 判断不会成立，即 newCapacity 不比 MAX_ARRAY_SIZE 大，则不会进入 `hugeCapacity` 方法。数组容量为 10，add 方法中 return true,size 增为 1。\n- 当 add 第 11 个元素进入 grow 方法时，newCapacity 为 15，比 minCapacity（为 11）大，第一个 if 判断不成立。新容量没有大于数组最大 size，不会进入 hugeCapacity 方法。数组容量扩为 15，add 方法中 return true,size 增为 11。\n- 以此类推······\n\n**这里补充一点比较重要，但是容易被忽视掉的知识点：**\n\n- java 中的 `length`属性是针对数组说的,比如说你声明了一个数组,想知道这个数组的长度则用到了 length 这个属性.\n- java 中的 `length()` 方法是针对字符串说的,如果想看这个字符串的长度则用到 `length()` 这个方法.\n- java 中的 `size()` 方法是针对泛型集合说的,如果想看这个泛型有多少个元素,就调用此方法来查看!\n\n### LinkedList\n\n\n> LinkedList是一个实现了List接口和Deque接口的双端链表。 LinkedList底层的链表结构使它支持高效的插入和删除操作，另外它实现了Deque接口，使得LinkedList类也具有队列的特性; LinkedList不是线程安全的，如果想使LinkedList变成线程安全的，可以调用静态类Collections类中的synchronizedList方法：\n\n\n**如下图所示：**\n\n\n![](https://blog-file.hehouhui.cn/202203222157254.png)\n\n\n看完了图之后，我们再看LinkedList类中的一个\n\n\n**内部私有类Node**\n\n\n就很好理解了：\n\n\n```java\nprivate static class Node<E> {\n        E item;//节点值\n        Node<E> next;//后继节点\n        Node<E> prev;//前驱节点\n\n        Node(Node<E> prev, E element, Node<E> next) {\n            this.item = element;\n            this.next = next;\n            this.prev = prev;\n        }\n    }\n\n```\n\n\n这个类就代表双端链表的节点Node。这个类有三个属性，分别是前驱节点，本节点的值，后继结点。\n\n\n## HashMap\n\n\nHashMap 主要用来存放键值对，它基于哈希表的Map接口实现，是常用的Java集合之一。\n\n\nJDK1.8 之前 HashMap 由 数组+链表 组成的，数组是 HashMap 的主体，链表则是主要为了解决哈希冲突而存在的（“拉链法”解决冲突）.JDK1.8 以后在解决哈希冲突时有了较大的变化，当链表长度大于阈值（默认为 8）时，将链表转化为红黑树（将链表转换成红黑树前会判断，如果当前数组的长度小于 64，那么会选择先进行数组扩容，而不是转换为红黑树），以减少搜索时间，具体可以参考 `treeifyBin`方法。\n\n\n### 底层结构\n\n\n### 1.8前\n\n\nJDK1.8 之前 HashMap 底层是 **数组和链表** 结合在一起使用也就是 **链表散列**。**HashMap 通过 key 的 hashCode 经过扰动函数处理过后得到 hash 值，然后通过** **`(n - 1) & hash`** **判断当前元素存放的位置（这里的 n 指的是数组的长度），如果当前位置存在元素的话，就判断该元素与要存入的元素的 hash 值以及 key 是否相同，如果相同的话，直接覆盖，不相同就通过拉链法解决冲突。**\n\n\n**所谓扰动函数指的就是 HashMap 的 hash 方法。使用 hash 方法也就是扰动函数是为了防止一些实现比较差的 hashCode() 方法 换句话说使用扰动函数之后可以减少碰撞。**\n\n\nJDK 1.8 的 hash方法 相比于 JDK 1.7 hash 方法更加简化，但是原理不变。\n\n\n```java\n    static final int hash(Object key) {\n      int h;\n      // key.hashCode()：返回散列值也就是hashcode\n      // ^ ：按位异或\n      // >>>:无符号右移，忽略符号位，空位都以0补齐\n      return (key == null) ? 0 : (h = key.hashCode()) ^ (h >>> 16);\n  }\n\n```\n\n\n对比一下 JDK1.7的 HashMap 的 hash 方法源码.\n\n\n```java\nstatic int hash(int h) {\n    // This function ensures that hashCodes that differ only by\n    // constant multiples at each bit position have a bounded\n    // number of collisions (approximately 8 at default load factor).\n\n    h ^= (h >>> 20) ^ (h >>> 12);\n    return h ^ (h >>> 7) ^ (h >>> 4);\n}\n\n```\n\n\n相比于 JDK1.8 的 hash 方法 ，JDK 1.7 的 hash 方法的性能会稍差一点点，因为毕竟扰动了 4 次。\n\n\n所谓 **“拉链法”** 就是：将链表和数组相结合。也就是说创建一个链表数组，数组中每一格就是一个链表。若遇到哈希冲突，则将冲突的值加到链表中即可。\n\n\n![](https://blog-file.hehouhui.cn/202203222203869.png)\n\n\n### 1.8后\n\n\n相比于之前的版本，jdk1.8在解决哈希冲突时有了较大的变化，当链表长度大于阈值（默认为8）时，将链表转化为红黑树，以减少搜索时间。\n\n\n![](https://blog-file.hehouhui.cn/202203222203079.jpg)\n\n\n**类的属性：**\n\n\n```java\npublic class HashMap<K,V> extends AbstractMap<K,V> implements Map<K,V>, Cloneable, Serializable {\n    // 序列号\n    private static final long serialVersionUID = 362498820763181265L;\n    // 默认的初始容量是16\n    static final int DEFAULT_INITIAL_CAPACITY = 1 << 4;\n    // 最大容量\n    static final int MAXIMUM_CAPACITY = 1 << 30;\n    // 默认的填充因子\n    static final float DEFAULT_LOAD_FACTOR = 0.75f;\n    // 当桶(bucket)上的结点数大于这个值时会转成红黑树\n    static final int TREEIFY_THRESHOLD = 8;\n    // 当桶(bucket)上的结点数小于这个值时树转链表\n    static final int UNTREEIFY_THRESHOLD = 6;\n    // 桶中结构转化为红黑树对应的table的最小大小\n    static final int MIN_TREEIFY_CAPACITY = 64;\n    // 存储元素的数组，总是2的幂次倍\n    transient Node<k,v>[] table;\n    // 存放具体元素的集\n    transient Set<map.entry<k,v>> entrySet;\n    // 存放元素的个数，注意这个不等于数组的长度。\n    transient int size;\n    // 每次扩容和更改map结构的计数器\n    transient int modCount;\n    // 临界值 当实际大小(容量*填充因子)超过临界值时，会进行扩容\n    int threshold;\n    // 加载因子\n    final float loadFactor;\n}\n\n```\n\n- **loadFactor加载因子**\n\n\tloadFactor加载因子是控制数组存放数据的疏密程度，loadFactor越趋近于1，那么 数组中存放的数据(entry)也就越多，也就越密，也就是会让链表的长度增加，loadFactor越小，也就是趋近于0，数组中存放的数据(entry)也就越少，也就越稀疏。\n\n\n\t**loadFactor太大导致查找元素效率低，太小导致数组的利用率低，存放的数据会很分散。loadFactor的默认值为0.75f是官方给出的一个比较好的临界值**。\n\n\n\t给定的默认容量为 16，负载因子为 0.75。Map 在使用过程中不断的往里面存放数据，当数量达到了 16 * 0.75 = 12 就需要将当前 16 的容量进行扩容，而扩容这个过程涉及到 rehash、复制数据等操作，所以非常消耗性能。\n\n- **threshold**\n\n\t_threshold = capacity \\_ _loadFactor_*，**当Size>=threshold**的时候，那么就要考虑对数组的扩增了，也就是说，这个的意思就是 **衡量数组是否需要扩增的一个标准**。\n\n\n### put方法（扩容）\n\n\nHashMap只提供了put用于添加元素，putVal方法只是给put方法调用的一个方法，并没有提供给用户使用。\n\n\n**对putVal方法添加元素的分析如下：**\n\n- ①如果定位到的数组位置没有元素 就直接插入。\n- ②如果定位到的数组位置有元素就和要插入的key比较，如果key相同就直接覆盖，如果key不相同，就判断p是否是一个树节点，如果是就调用`e = ((TreeNode)p).putTreeVal(this, tab, hash, key, value)`将元素添加进入。如果不是就遍历链表插入(插入的是链表尾部)。\n\nps:下图有一个小问题，来自 [issue#608](https://github.com/Snailclimb/JavaGuide/issues/608)指出：直接覆盖之后应该就会 return，不会有后续操作。参考 JDK8 HashMap.java 658 行。\n\n\n![](https://blog-file.hehouhui.cn/202203222203154.png)\n\n\n**我们再来对比一下 JDK1.7 put方法的代码**\n\n\n**对于put方法的分析如下：**\n\n- ①如果定位到的数组位置没有元素 就直接插入。\n- ②如果定位到的数组位置有元素，遍历以这个元素为头结点的链表，依次和插入的key比较，如果key相同就直接覆盖，不同就采用头插法插入元素。\n\n### resize方法\n\n\n进行扩容，会伴随着一次重新hash分配，并且会遍历hash表中所有的元素，是非常耗时的。在编写程序中，要尽量避免resize。\n\n- LinkedList的底层结构是双端链表，支持高效的插入和删除操作，同时实现了Deque接口，也具有队列的特性。\n- HashMap主要用来存放键值对，基于哈希表的Map接口实现，是常用的Java集合之一。JDK1.8之前的HashMap由数组+链表组成，数组是HashMap的主体，链表则是主要为了解决哈希冲突而存在的。JDK1.8以后，当链表长度大于阈值（默认为8）时，将链表转化为红黑树，以减少搜索时间。\n- 在进行扩容时，会伴随着一次重新hash分配，并且会遍历hash表中所有的元素，是非常耗时的，要尽量避免resize。\n",
      "properties": {
        "password": "",
        "icon": "",
        "date": "2020-03-08",
        "type": "Post",
        "category": "技术分享",
        "urlname": "44",
        "catalog": [
          "archives"
        ],
        "tags": [
          "Java",
          "数据结构"
        ],
        "summary": "这篇文章讲解了Java集合中的ArrayList，它是一种动态数组，可以快速随机访问。相比于Vector，ArrayList不是线程安全的，但是LinkedList则不支持高效的随机元素访问。此外，ArrayList实现了多个接口，包括List、RandomAccess、Cloneable和Serializable。因此，ArrayList在实际应用中非常常见。\n除了ArrayList，文章还介绍了Java中的其他集合类型，如LinkedList和HashMap。LinkedList是一种双向链表，可以在任意位置进行插入和删除操作，但是不支持随机访问。而HashMap是一种基于哈希表实现的Map，可以用于存储键值对。HashMap使用数组和链表结合的方式，来解决哈希冲突的问题，JDK1.8之后还引入了红黑树来优化性能。\n文章还介绍了一些Java集合中的底层实现细节，例如HashMap中的扰动函数、loadFactor加载因子、threshold临界值等。这些细节对于理解集合的工作原理非常重要，也可以帮助我们更好地使用Java集合。\n总之，Java集合是Java编程中非常常用的一部分，掌握好集合的使用方法和底层实现细节，可以帮助我们编写更高效、更易维护的Java代码。",
        "sort": "",
        "title": "Java基础-集合",
        "status": "Published",
        "updated": "2023-10-08 14:42:00"
      },
      "catalog": [
        {
          "title": "archives",
          "doc_id": "ba68c9c4-7d5d-4640-80d9-c123616ea287"
        }
      ],
      "body": "",
      "realName": "Java基础-集合",
      "relativePath": "/archives/Java基础-集合.md"
    },
    {
      "id": "387ccffe-e894-442f-9471-2460619d4e62",
      "doc_id": "387ccffe-e894-442f-9471-2460619d4e62",
      "title": "387ccffe-e894-442f-9471-2460619d4e62",
      "updated": 1696747320000,
      "body_original": "\n# Redis 面试宝典🤪\n\n\n> 💡 Redis是一种高性能的key-value存储系统，具有以下几个特点：  \n> 1. 内存存储：Redis将数据存储在内存中，因此读写速度非常快，通常比基于磁盘的存储系统快几个数量级。  \n>   \n> 2. 数据结构多样：Redis支持多种数据结构，包括字符串、哈希、列表、集合、有序集合等，丰富的数据结构使Redis可以支持更多的应用场景。  \n>   \n> 3. 持久化：Redis支持将数据持久化到磁盘上，以保证数据的可靠性。  \n>   \n> 4. 高并发性：Redis采用单线程模型，减少了多线程之间的竞争，从而使得Redis具有更高的并发性。  \n>   \n> 5. 分布式：Redis提供了集群模式，可以将数据分布到不同的节点上，从而实现水平扩展。\n\n\n> 💡 Redis的应用场景非常广泛，主要包括以下几个方面：  \n>   \n> 1. 缓存：Redis常用于作为缓存系统，用于加速数据的读取。通过将热数据缓存到内存中，可以减少磁盘读取次数，从而提高应用程序的响应速度。  \n>   \n> 2. 计数器：Redis支持原子操作，可以用于实现计数器功能。  \n>   \n> 3. 消息队列：Redis支持发布/订阅模式，可以用于实现简单的消息队列功能。  \n>   \n> 4. 分布式锁：Redis可以通过原子操作实现分布式锁，用于实现分布式系统中的互斥访问。  \n>   \n> 5. 会话管理：Redis可以用于存储会话数据，从而实现分布式应用程序的会话管理。\n\n\n## **一致性hash**\n\n\n> 在Redis 集群模式Cluster中，Redis采用的是分片Sharding的方式，也就是将数据采用一定的分区策略，分发到相应的集群节点中。但是我们使用上述HASH算法进行缓存时，会出现一些缺陷，主要体现在服务器数量变动的时候，所有缓存的位置都要发生改变！具体来讲就是说第一当缓存服务器数量发生变化时，会引起缓存的雪崩，可能会引起整体系统压力过大而崩溃（大量缓存同一时间失效）。第二当缓存服务器数量发生变化时，几乎所有缓存的位置都会发生改变。\n\n\n一致性Hash算法也是使用取模的方法，只是，刚才描述的取模法是对服务器的数量进行取模，而一致性Hash算法是对232取模，什么意思呢？简单来说，一致性Hash算法将整个哈希值空间组织成一个虚拟的圆环，如假设某哈希函数H的值空间为0-232-1（即哈希值是一个32位无符号整形），整个哈希环如下：\n\n\n![](https://s3.us-west-2.amazonaws.com/secure.notion-static.com/8a35a23c-1341-49da-a621-55a436622c1d/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20231031%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20231031T090555Z&X-Amz-Expires=3600&X-Amz-Signature=56ed1dd1e3db26bc28cea11990eb4d5a3ca9a077d6ca90e9cb3f7ed5584d2329&X-Amz-SignedHeaders=host&x-id=GetObject)\n\n\n整个空间按顺时针方向组织，圆环的正上方的点代表0，0点右侧的第一个点代表1，以此类推，2、3、4、5、6……直到232-1，也就是说0点左侧的第一个点代表232-1， 0和232-1在零点中方向重合，我们把这个由232个点组成的圆环称为Hash环。\n\n\n那么，一致性哈希算法与上图中的圆环有什么关系呢？我们继续聊，仍然以之前描述的场景为例，假设我们有4台缓存服务器，服务器A、服务器B、服务器C，服务器D，那么，在生产环境中，这4台服务器肯定有自己的IP地址或主机名，我们使用它们各自的IP地址或主机名作为关键字进行哈希计算，使用哈希后的结果对2^32取模，可以使用如下公式示意：\n\n\n$$\nhash（服务器A的IP地址） %  2^32\n$$\n\n\n通过上述公式算出的结果一定是一个0到232-1之间的一个整数，我们就用算出的这个整数，代表服务器A，既然这个整数肯定处于0到232-1之间，那么，上图中的hash环上必定有一个点与这个整数对应，而我们刚才已经说明，使用这个整数代表服务器A，那么，服务器A就可以映射到这个环上。\n\n\n以此类推，下一步将各个服务器使用类似的Hash算式进行一个哈希，这样每台机器就能确定其在哈希环上的位置，这里假设将上文中四台服务器使用IP地址哈希后在环空间的位置如下：\n\n\n接下来使用如下算法定位数据访问到相应服务器： 将数据key使用相同的函数Hash计算出哈希值，并确定此数据在环上的位置，从此位置沿环顺时针“行走”，第一台遇到的服务器就是其应该定位到的服务器！\n\n\n### **Hash算法的容错性和可扩展性**\n\n\n现假设Node C不幸宕机，可以看到此时对象A、B、D不会受到影响，只有C对象被重定位到Node D。一般的，在一致性Hash算法中，如果一台服务器不可用，则受影响的数据仅仅是此服务器到其环空间中前一台服务器（即沿着逆时针方向行走遇到的第一台服务器）之间数据，其它不会受到影响，如下所示：\n\n\n![](https://s3.us-west-2.amazonaws.com/secure.notion-static.com/d578620f-e4b5-41c5-990a-8c7bc1aa4fdc/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20231031%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20231031T090555Z&X-Amz-Expires=3600&X-Amz-Signature=b9eeb7b958cf50850f0215debb777ea9d3c0e5b688b1ee07234f11f29c91f3e3&X-Amz-SignedHeaders=host&x-id=GetObject)\n\n\n下面考虑另外一种情况，如果在系统中增加一台服务器Node X，如下图所示：\n\n\n![](https://s3.us-west-2.amazonaws.com/secure.notion-static.com/5dc25236-bddd-4d82-928a-30497f92890d/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20231031%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20231031T090555Z&X-Amz-Expires=3600&X-Amz-Signature=d5e082ad528629fa7f4364bab377e59535918c2c896b565f10e189c47222ff05&X-Amz-SignedHeaders=host&x-id=GetObject)\n\n\n此时对象Object A、B、D不受影响，只有对象C需要重定位到新的Node X ！一般的，在一致性Hash算法中，如果增加一台服务器，则受影响的数据仅仅是新服务器到其环空间中前一台服务器（即沿着逆时针方向行走遇到的第一台服务器）之间数据，其它数据也不会受到影响。\n\n\n综上所述，一致性Hash算法对于节点的增减都只需重定位环空间中的一小部分数据，具有较好的容错性和可扩展性。\n\n\n### **数据倾斜问题**\n\n\n一致性Hash算法在服务节点太少时，容易因为节点分部不均匀而造成数据倾斜（被缓存的对象大部分集中缓存在某一台服务器上）问题，例如系统中只有两台服务器，其环分布如下：\n\n\n![](https://s3.us-west-2.amazonaws.com/secure.notion-static.com/5d967984-598b-4263-b0e8-7d5d4d08cc3e/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20231031%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20231031T090555Z&X-Amz-Expires=3600&X-Amz-Signature=1458f39bff9625ff6a497e21a843a5badc34d56058b053fc89a9a5bf59c4c40e&X-Amz-SignedHeaders=host&x-id=GetObject)\n\n\n此时必然造成大量数据集中到Node A上，而只有极少量会定位到Node B上，从而出现hash环偏斜的情况，当hash环偏斜以后，缓存往往会极度不均衡的分布在各服务器上，如果想要均衡的将缓存分布到2台服务器上，最好能让这2台服务器尽量多的、均匀的出现在hash环上，但是，真实的服务器资源只有2台，我们怎样凭空的让它们多起来呢，没错，就是凭空的让服务器节点多起来，既然没有多余的真正的物理服务器节点，我们就只能将现有的物理节点通过虚拟的方法复制出来。\n\n\n这些由实际节点虚拟复制而来的节点被称为\"虚拟节点\"，即对每一个服务节点计算多个哈希，每个计算结果位置都放置一个此服务节点，称为虚拟节点。具体做法可以在服务器IP或主机名的后面增加编号来实现。\n\n\n例如上面的情况，可以为每台服务器计算三个虚拟节点，于是可以分别计算 “Node A#1”、“Node A#2”、“Node A#3”、“Node B#1”、“Node B#2”、“Node B#3”的哈希值，于是形成六个虚拟节点：\n\n\n![](https://s3.us-west-2.amazonaws.com/secure.notion-static.com/a0cf35dd-13c4-40da-9eb5-e219a514f3b5/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20231031%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20231031T090555Z&X-Amz-Expires=3600&X-Amz-Signature=364f7f5225eb453c08f298c29baf4f03c63bc2ab6cc06d43ddc8ffc4e3a4e911&X-Amz-SignedHeaders=host&x-id=GetObject)\n\n\n同时数据定位算法不变，只是多了一步虚拟节点到实际节点的映射，例如定位到“Node A#1”、“Node A#2”、“Node A#3”三个虚拟节点的数据均定位到Node A上。这样就解决了服务节点少时数据倾斜的问题。在实际应用中，通常将虚拟节点数设置为32甚至更大，因此即使很少的服务节点也能做到相对均匀的数据分布。\n\n\n## **集群情况下什么时候不可用**\n\n- 如果集群任意_master_挂掉_,_且当前_master_没有_slave._集群进入_fail_状态\n- 有A,B,C三个节点的集群，在没有复制模型的情况下，如果节点B失败了，那么整个集群就会以为缺少5501-11000这个范围的槽而不可用。\n- 如果集群超过半数以上_master_挂掉，无论是否有_slave_集群进入_fail_状态\n- 集群某一节点的主从全数宕机 （与2相似）\n\n## **故障的处理过程**\n\n1. 查看业务日志（微服务）\n2. 首先查看redis集群状态。\n3. 继续查看redis集群节点的状态。\n\n\t**处理过程**\n\n\n1、先停止所有redis节点。\n2、删除每个节点的缓存文件，包括node-6380.conf dump.rdp等文件。\n3、重启每个redis节点。\n4、重新创建redis集群。\n\n\n## **集群**\n\n\n> 发现我们当前项目用的redis是主从，但是跟单点其实没有什么区别，因为我们在应用层面没有做读写分离，所以其实从服务器只是做了一个主从复制的工作，其他的什么都没有做。\n\n\n\t那么如果我们的系统升级，用户量上升，那么一主一从可能扛不住那么大的压力，可能需要一主多从做备机，那么假如主服务器宕机了，选举哪台从服务器做主呢？这就是一个问题，需要一个第三个人来解决，所以我查了一下，哨兵模式可以解决这个问题。哨兵模式的细节下面会讲到。\n\n\n\t然后我又想了，那如果单台服务器无法承受100%的存储压力，那就应该将存储压力分散开来，所以集群就可以解决这个问题 了，比如我们用6台服务器做集群，3主3从，那么每台服务器只需要存储1/3即可。好，那么我们就来详细看一下这些具体怎么做的。\n\n\n### **单点主从**\n\n\n基本上就是一主一从，我们应用层主要使用的是主节点，从节点的主要工作是从主节点做主从复制。关键时刻，如果主服务器挂掉，可以手动启动从服务器，然后更改应用层的redis的ip即可\n\n\n> 实现主从复制（Master-Slave Replication）的工作原理：Slave从节点服务启动并连接到Master之后，它将主动发送一个SYNC命令。Master服务主节点收到同步命令后将启动后台存盘进程，同时收集所有接收到的用于修改数据集的命令，在后台进程执行完毕后，Master将传送整个数据库文件到Slave，以完成一次完全同步。而Slave从节点服务在接收到数据库文件数据之后将其存盘并加载到内存中。此后，Master主节点继续将所有已经收集到的修改命令，和新的修改命令依次传送给Slaves，Slave将在本次执行这些数据修改命令，从而达到最终的数据同步。\n\n\n如果Master和Slave之间的链接出现断连现象，Slave可以自动重连Master，但是在连接成功之后，一次完全同步将被自动执行。\n\n\n**主从复制配置**\n\n\n修改从节点的配置文件：slaveof masterip masterport\n如果设置了密码，就要设置：masterauth master-password\n\n\n主从模式的优缺点\n\n\n**优点：**\n\n- 同一个Master可以同步多个Slaves。\n- Slave同样可以接受其它Slaves的连接和同步请求，这样可以有效的分载Master的同步压力。因此我们可以将Redis的Replication架构视为图结构。\n- Master Server是以非阻塞的方式为Slaves提供服务。所以在Master-Slave同步期间，客户端仍然可以提交查询或修改请求。\n- Slave Server同样是以非阻塞的方式完成数据同步。在同步期间，如果有客户端提交查询请求，Redis则返回同步之前的数据\n- 为了分载Master的读操作压力，Slave服务器可以为客户端提供只读操作的服务，写服务仍然必须由Master来完成。即便如此，系统的伸缩性还是得到了很大的提高。\n- Master可以将数据保存操作交给Slaves完成，从而避免了在Master中要有独立的进程来完成此操作。\n支持主从复制，主机会自动将数据同步到从机，可以进行读写分离。\n\n**缺点：**\n\n- Redis不具备自动容错和恢复功能，主机从机的宕机都会导致前端部分读写请求失败，需要等待机器重启或者手动切换前端的IP才能恢复。\n- 主机宕机，宕机前有部分数据未能及时同步到从机，切换IP后还会引入数据不一致的问题，降低了系统的可用性。\n- Redis的主从复制采用全量复制，复制过程中主机会fork出一个子进程对内存做一份快照，并将子进程的内存快照保存为文件发送给从机，这一过程需要确保主机有足够多的空余内存。若快照文件较大，对集群的服务能力会产生较大的影响，而且复制过程是在从机新加入集群或者从机和主机网络断开重连时都会进行，也就是网络波动都会造成主机和从机间的一次全量的数据复制，这对实际的系统运营造成了不小的麻烦。\n- Redis较难支持在线扩容，在集群容量达到上限时在线扩容会变得很复杂。为避免这一问题，运维人员在系统上线时必须确保有足够的空间，这对资源造成了很大的浪费。\n- 其实redis的主从模式很简单，在实际的生产环境中是很少使用的，我也不建议在实际的生产环境中使用主从模式来提供系统的高可用性，之所以不建议使用都是由它的缺点造成的，在数据量非常大的情况，或者对系统的高可用性要求很高的情况下，主从模式也是不稳定的。\n\n### **读写分离**\n\n\n> 对于读占比较高的场景，可以通过把一部分流量分摊导出从节点(salve) 来减轻主节点（master）压力，同时需要主要只对主节点执行写操作。\n\n\n常见的应用场景下我觉得redis没必要进行读写分离。\n\n\n先来讨论一下为什么要读写分离：\n\n\n> 读写分离使用于大量读请求的情况，通过多个slave分摊了读的压力，从而增加了读的性能。过多的select会阻塞住数据库，使你增删改不能执行，而且到并发量过大时，数据库会拒绝服务。\n\n\n\t因而通过读写分离，从而增加性能，避免拒绝服务的发生。\n\n\n\t我认为需要读写分离的应用场景是：写请求在可接受范围内，但读请求要远大于写请求的场景。\n\n\n再来讨论一下redis常见的应用场景：\n\n1. 缓存\n2. 排名型的应用，访问计数型应用\n3. 实时消息系统\n\n首先说一下缓存集群，这也是非常常见的应用场景：\n\n1. 缓存主要解决的是用户访问时，怎么以更快的速度得到数据。\n2. 单机的内存资源是很有限的，所以缓存集群会通过某种算法将不同的数据放入到不同的机器中。\n3. 不同持久化数据库，一般来说，内存数据库单机可以支持大量的增删查改。\n4. 如果一台机器支持不住，可以用主从复制，进行缓存的方法解决。\n5. 综上，在这个场景下应用redis 进行读写分离，完全就失去了读写分离的意义。\n\n> 当然，也有可能考虑不到的地方需要读写分离，毕竟“存在即合理”嘛，那么就来介绍一下这个读写分离吧。\n\n\n`当使用从节点响应读请求时，业务端可能会遇到以下问题`\n\n- 复制数据延迟\n- 读到过期数据\n- 从节点故障\n\n**数据延迟**\n\n\n> Redis 复制数的延迟由于异步复制特性是无法避免的，延迟取决于网络带宽和命令阻塞情况，对于无法容忍大量延迟场景，可以编写外部监控程序监听主从节点的复制偏移量，当延迟较大时触发报警或者通知客户端避免读取延迟过高的从节点，实现逻辑如下：\n\n1. 监控程序(monitor) 定期检查主从节点的偏移量，主节点偏移量在info replication 的master_repl_offset 指标记录，从节点 偏移量可以查询主节点的slave0 字段的offset指标，它们的差值就是主从节点延迟的字节 量。\n2. 当延迟字节量过高时，比如超过10M。监控程序触发报警并通知客户端从节点延迟过高。可以采用Zookeeper的监听回调机制实现客户端通知。\n3. 客户端接到具体的从节点高延迟通知后，修改读命令路由到其他从节点或主节点上。当延迟回复后，再次通知客户端，回复从节点的读命令请求。\n\n这种方案成本较高，需要单独修改适配Redis的客户端类库。\n\n\n**读到过期数据**\n\n\n当主节点存储大量设置超时的数据时，如缓存数据，Redis内部需要维护过期数据删除策略，删除策略主要有两种：`惰性删除和定时删除`。\n\n\n`惰性删除`：主节点每次处理读取命令时，都会检查键是否超时，如果超时则执行del命令删除键对象那个，之后del命令也会异步 发送给 从节点\n\n\n> 需要注意的是为了保证复制的一致性，从节点自身永远不会主动删除超时数据，\n\n\n`定时删除`：\n\n\nRedis主节点在内部定时任务会循环采样一定数量的键，当发现采样的键过期就执行del命令，之后再同步给从节点\n\n\n> 如果此时 数据的大量超时，主节点采样速度跟不上过期速度且主节点没有读取过期键的操作，那么从节点将无法收到del命令，这时在从节点 上可以读取到已经超时的数据。Redis在3.2 版本解决了这个问题，从节点 读取数据之前会检查键的过期时间来决定是否返回数据，可以升级到3.2版本来规避这个问题。\n\n\n### **哨兵模式**\n\n\n> 该模式是从Redis的2.6版本开始提供的，但是当时这个版本的模式是不稳定的，直到Redis的2.8版本以后，这个哨兵模式才稳定下来，无论是主从模式，还是哨兵模式，这两个模式都有一个问题，不能水平扩容，并且这两个模式的高可用特性都会受到Master主节点内存的限制。\n\n\nSentinel(哨兵)进程是用于监控redis集群中Master主服务器工作的状态，在Master主服务器发生故障的时候，可以实现Master和Slave服务器的切换，保证系统的高可用。\n\n\n**Sentinel（哨兵）进程的作用**\n\n- 监控(Monitoring): 哨兵(sentinel) 会不断地检查你的Master和Slave是否运作正常。\n- 提醒(Notification)：当被监控的某个Redis节点出现问题时, 哨兵(sentinel) 可以通过 API 向管理员或者其他应用程序发送通知。\n- 自动故障迁移(Automatic failover)：\n\n\t> 当一个Master不能正常工作时，哨兵(sentinel) 会开始一次自动故障迁移操作，它会将失效Master的其中一个Slave升级为新的Master, 并让失效Master的其他Slave改为复制新的Master；当客户端试图连接失效的Master时，集群也会向客户端返回新Master的地址，使得集群可以使用现在的Master替换失效Master。\n\n\n\t\tMaster和Slave服务器切换后，Master的redis.conf、Slave的redis.conf和sentinel.conf的配置文件的内容都会发生相应的改变，即，Master主服务器的redis.conf配置文件中会多一行slaveof的配置，sentinel.conf的监控目标会随之调换。\n\n\n**Sentinel（哨兵）进程的工作方式**\n\n- 每个Sentinel（哨兵）进程以每秒钟一次的频率向整个集群中的Master主服务器，Slave从服务器以及其他Sentinel（哨兵）进程发送一个 PING 命令。\n- 如果一个实例（instance）距离最后一次有效回复 PING 命令的时间超过 down-after-milliseconds 选项所指定的值， 则这个实例会被 Sentinel（哨兵）进程标记为主观下线（SDOWN）\n- 如果一个Master主服务器被标记为主观下线（SDOWN），则正在监视这个Master主服务器的所有 Sentinel（哨兵）进程要以每秒一次的频率确认Master主服务器的确进入了主观下线状态\n- 当有足够数量的 Sentinel（哨兵）进程（大于等于配置文件指定的值）在指定的时间范围内确认Master主服务器进入了主观下线状态（SDOWN）， 则Master主服务器会被标记为客观下线（ODOWN）\n- 在一般情况下， 每个 Sentinel（哨兵）进程会以每 10 秒一次的频率向集群中的所有Master主服务器、Slave从服务器发送 INFO 命令。\n- 当Master主服务器被 Sentinel（哨兵）进程标记为客观下线（ODOWN）时，Sentinel（哨兵）进程向下线的 Master主服务器的所有 Slave从服务器发送 INFO 命令的频率会从 10 秒一次改为每秒一次。\n- 若没有足够数量的 Sentinel（哨兵）进程同意 Master主服务器下线， Master主服务器的客观下线状态就会被移除。若 Master主服务器重新向 Sentinel（哨兵）进程发送 PING 命令返回有效回复，Master主服务器的主观下线状态就会被移除。\n哨兵模式的优缺点\n\n**优点:**\n\n- 哨兵集群模式是基于主从模式的，所有主从的优点，哨兵模式同样具有。\n- 主从可以切换，故障可以转移，系统可用性更好。\n- 哨兵模式是主从模式的升级，系统更健壮，可用性更高。\n\n**缺点:**\n\n- Redis较难支持在线扩容，在集群容量达到上限时在线扩容会变得很复杂。为避免这一问题，运维人员在系统上线时必须确保有足够的空间，这对资源造成了很大的浪费。\n- 配置复杂\n\n`工作原理:`\n\n1. 用户链接时先通过哨兵获取主机Master的信息\n2. 获取Master的链接后实现redis的操作(set/get)\n3. 当master出现宕机时,哨兵的心跳检测发现主机长时间没有响应.这时哨兵会进行推选.推选出新的主机完成任务.\n4. 当新的主机出现时,其余的全部机器都充当该主机的从机\n\n> 这就有一个问题，就是添加哨兵以后，所有的请求都会经过哨兵询问当前的主服务器是谁，所以如果哨兵部在主服务器上面的话可能会增加服务器的压力，所以最好是将哨兵单独放在一个服务器上面。以分解压力。\n\n\n\t然后可能还有人担心哨兵服务器宕机了怎么办啊，首先哨兵服务器宕机的可能性很小，然后是如果哨兵服务器宕机了，使用人工干预重启即可，就会导致主从服务器监控的暂时不可用，不影响主从服务器的正常运行。\n\n\n先配置服务器（本地）哨兵模式，直接从redis官网下载安装或者解压版，安装后的目录结构\n\n\n![](https://images2018.cnblogs.com/blog/764785/201806/764785-20180629092443832-206225549.png)\n\n\n然后配置哨兵模式\n\n\n测试采用3个哨兵，1个主redis，2个从redis。\n\n\n复制6份redis.windows.conf文件并重命名如下（开发者可根据自己的开发习惯进行重命名）\n\n\n![](https://images2018.cnblogs.com/blog/764785/201806/764785-20180629092420294-1751487154.png)\n\n\n**配置master.6378.conf**\n\n\n```text\nport:6379\n\n\n\n#设置连接密码\n\n\n\nrequirepass:grs\n\n\n\n#连接密码\n\n\n\nmasterauth:grs\n```\n\n\n**slave.6380.conf配置**\n\n\n```text\nport:6380\n\n\n\ndbfilename dump6380.rdb\n\n\n\n#配置master\n\n\n\nslaveof 127.0.0.1 6379\n```\n\n\n**slave.6381.conf配置**\n\n\n```text\nport 6381\n\n\n\nslaveof 127.0.0.1 6379\n\n\n\ndbfilename \"dump.rdb\"\n```\n\n\n**配置哨兵sentinel.63791.conf（其他两个哨兵配置文件一致，只修改端口号码即可）**\n\n\n```text\nport 63791\n\n\n\n#主master，2个sentinel选举成功后才有效，这里的master-1是名称，在整合的时候需要一致，这里可以随便更改\n\n\n\nsentinel monitor master-1 127.0.0.1 6379 2\n\n\n\n#判断主master的挂机时间（毫秒），超时未返回正确信息后标记为sdown状态\n\n\n\nsentinel down-after-milliseconds master-1 5000\n\n\n\n#若sentinel在该配置值内未能完成failover操作（即故障时master/slave自动切换），则认为本次failover失败。\n\n\n\nsentinel failover-timeout master-1 18000\n\n\n\n#选项指定了在执行故障转移时， 最多可以有多少个从服务器同时对新的主服务器进行同步，这个数字越小，完成故障转移所需的时间就越长\n\n\n\nsentinel config-epoch master-1 2\n```\n\n\n**需要注意的地方**\n\n\n1、若通过redis-cli -h 127.0.0.1 -p 6379连接，无需改变配置文件，配置文件默认配置为bind 127.0.0.1(只允许127.0.0.1连接访问）若通过redis-cli -h 192.168.180.78 -p 6379连接，需改变配置文件，配置信息为bind 127.0.0.1 192.168.180.78（只允许127.0.0.1和192.168.180.78访问）或者将bind 127.0.0.1注释掉（允许所有远程访问）\n\n\n2、masterauth为所要连接的master服务器的requirepass,如果一个redis集群中有一个master服务器，两个slave服务器，当master服务器挂掉时，sentinel哨兵会随机选择一个slave服务器充当master服务器，鉴于这种机制，解决办法是将所有的主从服务器的requirepass和masterauth都设置为一样。\n\n\n3、sentinel monitor master-1 127.0.0.1 6379 2 行尾最后的一个2代表什么意思呢？我们知道，网络是不可靠的，有时候一个sentinel会因为网络堵塞而误以为一个master redis已经死掉了，当sentinel集群式，解决这个问题的方法就变得很简单，只需要多个sentinel互相沟通来确认某个master是否真的死了，这个2代表，当集群中有2个sentinel认为master死了时，才能真正认为该master已经不可用了。（sentinel集群中各个sentinel也有互相通信，通过gossip协议）。\n\n\n**依次启动redis**\n\n\nredis-server master.6379.conf\n\n\n![](https://images2018.cnblogs.com/blog/764785/201806/764785-20180629094553158-1599779576.png)\n\n\nredis-server slave.6380.conf\n\n\n![](https://images2018.cnblogs.com/blog/764785/201806/764785-20180629094725047-214223016.png)\n\n\nredis-server slave.6381.conf\n\n\n![](https://images2018.cnblogs.com/blog/764785/201806/764785-20180629094815575-184254002.png)\n\n\nredis-server sentinel.63791.conf --sentinel（linux:redis-sentinel sentinel.63791.conf）**其他两个哨兵也这样启动**\n\n\n![](https://images2018.cnblogs.com/blog/764785/201806/764785-20180629095126662-1063517024.png)\n\n\n使用客户端查看一下master状态\n\n\n![](https://images2018.cnblogs.com/blog/764785/201806/764785-20180629095541491-1901809546.png)\n\n\n查看一下哨兵状态\n\n\n![](https://images2018.cnblogs.com/blog/764785/201806/764785-20180629095632770-302235804.png)\n\n\n现在就可以在master插入数据，所有的redis服务都可以获取到，slave只能读\n\n\n**整合spring，导入依赖**\n\n\n```text\n\n<dependency>\n        <groupId>redis.clients</groupId>\n        <artifactId>jedis</artifactId>\n        <version>2.8.0</version>\n    </dependency>\n    <!-- spring-redis -->\n    <dependency>\n        <groupId>org.springframework.data</groupId>\n        <artifactId>spring-data-redis</artifactId>\n        <version>1.6.4.RELEASE</version>\n    </dependency>\n<dependency>\n    <groupId>org.apache.commons</groupId>\n    <artifactId>commons-pool2</artifactId>\n    <version>2.4.2</version>\n</dependency>\n\n```\n\n\n**redis.properties**\n\n\n```text\n#redis中心\nredis.host=127.0.0.1\n#redis.host=10.75.202.11\nredis.port=6379\nredis.password=\n#redis.password=123456\nredis.maxTotal=200\nredis.maxIdle=100\nredis.minIdle=8\nredis.maxWaitMillis=100000\nredis.maxActive=300\nredis.testOnBorrow=true\nredis.testOnReturn=true\n#Idle时进行连接扫描\nredis.testWhileIdle=true\n#表示idle object evitor两次扫描之间要sleep的毫秒数\nredis.timeBetweenEvictionRunsMillis=30000\n#表示idle object evitor每次扫描的最多的对象数\nredis.numTestsPerEvictionRun=10\n#表示一个对象至少停留在idle状态的最短时间，然后才能被idle object evitor扫描并驱逐；这一项只有在timeBetweenEvictionRunsMillis大于0时才有意义\nredis.minEvictableIdleTimeMillis=60000\nredis.timeout=100000\n```\n\n\n### **Cluster**\n\n\n> Redis3.0版本之后支持Cluster.\n\n\n**redis cluster的现状**\n\n\n目前redis支持的cluster特性：\n\n\n1):节点自动发现\n\n\n2):slave->master 选举,集群容错\n\n\n3):Hot resharding:在线分片\n\n\n4):进群管理:cluster xxx\n\n\n5):基于配置(nodes-port.conf)的集群管理\n\n\n6):ASK 转向/MOVED 转向机制.\n\n\n**redis cluster 架构**\n\n\n1)redis-cluster架构图\n\n\n![](https://images2015.cnblogs.com/blog/17405/201607/17405-20160729120110388-883077606.jpg)\n\n\n在这个图中，每一个蓝色的圈都代表着一个redis的服务器节点。它们任何两个节点之间都是相互连通的。客户端可以与任何一个节点相连接，然后就可以访问集群中的任何一个节点。对其进行存取和其他操作。\n\n\n**架构细节:**\n\n- 所有的redis节点彼此互联(PING-PONG机制),内部使用二进制协议优化传输速度和带宽.\n- 节点的fail是通过集群中超过半数的节点检测失效时才生效.\n- 客户端与redis节点直连,不需要中间proxy层.客户端不需要连接集群所有节点,连接集群中任何一个可用节点即可\n- redis-cluster把所有的物理节点映射到[0-16383]slot上,cluster 负责维护node<->slot<->value\n\n**redis-cluster选举:容错**\n\n\n![](https://images2015.cnblogs.com/blog/17405/201607/17405-20160729120154169-1347608301.jpg)\n\n- 领着选举过程是集群中所有master参与,如果半数以上master节点与master节点通信超过(cluster-node-timeout),认为当前master节点挂掉.\n- 什么时候整个集群不可用(cluster_state:fail),当集群不可用时,所有对集群的操作做都不可用，收到((error) CLUSTERDOWN The cluster is down)错误\n- 如果集群任意master挂掉,且当前master没有slave.集群进入fail状态,也可以理解成进群的slot映射[0-16383]不完成时进入fail状态.\n- 如果进群超过半数以上master挂掉，无论是否有slave集群进入fail状态.\n\n> 它们之间通过互相的ping-pong判断是否节点可以连接上。如果有一半以上的节点去ping一个节点的时候没有回应，集群就认为这个节点宕机了，然后去连接它的备用节点。\n\n\n\t如果某个节点和所有从节点全部挂掉，我们集群就进入faill状态。还有就是如果有一半以上的主节点宕机，那么我们集群同样进入发力了状态。这就是我们的redis的投票机制\n\n\n**Redis 3.0的集群方案有以下两个问题。**\n\n\n`一个Redis实例具备了“数据存储”和“路由重定向”，完全去中心化的设计。`\n\n\n> 这带来的好处是部署非常简单，直接部署Redis就行，不像Codis有那么多的组件和依赖。但带来的问题是很难对业务进行无痛的升级，如果哪天Redis集群出了什么严重的Bug，就只能回滚整个Redis集群。  \n> 对协议进行了较大的修改，对应的Redis客户端也需要升级。升级Redis客户端后谁能确保没有Bug？而且对于线上已经大规模运行的业务，升级代码中的Redis客户端也是一个很麻烦的事情。  \n> Redis Cluster是Redis 3.0以后才正式推出，时间较晚，目前能证明在大规模生产环境下成功的案例还不是很多，需要时间检验。\n\n\n### **Jedis sharding**\n\n\n> Redis Sharding可以说是在Redis cluster出来之前业界普遍的采用方式，其主要思想是采用hash算法将存储数据的key进行hash散列，这样特定的key会被定为到特定的节点上。\n\n\n**庆幸的是，Java Redis客户端驱动Jedis已支持Redis Sharding功能，即ShardedJedis以及结合缓存池的ShardedJedisPool**\n\n\n`Jedis的Redis Sharding实现具有如下特点：`\n\n1. 采用一致性哈希算法，将key和节点name同时hashing，然后进行映射匹配，采用的算法是MURMUR_HASH。采用一致性哈希而不是采用简单类似哈希求模映射的主要原因是当增加或减少节点时，不会产生由于重新匹配造成的rehashing。一致性哈希只影响相邻节点key分配，影响量小。\n2. 为了避免一致性哈希只影响相邻节点造成节点分配压力，ShardedJedis会对每个Redis节点根据名字(没有，Jedis会赋予缺省名字)会虚拟化出160个虚拟节点进行散列。根据权重weight，也可虚拟化出160倍数的虚拟节点。用虚拟节点做映射匹配，可以在增加或减少Redis节点时，key在各Redis节点移动再分配更均匀，而不是只有相邻节点受影响。\n3. ShardedJedis支持keyTagPattern模式，即抽取key的一部分keyTag做sharding，这样通过合理命名key，可以将一组相关联的key放入同一个Redis节点，这在避免跨节点访问相关数据时很重要。\n\n> 当然，Redis Sharding这种轻量灵活方式必然在集群其它能力方面做出妥协。比如扩容，当想要增加Redis节点时，尽管采用一致性哈希，毕竟还是会有key匹配不到而丢失，这时需要键值迁移。  \n> 作为轻量级客户端sharding，处理Redis键值迁移是不现实的，这就要求应用层面允许Redis中数据丢失或从后端数据库重新加载数据。但有些时候，击穿缓存层，直接访问数据库层，会对系统访问造成很大压力。\n\n\n### **利用中间件代理**\n\n\n中间件的作用是将我们需要存入redis中的数据的key通过一套算法计算得出一个值。然后根据这个值找到对应的redis节点，将这些数据存在这个redis的节点中。\n\n\n常用的中间件有这几种\n\n- Twemproxy\n- Codis\n- nginx\n\n具体用法就不赘述了，可以自行百度。\n\n\n### **总结**\n\n1. 客户端分片（sharding）需要客户端维护分片算法，这是一种静态的分片方案，需要增加或者减少Redis实例的数量，需要手工调整分片的程序。\n2. 利用中间件的情况则会影响到redis的性能，具体看中间件而定，毕竟所有请求都要经过中间件一层过滤\n3. 官方提供方案 （Cluster），现时点成功案例不多。\n\n## **Redis分片**\n\n\nRedis 的分片承担着两个主要目标：\n\n1. 允许使用很多电脑的内存总和来支持更大的数据库。没有分片，你就被局限于单机能支持的内存容量\n2. 允许伸缩计算能力到多核或多服务器，伸缩网络带宽到多服务器或多网络适配器\n范围分片的替代方案是哈希分片(hash partitioning)。这种模式适用于任何键\t哈希槽设置\tkey-->hashcode-->16384\n\n> 在redis的每一个节点上，都有这么两个东西\n\n\n\t一个是插槽（slot）可以理解为是一个可以存储两个数值的一个变量这个变量的取值范围是：0-16383。\n\n\n\t还有一个就是cluster我个人把这个cluster理解为是一个集群管理的插件。\n\n\n\t当我们的存取的key到达的时候，redis会根据crc16的算法得出一个结果，然后把结果对 16384 求余数，这样每个 key 都会对应一个编号在 0-16383 之间的哈希槽，通过这个值，去找到对应的插槽所对应的节点，然后直接自动跳转到这个对应的节点上进行存取操作。\n\n\n\t还有就是因为如果集群的话，是有好多个redis一起工作的，那么，就需要这个集群不是那么容易挂掉，所以呢，理论上就应该给集群中的每个节点至少一个备用的redis服务。这个备用的redis称为从节点（slave）。那么这个集群是如何判断是否有某个节点挂掉了呢？\n\n\n首先要说的是，每一个节点都存有这个集群所有主节点以及从节点的信息。\n\n\n## **Redis持久化**\n\n\n### **RDB**\n\n\n> Redis Database，就是快照snapshots。缺省情况情况下，Redis把数据快照存放在磁盘上的二进制文件中，文件名为dump.rdb。可以配置Redis的持久化策略，例如数据集中每N秒钟有超过M次更新，就将数据写入磁盘；或者你可以手工调用命令SAVE或BGSAVE。\n\n\nRedis是使用fork函数复制一份当前进程(父进程)的副本(子进程) \t子进程开始将数据写到临时RDB文件中 \t当子进程完成写RDB文件，用新文件替换老文件 \t这种方式可以使Redis使用copy-on-write技术。\n\n\n### **AOF**\n\n\n> Append Only File。快照模式并不十分健壮，当系统停止或者无意中Redis被kill掉，最后写入Redis的数据就会丢失。这对某些应用也许不是大问题，但对于要求高可靠性的应用来说，Redis就不是一个合适的选择。Append-only文件模式是另一种选择。可以在配置文件中打开AOF模式  \n> Redis中提供了3中同步策略，即每秒同步、每修改同步和不同步。事实上每秒同步也是异步完成的，其效率也是非常高的，所差的是一旦系统出现宕机现象，那么这一秒钟之内修改的数据将会丢失。\n\n1. appendfsync always每次有数据修改发生时都会写入AOF文件\n2. appendfsync everysec 每秒钟同步一次，该策略为AOF的缺省策略。在性能和持久化方面作了很好的折中\n3. appendfsync no从不同步。高效但是数据不会被持久化。\n\n### **虚拟内存方式**\n\n\n> 当key很小而value很大时，使用VM的效果会比较好，因为这样节约的内存比较大。当key不小时可以考虑使用一些非常方法将很大的key变成很大的value，如可以考虑将key/value组合成一个新value.  \n> vm-max-threads这个参数,可以设置访问swap文件的线程数,设置最好不要超过机器的核数,如果设置为0,那么所有对swap文件的操作都是串行的.可能会造成比较长时间的延迟,但是对数据完整性有很好的保证.用虚拟内存性能也不错。如果数据量很大，可以考虑分布式或者其他数据库\n\n\nredis.windows.conf\ndaemonize no默认情况下redis不是作为守护进程运行的，如果想让它在后台运行，就把它改成yes。当redis作为守护进程运行的时候，它会写一个pid到/var/run/redis.pid文件里面\n\n\n### **建议**\n\n- 更新频繁: 一致性要求比较高，AOF策略为主\n- 更新不频繁: 可以容忍少量数据丢失或错误，snapshot（快照）策略为主\n\n## **Redis事务**\n\n\nredis事务是通过MULTI，EXEC，DISCARD和WATCH四个原语实现的。\n\n\n```text\nMULTI命令用于开启一个事务，它总是返回OK。\n    MULTI执行之后，客户端可以继续向服务器发送任意多条命令，这些命令不会立即被执行，而是被放到一个队列中，当EXEC命令被调用时，所有队列中的命令才会被执行。\n    另一方面，通过调用DISCARD，客户端可以清空事务队列，并放弃执行事务。\n\nredis事务范围\n  从multi命令开始，到exec或者discard为止，整个操作过程是原子性的，不能打乱顺序，也不能插入操作\n  但是出错之前的操作会正常提交\n\nWATCH 命令可以为 Redis 事务提供 check-and-set （CAS）行为。\n  被 WATCH 的键会被监视，并会发觉这些键是否被改动过了。 如果有至少一个被监视的键在 EXEC 执行之前\n被修改了， 那么整个事务都会被取消， EXEC 返回空多条批量回复（null multi-bulk reply）来表示事务已\n经失败。\n```\n\n\n### **使用Redis实现分布式锁**\n\n\n```text\n1、向Redis中存放固定key的值，如果key不存在则实现存放并获取锁；如果key已经存在则不能获取锁\n      （依靠Redis中的原子操作进行CAS比对，实现锁的互斥）\n2、获取key所对应的时间，时间是锁预期的实效时间，如果已经实效，则存储新值，并获取锁\n3、否则获取锁失败\n\n解锁：\n  删除指定key的redis列\n\n```\n\n\n抢购、秒杀是如今很常见的一个应用场景，主要需要解决的问题有两个：\n\n1. 高并发对数据库产生的压力\n2. 竞争状态下如何解决库存的正确减少（\"超卖\"问题）\n\n对于第一个问题，已经很容易想到用缓存来处理抢购，避免直接操作数据库，例如使用Redis。\n\n\n### **Redis使用watch完成秒杀抢购功能：**\n\n\n使用redis中两个key完成秒杀抢购功能，mywatchkey用于存储抢购数量和mywatchlist用户存储抢购列表。\n\n\n```text\n优点：\n  1、首先选用内存数据库来抢购速度极快\n  2、速度快并发自然没不是问题\n  3、使用悲观锁，会迅速增加系统资源\n  4、比队列强的多，队列会使内存数据库资源瞬间爆棚\n  5、使用乐观锁，达到综合需求\n```\n\n\n## **与关系型数据库的区别**\n\n\n### **数据bai存储方式不同。**\n\n\n关系型和非关系型数据库的主要差异是数据存储的方式。关系型数据天然就是表格式的，因此存储在数据表的行和列中。数据表可以彼此关联协作存储，也很容易提取数据。\n\n\n与其相反，非关系型数据不适合存储在数据表的行和列中，而是大块组合在一起。非关系型数据通常存储在数据集中，就像文档、键值对或者图结构。你的数据及其特性是选择数据存储和提取方式的首要影响因素。\n\n\n### **扩展方式不同。**\n\n\nSQL和NoSQL数据库最大的差别可能是在扩展方式上，要支持日益增长的需求当然要扩展。\n\n\n要支持更多并发量，SQL数据库是纵向扩展，也就是说提高处理能力，使用速度更快速的计算机，这样处理相同的数据集就更快了。\n\n\n因为数据存储在关系表中，操作的性能瓶颈可能涉及很多个表，这都需要通过提高计算机性能来客服。虽然SQL数据库有很大扩展空间，但最终肯定会达到纵向扩展的上限。而NoSQL数据库是横向扩展的。\n\n\n而非关系型数据存储天然就是分布式的，NoSQL数据库的扩展可以通过给资源池添加更多普通的数据库服务器(节点)来分担负载。\n\n\n### **对事务性的支持不同。**\n\n\n如果数据操作需要高事务性或者复杂数据查询需要控制执行计划，那么传统的SQL数据库从性能和稳定性方面考虑是你的最佳选择。SQL数据库支持对事务原子性细粒度控制，并且易于回滚事务。\n\n\n虽然NoSQL数据库也可以使用事务操作，但稳定性方面没法和关系型数据库比较，所以它们真正闪亮的价值是在操作的扩展性和大数据量处理方面。\n\n\n### **关系型**\n\n\n**优点：**\n\n1. 易于维护：都是使用表结构，格式一致；\n2. 使用方便：SQL语言通用，可用于复杂查询；\n3. 复杂操作：支持SQL，可用于一个表以及多个表之间非常复杂的查询。\n\n**缺点：**\n\n1. 读写性能比较差，尤其是海量数据的高效率读写；\n2. 固定的表结构，灵活度稍欠；\n3. 高并发读写需求，传统关系型数据库来说，硬盘I/O是一个很大的瓶颈。\n\n### **非关系型**\n\n\n非关系型数据库严格上不是一种数据库，应该是一种数据结构化存储方法的集合，可以是文档或者键值对等。\n\n\n**优点：**\n\n1. 格式灵活：存储数据的格式可以是key,value形式、文档形式、图片形式等等，文档形式、图片形式等等，使用灵活，应用场景广泛，而关系型数据库则只支持基础类型。\n2. 速度快：nosql可以使用硬盘或者随机存储器作为载体，而关系型数据库只能使用硬盘；\n3. 高扩展性；\n4. 成本低：nosql数据库部署简单，基本都是开源软件。\n\n**缺点：**\n\n1. 不提供sql支持，学习和使用成本较高；\n2. 无事务处理；\n3. 数据结构相对复杂，复杂查询方面稍欠。\n",
      "properties": {
        "password": "",
        "icon": "",
        "date": "2023-04-15",
        "type": "Post",
        "category": "技术分享",
        "urlname": "redis-01",
        "catalog": [
          "archives"
        ],
        "tags": [
          "开发",
          "微服务",
          "分布式",
          "Redis",
          "缓存"
        ],
        "summary": "Redis是一种高性能的key-value存储系统，具有以下几个特点：\n1. 内存存储：Redis将数据存储在内存中，因此读写速度非常快，通常比基于磁盘的存储系统快几个数量级。\n2. 数据结构多样：Redis支持多种数据结构，包括字符串、哈希、列表、集合、有序集合等，丰富的数据结构使Redis可以支持更多的应用场景。\n3. 持久化：Redis支持将数据持久化到磁盘上，以保证数据的可靠性。\n4. 高并发性：Redis采用单线程模型，减少了多线程之间的竞争，从而使得Redis具有更高的并发性。\n5. 分布式：Redis提供了集群模式，可以将数据分布到不同的节点上，从而实现水平扩展。",
        "sort": "",
        "title": "Redis 入手知识点",
        "status": "Published",
        "updated": "2023-10-08 14:42:00"
      },
      "catalog": [
        {
          "title": "archives",
          "doc_id": "387ccffe-e894-442f-9471-2460619d4e62"
        }
      ],
      "body": "",
      "realName": "Redis 入手知识点",
      "relativePath": "/archives/Redis 入手知识点.md"
    },
    {
      "id": "f2eefe59-c1ef-4dba-847d-6525d1ddc518",
      "doc_id": "f2eefe59-c1ef-4dba-847d-6525d1ddc518",
      "title": "f2eefe59-c1ef-4dba-847d-6525d1ddc518",
      "updated": 1696747320000,
      "body_original": "\n## JMM\n\n\n### 内存模型\n\n\n在 JDK1.2 之前，Java 的内存模型实现总是从**主存**（即共享内存）读取变量，是不需要进行特别的注意的。而在当前的 Java 内存模型下，线程可以把变量保存**本地内存**（比如机器的寄存器）中，而不是直接在主存中进行读写。这就可能造成一个线程在主存中修改了一个变量的值，而另外一个线程还继续使用它在寄存器中的变量值的拷贝，造成**数据的不一致**。\n\n\n![](https://blog-file.hehouhui.cn/202203222203113.png)\n\n\n要解决这个问题，就需要把变量声明为**`volatile`**，这就指示 JVM，这个变量是共享且不稳定的，每次使用它都到主存中进行读取。\n\n\n所以，**`volatile`** **关键字 除了防止 JVM 的指令重排 ，还有一个重要的作用就是保证变量的可见性。**\n\n\n![](https://blog-file.hehouhui.cn/202203222203555.png)\n\n\n## JVM\n\n\n> 对于 Java 程序员来说，在虚拟机自动内存管理机制下，不再需要像 C/C++程序开发程序员这样为每一个 new 操作去写对应的 delete/free 操作，不容易出现内存泄漏和内存溢出问题。正是因为 Java 程序员把内存控制权利交给 Java 虚拟机，一旦出现内存泄漏和溢出方面的问题，如果不了解虚拟机是怎样使用内存的，那么排查错误将会是一个非常艰巨的任务。\n\n\n### 基本问题\n\n- **介绍下 Java 内存区域（运行时数据区）**\n- **Java 对象的创建过程（五步，建议能默写出来并且要知道每一步虚拟机做了什么）**\n- **对象的访问定位的两种方式（句柄和直接指针两种方式）**\n\n**其他问题**\n\n- **String 类和常量池**\n- **8 种基本类型的包装类和常量池**\n\n### 运行时内存区\n\n\nava 虚拟机在执行 Java 程序的过程中会把它管理的内存划分成若干个不同的数据区域。JDK. 1.8 和之前的版本略有不同，下面会介绍到。\n\n\n**JDK 1.8 之前：**\n\n\n![](https://blog-file.hehouhui.cn/202203222203825.png)\n\n\n**JDK 1.8 ：**\n\n\n![](https://blog-file.hehouhui.cn/202203222129191.png)\n\n\n**线程私有的：**\n\n- 程序计数器\n- 虚拟机栈\n- 本地方法栈\n\n**线程共享的：**\n\n- 堆\n- 方法区\n- 直接内存 (非运行时数据区的一部分)\n\n### 程序计数器\n\n\n程序计数器是一块较小的内存空间，可以看作是当前线程所执行的字节码的行号指示器。**字节码解释器工作时通过改变这个计数器的值来选取下一条需要执行的字节码指令，分支、循环、跳转、异常处理、线程恢复等功能都需要依赖这个计数器来完成。**\n\n\n另外，**为了线程切换后能恢复到正确的执行位置，每条线程都需要有一个独立的程序计数器，各线程之间计数器互不影响，独立存储，我们称这类内存区域为“线程私有”的内存。**\n\n\n**从上面的介绍中我们知道程序计数器主要有两个作用：**\n\n1. 字节码解释器通过改变程序计数器来依次读取指令，从而实现代码的流程控制，如：顺序执行、选择、循环、异常处理。\n2. 在多线程的情况下，程序计数器用于记录当前线程执行的位置，从而当线程被切换回来的时候能够知道该线程上次运行到哪儿了。\n\n**注意：程序计数器是唯一一个不会出现** **`OutOfMemoryError`** **的内存区域，它的生命周期随着线程的创建而创建，随着线程的结束而死亡。**\n\n\n### 虚拟机栈\n\n\n**与程序计数器一样，Java 虚拟机栈也是线程私有的，它的生命周期和线程相同，描述的是 Java 方法执行的内存模型，每次方法调用的数据都是通过栈传递的。**\n\n\n**Java 内存可以粗糙的区分为堆内存（Heap）和栈内存 (Stack),其中栈就是现在说的虚拟机栈，或者说是虚拟机栈中局部变量表部分。** （实际上，Java 虚拟机栈是由一个个栈帧组成，而每个栈帧中都拥有：局部变量表、操作数栈、动态链接、方法出口信息。）\n\n\n**局部变量表主要存放了编译期可知的各种数据类型**（boolean、byte、char、short、int、float、long、double）、**对象引用**（reference 类型，它不同于对象本身，可能是一个指向对象起始地址的引用指针，也可能是指向一个代表对象的句柄或其他与此对象相关的位置）。\n\n\n**Java 虚拟机栈会出现两种错误：****`StackOverFlowError`** **和** **`OutOfMemoryError`****。**\n\n- **`StackOverFlowError`****：** 若 Java 虚拟机栈的内存大小不允许动态扩展，那么当线程请求栈的深度超过当前 Java 虚拟机栈的最大深度的时候，就抛出 StackOverFlowError 错误。\n- **`OutOfMemoryError`****：** 若 Java 虚拟机堆中没有空闲内存，并且垃圾回收器也无法提供更多内存的话。就会抛出 OutOfMemoryError 错误。\n\nJava 虚拟机栈也是线程私有的，每个线程都有各自的 Java 虚拟机栈，而且随着线程的创建而创建，随着线程的死亡而死亡。\n\n\n**扩展：那么方法/函数如何调用？**\n\n\nJava 栈可用类比数据结构中栈，Java 栈中保存的主要内容是栈帧，每一次函数调用都会有一个对应的栈帧被压入 Java 栈，每一个函数调用结束后，都会有一个栈帧被弹出。\n\n\nJava 方法有两种返回方式：\n\n1. return 语句。\n2. 抛出异常。\n\n不管哪种返回方式都会导致栈帧被弹出。\n\n\n### 本地方法栈\n\n\n和虚拟机栈所发挥的作用非常相似，区别是： **虚拟机栈为虚拟机执行 Java 方法 （也就是字节码）服务，而本地方法栈则为虚拟机使用到的 Native 方法服务。** 在 HotSpot 虚拟机中和 Java 虚拟机栈合二为一。\n\n\n本地方法被执行的时候，在本地方法栈也会创建一个栈帧，用于存放该本地方法的局部变量表、操作数栈、动态链接、出口信息。\n\n\n方法执行完毕后相应的栈帧也会出栈并释放内存空间，也会出现 `StackOverFlowError` 和 `OutOfMemoryError` 两种错误。\n\n\n### 堆\n\n\nJava 虚拟机所管理的内存中最大的一块，Java 堆是所有线程共享的一块内存区域，在虚拟机启动时创建。**此内存区域的唯一目的就是存放对象实例，几乎所有的对象实例以及数组都在这里分配内存。**\n\n\n**Java世界中“几乎”所有的对象都在堆中分配，但是，随着JIT编译期的发展与逃逸分析技术逐渐成熟，栈上分配、标量替换优化技术将会导致一些微妙的变化，所有的对象都分配到堆上也渐渐变得不那么“绝对”了。从jdk 1.7开始已经默认开启逃逸分析，如果某些方法中的对象引用没有被返回或者未被外面使用（也就是未逃逸出去），那么对象可以直接在栈上分配内存。**\n\n\nJava 堆是垃圾收集器管理的主要区域，因此也被称作**GC 堆（Garbage Collected Heap）**.从垃圾回收的角度，由于现在收集器基本都采用分代垃圾收集算法，所以 Java 堆还可以细分为：新生代和老年代：再细致一点有：Eden 空间、From Survivor、To Survivor 空间等。**进一步划分的目的是更好地回收内存，或者更快地分配内存。**\n\n\n在 JDK 7 版本及JDK 7 版本之前，堆内存被通常被分为下面三部分：\n\n1. 新生代内存(Young Generation)\n2. 老生代(Old Generation)\n3. 永生代(Permanent Generation)\n\n![](https://blog-file.hehouhui.cn/202203222203705.png)\n\n\n![](https://blog-file.hehouhui.cn/202203222203389.png)\n\n\nJDK 8 版本之后方法区（HotSpot 的永久代）被彻底移除了（JDK1.7 就已经开始了），取而代之是元空间，元空间使用的是直接内存。\n\n\n![](https://blog-file.hehouhui.cn/202203222203514.png)\n\n\n**上图所示的 Eden 区、两个 Survivor 区都属于新生代（为了区分，这两个 Survivor 区域按照顺序被命名为 from 和 to），中间一层属于老年代。**\n\n\n大部分情况，对象都会首先在 Eden 区域分配，在一次新生代垃圾回收后，如果对象还存活，则会进入 s0 或者 s1，并且对象的年龄还会加 1(Eden 区->Survivor 区后对象的初始年龄变为 1)，当它的年龄增加到一定程度（默认为 15 岁），就会被晋升到老年代中。对象晋升到老年代的年龄阈值，可以通过参数 `-XX:MaxTenuringThreshold` 来设置。\n\n\n> 修正（issue552）：“Hotspot遍历所有对象时，按照年龄从小到大对其所占用的大小进行累积，当累积的某个年龄大小超过了survivor区的一半时，取这个年龄和MaxTenuringThreshold中更小的一个值，作为新的晋升年龄阈值”。\n\n\n\t**动态年龄计算的代码如下**\n\n\n\t```text\n\tuint ageTable::compute_tenuring_threshold(size_t survivor_capacity) {\n\t//survivor_capacity是survivor空间的大小\n\tsize_t desired_survivor_size = (size_t)((((double) survivor_capacity)*TargetSurvivorRatio)/100);\n\tsize_t total = 0;\n\tuint age = 1;\n\twhile (age < table_size) {\n\ttotal += sizes[age];//sizes数组是每个年龄段对象大小\n\tif (total > desired_survivor_size) break;\n\tage++;\n\t}\n\tuint result = age < MaxTenuringThreshold ? age : MaxTenuringThreshold;\n\t...\n\t}\n\tCopy to clipboardErrorCopied\n\t\n\t```\n\n\n堆这里最容易出现的就是 OutOfMemoryError 错误，并且出现这种错误之后的表现形式还会有几种，比如：\n\n1. **`OutOfMemoryError: GC Overhead Limit Exceeded`** ： 当JVM花太多时间执行垃圾回收并且只能回收很少的堆空间时，就会发生此错误。\n2. **`java.lang.OutOfMemoryError: Java heap space`** :假如在创建新的对象时, 堆内存中的空间不足以存放新创建的对象, 就会引发`java.lang.OutOfMemoryError: Java heap space` 错误。(和本机物理内存无关，和你配置的内存大小有关！)\n3. ......\n\n### 方法区\n\n\n方法区与 Java 堆一样，是各个线程共享的内存区域，它用于存储已被虚拟机加载的类信息、常量、静态变量、即时编译器编译后的代码等数据。虽然 **Java 虚拟机规范把方法区描述为堆的一个逻辑部分**，但是它却有一个别名叫做 **Non-Heap（非堆）**，目的应该是与 Java 堆区分开来。\n\n\n方法区也被称为永久代。很多人都会分不清方法区和永久代的关系，为此我也查阅了文献。\n\n\n**方法区和永生代的关系**\n\n\n> 《Java 虚拟机规范》只是规定了有方法区这么个概念和它的作用，并没有规定如何去实现它。那么，在不同的 JVM 上方法区的实现肯定是不同的了。 方法区和永久代的关系很像 Java 中接口和类的关系，类实现了接口，而永久代就是 HotSpot 虚拟机对虚拟机规范中方法区的一种实现方式。 也就是说，永久代是 HotSpot 的概念，方法区是 Java 虚拟机规范中的定义，是一种规范，而永久代是一种实现，一个是标准一个是实现，其他的虚拟机实现并没有永久代这一说法。\n\n\n**参数**\n\n\nJDK 1.8 之前永久代还没被彻底移除的时候通常通过下面这些参数来调节方法区大小\n\n\n```text\n-XX:PermSize=N //方法区 (永久代) 初始大小\n-XX:MaxPermSize=N //方法区 (永久代) 最大大小,超过这个值将会抛出 OutOfMemoryError 异常:java.lang.OutOfMemoryError: PermGenCopy to clipboardErrorCopied\n\n```\n\n\n相对而言，垃圾收集行为在这个区域是比较少出现的，但并非数据进入方法区后就“永久存在”了。\n\n\nJDK 1.8 的时候，方法区（HotSpot 的永久代）被彻底移除了（JDK1.7 就已经开始了），取而代之是元空间，元空间使用的是直接内存。\n\n\n下面是一些常用参数：\n\n\n```text\n-XX:MetaspaceSize=N //设置 Metaspace 的初始（和最小大小）\n-XX:MaxMetaspaceSize=N //设置 Metaspace 的最大大小Copy to clipboardErrorCopied\n\n```\n\n\n与永久代很大的不同就是，如果不指定大小的话，随着更多类的创建，虚拟机会耗尽所有可用的系统内存。\n\n\n### 元空间\n\n1. 整个永久代有一个 JVM 本身设置固定大小上限，无法进行调整，而元空间使用的是直接内存，受本机可用内存的限制，虽然元空间仍旧可能溢出，但是比原来出现的几率会更小。\n\n\t> 当你元空间溢出时会得到如下错误： java.lang.OutOfMemoryError: MetaSpace\n\n\n你可以使用 `-XX：MaxMetaspaceSize` 标志设置最大元空间大小，默认值为 unlimited，这意味着它只受系统内存的限制。`-XX：MetaspaceSize` 调整标志定义元空间的初始大小如果未指定此标志，则 Metaspace 将根据运行时的应用程序需求动态地重新调整大小。\n\n1. 元空间里面存放的是类的元数据，这样加载多少类的元数据就不由 `MaxPermSize` 控制了, 而由系统的实际可用空间来控制，这样能加载的类就更多了。\n2. 在 JDK8，合并 HotSpot 和 JRockit 的代码时, JRockit 从来没有一个叫永久代的东西, 合并之后就没有必要额外的设置这么一个永久代的地方了。\n\n### 运行时常量池\n\n\n运行时常量池是方法区的一部分。Class 文件中除了有类的版本、字段、方法、接口等描述信息外，还有常量池表（用于存放编译期生成的各种字面量和符号引用）\n\n\n既然运行时常量池是方法区的一部分，自然受到方法区内存的限制，当常量池无法再申请到内存时会抛出 OutOfMemoryError 错误。\n\n\n~~**JDK1.7 及之后版本的 JVM 已经将运行时常量池从方法区中移了出来，在 Java 堆（Heap）中开辟了一块区域存放运行时常量池。**~~\n\n\n> 修正(issue747，reference)：\n\n\t1. **JDK1.7之前运行时常量池逻辑包含字符串常量池存放在方法区, 此时hotspot虚拟机对方法区的实现为永久代**\n\t2. **JDK1.7 字符串常量池被从方法区拿到了堆中, 这里没有提到运行时常量池,也就是说字符串常量池被单独拿到堆,运行时常量池剩下的东西还在方法区, 也就是hotspot中的永久代** 。\n\t3. **JDK1.8 hotspot移除了永久代用元空间(Metaspace)取而代之, 这时候字符串常量池还在堆, 运行时常量池还在方法区, 只不过方法区的实现从永久代变成了元空间(Metaspace)**\n\n相关问题：JVM 常量池中存储的是对象还是引用呢？： [https://www.zhihu.com/question/57109429/answer/151717241](https://www.zhihu.com/question/57109429/answer/151717241) by RednaxelaFX\n\n\n**特别注意⚠️**\n\n\n**Java 基本类型的包装类的大部分都实现了常量池技术，即 Byte,Short,Integer,Long,Character,Boolean；前面 4 种包装类默认创建了数值[-128，127] 的相应类型的缓存数据，Character创建了数值在[0,127]范围的缓存数据，Boolean 直接返回True Or False。如果超出对应范围仍然会去创建新的对象。** 为啥把缓存设置为[-128，127]区间？（[参见issue/461](https://github.com/Snailclimb/JavaGuide/issues/461)）性能和资源之间的权衡。\n\n\n```text\npublic static Boolean valueOf(boolean b) {\n    return (b ? TRUE : FALSE);\n}Copy to clipboardErrorCopied\nprivate static class CharacterCache {\n    private CharacterCache(){}\n\n    static final Character cache[] = new Character[127 + 1];\n    static {\n        for (int i = 0; i < cache.length; i++)\n            cache[i] = new Character((char)i);\n    }\n}Copy to clipboardErrorCopied\n\n```\n\n\n两种浮点数类型的包装类 Float,Double 并没有实现常量池技术。\n\n\n```text\n        Integer i1 = 33;\n        Integer i2 = 33;\n        System.out.println(i1 == i2);// 输出 true\n        Integer i11 = 333;\n        Integer i22 = 333;\n        System.out.println(i11 == i22);// 输出 false\n        Double i3 = 1.2;\n        Double i4 = 1.2;\n        System.out.println(i3 == i4);// 输出 falseCopy to clipboardErrorCopied\n\n```\n\n\n**Integer 缓存源代码：**\n\n\n```text\n/**\n*此方法将始终缓存-128 到 127（包括端点）范围内的值，并可以缓存此范围之外的其他值。\n*/\n    public static Integer valueOf(int i) {\n        if (i >= IntegerCache.low && i <= IntegerCache.high)\n            return IntegerCache.cache[i + (-IntegerCache.low)];\n        return new Integer(i);\n    }\nCopy to clipboardErrorCopied\n\n```\n\n\n**应用场景：**\n\n1. Integer i1=40；Java 在编译的时候会直接将代码封装成 Integer i1=Integer.valueOf(40);，从而使用常量池中的对象。\n2. Integer i1 = new Integer(40);这种情况下会创建新的对象。\n\n```text\n  Integer i1 = 40;\n  Integer i2 = new Integer(40);\n  System.out.println(i1==i2);//输出 falseCopy to clipboardErrorCopied\n\n```\n\n\n**Integer 比较更丰富的一个例子:**\n\n\n```text\n  Integer i1 = 40;\n  Integer i2 = 40;\n  Integer i3 = 0;\n  Integer i4 = new Integer(40);\n  Integer i5 = new Integer(40);\n  Integer i6 = new Integer(0);\n\n  System.out.println(\"i1=i2   \" + (i1 == i2));\n  System.out.println(\"i1=i2+i3   \" + (i1 == i2 + i3));\n  System.out.println(\"i1=i4   \" + (i1 == i4));\n  System.out.println(\"i4=i5   \" + (i4 == i5));\n  System.out.println(\"i4=i5+i6   \" + (i4 == i5 + i6));\n  System.out.println(\"40=i5+i6   \" + (40 == i5 + i6));     Copy to clipboardErrorCopied\n\n```\n\n\n结果：\n\n\n```text\ni1=i2   true\ni1=i2+i3   true\ni1=i4   false\ni4=i5   false\ni4=i5+i6   true\n40=i5+i6   trueCopy to clipboardErrorCopied\n\n```\n\n\n解释：\n\n\n语句 i4 == i5 + i6，因为+这个操作符不适用于 Integer 对象，首先 i5 和 i6 进行自动拆箱操作，进行数值相加，即 i4 == 40。然后 Integer 对象无法与数值进行直接比较，所以 i4 自动拆箱转为 int 值 40，最终这条语句转为 40 == 40 进行数值比较。\n\n\n### 直接内存\n\n\n**直接内存并不是虚拟机运行时数据区的一部分，也不是虚拟机规范中定义的内存区域，但是这部分内存也被频繁地使用。而且也可能导致 OutOfMemoryError 错误出现。**\n\n\nJDK1.4 中新加入的 **NIO(New Input/Output) 类**，引入了一种基于**通道（Channel）** 与**缓存区（Buffer）** 的 I/O 方式，它可以直接使用 Native 函数库直接分配堆外内存，然后通过一个存储在 Java 堆中的 DirectByteBuffer 对象作为这块内存的引用进行操作。这样就能在一些场景中显著提高性能，因为**避免了在 Java 堆和 Native 堆之间来回复制数据**。\n\n\n本机直接内存的分配不会受到 Java 堆的限制，但是，既然是内存就会受到本机总内存大小以及处理器寻址空间的限制。\n\n\n### 垃圾回收\n\n\nJava 的自动内存管理主要是针对对象内存的回收和对象内存的分配。同时，Java 自动内存管理最核心的功能是 **堆** 内存中对象的分配与回收。\n\n\nJava 堆是垃圾收集器管理的主要区域，因此也被称作**GC 堆（Garbage Collected Heap）**.从垃圾回收的角度，由于现在收集器基本都采用分代垃圾收集算法，所以 Java 堆还可以细分为：新生代和老年代：再细致一点有：Eden 空间、From Survivor、To Survivor 空间等。**进一步划分的目的是更好地回收内存，或者更快地分配内存。**\n\n\n**堆空间的基本结构：**\n\n\n![](https://snailclimb.gitee.io/javaguide/docs/java/jvm/pictures/jvm%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6/01d330d8-2710-4fad-a91c-7bbbfaaefc0e.png)\n\n\n上图所示的 Eden 区、From Survivor0(\"From\") 区、To Survivor1(\"To\") 区都属于新生代，Old Memory 区属于老年代。\n\n\n大部分情况，对象都会首先在 Eden 区域分配，在一次新生代垃圾回收后，如果对象还存活，则会进入 s0 或者 s1，并且对象的年龄还会加 1(Eden 区->Survivor 区后对象的初始年龄变为 1)，当它的年龄增加到一定程度（默认为 15 岁），就会被晋升到老年代中。对象晋升到老年代的年龄阈值，可以通过参数 `-XX:MaxTenuringThreshold` 来设置。\n\n\n> 修正（issue552）：“Hotspot 遍历所有对象时，按照年龄从小到大对其所占用的大小进行累积，当累积的某个年龄大小超过了 survivor 区的一半时，取这个年龄和 MaxTenuringThreshold 中更小的一个值，作为新的晋升年龄阈值”。\n\n\n\t**动态年龄计算的代码如下**\n\n\n\t```text\n\tuint ageTable::compute_tenuring_threshold(size_t survivor_capacity) {\n\t//survivor_capacity是survivor空间的大小\n\tsize_t desired_survivor_size = (size_t)((((double)survivor_capacity)*TargetSurvivorRatio)/100);\n\tsize_t total = 0;\n\tuint age = 1;\n\twhile (age < table_size) {\n\t  //sizes数组是每个年龄段对象大小\n\t  total += sizes[age];\n\t  if (total > desired_survivor_size) {\n\t      break;\n\t  }\n\t  age++;\n\t}\n\tuint result = age < MaxTenuringThreshold ? age : MaxTenuringThreshold;\n\t...\n\t}\n\tCopy to clipboardErrorCopied\n\t\n\t```\n\n\n经过这次 GC 后，Eden 区和\"From\"区已经被清空。这个时候，\"From\"和\"To\"会交换他们的角色，也就是新的\"To\"就是上次 GC 前的“From”，新的\"From\"就是上次 GC 前的\"To\"。不管怎样，都会保证名为 To 的 Survivor 区域是空的。Minor GC 会一直重复这样的过程，直到“To”区被填满，\"To\"区被填满之后，会将所有对象移动到老年代中。\n\n\n![](https://blog-file.hehouhui.cn/202203222203744.png)\n\n\n![](https://blog-file.hehouhui.cn/202203222203096.png)\n\n\n![](https://blog-file.hehouhui.cn/202203222203366.png)\n\n\n![](https://blog-file.hehouhui.cn/202203222203478.png)\n\n\n> 当需要排查各种内存溢出问题、当垃圾收集成为系统达到更高并发的瓶颈时，我们就需要对这些“自动化”的技术实施必要的监控和调节。\n\n\n问题答案在文中都有提到\n\n- 如何判断对象是否死亡（两种方法）。\n- 简单的介绍一下强引用、软引用、弱引用、虚引用（虚引用与软引用和弱引用的区别、使用软引用能带来的好处）。\n- 如何判断一个常量是废弃常量\n- 如何判断一个类是无用的类\n- 垃圾收集有哪些算法，各自的特点？\n- HotSpot 为什么要分为新生代和老年代？\n- 常见的垃圾回收器有哪些？\n- 介绍一下 CMS,G1 收集器。\n- Minor Gc 和 Full GC 有什么不同呢？\n\n### 内存分配与回收机制\n\n\n### 对象优先在eden区分配\n\n\n目前主流的垃圾收集器都会采用分代回收算法，因此需要将堆内存分为新生代和老年代，这样我们就可以根据各个年代的特点选择合适的垃圾收集算法。\n\n\n大多数情况下，对象在新生代中 eden 区分配。当 eden 区没有足够空间进行分配时，虚拟机将发起一次 Minor GC.下面我们来进行实际测试以下。\n\n\n**测试：**\n\n\n```text\npublic class GCTest {\n\n    public static void main(String[] args) {\n        byte[] allocation1, allocation2;\n        allocation1 = new byte[30900*1024];\n        //allocation2 = new byte[900*1024];\n    }\n}Copy to clipboardErrorCopied\n\n```\n\n\n通过以下方式运行：\n\n\n![](https://blog-file.hehouhui.cn/202203222136423.png)\n\n\n添加的参数：\n\n\n```text\n-XX:+PrintGCDetails\n```\n\n\n![](https://snailclimb.gitee.io/javaguide/docs/java/jvm/pictures/jvm%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6/10317146.png)\n\n\n运行结果 (红色字体描述有误，应该是对应于 JDK1.7 的永久代)：\n\n\n![](https://blog-file.hehouhui.cn/202203222203083.jpg)\n\n\n从上图我们可以看出 eden 区内存几乎已经被分配完全（即使程序什么也不做，新生代也会使用 2000 多 k 内存）。假如我们再为 allocation2 分配内存会出现什么情况呢？\n\n\n```text\nallocation2 = new byte[900*1024];Copy to clipboardErrorCopied\n\n```\n\n\n![](https://blog-file.hehouhui.cn/202203222203234.png)\n\n\n**简单解释一下为什么会出现这种情况：** 因为给 allocation2 分配内存的时候 eden 区内存几乎已经被分配完了，我们刚刚讲了当 Eden 区没有足够空间进行分配时，虚拟机将发起一次 Minor GC.GC 期间虚拟机又发现 allocation1 无法存入 Survivor 空间，所以只好通过 **分配担保机制** 把新生代的对象提前转移到老年代中去，老年代上的空间足够存放 allocation1，所以不会出现 Full GC。执行 Minor GC 后，后面分配的对象如果能够存在 eden 区的话，还是会在 eden 区分配内存。可以执行如下代码验证：\n\n\n```text\npublic class GCTest {\n\n    public static void main(String[] args) {\n        byte[] allocation1, allocation2,allocation3,allocation4,allocation5;\n        allocation1 = new byte[32000*1024];\n        allocation2 = new byte[1000*1024];\n        allocation3 = new byte[1000*1024];\n        allocation4 = new byte[1000*1024];\n        allocation5 = new byte[1000*1024];\n    }\n}\nCopy to clipboardErrorCopied\n\n```\n\n\n### 大对象直接进入老年代\n\n\n> 大对象就是需要大量连续内存空间的对象（比如：字符串、数组）。\n\n\n**为什么要这样呢？**\n\n\n为了避免为大对象分配内存时由于分配担保机制带来的复制而降低效率\n\n\n### 长期存活的对象进入老年代\n\n\n既然虚拟机采用了分代收集的思想来管理内存，那么内存回收时就必须能识别哪些对象应放在新生代，哪些对象应放在老年代中。为了做到这一点，虚拟机给每个对象一个对象年龄（Age）计数器。\n\n\n如果对象在 Eden 出生并经过第一次 Minor GC 后仍然能够存活，并且能被 Survivor 容纳的话，将被移动到 Survivor 空间中，并将对象年龄设为 1.对象在 Survivor 中每熬过一次 MinorGC,年龄就增加 1 岁，当它的年龄增加到一定程度（默认为 15 岁），就会被晋升到老年代中。对象晋升到老年代的年龄阈值，可以通过参数 `-XX:MaxTenuringThreshold` 来设置。\n\n\n### 动态判断对象年龄\n\n\n大部分情况，对象都会首先在 Eden 区域分配，在一次新生代垃圾回收后，如果对象还存活，则会进入 s0 或者 s1，并且对象的年龄还会加 1(Eden 区->Survivor 区后对象的初始年龄变为 1)，当它的年龄增加到一定程度（默认为 15 岁），就会被晋升到老年代中。对象晋升到老年代的年龄阈值，可以通过参数 `-XX:MaxTenuringThreshold` 来设置。\n\n\n> 修正（issue552）：“Hotspot 遍历所有对象时，按照年龄从小到大对其所占用的大小进行累积，当累积的某个年龄大小超过了 survivor 区的一半时，取这个年龄和 MaxTenuringThreshold 中更小的一个值，作为新的晋升年龄阈值”。\n\n\n\t**动态年龄计算的代码如下**\n\n\n\t```text\n\tuint ageTable::compute_tenuring_threshold(size_t survivor_capacity) {\n\t//survivor_capacity是survivor空间的大小\n\tsize_t desired_survivor_size = (size_t)((((double)survivor_capacity)*TargetSurvivorRatio)/100);\n\tsize_t total = 0;\n\tuint age = 1;\n\twhile (age < table_size) {\n\t  //sizes数组是每个年龄段对象大小\n\t  total += sizes[age];\n\t  if (total > desired_survivor_size) {\n\t      break;\n\t  }\n\t  age++;\n\t}\n\tuint result = age < MaxTenuringThreshold ? age : MaxTenuringThreshold;\n\t...\n\t}\n\tCopy to clipboardErrorCopied\n\t\n\t```\n\n\n\t额外补充说明([issue672](https://github.com/Snailclimb/JavaGuide/issues/672))：**关于默认的晋升年龄是 15，这个说法的来源大部分都是《深入理解 Java 虚拟机》这本书。** 如果你去 Oracle 的官网阅读[相关的虚拟机参数](https://docs.oracle.com/javase/8/docs/technotes/tools/unix/java.html)，你会发现`-XX:MaxTenuringThreshold=threshold`这里有个说明\n\n\n\t**Sets the maximum tenuring threshold for use in adaptive GC sizing. The largest value is 15. The default value is 15 for the parallel (throughput) collector, and 6 for the CMS collector.默认晋升年龄并不都是 15，这个是要区分垃圾收集器的，CMS 就是 6.**\n\n\n### GC区域\n\n\n周志明先生在《深入理解 Java 虚拟机》第二版中 P92 如是写道：\n\n\n> “老年代 GC（Major GC/Full GC），指发生在老年代的 GC……”\n\n\n上面的说法已经在《深入理解 Java 虚拟机》第三版中被改正过来了。感谢 R 大的回答：\n\n\n![](https://blog-file.hehouhui.cn/202203222203561.png)\n\n\n**总结：**\n\n\n针对 HotSpot VM 的实现，它里面的 GC 其实准确分类只有两大种：\n\n\n部分收集 (Partial GC)：\n\n- 新生代收集（Minor GC / Young GC）：只对新生代进行垃圾收集；\n- 老年代收集（Major GC / Old GC）：只对老年代进行垃圾收集。需要注意的是 Major GC 在有的语境中也用于指代整堆收集；\n- 混合收集（Mixed GC）：对整个新生代和部分老年代进行垃圾收集。\n\n整堆收集 (Full GC)：收集整个 Java 堆和方法区。\n\n\n### 对象是否死亡\n\n\n堆中几乎放着所有的对象实例，对堆垃圾回收前的第一步就是要判断哪些对象已经死亡（即不能再被任何途径使用的对象）。\n\n\n![](https://blog-file.hehouhui.cn/202203222203717.png)\n\n\n### 引用计数器\n\n\n给对象中添加一个引用计数器，每当有一个地方引用它，计数器就加 1；当引用失效，计数器就减 1；任何时候计数器为 0 的对象就是不可能再被使用的。\n\n\n**这个方法实现简单，效率高，但是目前主流的虚拟机中并没有选择这个算法来管理内存，其最主要的原因是它很难解决对象之间相互循环引用的问题。** 所谓对象之间的相互引用问题，如下面代码所示：除了对象 objA 和 objB 相互引用着对方之外，这两个对象之间再无任何引用。但是他们因为互相引用对方，导致它们的引用计数器都不为 0，于是引用计数算法无法通知 GC 回收器回收他们。\n\n\n```text\npublic class ReferenceCountingGc {\n    Object instance = null;\n    public static void main(String[] args) {\n        ReferenceCountingGc objA = new ReferenceCountingGc();\n        ReferenceCountingGc objB = new ReferenceCountingGc();\n        objA.instance = objB;\n        objB.instance = objA;\n        objA = null;\n        objB = null;\n\n    }\n}Copy to clipboardErrorCopied\n\n```\n\n\n### 可达性分析\n\n\n这个算法的基本思想就是通过一系列的称为 **“GC Roots”** 的对象作为起点，从这些节点开始向下搜索，节点所走过的路径称为引用链，当一个对象到 GC Roots 没有任何引用链相连的话，则证明此对象是不可用的。\n\n\n![](https://blog-file.hehouhui.cn/202203222203882.png)\n\n\n可作为 GC Roots 的对象包括下面几种:\n\n- 虚拟机栈(栈帧中的本地变量表)中引用的对象\n- 本地方法栈(Native 方法)中引用的对象\n- 方法区中类静态属性引用的对象\n- 方法区中常量引用的对象\n- 所有被同步锁持有的对象\n\n### 引用\n\n\n无论是通过引用计数法判断对象引用数量，还是通过可达性分析法判断对象的引用链是否可达，判定对象的存活都与“引用”有关。\n\n\nJDK1.2 之前，Java 中引用的定义很传统：如果 reference 类型的数据存储的数值代表的是另一块内存的起始地址，就称这块内存代表一个引用。\n\n\nJDK1.2 以后，Java 对引用的概念进行了扩充，将引用分为强引用、软引用、弱引用、虚引用四种（引用强度逐渐减弱）\n\n\n**1．强引用（StrongReference）**\n\n\n以前我们使用的大部分引用实际上都是强引用，这是使用最普遍的引用。如果一个对象具有强引用，那就类似于**必不可少的生活用品**，垃圾回收器绝不会回收它。当内存空间不足，Java 虚拟机宁愿抛出 OutOfMemoryError 错误，使程序异常终止，也不会靠随意回收具有强引用的对象来解决内存不足问题。\n\n\n**2．软引用（SoftReference）**\n\n\n如果一个对象只具有软引用，那就类似于**可有可无的生活用品**。如果内存空间足够，垃圾回收器就不会回收它，如果内存空间不足了，就会回收这些对象的内存。只要垃圾回收器没有回收它，该对象就可以被程序使用。软引用可用来实现内存敏感的高速缓存。\n\n\n软引用可以和一个引用队列（ReferenceQueue）联合使用，如果软引用所引用的对象被垃圾回收，JAVA 虚拟机就会把这个软引用加入到与之关联的引用队列中。\n\n\n**3．弱引用（WeakReference）**\n\n\n如果一个对象只具有弱引用，那就类似于**可有可无的生活用品**。弱引用与软引用的区别在于：只具有弱引用的对象拥有更短暂的生命周期。在垃圾回收器线程扫描它所管辖的内存区域的过程中，一旦发现了只具有弱引用的对象，不管当前内存空间足够与否，都会回收它的内存。不过，由于垃圾回收器是一个优先级很低的线程， 因此不一定会很快发现那些只具有弱引用的对象。\n\n\n弱引用可以和一个引用队列（ReferenceQueue）联合使用，如果弱引用所引用的对象被垃圾回收，Java 虚拟机就会把这个弱引用加入到与之关联的引用队列中。\n\n\n**4．虚引用（PhantomReference）**\n\n\n\"虚引用\"顾名思义，就是形同虚设，与其他几种引用都不同，虚引用并不会决定对象的生命周期。如果一个对象仅持有虚引用，那么它就和没有任何引用一样，在任何时候都可能被垃圾回收。\n\n\n**虚引用主要用来跟踪对象被垃圾回收的活动**。\n\n\n**虚引用与软引用和弱引用的一个区别在于：** 虚引用必须和引用队列（ReferenceQueue）联合使用。当垃圾回收器准备回收一个对象时，如果发现它还有虚引用，就会在回收对象的内存之前，把这个虚引用加入到与之关联的引用队列中。程序可以通过判断引用队列中是否已经加入了虚引用，来了解被引用的对象是否将要被垃圾回收。程序如果发现某个虚引用已经被加入到引用队列，那么就可以在所引用的对象的内存被回收之前采取必要的行动。\n\n\n特别注意，在程序设计中一般很少使用弱引用与虚引用，使用软引用的情况较多，这是因为**软引用可以加速 JVM 对垃圾内存的回收速度，可以维护系统的运行安全，防止内存溢出（OutOfMemory）等问题的产生**。\n\n\n### 不可达对象别非“非死不可“\n\n\n即使在可达性分析法中不可达的对象，也并非是“非死不可”的，这时候它们暂时处于“缓刑阶段”，要真正宣告一个对象死亡，至少要经历两次标记过程；可达性分析法中不可达的对象被第一次标记并且进行一次筛选，筛选的条件是此对象是否有必要执行 finalize 方法。当对象没有覆盖 finalize 方法，或 finalize 方法已经被虚拟机调用过时，虚拟机将这两种情况视为没有必要执行。\n\n\n被判定为需要执行的对象将会被放在一个队列中进行第二次标记，除非这个对象与引用链上的任何一个对象建立关联，否则就会被真的回收。\n\n\n### 如何判断常量是否废弃?\n\n\n运行时常量池主要回收的是废弃的常量。那么，我们如何判断一个常量是废弃常量呢？\n\n\n~~**JDK1.7 及之后版本的 JVM 已经将运行时常量池从方法区中移了出来，在 Java 堆（Heap）中开辟了一块区域存放运行时常量池。**~~\n\n\n> 修正(issue747，reference)：\n\n\t1. **JDK1.7 之前运行时常量池逻辑包含字符串常量池存放在方法区, 此时 hotspot 虚拟机对方法区的实现为永久代**\n\t2. **JDK1.7 字符串常量池被从方法区拿到了堆中, 这里没有提到运行时常量池,也就是说字符串常量池被单独拿到堆,运行时常量池剩下的东西还在方法区, 也就是 hotspot 中的永久代** 。\n\t3. **JDK1.8 hotspot 移除了永久代用元空间(Metaspace)取而代之, 这时候字符串常量池还在堆, 运行时常量池还在方法区, 只不过方法区的实现从永久代变成了元空间(Metaspace)**\n\n假如在字符串常量池中存在字符串 \"abc\"，如果当前没有任何 String 对象引用该字符串常量的话，就说明常量 \"abc\" 就是废弃常量，如果这时发生内存回收的话而且有必要的话，\"abc\" 就会被系统清理出常量池了。\n\n\n### 如何判断类为无用类\n\n\n方法区主要回收的是无用的类，那么如何判断一个类是无用的类的呢？\n\n\n判定一个常量是否是“废弃常量”比较简单，而要判定一个类是否是“无用的类”的条件则相对苛刻许多。类需要同时满足下面 3 个条件才能算是 **“无用的类”** ：\n\n- 该类所有的实例都已经被回收，也就是 Java 堆中不存在该类的任何实例。\n- 加载该类的 `ClassLoader` 已经被回收。\n- 该类对应的 `java.lang.Class` 对象没有在任何地方被引用，无法在任何地方通过反射访问该类的方法。\n\n虚拟机可以对满足上述 3 个条件的无用类进行回收，这里说的仅仅是“可以”，而并不是和对象一样不使用了就会必然被回收。\n\n\n### 垃圾收集算法\n\n\n![](https://blog-file.hehouhui.cn/202203222203133.png)\n\n\n### 标记-清除术算法\n\n\n该算法分为“标记”和“清除”阶段：首先标记出所有不需要回收的对象，在标记完成后统一回收掉所有没有被标记的对象。它是最基础的收集算法，后续的算法都是对其不足进行改进得到。这种垃圾收集算法会带来两个明显的问题：\n\n1. **效率问题**\n2. **空间问题（标记清除后会产生大量不连续的碎片）**\n\n![](https://blog-file.hehouhui.cn/202203222203288.jpeg)\n\n\n### 标记-复制算法\n\n\n为了解决效率问题，“标记-复制”收集算法出现了。它可以将内存分为大小相同的两块，每次使用其中的一块。当这一块的内存使用完后，就将还存活的对象复制到另一块去，然后再把使用的空间一次清理掉。这样就使每次的内存回收都是对内存区间的一半进行回收。\n\n\n![](https://blog-file.hehouhui.cn/202203222146680.png)\n\n\n### 标记-整理算法\n\n\n根据老年代的特点提出的一种标记算法，标记过程仍然与“标记-清除”算法一样，但后续步骤不是直接对可回收对象回收，而是让所有存活的对象向一端移动，然后直接清理掉端边界以外的内存。\n\n\n![](https://blog-file.hehouhui.cn/202203222203632.png)\n\n\n### 分代收集算法\n\n\n当前虚拟机的垃圾收集都采用分代收集算法，这种算法没有什么新的思想，只是根据对象存活周期的不同将内存分为几块。一般将 java 堆分为新生代和老年代，这样我们就可以根据各个年代的特点选择合适的垃圾收集算法。\n\n\n**比如在新生代中，每次收集都会有大量对象死去，所以可以选择”标记-复制“算法，只需要付出少量对象的复制成本就可以完成每次垃圾收集。而老年代的对象存活几率是比较高的，而且没有额外的空间对它进行分配担保，所以我们必须选择“标记-清除”或“标记-整理”算法进行垃圾收集。**\n\n\n**延伸面试问题：** HotSpot 为什么要分为新生代和老年代？\n\n\n根据上面的对分代收集算法的介绍回答。\n\n\n### 垃圾收集器\n\n\n![](https://blog-file.hehouhui.cn/202203222150111.png)\n\n\nHotSpot VM中的垃圾回收器，以及适用场景\n\n\n![](https://blog-file.hehouhui.cn/202203222121319.png)\n\n\n到jdk8为止，默认的垃圾收集器是Parallel Scavenge 和 Parallel Old\n\n\n从jdk9开始，G1收集器成为默认的垃圾收集器 目前来看，G1回收器停顿时间最短而且没有明显缺点，非常适合Web应用。在jdk8中测试Web应用，堆内存6G，新生代4.5G的情况下，Parallel Scavenge 回收新生代停顿长达1.5秒。G1回收器回收同样大小的新生代只停顿0.2秒。\n\n\n**如果说收集算法是内存回收的方法论，那么垃圾收集器就是内存回收的具体实现。**\n\n\n虽然我们对各个收集器进行比较，但并非要挑选出一个最好的收集器。因为直到现在为止还没有最好的垃圾收集器出现，更加没有万能的垃圾收集器，**我们能做的就是根据具体应用场景选择适合自己的垃圾收集器**。试想一下：如果有一种四海之内、任何场景下都适用的完美收集器存在，那么我们的 HotSpot 虚拟机就不会实现那么多不同的垃圾收集器了。\n\n\n### Serial（串行）收集器\n\n\nSerial（串行）收集器是最基本、历史最悠久的垃圾收集器了。大家看名字就知道这个收集器是一个单线程收集器了。它的 **“单线程”** 的意义不仅仅意味着它只会使用一条垃圾收集线程去完成垃圾收集工作，更重要的是它在进行垃圾收集工作的时候必须暂停其他所有的工作线程（ **\"Stop The World\"** ），直到它收集结束。\n\n\n**新生代采用标记-复制算法，老年代采用标记-整理算法。**\n\n\n![](https://blog-file.hehouhui.cn/202203222203314.png)\n\n\n虚拟机的设计者们当然知道 Stop The World 带来的不良用户体验，所以在后续的垃圾收集器设计中停顿时间在不断缩短（仍然还有停顿，寻找最优秀的垃圾收集器的过程仍然在继续）。\n\n\n但是 Serial 收集器有没有优于其他垃圾收集器的地方呢？当然有，它**简单而高效（与其他收集器的单线程相比）**。Serial 收集器由于没有线程交互的开销，自然可以获得很高的单线程收集效率。Serial 收集器对于运行在 Client 模式下的虚拟机来说是个不错的选择。\n\n\n### **ParNew**收集器\n\n\n**ParNew 收集器其实就是 Serial 收集器的多线程版本，除了使用多线程进行垃圾收集外，其余行为（控制参数、收集算法、回收策略等等）和 Serial 收集器完全一样。**\n\n\n**新生代采用标记-复制算法，老年代采用标记-整理算法。**\n\n\n![](https://blog-file.hehouhui.cn/202203222203393.png)\n\n\n它是许多运行在 Server 模式下的虚拟机的首要选择，除了 Serial 收集器外，只有它能与 CMS 收集器（真正意义上的并发收集器，后面会介绍到）配合工作。\n\n\n**并行和并发概念补充：**\n\n- **并行（Parallel）** ：指多条垃圾收集线程并行工作，但此时用户线程仍然处于等待状态。\n- **并发（Concurrent）**：指用户线程与垃圾收集线程同时执行（但不一定是并行，可能会交替执行），用户程序在继续运行，而垃圾收集器运行在另一个 CPU 上。\n\n### Parallel Scavenge 收集器\n\n\nParallel Scavenge 收集器也是使用标记-复制算法的多线程收集器，它看上去几乎和 ParNew 都一样。 **那么它有什么特别之处呢？**\n\n\n```text\n-XX:+UseParallelGC\n\n    使用 Parallel 收集器+ 老年代串行\n\n-XX:+UseParallelOldGC\n\n    使用 Parallel 收集器+ 老年代并行\nCopy to clipboardErrorCopied\n\n```\n\n\n**Parallel Scavenge 收集器关注点是吞吐量（高效率的利用 CPU）。CMS 等垃圾收集器的关注点更多的是用户线程的停顿时间（提高用户体验）。所谓吞吐量就是 CPU 中用于运行用户代码的时间与 CPU 总消耗时间的比值。** Parallel Scavenge 收集器提供了很多参数供用户找到最合适的停顿时间或最大吞吐量，如果对于收集器运作不太了解，手工优化存在困难的时候，使用 Parallel Scavenge 收集器配合自适应调节策略，把内存管理优化交给虚拟机去完成也是一个不错的选择。\n\n\n**新生代采用标记-复制算法，老年代采用标记-整理算法。**\n\n\n![](https://blog-file.hehouhui.cn/202203222152500.png)\n\n\n**这是 JDK1.8 默认收集器**\n\n\n使用 java -XX:+PrintCommandLineFlags -version 命令查看\n\n\n```text\n-XX:InitialHeapSize=262921408 -XX:MaxHeapSize=4206742528 -XX:+PrintCommandLineFlags -XX:+UseCompressedClassPointers -XX:+UseCompressedOops -XX:+UseParallelGC\njava version \"1.8.0_211\"\nJava(TM) SE Runtime Environment (build 1.8.0_211-b12)\nJava HotSpot(TM) 64-Bit Server VM (build 25.211-b12, mixed mode)Copy to clipboardErrorCopied\n\n```\n\n\nJDK1.8 默认使用的是 Parallel Scavenge + Parallel Old，如果指定了-XX:+UseParallelGC 参数，则默认指定了-XX:+UseParallelOldGC，可以使用-XX:-UseParallelOldGC 来禁用该功能\n\n\n### **Serial** Old收集器\n\n\n**Serial 收集器的老年代版本**，它同样是一个单线程收集器。它主要有两大用途：一种用途是在 JDK1.5 以及以前的版本中与 Parallel Scavenge 收集器搭配使用，另一种用途是作为 CMS 收集器的后备方案。\n\n\n### **Parallel Scavenge**收集器\n\n\n**Parallel Scavenge 收集器的老年代版本**。使用多线程和“标记-整理”算法。在注重吞吐量以及 CPU 资源的场合，都可以优先考虑 Parallel Scavenge 收集器和 Parallel Old 收集器。\n\n\n### CMS收集器\n\n\n**CMS（Concurrent Mark Sweep）收集器是一种以获取最短回收停顿时间为目标的收集器。它非常符合在注重用户体验的应用上使用。**\n\n\n**CMS（Concurrent Mark Sweep）收集器是 HotSpot 虚拟机第一款真正意义上的并发收集器，它第一次实现了让垃圾收集线程与用户线程（基本上）同时工作。**\n\n\n从名字中的**Mark Sweep**这两个词可以看出，CMS 收集器是一种 **“标记-清除”算法**实现的，它的运作过程相比于前面几种垃圾收集器来说更加复杂一些。整个过程分为四个步骤：\n\n- **初始标记：** 暂停所有的其他线程，并记录下直接与 root 相连的对象，速度很快 ；\n- **并发标记：** 同时开启 GC 和用户线程，用一个闭包结构去记录可达对象。但在这个阶段结束，这个闭包结构并不能保证包含当前所有的可达对象。因为用户线程可能会不断的更新引用域，所以 GC 线程无法保证可达性分析的实时性。所以这个算法里会跟踪记录这些发生引用更新的地方。\n- **重新标记：** 重新标记阶段就是为了修正并发标记期间因为用户程序继续运行而导致标记产生变动的那一部分对象的标记记录，这个阶段的停顿时间一般会比初始标记阶段的时间稍长，远远比并发标记阶段时间短\n- **并发清除：** 开启用户线程，同时 GC 线程开始对未标记的区域做清扫。\n\n![](https://blog-file.hehouhui.cn/image-1647957148871.png)\n\n\n从它的名字就可以看出它是一款优秀的垃圾收集器，主要优点：**并发收集、低停顿**。但是它有下面三个明显的缺点：\n\n- **对 CPU 资源敏感；**\n- **无法处理浮动垃圾；**\n- **它使用的回收算法-“标记-清除”算法会导致收集结束时会有大量空间碎片产生。**\n\n### G1收集器\n\n\n**G1 (Garbage-First) 是一款面向服务器的垃圾收集器,主要针对配备多颗处理器及大容量内存的机器. 以极高概率满足 GC 停顿时间要求的同时,还具备高吞吐量性能特征.**\n\n\n被视为 JDK1.7 中 HotSpot 虚拟机的一个重要进化特征。它具备一下特点：\n\n- **并行与并发**：G1 能充分利用 CPU、多核环境下的硬件优势，使用多个 CPU（CPU 或者 CPU 核心）来缩短 Stop-The-World 停顿时间。部分其他收集器原本需要停顿 Java 线程执行的 GC 动作，G1 收集器仍然可以通过并发的方式让 java 程序继续执行。\n- **分代收集**：虽然 G1 可以不需要其他收集器配合就能独立管理整个 GC 堆，但是还是保留了分代的概念。\n- **空间整合**：与 CMS 的“标记-清理”算法不同，G1 从整体来看是基于“标记-整理”算法实现的收集器；从局部上来看是基于“标记-复制”算法实现的。\n- **可预测的停顿**：这是 G1 相对于 CMS 的另一个大优势，降低停顿时间是 G1 和 CMS 共同的关注点，但 G1 除了追求低停顿外，还能建立可预测的停顿时间模型，能让使用者明确指定在一个长度为 M 毫秒的时间片段内。\n\nG1 收集器的运作大致分为以下几个步骤：\n\n- **初始标记**\n- **并发标记**\n- **最终标记**\n- **筛选回收**\n\n**G1 收集器在后台维护了一个优先列表，每次根据允许的收集时间，优先选择回收价值最大的 Region(这也就是它的名字 Garbage-First 的由来)** 。这种使用 Region 划分内存空间以及有优先级的区域回收方式，保证了 G1 收集器在有限时间内可以尽可能高的收集效率（把内存化整为零）。\n\n\n### ZGC收集器\n\n\n与 CMS 中的 ParNew 和 G1 类似，ZGC 也采用标记-复制算法，不过 ZGC 对该算法做了重大改进。\n\n\n在 ZGC 中出现 Stop The World 的情况会更少！\n\n\n详情可以看 ： [《新一代垃圾回收器 ZGC 的探索与实践》](https://tech.meituan.com/2020/08/06/new-zgc-practice-in-meituan.html)\n\n\n### 参考\n\n- 《深入理解 Java 虚拟机：JVM 高级特性与最佳实践（第二版》\n- [https://my.oschina.net/hosee/blog/644618](https://my.oschina.net/hosee/blog/644618)\n- [https://docs.oracle.com/javase/specs/jvms/se8/html/index.html](https://docs.oracle.com/javase/specs/jvms/se8/html/index.html)\n",
      "properties": {
        "password": "",
        "icon": "",
        "date": "2020-03-12",
        "type": "Post",
        "category": "技术分享",
        "urlname": "42",
        "catalog": [
          "archives"
        ],
        "tags": [
          "Java",
          "Jvm"
        ],
        "summary": "Java基础-JVM是Java开发者必须要掌握的重要知识点之一，JVM全称为Java Virtual Machine（Java虚拟机），它是Java程序运行的环境，在Java编程中具有极其重要的作用。\n作为Java语言的核心，JVM能够通过将Java代码编译为字节码，再通过字节码的解释器实现Java程序的运行。JVM负责管理应用程序的内存、多线程、垃圾回收等操作，是实现跨平台、自动垃圾回收、安全性高等特性的关键。\n要深入理解JVM，需要掌握JVM的内部机制，包括虚拟机类加载器、运行时数据区域、字节码执行引擎等方面。此外，对于JVM的性能调优及故障排查也非常重要，能够帮助开发者优化应用程序的运行效率。\n总之，掌握Java基础-JVM是Java开发者必须要具备的知识技能，它可以帮助你更好地理解Java程序的运行机制，提高开发效率，提升应用程序的性能以及稳定性。",
        "sort": "",
        "title": "Java基础-JVM",
        "status": "Published",
        "updated": "2023-10-08 14:42:00"
      },
      "catalog": [
        {
          "title": "archives",
          "doc_id": "f2eefe59-c1ef-4dba-847d-6525d1ddc518"
        }
      ],
      "body": "",
      "realName": "Java基础-JVM",
      "relativePath": "/archives/Java基础-JVM.md"
    },
    {
      "id": "7d7311ba-8ed2-4a90-91ac-5d2b4a503c4c",
      "doc_id": "7d7311ba-8ed2-4a90-91ac-5d2b4a503c4c",
      "title": "7d7311ba-8ed2-4a90-91ac-5d2b4a503c4c",
      "updated": 1696747320000,
      "body_original": "\n启用身份验证和授权涉及复杂的功能，而不仅仅是简单的登录 API。[在之前的文章（使用 Keycloak 的 API 登录和 JWT 令牌生成）中](https://blog.hehouhui.cn/archives/api-login-and-jwt-token-generation-using-keycloak)，我描述了 Keycloak REST 登录 API 端点，它只处理一些身份验证任务。在本文中，我描述了如何使用开箱即用的 Keycloak REST API 功能启用身份验证和授权的其他方面。\n\n\n# 身份验证与授权\n\n\n但首先，身份验证和授权之间有什么区别？简单地说，身份验证意味着_你是谁_，而授权意味着_你可以做什么_，每种方法都使用不同的方法进行验证。例如，身份验证使用用户管理和登录表单，授权使用基于角色的访问控制 (RBAC) 或访问控制列表 (ACL)。幸运的是，这些验证方法在 Red Hat 的单点登录 (SSO) 工具中提供，或者在他们的上游开源项目 Keycloak 的 REST API 中提供。\n\n\n# Keycloak SSO 案例研究\n\n\n为了更好地理解使用 Keycloak 进行身份验证和授权，让我们从一个简单的案例研究开始。假设印度尼西亚教育部计划创建与多所学校的单点登录集成。他们计划使用集中式平台在多个学校维护学生和教师的单一帐户 ID。这让每个用户都具有相同的角色，但在每所学校具有不同的访问权限和特权，如图 1 所示。\n\n\n![](https://developers.redhat.com/sites/default/files/styles/article_floated/public/blog/2020/08/keycloak-education-00.png?itok=aKKqoLgU)\n\n\n图 1：每个用户都可以使用相同的角色，但在每所学校具有不同的访问权限和权限。\">\n\n\n# Keycloak SSO 演示\n\n\n让我们通过创建一个 Keycloak 领域来开始演示。为该部门使用 Add realm 对话框（如图 2 所示）。为领域命名`education`，将**Enabled**设置为**ON**，然后单击**Create**。\n\n\n![](https://developers.redhat.com/sites/default/files/styles/article_floated/public/blog/2020/08/keycloak-education-01.png?itok=TFFotz_T)\n\n\n图 2：为教育部创建名为“教育”的 Keycloak 领域。\">\n\n\n接下来，转到 Roles 页面并确保选择了**Realm Roles**选项卡，如图 3 所示。\n\n\n![](https://developers.redhat.com/sites/default/files/styles/article_floated/public/blog/2020/08/keycloak-education-06.png?itok=kN1XyC0P)\n\n\n图 3：为教育领域创建两个独立的角色：教师和学生。\n\n\n单击**添加角色**为该领域创建两个单独的角色，称为“教师”和“学生”。然后，这些新角色将出现在**Realm Roles**选项卡中，如图 4 所示。\n\n\n![](https://developers.redhat.com/sites/default/files/styles/article_floated/public/blog/2020/08/keycloak-education-07.png?itok=KBPu-x5E)\n\n\n图 4：添加教师和学生角色。\">\n\n\n然后，使用 Clients 页面，单击**Create**添加客户端，如图 5 所示。\n\n\n![](https://developers.redhat.com/sites/default/files/styles/article_floated/public/blog/2020/08/keycloak-education-02.png?itok=_XcRiyxZ)\n\n\n图 5：添加客户端。\n\n\n在 Add Client 页面上，创建一个名为“jakarta-school”的客户端，然后单击**Save**添加该客户端，如图 6 所示。\n\n\n![](https://developers.redhat.com/sites/default/files/styles/article_floated/public/blog/2020/08/keycloak-education-03.png?itok=ioRUgKYl)\n\n\n在 jakarta-school 详细信息页面上，转到**“设置”**选项卡并输入以下客户端配置，如图 7 所示：\n\n- **客户 ID**\n\n\t: jakarta-school\n\n- **启用**\n\n\t：开\n\n- **需要同意**\n\n\t：关闭\n\n- **客户端协议**\n\n\t：openid-connect\n\n- **访问类型**\n\n\t：机密\n\n- **启用标准流量**\n\n\t：开\n\n- **启用影响流**\n\n\t：关闭\n\n- **已启用直接访问授权**\n\n\t：ON\n\n\n![](https://developers.redhat.com/sites/default/files/styles/article_floated/public/blog/2020/08/keycloak-education-04.png?itok=UAhXaIL2)\n\n\n图 7：在 jakarta-school 详细信息页面上，输入客户端配置。\n\n\n在同一页面的底部，在 Authentication Flow Overrides 部分，我们可以设置如下，如图 8 所示：\n\n- **浏览器流程**\n\n\t：浏览器\n\n- **直接拨款流程**\n\n\t：直接拨款\n\n\n![](https://developers.redhat.com/sites/default/files/styles/article_floated/public/blog/2020/08/keycloak-education-05.png?itok=bjT_1W98)\n\n\n图 8：配置身份验证流程覆盖。\">\n\n\n转到**Roles**选项卡，单击**Add Role**，然后为此客户端创建 create-student-grade、view-student-grade 和 view-student-profile 角色，如图 9 所示。每个角色都应设置为 Composite **False**。\n\n\n![](https://developers.redhat.com/sites/default/files/styles/article_floated/public/blog/2020/08/keycloak-education-11.png?itok=MySy1CkW)\n\n\n图 9：为此客户创建角色。\n\n\n接下来，转到**Client Scopes**选项卡，在**Default Client Scopes**部分中，将“roles”和“profile”添加到**Assigned Default Client Scopes**中，如图 10 所示。\n\n\n![](https://developers.redhat.com/sites/default/files/styles/article_floated/public/blog/2020/08/keycloak-education-08.png?itok=EQBlb8Yo)\n\n\n图 10：设置客户端范围。\n\n\n在jakarta-school详情页面，选择**Mappers**，然后选择**Create Protocol Mappers**，设置mappers在Userinfo API上显示客户端角色，如图11所示：\n\n- **名称**\n\n\t：角色\n\n- **映射器类型**\n\n\t：用户领域角色\n\n- **多值**\n\n\t：开\n\n- **令牌声明名称**\n\n\t：角色\n\n- **声明 JSON 类型**\n\n\t：字符串\n\n- **添加到 ID 令牌**\n\n\t：关闭\n\n- **添加到访问令牌**\n\n\t：关闭\n\n- **添加到用户信息**\n\n\t：ON\n\n\n![](https://developers.redhat.com/sites/default/files/styles/article_floated/public/blog/2020/08/keycloak-education-09.png?itok=zZAbM6Sr)\n\n\n图 11：设置映射器以显示客户端角色。\n\n\n接下来，转到**Users**页面，选择**Add user**，创建新用户，然后单击**Save，**如图 12 所示：\n\n- **用户名**\n\n\t：埃德温\n\n- **电子邮件**\n\n\t：edwin@redhat.com\n\n- **名字**\n\n\t：埃德温\n\n- **姓**\n\n\t: M\n\n- **用户启用**\n\n\t：ON\n\n- **电子邮件已验证**\n\n\t：关闭\n\n\n![](https://developers.redhat.com/sites/default/files/styles/article_floated/public/blog/2020/08/keycloak-education-10.png?itok=rOzI0tsb)\n\n\n图 12：创建新用户。\n\n\n最后，在**Role Mappings**选项卡中，为 jakarta-school 中的每个用户选择**Client Roles**，如图 13 所示。它们应该是 create-student-grade、view-student-grade 和 view-student-profile。\n\n\n![](https://developers.redhat.com/sites/default/files/styles/article_floated/public/blog/2020/08/keycloak-education-12.png?itok=3QIR03u9)\n\n\n图 13：为 jakarta-school 中的每个用户选择客户端角色。\n\n\n我的 Keycloak 配置演示到此结束。\n\n\n# 使用 Java 应用程序的 Keycloak 连接\n\n\n现在我想演示如何开发一个非常简单的 Java 应用程序。此应用程序连接到您的 Keycloak 实例，并通过其 REST API 使用 Keycloak 的身份验证和授权功能。\n\n\n首先，从 pom.xml 文件开始开发 Java 应用程序，如以下示例所示：\n\n\n```xml\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<project xmlns=\"http://maven.apache.org/POM/4.0.0\"\n         xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n         xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\">\n    <modelVersion>4.0.0</modelVersion>\n\n    <groupId>EducationService</groupId>\n    <artifactId>com.edw</artifactId>\n    <version>1.0-SNAPSHOT</version>\n\n    <parent>\n        <groupId>org.springframework.boot</groupId>\n        <artifactId>spring-boot-starter-parent</artifactId>\n        <version>2.2.1.RELEASE</version>\n        <relativePath/>\n    </parent>\n\n    <properties>\n        <java.version>11</java.version>\n    </properties>\n\n    <dependencies>\n        <dependency>\n            <groupId>org.springframework.boot</groupId>\n            <artifactId>spring-boot-starter</artifactId>\n        </dependency>\n        <dependency>\n            <groupId>org.springframework.boot</groupId>\n            <artifactId>spring-boot-starter-web</artifactId>\n        </dependency>\n\n        <dependency>\n            <groupId>com.auth0</groupId>\n            <artifactId>jwks-rsa</artifactId>\n            <version>0.12.0</version>\n        </dependency>\n\n        <dependency>\n            <groupId>com.auth0</groupId>\n            <artifactId>java-jwt</artifactId>\n            <version>3.8.3</version>\n        </dependency>\n\n    </dependencies>\n\n    <build>\n        <plugins>\n            <plugin>\n                <groupId>org.springframework.boot</groupId>\n                <artifactId>spring-boot-maven-plugin</artifactId>\n            </plugin>\n        </plugins>\n    </build>\n\n</project>\n```\n\n\nJava应用程序还要求您开发一个简单的属性文件：\n\n\n```yaml\nserver:\n    error:\n        whitelabel:\n            enabled:false\n    port: 8082\nspring:\n    mvc:\n        favicon:\n            enabled:false\nkeycloak:\n    client-id: jakarta-school\n    client-secret: 197bc3b4-64b0-452f-9bdb-fcaea0988e90\n    scope: openid, profile\n    authorization-grant-type: password\n    authorization-uri: http://localhost:8080/auth/realms/education/protocol/openid-connect/auth\n    user-info-uri: http://localhost:8080/auth/realms/education/protocol/openid-connect/userinfo\n    token-uri: http://localhost:8080/auth/realms/education/protocol/openid-connect/token\n    logout: http://localhost:8080/auth/realms/education/protocol/openid-connect/logout\n    jwk-set-uri: http://localhost:8080/auth/realms/education/protocol/openid-connect/certs\n    certs-id: vdaec4Br3ZnRFtZN-pimK9v1eGd3gL2MHu8rQ6M5SiE\nlogging:\n    level:\n        root: INFO\n```\n\n\n接下来，从图 14 所示的表单中获取 Keycloak 证书 ID。\n\n\n![](https://developers.redhat.com/sites/default/files/styles/article_floated/public/blog/2020/11/keycloak-education-13.png?itok=KkNUCm4W)\n\n\n图 14：找到 Keycloak 证书 ID。\n\n\n之后，最重要的是，您的下一个任务是开发集成代码；此操作涉及多个 Keycloak API。请注意，我没有详细介绍 Keycloak 登录 API，因为它已经在我之前的文章中进行了描述。\n\n\n从一个简单的注销 API 开始：\n\n\n```java\n    @Value(\"${keycloak.logout}\") \n    private String keycloakLogout;\n\n    public void logout(String refreshToken) throws Exception { \n        MultiValueMap<String, String> map = new LinkedMultiValueMap<>(); \n        map.add(\"client_id\",clientId); \n        map.add(\"client_secret\",clientSecret); \n        map.add(\"refresh_token\",refreshToken); \n\n        HttpEntity<MultiValueMap<String, String>> request = new HttpEntity<>(map, null); \n        restTemplate.postForObject(keycloakLogout, request, String.class); \n    }\n```\n\n\n首先，我想指出，对于注销，使用参数`refresh_token`而不是是至关重要的`access_token`。现在，使用 API 检查不记名令牌是否有效和活跃，以验证请求是否带来有效凭证。\n\n\n```java\n    @Value(\"${keycloak.user-info-uri}\")\n    private String keycloakUserInfo;\n\n    private String getUserInfo(String token) {\n        MultiValueMap<String, String> headers = new LinkedMultiValueMap<>();\n        headers.add(\"Authorization\", token);\n\n        HttpEntity<MultiValueMap<String, String>> request = new HttpEntity<>(null, headers);\n        return restTemplate.postForObject(keycloakUserInfo, request, String.class);\n    }\n```\n\n\n对于授权，您可以使用两种方法来决定给定角色是否有资格访问特定 API。第一种方法是通过针对 Keycloak 的 userinfo API 进行验证来确定不记名令牌带来的角色，下一个方法是验证不记名令牌中的角色。\n\n\n对于第一种方法，您可以期待 Keycloak 的以下响应：\n\n\n```json\n{\n    \"sub\": \"ef2cbe43-9748-40e5-aed9-fe981e3082d5\",\n    \"roles\": [\n        \"teacher\"\n    ],\n    \"name\": \"Edwin M\",\n    \"preferred_username\": \"edwin\",\n    \"given_name\": \"Edwin \",\n    \"family_name\": \"M\"\n}\n```\n\n\n如您所见，那里有一个`roles`标签，一种方法是根据它来验证访问权限。缺点是每个请求在您的应用程序和 Keycloak 之间有多次往返请求，这会导致更高的延迟。\n\n\n另一种方法是读取通过每个请求发送的 JWT 令牌的内容。为了成功解码您的 JWT 令牌，您必须知道用于对其签名的公钥。这就是 Keycloak 提供 JWKS 端点的原因。您可以使用 curl 命令查看其内容，如以下示例所示：\n\n\n```json\n{\n    \"keys\": [\n        {\n            \"kid\": \"vdaec4Br3ZnRFtZN-pimK9v1eGd3gL2MHu8rQ6M5SiE\",\n            \"kty\": \"RSA\",\n            \"alg\": \"RS256\",\n            \"use\": \"sig\",\n            \"n\": \"4OPCc_LDhU6ADQj7cEgRei4VUf4PZH8GYsxsR6RSdeKmDvZ48hCSEFiEgfc3FIfh-gC4r9PtKucc_nkRofrAKR4qL8KNNoSuzQAOC92Yz6r7Ao4HppHJ8-QVdo5H-d9wfNSlDLBSo5My4b4EnHb1HLuFxDqyhFpGvsoUJdgbt3m_Q3WAVb2yrM83S6HX__vrQvqUk2e7z5RNrI7LSsW3ZOz9fU7pvm8-kFFAIPJ7fOJIC7UQ9wBWg3YdwQ0B2b24jXjVr0QCGzqJ6o1G_UZYSJCDMGQDpDcEuYnvSKBLfVR-0EcAjolRhcSPjHlW0Cp0YU8qwWDHpjkbrMrFmxlO4Q\",\n            \"e\": \"AQAB\"\n        }\n    ]\n}\n```\n\n\n请注意，在前面的示例中，kid代表key id，alg代表加密算法，n代表用于此领域的公钥。您可以使用此公钥轻松解码我们的JWT令牌，并从JWT声明中读取roles。下面显示了解码后的样本JWT令牌：\n\n\n```json\n{\n  \"jti\": \"85edca8c-a4a6-4a4c-b8c0-356043e7ba7d\",\n  \"exp\": 1598079154,\n  \"nbf\": 0,\n  \"iat\": 1598078854,\n  \"iss\": \"http://localhost:8080/auth/realms/education\",\n  \"sub\": \"ef2cbe43-9748-40e5-aed9-fe981e3082d5\",\n  \"typ\": \"Bearer\",\n  \"azp\": \"jakarta-school\",\n  \"auth_time\": 0,\n  \"session_state\": \"f8ab78f8-15ee-403d-8db7-7052a8647c65\",\n  \"acr\": \"1\",\n  \"realm_access\": {\n    \"roles\": [\n      \"teacher\"\n    ]\n  },\n  \"resource_access\": {\n    \"jakarta-school\": {\n      \"roles\": [\n        \"create-student-grade\",\n        \"view-student-profile\",\n        \"view-student-grade\"\n      ]\n    }\n  },\n  \"scope\": \"profile\",\n  \"name\": \"Edwin M\",\n  \"preferred_username\": \"edwin\",\n  \"given_name\": \"Edwin\",\n  \"family_name\": \"M\"\n}\n```\n\n\n您可以使用以下示例中显示的代码读取角色标签：\n\n\n```java\n   @GetMapping(\"/teacher\")\n    public HashMap teacher(@RequestHeader(\"Authorization\") String authHeader) {\n        try {\n            DecodedJWT jwt = JWT.decode(authHeader.replace(\"Bearer\", \"\").trim());\n\n            // check JWT is valid\n            Jwk jwk = jwtService.getJwk();\n            Algorithm algorithm = Algorithm.RSA256((RSAPublicKey) jwk.getPublicKey(), null);\n\n            algorithm.verify(jwt);\n\n            // check JWT role is correct\n            List<String> roles = ((List)jwt.getClaim(\"realm_access\").asMap().get(\"roles\"));\n            if(!roles.contains(\"teacher\"))\n                throw new Exception(\"not a teacher role\");\n\n            // check JWT is still active\n            Date expiryDate = jwt.getExpiresAt();\n            if(expiryDate.before(new Date()))\n                throw new Exception(\"token is expired\");\n\n            // all validation passed\n            return new HashMap() {{\n                put(\"role\", \"teacher\");\n            }};\n        } catch (Exception e) {\n            logger.error(\"exception : {} \", e.getMessage());\n            return new HashMap() {{\n                put(\"status\", \"forbidden\");\n            }};\n        }\n    }\n```\n\n\n这种方法最好的部分是您可以将来自 Keycloak 的公钥放在缓存中，从而减少往返请求，这种做法最终会增加应用程序延迟和性能。[可以在我的 GitHub 存储库中](https://github.com/edwin/java-keycloak-integration)找到本文的完整代码。\n\n\n# 结论\n\n\n总之，我准备这篇文章首先是为了解释启用身份验证和授权涉及复杂的功能，而不仅仅是一个简单的登录 API。然后我演示了如何使用开箱即用的 Keycloak REST API 功能启用身份验证和授权的许多方面。\n\n\n此外，我还演示了如何开发一个连接到您的 Keycloak 实例的简单 Java 应用程序，并通过其 REST API 使用 Keycloak 的身份验证和授权功能。\n\n",
      "properties": {
        "password": "",
        "icon": "",
        "date": "2023-04-28T13:13:00.000+08:00",
        "type": "Post",
        "category": "技术分享",
        "urlname": "authentication-and-authorization-using-the-keycloak-rest-api",
        "catalog": [
          "archives"
        ],
        "tags": [
          "开发",
          "建站",
          "Java",
          "oauth",
          "keycloak"
        ],
        "summary": "Keycloak REST API 可以被用来进行身份验证和授权。使用该API，开发人员可以轻松地在其应用程序中实现安全性验证和授权功能，同时使用Keycloak的内置功能进行管理和配置。Keycloak的REST API还提供了许多不同的终端点来进行用户和角色管理、认证事件和SAML元数据的访问等。 作为一个基于开源的身份和访问管理解决方案，Keycloak的REST API对于任何需要对应用程序进行认证和授权的开发人员都非常有帮助。",
        "sort": "",
        "title": "使用 Keycloak REST API 进行身份验证和授权",
        "status": "Published",
        "updated": "2023-10-08 14:42:00"
      },
      "catalog": [
        {
          "title": "archives",
          "doc_id": "7d7311ba-8ed2-4a90-91ac-5d2b4a503c4c"
        }
      ],
      "body": "",
      "realName": "使用 Keycloak REST API 进行身份验证和授权",
      "relativePath": "/archives/使用 Keycloak REST API 进行身份验证和授权.md"
    },
    {
      "id": "bcd4f723-5f6a-4d94-90a1-63bb8c6dbfcf",
      "doc_id": "bcd4f723-5f6a-4d94-90a1-63bb8c6dbfcf",
      "title": "bcd4f723-5f6a-4d94-90a1-63bb8c6dbfcf",
      "updated": 1696747320000,
      "body_original": "\n> 💡 Spring Cache是一个强大的缓存框架，可用于提高应用程序的性能和可靠性。它是Spring框架的一部分，基于标准的缓存抽象，提供了一个统一的、可扩展的缓存解决方案。Spring Cache支持多种缓存提供者，包括Ehcache、Redis、Couchbase等。它还提供了灵活的配置选项，使开发人员可以根据应用程序的需要选择最适合的缓存解决方案。  \n>   \n> 使用Spring Cache可以帮助应用程序减少对数据库和其他外部资源的访问次数，从而提高应用程序的响应速度和吞吐量。除了提高性能之外，Spring Cache还可以用于添加应用程序级别的缓存逻辑，例如缓存用户会话、结果集等。  \n>   \n> 总之，Spring Cache是一个十分强大的工具，可用于提高应用程序的性能和可靠性。它在Spring框架中的定位十分重要，是一个不可或缺的组件。如果您想要在应用程序中使用缓存，那么Spring Cache绝对是一个值得考虑的选择。\n\n\n### **@Cacheable**\n\n\n@Cacheable 的作用 主要针对方法配置，**能够根据方法的请求参数对其结果进行缓存**\n\n\n@Cacheable 作用和配置方法\n\n\n| 参数        | 解释                                                                             | example                                                              |\n| --------- | ------------------------------------------------------------------------------ | -------------------------------------------------------------------- |\n| value     | 缓存的名称，在 [spring](http://www.aspku.com/kaifa/java/237386.html) 配置文件中定义，必须指定至少一个 | 例如: @Cacheable(value=”mycache”) @Cacheable(value={”cache1”,”cache2”} |\n| key       | 缓存的 key，可以为空，如果指定要按照 SpEL 表达式编写，如果不指定，则缺省按照方法的所有参数进行组合                         | @Cacheable(value=”testcache”,key=”#userName”)                        |\n| condition | 缓存的条件，可以为空，使用 SpEL 编写，返回 true 或者 false，只有为 true 才进行缓存                          | @Cacheable(value=”testcache”,condition=”#userName.length()>2”)       |\n\n\n---\n\n\n**实例**\n\n\n> @Cacheable(value=”accountCache”)，这个注释的意思是：\n\n\n当调用这个方法的时候，**会先从一个名叫 accountCache 的缓存中查询**，如果没有，则执行实际的方法（即查询数据库），并将执行的结果存入缓存中，否则返回缓存中的对象。**这里的缓存中的 key 就是参数 userName，value 就是 Account 对象。**\n\n\n**“accountCache”缓存是在 \\****spring\\*.xml\\** **中定义的名称。**\n\n\n```java\n@Cacheable(value=\"accountCache\")// 使用了一个缓存名叫 accountCache\npublic Account getAccountByName(String userName) {\n   // 方法内部实现不考虑缓存逻辑，直接实现业务\n   System.out.println(\"real query account.\"+userName);\n   return getFromDB(userName);\n\n}\n\n```\n\n\n**而在SpringBoot中使用的话是存放在resources下的\\****ehcache.xml\\****。**\n\n\n```xml\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<ehcache xmlns:xsi=\"<http://www.w3.org/2001/XMLSchema-instance>\"\n         xsi:noNamespaceSchemaLocation=\"<http://ehcache.org/ehcache.xsd>\"\n\n         updateCheck=\"false\">\n\n    <!--账号缓存-->\n\n    <cache name=\"accountCache\" maxElementsInMemory=\"10000\" timeToLiveSeconds=\"300\"  timeToIdleSeconds=\"310\"   eternal=\"false\" overflowToDisk=\"true\"/>\n\n</ehcache>\n\n```\n\n\n---\n\n\n### **@CachePut**\n\n\n> @CachePut 作用和配置方法：\n\n\n@CachePut 的作用 主要针对方法配置，能够根据方法的请求参数对其结果进行缓存，和 @Cacheable 不同的是，**它每次都会触发真实方法的调用**\n\n\n| 参数        | 解释                                                     | example                                                       |\n| --------- | ------------------------------------------------------ | ------------------------------------------------------------- |\n| value     | 缓存的名称，在 spring 配置文件中定义，必须指定至少一个                        | @CachePut(value=”my cache”)                                   |\n| key       | 缓存的 key，可以为空，如果指定要按照 SpEL 表达式编写，如果不指定，则缺省按照方法的所有参数进行组合 | @CachePut(value=”testcache”,key=”#userName”)                  |\n| condition | 缓存的条件，可以为空，使用 SpEL 编写，返回 true 或者 false，只有为 true 才进行缓存  | @CachePut(value=”testcache”,condition=”#userName.length()>2”) |\n\n\n**实例**\n\n\n**@CachePut 注释，这个注释可以确保方法被执行，同时方法的返回值也被记录到缓存中，实现缓存与数据库的同步更新。**\n\n\n```java\n@CachePut(value=\"accountCache\",key=\"#account.getName()\")// 更新accountCache 缓存\npublic Account updateAccount(Account account) {\n  return updateDB(account);\n}\n\n```\n\n\n---\n\n\n### **@CacheEvict**\n\n\n> @CacheEvict 作用和配置方法\n\n\n@CachEvict 的作用 主要针对方法配置，**能够根据一定的条件对缓存进行清空**\n\n\n| 参数               | 解释                                                                          | example                                                         |\n| ---------------- | --------------------------------------------------------------------------- | --------------------------------------------------------------- |\n| value            | 缓存的名称，在 spring 配置文件中定义，必须指定至少一个                                             | @CacheEvict(value=”my cache”)                                   |\n| key              | 缓存的 key，可以为空，如果指定要按照 SpEL 表达式编写，如果不指定，则缺省按照方法的所有参数进行组合                      | @CacheEvict(value=”testcache”,key=”#userName”)                  |\n| condition        | 缓存的条件，可以为空，使用 SpEL 编写，返回 true 或者 false，只有为 true 才进行缓存                       | @CacheEvict(value=”testcache”,condition=”#userName.length()>2”) |\n| allEntries       | 是否清空所有缓存内容，缺省为 false，如果指定为 true，则方法调用后将立即清空所有缓存                             | @CachEvict(value=”testcache”,allEntries=true)                   |\n| beforeInvocation | 是否在方法执行前就清空，缺省为 false，如果指定为 true，则在方法还没有执行的时候就清空缓存，缺省情况下，如果方法执行抛出异常，则不会清空缓存 | @CachEvict(value=”testcache”，beforeInvocation=true)             |\n\n\n实例\n\n\n```java\n@CacheEvict(value=\"accountCache\",key=\"#account.getName()\")// 清空accountCache 缓存\npublic void updateAccount(Account account) {\n   updateDB(account);\n}\n\n\n@CacheEvict(value=\"accountCache\",allEntries=true)// 清空accountCache 缓存\npublic void reload() {\n\n   reloadAll()\n\n}\n\n\n\n\n@Cacheable(value=\"accountCache\",condition=\"#userName.length() <=4\")// 缓存名叫 accountCache\npublic Account getAccountByName(String userName) {\n\n // 方法内部实现不考虑缓存逻辑，直接实现业务\n return getFromDB(userName);\n\n}\n\n```\n\n\n---\n\n\n### **@CacheConfig**\n\n\n所有的@Cacheable（）里面都有一个value＝“xxx”的属性，这显然如果方法多了，写起来也是挺累的，如果可以一次性声明完 那就省事了， 所以，有了@CacheConfig这个配置，@CacheConfig is a class-level annotation that allows to share the cache names，如果你在你的方法写别的名字，那么依然以方法的名字为准。\n\n\n```java\n@CacheConfig(\"books\")\npublic class BookRepositoryImpl implements BookRepository {\n\n\n  @Cacheable\n  public Book findBook(ISBN isbn) {...}\n\n}\n```\n\n\n---\n\n\n### **条件缓存**\n\n\n下面提供一些常用的条件缓存\n\n\n```java\n//@Cacheable将在执行方法之前( #result还拿不到返回值)判断condition，如果返回true，则查缓存；\n@Cacheable(value = \"user\", key = \"#id\", condition = \"#id lt 10\")\npublic User conditionFindById(final Long id)\n\n\n\n\n//@CachePut将在执行完方法后（#result就能拿到返回值了）判断condition，如果返回true，则放入缓存；\n@CachePut(value = \"user\", key = \"#id\", condition = \"#result.username ne 'zhang'\")\npublic User conditionSave(final User user)\n\n\n\n//@CachePut将在执行完方法后（#result就能拿到返回值了）判断unless，如果返回false，则放入缓存；（即跟condition相反）\n@CachePut(value = \"user\", key = \"#user.id\", unless = \"#result.username eq 'zhang'\")\npublic User conditionSave2(final User user)\n\n\n\n//@CacheEvict， beforeInvocation=false表示在方法执行之后调用（#result能拿到返回值了）；且判断condition，如果返回true，则移除缓存；\n@CacheEvict(value = \"user\", key = \"#user.id\", beforeInvocation = false, condition = \"#result.username ne 'zhang'\")\npublic User conditionDelete(final User user)\n\n```\n\n\n---\n\n\n### **@Caching**\n\n\n有时候我们可能组合多个Cache注解使用；比如用户新增成功后，我们要添加id–>user；username—>user；email—>user的缓存；此时就需要@Caching组合多个注解标签了。\n\n\n```java\n@Caching(put = {\n@CachePut(value = \"user\", key = \"#user.id\"),\n@CachePut(value = \"user\", key = \"#user.username\"),\n@CachePut(value = \"user\", key = \"#user.email\")\n})\n\npublic User save(User user) {\n\n```\n\n\n---\n\n\n### **自定义缓存注解**\n\n\n比如之前的那个@Caching组合，会让方法上的注解显得整个代码比较乱，此时可以使用自定义注解把这些注解组合到一个注解中，如：\n\n\n```java\n@Caching(put = {\n@CachePut(value = \"user\", key = \"#user.id\"),\n@CachePut(value = \"user\", key = \"#user.username\"),\n @CachePut(value = \"user\", key = \"#user.email\")\n})\n\n@Target({ElementType.METHOD, ElementType.TYPE})\n@Retention(RetentionPolicy.RUNTIME)\n@Inherited\npublic @interface UserSaveCache {\n}\n```\n\n\n这样我们在方法上使用如下代码即可，整个代码显得比较干净。\n\n\n```java\n@UserSaveCache\npublic User save(User user)\n\n```\n\n\n---\n\n\n### 扩展\n\n\n比如findByUsername时，不应该只放username–>user，应该连同id—>user和email—>user一起放入；这样下次如果按照id查找直接从缓存中就命中了\n\n\n```java\n@Caching(\n  cacheable = {\n    @Cacheable(value = \"user\", key = \"#username\")\n  },\n  put = {\n    @CachePut(value = \"user\", key = \"#result.id\", condition = \"#result != null\"),\n    @CachePut(value = \"user\", key = \"#result.email\", condition = \"#result != null\")\n  }\n)\npublic User findByUsername(final String username) {\n  System.out.println(\"cache miss, invoke find by username, username:\" + username);\n  for (User user : users) {\n    if (user.getUsername().equals(username)) {\n      return user;\n    }\n  }\n  return null;\n}\n\n```\n\n\n其实对于：id—>user；username—->user；email—>user；更好的方式可能是：id—>user；username—>id；email—>id；保证user只存一份；如：\n\n\n```java\n@CachePut(value=\"cacheName\", key=\"#user.username\", cacheValue=\"#user.username\")\npublic void save(User user)\n\n\n@Cacheable(value=\"cacheName\", key=\"#user.username\", cacheValue=\"#caches[0].get(#caches[0].get(#username).get())\")\n\npublic User findByUsername(String username)\n\n```\n\n\n**SpEL上下文数据**\n\n\nSpring Cache提供了一些供我们使用的SpEL上下文数据，下表直接摘自Spring官方文档：\n\n\n| 名称            | 位置     | 描述                                                                      | 示例                                           |\n| ------------- | ------ | ----------------------------------------------------------------------- | -------------------------------------------- |\n| methodName    | root对象 | 当前被调用的方法名                                                               | root.methodName                              |\n| method        | root对象 | 当前被调用的方法                                                                | [root.method.name](http://root.method.name/) |\n| target        | root对象 | 当前被调用的目标对象                                                              | root.target                                  |\n| targetClass   | root对象 | 当前被调用的目标对象类                                                             | root.targetClass                             |\n| args          | root对象 | 当前被调用的方法的参数列表                                                           | root.args[0]                                 |\n| caches        | root对象 | 当前方法调用使用的缓存列表（如@Cacheable(value={“cache1”, “cache2”})），则有两个cache        | root.caches[0].name                          |\n| argument name | 执行上下文  | 当前被调用的方法的参数，如findById(Long id)，我们可以通过#id拿到参数                            | [user.id](http://user.id/)                   |\n| result        | 执行上下文  | 方法执行后的返回值（仅当方法执行之后的判断有效，如‘unless'，'cache evict'的beforeInvocation=false） | result                                       |\n\n\n```java\n@CacheEvict(value = \"user\", key = \"#user.id\", condition = \"#root.target.canCache() and #root.caches[0].get(#user.id).get().username ne #user.username\", beforeInvocation = true)\npublic void conditionUpdate(User user)\n```\n\n",
      "properties": {
        "password": "",
        "icon": "",
        "date": "2020-06-02",
        "type": "Post",
        "category": "技术分享",
        "urlname": "18",
        "catalog": [
          "archives"
        ],
        "tags": [
          "Spring",
          "缓存"
        ],
        "summary": "Spring Cache是Spring框架用于支持缓存的模块。它提供了一组缓存抽象，使得我们可以将不同的缓存技术集成到应用程序中，从而提高性能和可扩展性。Spring Cache通过使用轻量级的注释来定义缓存的行为，从而减少了缓存操作的复杂性。此外，Spring Cache还支持事务性缓存，这样可以保证缓存与数据库之间的一致性。总体来说，Spring Cache是一个强大的工具，可以极大地提高应用程序的性能和可用性。",
        "sort": "",
        "title": "spring cache",
        "status": "Published",
        "updated": "2023-10-08 14:42:00"
      },
      "catalog": [
        {
          "title": "archives",
          "doc_id": "bcd4f723-5f6a-4d94-90a1-63bb8c6dbfcf"
        }
      ],
      "body": "",
      "realName": "spring cache",
      "relativePath": "/archives/spring cache.md"
    },
    {
      "id": "76be662e-68e2-4e75-8d51-c66197e850d3",
      "doc_id": "76be662e-68e2-4e75-8d51-c66197e850d3",
      "title": "76be662e-68e2-4e75-8d51-c66197e850d3",
      "updated": 1696747320000,
      "body_original": "\n> 记录一次scan和keys的使用,scan和key都是redis搜索key的值函数,但实现却完全不同。生产环境用key的同学准备好跑路吧~\n\n\n# keys\n\n\n> Warning: consider KEYS as a command that should only be used in production environments with extreme care. It may ruin performance when it is executed against large databases. This command is intended for debugging and special operations, such as changing your keyspace layout. Don't use KEYS in your regular application code. If you're looking for a way to find keys in a subset of your keyspace, consider using sets.\n\n\n> 上面是官方文档声明，KEYS命令不能用在生产的环境中，这个时候如果数量过大效率是十分低的。\n\n\n```text\n       /**\n\t * Find all keys matching the given {@code pattern}.\n\t *\n\t * @param pattern must not be {@literal null}.\n\t * @return {@literal null} when used in pipeline / transaction.\n\t * @see <a href=\"<https://redis.io/commands/keys>\">Redis Documentation: KEYS</a>\n\t */\n\t@Nullable\n\tSet<K> keys(K pattern);\n\n\n```\n\n\n> keys函数支持传入一个正则字符串，遍历redis中所有匹配到的key。命令会引起阻塞,性能随着数据库数据的增多而越来越慢。\n\n\n# SCAN\n\n\n> Redis从2.8版本开始支持scan命令，SCAN命令的基本用法如下\n\n1. 复杂度虽然也是 O(n)，通过游标分步进行不会阻塞线程;\n2. 有限制参数 COUNT ；\n3. 同 keys命令 一样提供模式匹配功能;\n4. 服务器不需要为游标保存状态，游标的唯一状态就是 scan 返回给客户端的游标整数;\n\n> 但是需要注意返回结果可能重复，需要客户端去重。如果遍历过程中有数据修改，改动后的数据不保证同步。\n\n\n_scan 命令提供三个参数，第一个是cursor，第二个是要匹配的正则，第三个是单次遍历的槽位_\n\n\n## RedisOperations sacn\n\n\n> spring中redisOperations只有HashOperations类提供scan方法\n\n\n```text\n      /**\n\t * Use a {@link Cursor} to iterate over entries in hash at {@code key}. <br />\n\t * <strong>Important:</strong> Call {@link Cursor#close()} when done to avoid resource leak.\n\t *\n\t * @param key must not be {@literal null}.\n\t * @param options\n\t * @return {@literal null} when used in pipeline / transaction.\n\t * @since 1.4\n\t */\n\tCursor<Map.Entry<HK, HV>> scan(H key, ScanOptions options);\n\n```\n\n\n## 使用\n\n\n```text\n       /**\n         *  搜索\n         */\n        Cursor<Map.Entry<ID, Double>> cursor = hash.scan(key,new ScanOptions.ScanOptionsBuilder().match(sb.toString()).count(keys.size()).build());\n\n        Map<ID, Double> map = new HashMap<>(keys.size());\n        while (cursor.hasNext()) {\n            Map.Entry<ID, Double> next = cursor.next();\n            map.put(next.getKey(), next.getValue());\n        }\n\n```\n\n\n> 第一个遍历是 cursor 值为0，然后将返回结果的第一个整数作为下一个遍历的游标，如果最后返回的到cursor的值为0就代表结束。\n\n",
      "properties": {
        "password": "",
        "icon": "",
        "date": "2020-07-23",
        "type": "Post",
        "category": "技术分享",
        "urlname": "9",
        "catalog": [
          "archives"
        ],
        "tags": [
          "Spring",
          "Java",
          "Redis"
        ],
        "summary": "记录一次scan和keys的使用,scan和key都是redis搜索key的值函数,但实现却完全不同。生产环境用key的同学准备好跑路吧~keysWarning: consider KEYS as a",
        "sort": "",
        "title": "RedisOperations scan 用法",
        "status": "Published",
        "updated": "2023-10-08 14:42:00"
      },
      "catalog": [
        {
          "title": "archives",
          "doc_id": "76be662e-68e2-4e75-8d51-c66197e850d3"
        }
      ],
      "body": "",
      "realName": "RedisOperations scan 用法",
      "relativePath": "/archives/RedisOperations scan 用法.md"
    },
    {
      "id": "a1eb98de-4986-46c0-b1ac-6846629d0819",
      "doc_id": "a1eb98de-4986-46c0-b1ac-6846629d0819",
      "title": "a1eb98de-4986-46c0-b1ac-6846629d0819",
      "updated": 1696747320000,
      "body_original": "\n# webFlux 初识\n\n\n## **Lambda**\n\n\nLambda 表达式，有时候也称为匿名函数或箭头函数，几乎在当前的各种主流的编程语言中都有它的身影。Java8 中引入 Lambda 表达式，使原本需要用**匿名类**实现接口来传递行为，现在通过 Lambda 可以更直观的表达。\n\n- Lambda 表达式，也可称为闭包。闭包就是一个定义在函数内部的函数，闭包使得变量即使脱离了该函数的作用域范围也依然能被访问到。\n- Lambda 表达式的本质只是一个”语法糖”，由编译器推断并帮你转换包装为常规的代码,因此你可以使用更少的代码来实现同样的功能。\n- Lambda 表达式是一个匿名函数，即没有函数名的函数。有些函数如果只是临时一用，而且它的业务逻辑也很简单时，就没必要非给它取个名字不可。\n- Lambda 允许把函数作为一个方法的参数（函数作为参数传递进方法中）.\n\nLambda 表达式语法如下：**形参列表=>函数体**（函数体多于一条语句的可用大括号括起）。在Java里就是**() -> {}**：\n\n\n```text\n(参数) -> 表达式\n\n```\n\n\n或\n\n\n```text\n(参数) ->{ 代码语句 }\n\n```\n\n\nLambda表达式的重要特征：\n\n- Lambda 表达式主要用来定义行内执行的方法类型接口，例如，一个简单方法接口。\n- Lambda表达式是通过函数式接口（必须**有且仅有一个抽象方法声明**）识别的\n- 可选类型声明：不需要声明参数类型，编译器可以统一识别参数值。\n- 可选的参数圆括号：一个参数无需定义圆括号，但多个参数需要定义圆括号。\n- 可选的大括号：如果主体包含了一个语句，就不需要使用大括号。\n- 可选的返回关键字：如果主体只有一个表达式返回值，则编译器会自动返回值，大括号需要指定表达式返回一个值。\n\nLambda表达式中的变量作用域：\n\n- 访问权限与匿名对象的方式非常类似。只能够访问**局部对应的外部区域的局部final变量，以及成员变量和静态变量**。\n- 在Lambda表达式中能访问域外的局部非final变量、但**不能修改Lambda域外的局部非final变量**。因为在Lambda表达式中，Lambda域外的局部非final变量会在编译的时候，会被**隐式地当做final变量来处理**。\n- Lambda表达式内部**无法访问接口默认（default）方法**\n\n例子：使用Java 8之前的方法来实现对一个列表进行排序：\n\n\n```text\nList<String> names = Arrays.asList(\"aaa\", \"cccc\", \"ddd\", \"bbb\");\nCollections.sort(names, new Comparator<String>() {\n    @Override\n    public int compare(String a, String b) {\n        return b.compareTo(a);\n    }\n});\n\n```\n\n\nJava 8 Lambda 表达式：\n\n\n```text\nCollections.sort(names, (String a, String b) -> {\n    return b.compareTo(a);\n});\n// 只有一条逻辑语句，可以省略大括号\nCollections.sort(names, (String a, String b) -> b.compareTo(a));\n// 可以省略入参类型\nCollections.sort(names, (a, b) -> b.compareTo(a));\n\n```\n\n\n### **类型推断**\n\n\n通常 Lambda 表达式的参数并不需要显示声明类型。那么对于给定的Lambda表达式，程序如何知道对应的是哪个函数接口以及参数的类型呢？编译器**通过 Lambda 表达式所在的上下文来进行目标类型推断，通过检查 Lambda 表达式的入参类型及返回类型，和对应的目标类型的方法签名是否一致，推导出合适的函数接口**。比如：\n\n\n```text\nStream.of(\"我是字符串A\", \"我是字符串B\").map(s -> s.length()).filter(l -> l == 3);\n\n```\n\n\n在上面的例子中，对于传入 map 方法的 Lamda 表达式，从 Stream 的类型上下文可以推导出入参是 String 类型，从函数的返回值可以推导出出参是整形类型，因此可推导出对应的函数接口类型为 Function；对于传入 filter 方法的 Lamda 表达式，从 pipeline 的上下文可得知入参是整形类型，因此可推导出函数接口 Predicate。\n\n\n### 方法引用\n\n\nJava 8 中还可以通过**方法引用**来**表示 Lambda 表达式**。方法引用是用来直接访问类或者实例的已经存在的方法或者构造方法。Java 8 允许你通过\"**::**\"关键字获取方法或者构造函数的引用。方法引用提供了一种**引用而不执行方法**的方式，它需要由兼容的函数式接口构成目标类型上下文。计算时，方法引用会创建函数式接口的一个实例。常用的方法引用有：\n\n- 静态方法引用：ClassName::methodName\n- 实例对象上的方法引用：instanceReference::methodName\n- 类上的方法引用：ClassName::methodName\n- 构造方法引用：Class::new\n- 数组构造方法引用：TypeName[]::new\n\n例子：\n\n\n```text\n// 静态方法引用\nStream.of(someStringArray).allMatch(StringUtils::isNotEmpty);\n// 实例对象上的方法引用\nStream.of(someStringArray).map(this::someTransform);\n// 类上的方法引用\nStream.of(someStringArray).mapToInt(String::length);\n// 构造方法引用\nStream.of(someStringArray).collect(Collectors.toCollection(LinkedList::new));\n// 数组构造方法引用\nStream.of(someStringArray).toArray(String[]::new);\n\n```\n\n\n### 函数式接口\n\n\nJava 8 中采用**函数式接口**作为**Lambda 表达式**的目标类型。**函数式接口(Functional Interface)****是一个****有且仅有一个抽象方法声明**的**接口**。任意只包含一个抽象方法的接口，我们都可以用来做成Lambda表达式。每个与之对应的lambda表达式必须要与抽象方法的声明相匹配。函数式接口与其他普通接口的区别：\n\n- 函数式接口中**只能有一个抽象方法**（这里不包括与Object的方法重名的方法）\n- 接口中唯一抽象方法的命名并不重要，因为函数式接口就是对某一行为进行抽象，主要目的就是支持 Lambda 表达式\n- 自定义函数式接口时，应当在接口前加上**@FunctionalInterface**标注（虽然不加也不会有错误）。编译器会注意到这个标注，如果你的接口中定义了第二个抽象方法的话，编译器会抛出异常。\n\n## 函数式编程\n\n\n> Java来讲，从命令式编程到函数式编程的关键转变是从Java8多了一个funtcion包开始，在此基础上的stream更好的诠释了这一点，而之后java 9 的reactor，再到spring5的webflux都是在其基础上一步步演变的\n\n\n### java.util.function\n\n\n![](https://pic3.zhimg.com/80/v2-aae13ab98b68ec811e4efa2808b499ae_1440w.jpg)\n\n\n```text\n        Function<T, R> stringIntegerFunction //输入T返回R的函数\n        Predicate<T> predicate //输入T，返回boolean值，断言（谓词）函数\n        Consumer<T> consumer; //消费者函数，消费一个数据\n        Supplier<T> supplier; // 生产者函数，提供数据\n\n```\n\n\n### Function\n\n\n```text\n    /**\n     * 将范型T对象应用到输入的参数上，然后返回计算结果\n     *\n     * @param t the function argument\n     * @return the function result\n     */\n    R apply(T t);\n\n   /**\n     *  返回一个先执行before函数对象apply方法再执行当前函数对象apply方法的函数对象\n     *\n     * @param <V> 前置函数的的输入类型，以及函数的输入类型 由函数\n     *\n     */\n    default <V> Function<V, R> compose(Function<? super V, ? extends T> before) {\n        Objects.requireNonNull(before);\n        return (V v) -> apply(before.apply(v));\n    }\n\n\n  /**\n    * 返回一个先执行当前函数对象apply方法再执行after函数对象apply方法的函数对象。\n    *  <br>\n    *    compose 和 andThen 的不同之处是函数执行的顺序不同。compose 函数先执行参数，\n    *     然后执行调用者，而 andThen 先执行调用者，然后再执行参数。\n    *  </br>\n    */\n   default <V> Function<T, V> andThen(Function<? super R, ? extends V> after) {\n        Objects.requireNonNull(after);\n        return (T t) -> after.apply(apply(t));\n    }\n\n   /**\n     *  返回输入结果\n     */\n    static <T> Function<T, T> identity() {\n        return t -> t;\n    }\n\n```\n\n\n> 标注为FunctionalInterface的接口被称为函数式接口，该接口只能有一个自定义方法，但是可以包括从object类继承而来的方法。如果一个接口只有一个方法，则编译器会认为这就是一个函数式接口。\n\n\n**是否是一个函数式接口，需要注意的有以下几点：**\n\n- 该注解只能标记在”有且仅有一个抽象方法”的接口上。\n- JDK8接口中的静态方法和默认方法，都不算是抽象方法。\n- 接口默认继承java.lang.Object，所以如果接口显示声明覆盖了Object中方法，那么也不算抽象方法。\n- 该注解不是必须的，如果一个接口符合”函数式接口”定义，那么加不加该注解都没有影响。加上该注解能够更好地让编译器进行检查。如果编写的不是函数式接口，但是加上了@FunctionInterface，那么编译器会报错。\n- 在一个接口中定义两个自定义的方法，就会产生Invalid ‘@FunctionalInterface’ annotation; FunctionalInterfaceTest is not a functional interface错误.\n\n## 响应式\n\n\n> 响应式流(Reactive Streams)通过定义一组实体，接口和互操作方法，给出了实现异步非阻塞背压的标准。第三方遵循这个标准来实现具体的解决方案，常见的有Reactor，RxJava，Akka Streams，Ratpack等。\n\n\n响应式编程（reactive programming）是一种基于数据流（data stream）和变化传递（propagation of change）的声明式（declarative）的编程范式\n\n\n一个通用的流处理架构一般会是这样的（**生产者产生数据，对数据进行中间处理，消费者拿到数据消费**)\n\n\n![](https://pic4.zhimg.com/80/v2-edb2811ded51ed4de4de5656b057fc07_1440w.png)\n\n- 数据来源，一般称为生产者（Producer）\n- 数据的目的地，一般称为消费者(Consumer)\n- 在处理时，对数据执行某些操作一个或多个处理阶段。（Processor)\n\n规范定义了4个接口\n\n\n![](https://pic4.zhimg.com/80/v2-d235427992d3feb5e49bcf2b210ba5eb_1440w.jpg)\n\n\n在响应式流上提到了back pressure（背压）这么一个概念。在响应式流实现异步非阻塞是基于生产者和消费者模式的，而生产者消费者很容易出现的一个问题就是：**生产者生产数据多了，就把消费者给压垮了**\n\n\n通俗就是： **消费者能告诉生产者自己需要多少量的数据**。这里就是**Subscription**接口所做的事\n\n\n**特质**\n\n\n[原文:](https://www.reactivemanifesto.org/zh-CN)[https://www.reactivemanifesto.org/zh-CN](https://www.reactivemanifesto.org/zh-CN)\n\n\n**即时响应性:** ：只要有可能， [系统](https://www.reactivemanifesto.org/zh-CN/glossary#System)就会及时地做出响应。 即时响应是可用性和实用性的基石， 而更加重要的是，即时响应意味着可以快速地检测到问题并且有效地对其进行处理。 即时响应的系统专注于提供快速而一致的响应时间， 确立可靠的反馈上限， 以提供一致的服务质量。 这种一致的行为转而将简化错误处理、 建立最终用户的信任并促使用户与系统作进一步的互动。\n\n- *回弹性：**系统在出现[失败](https://www.reactivemanifesto.org/zh-CN/glossary#Failure)时依然保持即时响应性。 这不仅适用于高可用的、 任务关键型系统——任何不具备回弹性的系统都将会在发生失败之后丢失即时响应性。 回弹性是通过[复制](https://www.reactivemanifesto.org/zh-CN/glossary#Replication)、 遏制、 [隔离](https://www.reactivemanifesto.org/zh-CN/glossary#Isolation)以及[委托](https://www.reactivemanifesto.org/zh-CN/glossary#Delegation)来实现的。 失败的扩散被遏制在了每个[组件](notion://www.notion.so/glossary.zh-cn.md#%E7%BB%84%E4%BB%B6)内部， 与其他组件相互隔离， 从而确保系统某部分的失败不会危及整个系统，并能独立恢复。 每个组件的恢复都被委托给了另一个（外部的）组件， 此外，在必要时可以通过复制来保证高可用性。 （因此）组件的客户端不再承担组件失败的处理。\n\n**弹性：** 系统在不断变化的工作负载之下依然保持即时响应性。 反应式系统可以对输入（负载）的速率变化做出反应，比如通过增加或者减少被分配用于服务这些输入（负载）的[资源](https://www.reactivemanifesto.org/zh-CN/glossary#Resource)。 这意味着设计上并没有争用点和中央瓶颈， 得以进行组件的分片或者复制， 并在它们之间分布输入（负载）。 通过提供相关的实时性能指标， 反应式系统能支持预测式以及反应式的伸缩算法。 这些系统可以在常规的硬件以及软件平台上实现成本高效的[弹性](https://www.reactivemanifesto.org/zh-CN/glossary#Elasticity)[。](https://www.reactivemanifesto.org/zh-CN/glossary#Elasticity)\n\n- [*消息驱动：**反应式系统依赖](https://www.reactivemanifesto.org/zh-CN/glossary#Elasticity)[异步的](https://www.reactivemanifesto.org/zh-CN/glossary#Asynchronous)[消息传递](https://www.reactivemanifesto.org/zh-CN/glossary#Message-Driven)，从而确保了松耦合、隔离、[位置透明](https://www.reactivemanifesto.org/zh-CN/glossary#Location-Transparency)的组件之间有着明确边界。 这一边界还提供了将[失败](https://www.reactivemanifesto.org/zh-CN/glossary#Failure)作为消息委托出去的手段。 使用显式的消息传递，可以通过在系统中塑造并监视消息流队列， 并在必要时应用[回压](https://www.reactivemanifesto.org/zh-CN/glossary#Back-Pressure)， 从而实现负载管理、 弹性以及流量控制。 使用位置透明的消息传递作为通信的手段， 使得跨集群或者在单个主机中使用相同的结构成分和语义来管理失败成为了可能。 [非阻塞的](https://www.reactivemanifesto.org/zh-CN/glossary#Non-Blocking)通信使得接收者可以只在活动时才消耗[资源](https://www.reactivemanifesto.org/zh-CN/glossary#Resource)， 从而减少系统开销。\n\n![](https://www.reactivemanifesto.org/images/reactive-traits-zh-cn.svg)\n\n\n大型系统由多个较小型的系统所构成， 因此整体效用取决于它们的构成部分的反应式属性。 这意味着， 反应式系统应用着一些设计原则，使这些属性能在所有级别的规模上生效，而且可组合。\n\n\n## Reactive\n\n\n[官网https://projectreactor.io/docs/core/release/reference/index.html#which.windowprojectreactor.io](https://projectreactor.io/docs/core/release/reference/index.html)\n\n\n在reactor中有两个最基本的概念，发布者和订阅者，可以理解为生产者和消费者的概念。在Reactor中发布者有两个，一个是**Flux**，一个是**Mono。** Flux代表的是0-N个元素的响应式序列，而Mono代表的是0-1个的元素的结果。\n\n\n在Reactive中\n\n- Publisher（发布者)相当于生产者(Producer)\n- Subscriber(订阅者)相当于消费者(Consumer)\n- Processor就是在发布者与订阅者之间处理数据用的\n\n```text\n// 发布者(生产者)public interface Publisher<T> {   // 可以被订阅多次，每次生成新的Subscriber，每个消费者只能订阅一次Publisher，执行过程出错会直接报error    public void subscribe(Subscriber<? super T> s);}// 订阅者(消费者)public interface Subscriber<T> {   //该方法在订阅Publisher之后执行，在订阅之前不会有数据流的消费    public void onSubscribe(Subscription s);   /**     * 消费下一个消息，在执行request方法之后通知Publisher，     *可被调用多次，有request（x），参数x决定执行几次     */    public void onNext(T t);    //执行出错调用方法    public void onError(Throwable t);     //执行完成之后调用方法    public void onComplete();}// 用于发布者与订阅者之间的通信(实现背压：订阅者能够告诉生产者需要多少数据)public interface Subscription {    \t//消费请求    public void request(long n);       //取消请求    public void cancel();}// 用于处理发布者 发布消息后，对消息进行处理，再交由消费者消费public interface Processor<T,R> extends Subscriber<T>, Publisher<R> {}\n\n```\n\n\n### **Mono** (返回0或1个元素)\n\n\nMono 是响应流 Publisher 具有基础 rx 操作符。可以成功发布元素或者错误。如图所示：\n\n\n![](https:////upload-images.jianshu.io/upload_images/1483536-03c853fb59816f3a.png)\n\n\n### 常用方法\n\n\n```text\nMono.create(); //：使用 MonoSink 来创建 MonoMono.justOrEmpty(); //：从一个 Optional 对象或 null 对象中创建 Mono。 只有 Optional 对象中包含值或对象不为 null 时，Mono 序列才产生对应的元素。Mono.error(); //：创建一个只包含错误消息的 MonoMono.never(); //：创建一个不包含任何消息通知的 MonoMono.delay(); //：在指定的延迟时间之后，创建一个 Mono，产生数字 0 作为唯一值Mono.just(); //创建一个不为null的数据流 声明的参数就是数据流的元素 创建出来的 Mono序列在发布这些元素之后会自动结束/**注释同下*/Mono.fromCallable(); // 从回调函数生产数据 CallableMono.fromCompletionStage(); //异步任务中 CompletionStage Mono.fromFuture(); //异步任务中 CompletableFutureMono.fromRunnable(); // 异步任务 RunnableMono.fromSupplier()：//Supplier 提供着\n\n```\n\n\n### *Flux **(返回0-n个元素)\n\n\nFlux 是响应流 Publisher 具有基础 rx 操作符。可以成功发布 0 到 N 个元素或者错误。Flux 其实是 Mono 的一个补充。如图所示：\n\n\n![](https:////upload-images.jianshu.io/upload_images/1483536-55fd0aeaaea2d023.png)\n\n\n所以要注意：如果知道 Publisher 是 0 或 1 个，则用 Mono。\n\n\nFlux 最值得一提的是 fromIterable 方法。   fromIterable(Iterable<? extends T> it)  可以发布 Iterable 类型的元素。\n\n\n![](https://pic1.zhimg.com/80/v2-a72c1db9d5564ae6e47ec22ad3e6b7f0_1440w.jpg)\n\n\n当调用just方法，查看源码可以得知，返回的是一个Flux<T>对象，当次数为0直接返回空，为1 的时候单独处理，其余的通过_onAssembly方法处理返回fluxarrary对象。_\n\n\n```text\n\t/**\t * Create a {@link Flux} that emits the items contained in the provided array.\t * <p>\t * <img class=\"marble\" src=\"doc-files/marbles/fromArray.svg\" alt=\"\">\t *\t * @param array the array to read data from\t * @param <T> The type of values in the source array and resulting Flux\t *\t * @return a new {@link Flux}\t */\tpublic static <T> Flux<T> fromArray(T[] array) {\t\tif (array.length == 0) {\t\t\treturn empty();\t\t}\t\tif (array.length == 1) {\t\t\treturn just(array[0]);\t\t}\t\treturn onAssembly(new FluxArray<>(array));\t}final class FluxArray<T> extends Flux<T> implements Fuseable, SourceProducer<T> {\tfinal T[] array; //存储数据\t@SafeVarargs\tpublic FluxArray(T... array) {\t\tthis.array = Objects.requireNonNull(array, \"array\");\t}\t@SuppressWarnings(\"unchecked\")        //订阅方法\tpublic static <T> void subscribe(CoreSubscriber<? super T> s, T[] array) {\t\tif (array.length == 0) {\t\t\tOperators.complete(s);\t\t\treturn;\t\t}\t\tif (s instanceof ConditionalSubscriber) {      // 此处是个啥？\t\t\ts.onSubscribe(new ArrayConditionalSubscription<>((ConditionalSubscriber<? super T>) s, array));\t\t}\t\telse {\t\t\ts.onSubscribe(new ArraySubscription<>(s, array));\t\t}\t}      // 正常消费者  static final class ArraySubscription<T>\t\t\timplements InnerProducer<T>, SynchronousSubscription<T> {\t\tfinal CoreSubscriber<? super T> actual;\t\tfinal T[] array; //存储数据\t\tint index;\t\tvolatile boolean cancelled; //记录是否取消\t\tvolatile long requested; //记录请求多少次\t\t@SuppressWarnings(\"rawtypes\")\t\tstatic final AtomicLongFieldUpdater<ArraySubscription> REQUESTED =\t\t\t\tAtomicLongFieldUpdater.newUpdater(ArraySubscription.class, \"requested\");\t\tArraySubscription(CoreSubscriber<? super T> actual, T[] array) {\t\t\tthis.actual = actual;\t\t\tthis.array = array;\t\t}\t\t@Override\t\tpublic void request(long n) {\t\t\tif (Operators.validate(n)) {\t\t\t\tif (Operators.addCap(REQUESTED, this, n) == 0) {\t\t\t\t\tif (n == Long.MAX_VALUE) {\t\t\t\t\t\tfastPath();\t\t\t\t\t}\t\t\t\t\telse {\t\t\t\t\t\tslowPath(n);\t\t\t\t\t}\t\t\t\t}\t\t\t}\t\t}\n\n```\n\n\n流程\n\n\n![](https://pic3.zhimg.com/80/v2-feadef87bacc918580803b777a21e042_1440w.jpg)\n\n\n### 内置的 Processor\n\n\n`Processor`既是一种特别的发布者（`Publisher`）又是一种订阅者（`Subscriber`）。 所以你能够订阅一个`Processor`，也可以调用它们提供的方法来手动插入数据到序列，或终止序列。\n\n\n> 一直在聊响应式流的四个接口中的三个：Publisher、Subscriber、Subscription，唯独Processor迟迟没有提及。原因在于想用好它们不太容易，多数情况下，我们应该进行避免使用Processor，通常来说仅用于一些特殊场景。\n\n\n`Reactor Core` 内置多种 `Processor`。这些 processor 具有不同的语法，大概分为三类。\n\n- **直接的（direct）**（DirectProcessor 和 UnicastProcessor）：这些 processors 只能通过直接 调用 Sink 的方法来推送数据。\n- **同步的（synchronous）**（EmitterProcessor 和 ReplayProcessor）：这些 processors 既可以直接调用 Sink 方法来推送数据，也可以通过订阅到一个上游的发布者来同步地产生数据。\n- **异步的（asynchronous）**（WorkQueueProcessor 和 TopicProcessor）：这些 processors 可以将从多个上游发布者得到的数据推送下去。由于使用了 RingBuffer 的数据结构来缓存多个来自上游的数据，因此更加有健壮性。\n\n异步的 processor 在实例化的时候最复杂，因为有许多不同的选项。因此它们暴露出一个 Builder 接口。 而简单的 processors 有静态的工厂方法。\n\n\n### DirectProcessor\n\n\n       `DirectProcessor` 可以将信号分发给零到多个订阅者（Subscriber）。它是最容易实例化的，使用静态方法 create() 即可。另一方面，它的不足是无法处理背压。所以，当DirectProcessor推送的是 N 个元素，而至少有一个订阅者的请求个数少于 N 的时候，就会发出一个IllegalStateException。\n\n\n一旦 Processor 结束（通常通过调用它的 Sink 的 error(Throwable) 或 complete() 方法）， 虽然它允许更多的订阅者订阅它，但是会立即向它们重新发送终止信号。\n\n\n### UnicastProcessor\n\n\n     `UnicastProcessor`可以使用一个内置的缓存来处理背压。代价就是它最多只能有一个订阅者（上一节的例子通过publish转换成了ConnectableFlux，所以可以接入两个订阅者）。\n\n\n> UnicastProcessor有多种选项，因此提供多种不同的create静态方法。例如，它默认是 无限的（unbounded） ：如果你在在订阅者还没有请求数据的情况下让它推送数据，它会缓存所有数据。\n\n\n\t可以通过提供一个自定义的 Queue 的具体实现传递给 create 工厂方法来改变默认行为。如果给出的队列是有限的（bounded）， 并且缓存已满，而且未收到下游的请求，processor 会拒绝推送数据。\n\n\n在“有限的”队列中，还可以在构造 processor 的时候提供一个回调方法，这个回调方法可以在每一个 被拒绝推送的元素上调用，从而让开发者有机会清理这些元素。\n\n\n### EmitterProcessor\n\n\n    `EmitterProcessor`能够向多个订阅者发送数据，并且可以对每一个订阅者进行背压处理。它本身也可以订阅一个发布者并同步获得数据。\n\n\n       最初如果没有订阅者，它仍然允许推送一些数据到缓存，缓存大小由bufferSize定义。 之后如果仍然没有订阅者订阅它并消费数据，对onNext的调用会阻塞，直到有订阅者接入 （这时只能并发地订阅了）。\n\n\n> 因此第一个订阅者会收到最多bufferSize个元素。然而之后，后续接入的订阅者只能获取到它们开始订阅之后推送的数据。这个内部的缓存会继续用于背压的目的。\n\n\n**默认情况下**，如果所有的订阅者都取消了订阅，它会清空内部缓存，并且不再接受更多的订阅者。这一点可以通过 create 静态工厂方法的 autoCancel 参数来配置。\n\n\n### ReplayProcessor\n\n\n`ReplayProcessor`会缓存直接通过自身的 Sink 推送的元素，以及来自上游发布者的元素， 并且后来的订阅者也会收到重发（replay）的这些元素。\n\n\n可以通过多种配置方式创建它：\n\n- 缓存一个元素（cacheLast）。\n- 缓存一定个数的历史元素（create(int)），所有的历史元素（create()）。\n- 缓存基于时间窗期间内的元素（createTimeout(Duration)）。\n- 缓存基于历史个数和时间窗的元素（createSizeOrTimeout(int, Duration)）。\n\n### TopicProcessor\n\n\n     TopicProcessor是一个异步的 processor，它能够重发来自多个上游发布者的元素， 这需要在创建它的时候配置shared（build() 的 share(boolean) 配置）。\n\n\n> 如果你企图在并发环境下通过并发的上游发布者调用TopicProcessor的onNext、 onComplete，或onError方法，就必须配置shared。否则，并发调用就是非法的，从而 processor 是完全兼容响应式流规范的。\n\n\n**TopicProcessor能够对多个订阅者发送数据**。它通过对每一个订阅者关联一个线程来实现这一点， 这个线程会一直执行直到 processor 发出onError或onComplete信号，或关联的订阅者被取消。 最多可以接受的订阅者个数由构造者方法executor指定，通过提供一个有限线程数的 ExecutorService来限制这一个数。\n\n\n这个 processor 基于一个RingBuffer数据结构来存储已发送的数据。每一个订阅者线程 自行管理其相关的数据在RingBuffer中的索引。\n\n\n这个 processor 也有一个autoCancel构造器方法：如果设置为true（默认的），那么当 所有的订阅者取消之后，上游发布者也就被取消了。\n\n\n### WorkQueueProcessor\n\n\n    WorkQueueProcessor也是一个异步的 processor，也能够重发来自多个上游发布者的元素， 同样在创建时需要配置shared（它多数构造器配置与TopicProcessor相同）。\n\n\n> 它放松了对响应式流规范的兼容，但是好处就在于相对于TopicProcessor来说需要更少的资源。 它仍然基于RingBuffer，但是不再要求每一个订阅者都关联一个线程，因此相对于TopicProcessor来说更具扩展性。\n\n\n\t    代价在于分发模式有些区别：来自订阅者的请求会汇总在一起，并且这个 processor 每次只对一个 订阅者发送数据，因此需要循环（round-robin）对订阅者发送数据，而不是一次全部发出的模式（无法保证完全公平的循环分发）。\n\n\nWorkQueueProcessor多数构造器方法与TopicProcessor相同，比如autoCancel、share， 以及waitStrategy。下游订阅者的最大数目同样由构造器executor配置的ExecutorService 决定。\n\n- *注意：**最好不要有太多订阅者订阅WorkQueueProcessor，因为这会锁住 processor。如果你需要限制订阅者数量，最好使用一个ThreadPoolExecutor或 ForkJoinPool。这个 processor 能够检测到（线程池）容量并在订阅者过多时抛出异常。\n\n### 完成信号\n\n\n对于 Flux和Mono来说,just 是数据完成的信号，那如果不是通过just声明的数据流，没有这种数据准备完成的信号，那么这个流就是一个无限流。除了我们手动声明数据准备的完成，错误信号也标志这整个流的完成。\n\n\n```text\n        Flux.error(new RuntimeException());\n\n```\n\n\n还有一种情况就是当Flux和Mono没有发出任何一个元素，而是直接发出了完成信号，那么这个流就是一个空的流，像这样。\n\n\n```text\n        Flux.error(new RuntimeException());        Flux.just();        Flux.empty();\n\n```\n\n\n还有很重要的一点就是 **Flux.just(1，2，4)** 只是定义了一个数据流而已，在**subscribe()** 之前的操作什么也不会发生，类似 **Stream**的惰性求值，在中止操作之前的操作都不会触发。\n\n\n例如打印声明的数据流需要这样做\n\n\n```text\n        Flux.just(1, 2, 3).subscribe(System.out::println);\n\n```\n\n\n另外 subscribe 时，还可以指定错误的回调处理，以及数据处理完的完成回调\n\n\n![](https://pic1.zhimg.com/80/v2-8a63e0f1cde490621e947f31466278ac_1440w.jpg)\n\n\n所以可以这样写\n\n\n```text\nFlux.error(new Exception(\"error\")).subscribe(        System.out::println,        System.err::println,        () -> System.out.println(\"Completed!\"));\n\n```\n\n\n流程：\n\n\n![](https://pic3.zhimg.com/80/v2-f7bb37cd9efec5a597c5f432e20d0142_1440w.jpg)\n\n\n### 流量控制（背压）\n\n\n上面提到了一个问题，当生产者生产的速度远远大于消费者消费的的速度的时候，会引发任务大量堆积的情况，最终压垮整个管道。\n\n\n![](https://pic2.zhimg.com/80/v2-ebcf6e57fce02a882ea9f8fe9880aa35_1440w.jpg)\n\n\n那么响应式是怎么解决这个问题的，通过背压（back pressure）的机制，如下图\n\n\n![](https://pic4.zhimg.com/80/v2-24e321527e474779b4090ffc46d2deb3_1440w.jpg)\n\n\n这种下游可以向上游反馈自己消费能力的机制就叫做背压，具体背压的原理和运行机制会在后面的实战中带入，因为很多刚接触这种概念的同学只听理论的话会一时很难理解。\n\n\n通过 Reactor提供的BaseSubscriber来进行自定义我们自己流量控制的subscriber\n\n\n```text\n        Flux.just(1,2)                .doOnRequest(s->System.out.println(\"no. \"+s))                .subscribe(new BaseSubscriber<Integer>() {                    @Override                    protected void hookOnSubscribe(Subscription subscription) {                        System.out.println(\"订阅开始了，我要请求几个元素\");                        request(1);                    }                    @Override                    protected void hookOnNext(Integer value) {                        System.out.println(\"收到一个元素，下一次请求几个元素\");                        request(1);                    }                });\n\n```\n\n\n### Reactor中的多线程\n\n\n在我们java的传统的编程中，对于线程之间的调度有封装好的线程池工具类供我们使用，或者我们可以通过线程池的构造函数定义自己的线程池，这一切都让多线程的调度都变得很容易，那么在reactor中怎么处理线程的调度\n\n\n**4.1 Schedulers**\n\n\n在reactor中处理线程调度的不叫thread pool，而是Schedulers（调度器），通过调度器就可以创建出供我们直接使用的多线程环境。\n\n\n**4.1.1 Schedulers.immediate()**\n\n\n在当前线程中使用\n\n\n**4.1.2 Schedulers.single()**\n\n\n创建一个可重用的单线程环境，该方法的所有调用者都会重复使用同一个线程。\n\n\n**4.1.3 Schedulers.elastic()**\n\n\n创建一个弹性线程池，会重用空闲线程，当线程池空闲时间过长就会自动废弃掉。通常使用的场景是给一个阻塞的任务分配自己的线程，从而不会影响到其他任务的执行。\n\n\n**4.1.4 Schedulers.parallel()**\n\n\n创建一个固定大小的线程池，线程池大小和cpu个数相等。\n\n\n来看一个具体使用的实例，通过 Schedulers.elastic() 将一个同步阻塞的方法改写成异步的。\n\n\n```text\n private Integer syncMethod(){         try {             TimeUnit.SECONDS.sleep(2);         } catch (InterruptedException e) {             e.printStackTrace();         }        return 123456;     }     @Test     public void switchSyncToAsyncTest(){         CountDownLatch countDownLatch = new CountDownLatch(1);         Mono.fromCallable(()->syncMethod())                 .subscribeOn(Schedulers.elastic())                 .subscribe(System.out::println,null,countDownLatch::countDown);         try {             countDownLatch.await();         } catch (InterruptedException e) {             e.printStackTrace();         }     }\n\n```\n\n\n简单分析上述代码，通过fromCallable声明 一个callable 的mono，然后通过subscribeOn 切换环境，调度任务到单独的弹性线程池工作。\n\n\n### 错误处理\n\n\n在传统的编程中，我们处理单个接口错误的方式，可能是 try-catch-finally的方式，也可能是try-winth-resource的语法糖，这些在reactor中变得不太一样。下面来说一说reactor中的几种错误处理方式。\n\n\n**5.1 onErrorReturn**\n\n\nonErrorReturn在发生错误的时候，会提供一个缺省值，类似于安全取值的问题，但是这个在响应式流里面会更加实用。\n\n\n```text\n Flux.just(1,2,0)              .map(v->2/v)              .onErrorReturn(0)              .map(v->v*2)              .subscribe(System.out::println,System.err::println);\n\n```\n\n\n这样就可以在处理一些未知元素的时候，又不想让未知因素中止程序的继续运行，就可以采取这种方式。\n\n\n**5.2 onErrorResume**\n\n\n在发生错误的时候，提供一个新的流或者值返回，这样说可能不太清楚，看代码。\n\n\n```text\n        Flux.just(1,2,0)                 //调用redis服务获取数据                .flatMap(id->redisService.getUserByid(id))                //当发生异常的时候，从DB用户服务获取数据                .onErrorResume(v->userService.getUserByCache(id));\n\n```\n\n\n类似于错误的一个callback；\n\n\n**5.3 onErrorMap**\n\n\n上面的都是我们去提供缺省的方法或值处理错误，但是有的时候，我们需要抛出错误，但是需要将错误包装一下，可读性好一点，也就是抛出自定义异常。\n\n\n```text\n                Flux.just(1,2,0)                .flatMap(id->getUserByid(id))                .onErrorMap(v->new CustomizeExcetion(\"服务器开小差了\",v));\n\n```\n\n\n**5.4 doOnError** **记录错误日志**\n\n\n在发生错误的时候我们需要记录日志，在reactor里面专门独立出api记录错误日志\n\n\n```text\n        Flux.just(1,2,0)                .flatMap(id->getUserByid(id))                .doOnError(e-> Log.error(\"this occur something error\"))                .onErrorMap(v->new CustomizeExcetion(\"服务器开小差了\",v));\n\n```\n\n\ndoOnError 对于流里面的元素只读，也就是他不会对流里面的任务元素操作，记录日志后，会讲错误信号继续抛到后面，让后面去处理。\n\n\n**5.5 finally 确保做一些事情**\n\n\n有的时候我们想要像传统的同步代码那样使用finally去做一些事情，比如关闭http连接，清理资源，那么在reactor中怎么去做finally\n\n\n```text\n        Flux.just(1,2,0)                .flatMap(id->getUserByid(id))                .doOnError(e-> Log.error(\"this occur something error\"))                .onErrorMap(v->new CustomizeExcetion(\"服务器开小差了\",v))                .doFinally(System.out.println(\"我会确保做一些事情\"))        ;\n\n```\n\n\n或者当我们打开一个连接需要关闭资源的时候，还可以这样写\n\n\n```text\n        Flux.using(                () -> createHttpClient(),                client -> Flux.just(client.sendRequest()),                createHttpClient::close        );\n\n```\n\n\n使用using函数的三个参数，获取client，发送请求，关闭资源。\n\n\n**5.6 retry 重试机制**\n\n\n当遇到一些不可控因素导致的程序失败，但是代码逻辑确实是正确的，这个时候需要重试机制。\n\n\n```text\n      Flux.just(1,2,0)              .map(v->2/v)              .retry(1)              .subscribe(System.out::println,System.err::println);\n\n```\n\n\n但是需要注意的是重试不是从错误的地方开始重试，相当于对publisher 的重订阅，也就是从零开始重新执行一遍，所以无法达到类似于断点续传的功能，所以使用场景还是有限制。\n\n\n### **如何调试reactor**\n\n\n在我们传统阻塞代码里面，调试（Debug）的时候是一件非常简单的事情，通过打断点，得到相关的stack 的信息，就可以很清楚的知道错误信息（不过在多线程的环境下去打断点，需要切换线程环境，也有点麻烦）。\n\n\n> 但是在reactor 环境下去调试代码并不是一件简单的事情，最常见的就是 一个 Flux流，怎么去得到每个元素信息，怎么去知道在管道里面下一个元素是什么，每个元素是否像期望的那样做了操作。所以这也是从传统编程切换到响应式编程的难点，开发人员需要花时间去学习这个操作，但是感觉难受总是好的，因为做什么都太容易的话，自己会长期止步于此，像早期的EJB到j2ee，ssh -> ssm -> spring boot -> spring cloud ，从微服务->service mesh -> serve less ,到现在一些一线大厂盛行的中台。也许这一次就是改变自己的时候。\n\n\n言归正传，关于比较复杂的调试后期再说，我们先从最基本的单元测试开始。官方推荐的工具是 **StepVerifier**\n\n\n```text\n    @Test    public void reactorTest(){        StepVerifier.create(Flux.just(1,2)) //1              .expectNext(1,2) //2               .expectComplete() //3               .verify(); //4     }\n\n```\n\n1. 创建测试的异步流\n2. 测试期望的值\n3. 测试是否完成\n4. 验证\n\n> 我们通常使用create方法创建基于Flux或Mono的StepVerifier，然后就可以进行以下测试：\n\n- 测试期望发出的下一个信号。如果收到其他信号（或者信号与期望不匹配），整个测试就会 失败（AssertionError），如expectNext(T...)或expectNextCount(long)。`\n- 处理（consume）下一个信号。当你想要跳过部分序列或者当你想对信号内容进行自定义的校验的时候会用到它，可以使用consumeNextWith(Consumer<T>)。\n- 其他操作，比如暂停或运行一段代码。比如，你想对测试状态或内容进行调整或处理， 你可能会用到thenAwait(Duration)和then(Runnable)。\n\n>       对于终止事件，相应的期望方法（如expectComplete()、expectError()，及其所有的变体方法） 使用之后就不能再继续增加别的期望方法了。最后你只能对 StepVerifier 进行一些额外的配置并 触发校验（通常调用verify()及其变体方法）。\n\n\n>        从StepVerifier内部实现来看，它订阅了待测试的 Flux 或 Mono，然后将序列中的每个信号与测试 场景的期望进行比对。如果匹配的话，测试成功。如果有不匹配的情况，则抛出AssertionError异常。\n\n\n>       响应式流是一种基于时间的数据流。许多时候，待测试的数据流存在延迟，从而持续一段时间。如果这种场景比较多的话，那么会导致自动化测试运行时间较长。因此StepVerifier提供了可以操作“虚拟时间”的测试方式，这时候需要使用StepVerifier.withVirtualTime来构造。\n\n\n为了提高 StepVerifier 正常起作用的概率，它一般不接收一个简单的 Flux 作为输入，而是接收 一个Supplier，从而可以在配置好订阅者之后 “懒创建”待测试的 flux，如：\n\n\n> StepVerifier.withVirtualTime(() -> Mono.delay(Duration.ofDays(1))) //... 继续追加期望方法\n\n\n**有两种处理时间的期望方法，无论是否配置虚拟时间都是可用的：**\n\n- thenAwait(Duration)会暂停校验步骤（允许信号延迟发出）。\n- expectNoEvent(Duration)同样让序列持续一定的时间，期间如果有任何信号发出则测试失败。\n在普通的测试中，两个方法都会基于给定的持续时间暂停线程的执行。而如果是在虚拟时间模式下就相应地使用虚拟时间。\n\n```text\nStepVerifier.withVirtualTime(() -> Mono.delay(Duration.ofDays(1)))    .expectSubscription()   // 1    .expectNoEvent(Duration.ofDays(1))  // 2    .expectNext(0L)    .verifyComplete();  // 3\n\n```\n\n1. expectNoEvent 将订阅（subscription）也认作一个事件。假设你用它作为第一步，如果检测 到有订阅信号，也会失败。这时候可以使用expectSubscription().expectNoEvent(duration) 来代替；\n2. 期待“一天”内没有信号发生；\n3. verify或变体方法最终会返回一个Duration，这是实际的测试时长。\n\n**3.1 map**\n\n\n这里的map和java 8 stream的map是同一个意思，用于元素的转换，像这样\n\n\n```text\n    @Test    public void reactorMapTest(){        StepVerifier.create(Flux.just(1,2)                .map(v->v+1))                .expectNext(2,3)                .expectComplete()                .verify();    }\n\n```\n\n\n还是之前的代码，只是对每一个元素都自增加一，这里就不多说了，对lambada熟悉的同学都了解。\n\n\n**3.2 flatmap**\n\n\nflatmap也是对元素的转换，但是不同的是flatmap是将元素转换为流，再将流合并为一个大的流。\n\n\n```text\n    @Test    public void reactorFlatMapTest(){        StepVerifier.create(Flux.just(\"crabman\",\"is\",\"hero\")                .flatMap(v->Flux.fromArray(v.split(\"\")))                .doOnNext(System.out::println))                .expectNextCount(13)                .verifyComplete();    }\n\n```\n\n\n> tips ：flatmap 和 map的区别  \n> 从源码上来看 map就是一个function函数，输入一个输出一个，对于flatmap来讲它接受的是输出为Publisher的function，也就是说对于flatmap来讲 输入一个值，输出的是一个Publisher，所以map是一对一的关系，而flatmap 是一对多或者多对多的关系，并且两者输出也不一样。那flatmap 的应用场景在哪里，例如一个接口，入参是List<id>,用户id 的集合，需求是返回每个id 对应的具体信息，所以代码看起来就是这样 xx.flatmap(id->getUserInfoById(id))\n\n\n**3.3 filter**\n\n\nreactor 的filter和java 8 stream 的filter是一样的，就不多说了，这里过滤掉值为2 的\n\n\n```text\n    @Test    public void reactorFilterTest(){        StepVerifier.create(Flux.just(1,2)                .map(v->v+1)                .filter(s->s!=2)                .doOnNext(System.out::println))                .expectNext(3)                .expectComplete()                .verify();    }\n\n```\n\n\n**3.4 zip**\n\n\n这个是操作可能看起来比较陌生，顾名思义，“压缩”就是将多个流一对一的合并起来，还有一个原因，因为在每个flux流或者mono流里面，各个流的速度是不一样，zip还有个作用就是将两个流进行同步对齐。例如我们这里在加入另一个流，这个流会不停的发出元素，为了让大家可以感受到同，这里限制另一个流的速度为没1秒发出一个元素，这样合并的流也会向另一个流对齐。\n\n\n```text\n    @Test    public void reactorZipTest(){        //定义一个Flux流        Flux<String> stringFlux = Flux.just(\"a\", \"b\", \"c\", \"d\");        //这里使用计时器，因为在单元测试里面，可能元素没执行完，他就会直接返回        CountDownLatch countDownLatch = new CountDownLatch(1);  // 2        Flux.zip(stringFlux,Flux.interval(Duration.ofSeconds(1)))                .subscribe(t->System.out.println(t.getT1())                        ,System.err::println                        ,countDownLatch::countDown);        try {            countDownLatch.await();        } catch (InterruptedException e) {            e.printStackTrace();        }    }\n\n```\n\n\n上面讲的这四个是比较常用的，还有很多。\n\n\n## jdk9的响应式规范\n\n\nJDK 9提供了对于Reactive的完整支持，JDK9也定义了上述提到的四个接口，在`java.util.concurrent`包上\n\n\nFlow的源码\n\n\n```text\npublic final class Flow {    private Flow() {} // uninstantiable    //发布者    @FunctionalInterface    public static interface Publisher<T> {                public void subscribe(Subscriber<? super T> subscriber);    }    //订阅者    public static interface Subscriber<T> {        public void onNext(T item);        public void onError(Throwable throwable);        public void onComplete();    }  //订阅消费对象    public static interface Subscription {        public void request(long n);        public void cancel();    }    //数据转换    public static interface Processor<T,R> extends Subscriber<T>, Publisher<R> {    }    static final int DEFAULT_BUFFER_SIZE = 256;    public static int defaultBufferSize() {        return DEFAULT_BUFFER_SIZE;    }}\n\n```\n\n\n## webFlux\n\n- WebFlux是Spring推出响应式编程的一部分(web端)\n\n\t> 响应式编程是异步非阻塞的(是一种基于数据流（data stream）和变化传递（propagation of change）的声明式（declarative）的编程范式)\n\n\n![](https://pic1.zhimg.com/v2-5be72cbcf804dc2953f1198a1365ed9d_r.jpg?source=172ae18b)\n\n\n> 以往根据不同的应用场景选择不同的技术，有的场景适合用于同步阻塞的，有的场景适合用于异步非阻塞的。而Spring5提供了一整套响应式(非阻塞)的技术栈供我们使用(包括Web控制器、权限控制、数据访问层等等)。\n\n\n响应式一般用Netty或者Servlet 3.1的容器(因为支持异步非阻塞)，而Servlet技术栈用的是Servlet容器\n\n\n![](https://spring.io/images/diagram-reactive-dark-31d740ed8e454af5f1b8d55ae716525d.svg)\n\n\nSpring官方为了让我们更加**快速/平滑**到WebFlux上，之前SpringMVC那套都是支持的。也就是说：**我们可以像使用SpringMVC一样使用着WebFlux**。\n\n\nWebFlux使用的响应式流并不是用JDK9平台的，而是**Reactor**响应式流库为啥？因为人家是兄弟公司！\n\n\n### 两种路由方式\n\n- **基于Spring web的注解声明**\n- **基于routing function的函数式开发**\n\n```text\n/**     * 阻塞5秒钟     * @return     */    private String createStr() {        try {            TimeUnit.SECONDS.sleep(5);        } catch (InterruptedException e) {        }        return \"……^ - ^\";    }    /**     *  原mvc     *     * @return {@link String}     */    @GetMapping(\"/mvc\")    private String mvc() {        long millis = System.currentTimeMillis();        log.info(\"请求1:{}\",millis);        String result = createStr();        log.info(\"结束1:{}\",System.currentTimeMillis() - millis);        return result;    }    /**     *  web flux     *     * @return {@link Mono<String>}     */    @GetMapping(\"/flux\")    private Mono<String> flux() {        long millis = System.currentTimeMillis();        log.info(\"请求2:{}\",millis);        Mono<String> result = Mono.fromSupplier(() -> createStr());        log.info(\"结束2:{}\",System.currentTimeMillis() - millis);        return result;    }\n\n```\n\n\n从调用者(浏览器)的角度而言，是感知不到有什么变化的，因为都是得等待5s才返回数据。但是，从服务端的日志我们可以看出，WebFlux是**直接返回Mono对象的**(而不是像SpringMVC一直同步阻塞5s，线程才返回)。\n\n\n这正是WebFlux的好处：能够以**固定的线程来处理高并发**（充分发挥机器的性能）。\n\n\nWebFlux还支持**服务器推送**(SSE - >Server Send Event)，我们来看个例子：\n\n\n```text\n/**     *     * 定时 返回0-n个元素     * 注：需要指定MediaType     *     * @return {@link Flux<String>}     */    @GetMapping(value = \"/timing\", produces = MediaType.TEXT_EVENT_STREAM_VALUE)    private Flux<String> timing() {        Flux<String> result = Flux            .fromStream(IntStream.range(1, 5).mapToObj(i -> {                try {                    TimeUnit.SECONDS.sleep(1);                } catch (InterruptedException e) {                }                return \"大内密探00\" + i;            }));        return result;    }\n\n```\n\n\n效果就是**每秒**会给浏览器推送数据：\n\n\n### 核心组件\n\n\n1.HttpHandler\n\n\n是一种带有处理 HTTP 请求和响应方法的简单契约。\n\n\n2.WebHandler\n\n\n![](https://img-blog.csdnimg.cn/20190610181144939.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2xpeWFudGlhbm1pbg==,size_16,color_FFFFFF,t_70)\n\n\nwebHandler显得有一些抽象，我们可以通过对比SpringMVC的一些组件帮助大家理解一下在WebFlux中各个组件的作用：\n\n\n![](https://img-blog.csdnimg.cn/20190610181202340.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2xpeWFudGlhbm1pbg==,size_16,color_FFFFFF,t_70)\n\n\n![](https://img-blog.csdnimg.cn/20190303103722917.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L21pa2V6em1lcmlj,size_16,color_FFFFFF,t_70)\n\n\n### 请求处理流程\n\n\n![](https://img-blog.csdnimg.cn/20190302175319337.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L21pa2V6em1lcmlj,size_16,color_FFFFFF,t_70)\n\n\n> RouterFunctionMapping中有private RouterFunction<?> routerFunction;这里面表面看起来只有一个Bean，其实它里面组合了非常多的RouterFunction，它是如何根据用户的请求找到对应的Function的呢？\n\n\n```text\n  // 查询处理器  @Override\tprotected Mono<?> getHandlerInternal(ServerWebExchange exchange) {    // 路由函数是否存在\t\tif (this.routerFunction != null) {      // 创建请求，并绑定\t\t\tServerRequest request = ServerRequest.create(exchange, this.messageReaders);\t\t\treturn this.routerFunction.route(request)\t\t\t\t\t.doOnNext(handler -> setAttributes(exchange.getAttributes(), request, handler));\t\t}\t\telse {      // 没有直接空\t\t\treturn Mono.empty();\t\t}\t}\n\n```\n\n\n> 关键部分就是通过它的成员变量routerFunction的route方法来找，其实就是通过用户写的predicate来判断请求是否相符合，如果符合就返回一个Mono<HandlerFunction<T>>\n\n\n```text\n\t\tpublic Mono<HandlerFunction<T>> route(ServerRequest request) {\t\t\t // routerFunction 中的路由谓词匹配      if (this.predicate.test(request)) {\t\t\t\tif (logger.isTraceEnabled()) {\t\t\t\t\tString logPrefix = request.exchange().getLogPrefix();\t\t\t\t\tlogger.trace(logPrefix + String.format(\"Matched %s\", this.predicate));\t\t\t\t}\t\t\t\treturn Mono.just(this.handlerFunction);\t\t\t}\t\t\telse {\t\t\t\treturn Mono.empty();\t\t\t}\t\t}\n\n```\n\n\n![](https://img-blog.csdnimg.cn/20190303103821980.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L21pa2V6em1lcmlj,size_16,color_FFFFFF,t_70)\n\n\n### 总结\n\n\n反应式编程框架主要采用了观察者模式，而 Spring Reactor的核心则是对观察者模式的一种衍伸。关于观察者模式的架构中被观察者(Observable)和观察者(Subscriber)处在不同的线程环境中时，由于者各自的工作量不一样，导致它们产生事件和处理事件的速度不一样，这时就出现了两种情况：\n\n- 被观察者产生事件慢一些，观察者处理事件很快。那么观察者就会等着被观察者发送事件好比观察者在等米下锅，程序等待）。\n- 被观察者产生事件的速度很快，而观察者处理很慢。那就出问题了，如果不作处理的话，事件会堆积起来，最终挤爆你的内存，导致程序崩溃。（好比被观察者生产的大米没人吃，堆积最后就会烂掉）。为了方便下面理解Mono和Flux，也可以理解为Publisher（发布者也可以理解为被观察者）主动推送数据给Subscriber（订阅者也可以叫观察者），如果Publisher发布消息太快，超过了Subscriber的处理速度，如何处理。这时就出现了Backpressure（背压—–指在异步场景中，被观察者发送事件速度远快于观察者的处理速度的情况下，一种告诉上游的被观察者降低发送速度的策略）\n- WebFlux提升的其实往往是系统的伸缩性，对于速度的提升没有太多的明显。\n- Reactive 编程尽管没有新增大量的代码，然而编码（和调试）却是变得更为复杂\n- 现在面临的最大问题是缺少文档。在开发应用中给我们造成了最大障碍。且Spring WebFlux 尚未证明自身明显地优于Spring MVC\n",
      "properties": {
        "password": "",
        "icon": "",
        "date": "2021-04-10",
        "type": "Post",
        "category": "技术分享",
        "urlname": "21",
        "catalog": [
          "archives"
        ],
        "tags": [
          "Spring",
          "Java",
          "响应式",
          "异步编程",
          "WebFlux"
        ],
        "summary": "webFlux 初识LambdaLambda 表达式，有时候也称为匿名函数或箭头函数，几乎在当前的各种主流的编程语言中都有它的身影。Java8 中引入 Lambda 表达式，使原本需要用匿名类实现接口",
        "sort": "",
        "title": "响应式开发之webFlux & Reactor",
        "status": "Published",
        "updated": "2023-10-08 14:42:00"
      },
      "catalog": [
        {
          "title": "archives",
          "doc_id": "a1eb98de-4986-46c0-b1ac-6846629d0819"
        }
      ],
      "body": "",
      "realName": "响应式开发之webFlux & Reactor",
      "relativePath": "/archives/响应式开发之webFlux & Reactor.md"
    },
    {
      "id": "807ec69b-8970-4659-8443-d224934a8e7f",
      "doc_id": "807ec69b-8970-4659-8443-d224934a8e7f",
      "title": "807ec69b-8970-4659-8443-d224934a8e7f",
      "updated": 1696747320000,
      "body_original": "\n# springBoot 三板斧\n\n\n## AOP\n\n\n> aop是一种面向切面编程 能够将那些与业务无关，却为业务模块所共同调用的逻辑或责任（缓存，锁） 封装起来，便于减少系统的重复代码，降低模块间的耦合度，并有利于未来的可拓展性和可维护性\n\n\n> Spring AOP就是基于动态代理的，如果要代理的对象，实现了某个接口，那么Spring AOP会使用JDK Proxy，去创建代理对象，而对于没有实现接口的对象，就无法使用 JDK Proxy 去进行代理了，这时候Spring AOP会使用Cglib ，这时候Spring AOP会使用 Cglib 生成一个被代理对象的子类来作为代理，如下图所示：\n\n\n![](https://blog-file.hehouhui.cn/SpringAOPProcess.jpg)\n\n\n## bean作用域\n\n- singleton : 唯一 bean 实例，Spring 中的 bean 默认都是单例的。\n- prototype : 每次请求都会创建一个新的 bean 实例。\n- request : 每一次HTTP请求都会产生一个新的bean，该bean仅在当前HTTP request内有效。\n- session : 每一次HTTP请求都会产生一个新的 bean，该bean仅在当前 HTTP session 内有效。\n- global-session： 全局session作用域，仅仅在基于portlet的web应用中才有意义，Spring5已经没有了。Portlet是能够生成语义代码(例如：HTML)片段的小型Java Web插件。它们基于portlet容器，可以像servlet一样处理HTTP请求。但是，与 servlet 不同，每个 portlet 都有不同的会话\n\n### bean是否线程安全\n\n\n的确是存在安全问题的。因为，当多个线程操作同一个对象的时候，对这个对象的成员变量的写操作会存在线程安全问题。\n\n\n但是，一般情况下，我们常用的 `Controller`、`Service`、`Dao` 这些 Bean 是无状态的。无状态的 Bean 不能保存数据，因此是线程安全的。\n\n\n常见的有 2 种解决办法：\n\n1. 在类中定义一个 `ThreadLocal` 成员变量，将需要的可变成员变量保存在 `ThreadLocal` 中（推荐的一种方式）。\n2. 改变 Bean 的作用域为 “prototype”：每次请求都会创建一个新的 bean 实例，自然不会存在线程安全问题。\n\n### 生命周期\n\n- Bean 容器找到配置文件中 Spring Bean 的定义。\n- Bean 容器利用 Java Reflection API 创建一个Bean的实例。\n- 如果涉及到一些属性值 利用 `set()`方法设置一些属性值。\n- 如果 Bean 实现了 `BeanNameAware` 接口，调用 `setBeanName()`方法，传入Bean的名字。\n- 如果 Bean 实现了 `BeanClassLoaderAware` 接口，调用 `setBeanClassLoader()`方法，传入 `ClassLoader`对象的实例。\n- 与上面的类似，如果实现了其他 `.Aware`接口，就调用相应的方法。\n- 如果有和加载这个 Bean 的 Spring 容器相关的 `BeanPostProcessor` 对象，执行`postProcessBeforeInitialization()` 方法\n- 如果Bean实现了`InitializingBean`接口，执行`afterPropertiesSet()`方法。\n- 如果 Bean 在配置文件中的定义包含 init-method 属性，执行指定的方法。\n- 如果有和加载这个 Bean的 Spring 容器相关的 `BeanPostProcessor` 对象，执行`postProcessAfterInitialization()` 方法\n- 当要销毁 Bean 的时候，如果 Bean 实现了 `DisposableBean` 接口，执行 `destroy()` 方法。\n- 当要销毁 Bean 的时候，如果 Bean 在配置文件中的定义包含 destroy-method 属性，执行指定的方法。\n\n![](https://blog-file.hehouhui.cn/48376272.jpg)\n\n\n![](https://blog-file.hehouhui.cn/5496407.jpg)\n\n\n## MVC\n\n\n![](https://blog-file.hehouhui.cn/49790288.jpg)\n\n\n**流程说明（重要）：**\n\n1. 客户端（浏览器）发送请求，直接请求到 `DispatcherServlet`。\n2. `DispatcherServlet` 根据请求信息调用 `HandlerMapping`，解析请求对应的 `Handler`。\n3. 解析到对应的 `Handler`（也就是我们平常说的 `Controller` 控制器）后，开始由 `HandlerAdapter` 适配器处理。\n4. `HandlerAdapter` 会根据 `Handler` 来调用真正的处理器来处理请求，并处理相应的业务逻辑。\n5. 处理器处理完业务后，会返回一个 `ModelAndView` 对象，`Model` 是返回的数据对象，`View` 是个逻辑上的 `View`。\n6. `ViewResolver` 会根据逻辑 `View` 查找实际的 `View`。\n7. `DispaterServlet` 把返回的 `Model` 传给 `View`（视图渲染）。\n8. 把 `View` 返回给请求者（浏览器）\n\n## 有哪些设计模式\n\n- **工厂设计模式** : Spring使用工厂模式通过 `BeanFactory`、`ApplicationContext` 创建 bean 对象。\n- **代理设计模式** : Spring AOP 功能的实现。\n- **单例设计模式** : Spring 中的 Bean 默认都是单例的。\n- **模板方法模式** : Spring 中 `jdbcTemplate`、`hibernateTemplate` 等以 Template 结尾的对数据库操作的类，它们就使用到了模板模式。\n- **包装器设计模式** : 我们的项目需要连接多个数据库，而且不同的客户在每次访问中根据需要会去访问不同的数据库。这种模式让我们可以根据客户的需求能够动态切换不同的数据源。\n- **观察者模式:** Spring 事件驱动模型就是观察者模式很经典的一个应用。\n- **适配器模式** :Spring AOP 的增强或通知(Advice)使用到了适配器模式、spring MVC 中也是用到了适配器模式适配`Controller`。\n\n## 事务\n\n\n### 事务方式\n\n1. 编程式事务，在代码中硬编码。(不推荐使用)\n2. 声明式事务，在配置文件中配置（推荐使用）\n\n**声明式事务又分为两种：**\n\n1. 基于XML的声明式事务\n2. 基于注解的声明式事务\n\n### 隔离级别\n\n\n**ransactionDefinition 接口中定义了五个表示隔离级别的常量：**\n\n- **TransactionDefinition.ISOLATION_DEFAULT:** 使用后端数据库默认的隔离级别，Mysql 默认采用的 REPEATABLE_READ隔离级别 Oracle 默认采用的 READ_COMMITTED隔离级别.\n- **TransactionDefinition.ISOLATION_READ_UNCOMMITTED:** 最低的隔离级别，允许读取尚未提交的数据变更，**可能会导致脏读、幻读或不可重复读**\n- **TransactionDefinition.ISOLATION_READ_COMMITTED:** 允许读取并发事务已经提交的数据，**可以阻止脏读，但是幻读或不可重复读仍有可能发生**\n- **TransactionDefinition.ISOLATION_REPEATABLE_READ:** 对同一字段的多次读取结果都是一致的，除非数据是被本身事务自己所修改，**可以阻止脏读和不可重复读，但幻读仍有可能发生。**\n- **TransactionDefinition.ISOLATION_SERIALIZABLE:** 最高的隔离级别，完全服从ACID的隔离级别。所有的事务依次逐个执行，这样事务之间就完全不可能产生干扰，也就是说，**该级别可以防止脏读、不可重复读以及幻读**。但是这将严重影响程序的性能。通常情况下也不会用到该级别。\n\n### 传播行为\n\n\n**支持当前事务的情况：**\n\n- **TransactionDefinition.PROPAGATION_REQUIRED：** 如果当前存在事务，则加入该事务；如果当前没有事务，则创建一个新的事务。\n- **TransactionDefinition.PROPAGATION_SUPPORTS：** 如果当前存在事务，则加入该事务；如果当前没有事务，则以非事务的方式继续运行。\n- **TransactionDefinition.PROPAGATION_MANDATORY：** 如果当前存在事务，则加入该事务；如果当前没有事务，则抛出异常。（mandatory：强制性）\n\n**不支持当前事务的情况：**\n\n- **TransactionDefinition.PROPAGATION_REQUIRES_NEW：** 创建一个新的事务，如果当前存在事务，则把当前事务挂起。\n- **TransactionDefinition.PROPAGATION_NOT_SUPPORTED：** 以非事务方式运行，如果当前存在事务，则把当前事务挂起。\n- **TransactionDefinition.PROPAGATION_NEVER：** 以非事务方式运行，如果当前存在事务，则抛出异常。\n\n**其他情况：**\n\n- **TransactionDefinition.PROPAGATION_NESTED：** 如果当前存在事务，则创建一个事务作为当前事务的嵌套事务来运行；如果当前没有事务，则该取值等价于TransactionDefinition.PROPAGATION_REQUIRED。\n\n## IOC\n\n\n> IOC是一种设计思想，它有一个容器用来存放对象引用。IoC 容器是 Spring 用来实现 IoC 的载体 将原本在程序中手动创建对象的控制权，交由Spring框架来管理 IoC 容器实际上就是个Map（key，value）,Map 中存放的是各种对象。\n\n\n> Spring IoC的初始化过程：\n\n\n![](https://blog-file.hehouhui.cn/SpringIOC%E5%88%9D%E5%A7%8B%E5%8C%96%E8%BF%87%E7%A8%8B.png)\n\n\n### spring怎么解决循环依赖\n\n\nSpring整个解决循环依赖问题的实现思路已经比较清楚了。对于整体过程，读者朋友只要理解两点：\n\n- Spring是通过递归的方式获取目标bean及其所依赖的bean的；\n- Spring实例化一个bean的时候，是分两步进行的，首先实例化目标bean，然后为其注入属性。\n\n结合这两点，也就是说，Spring在实例化一个bean的时候，是首先递归的实例化其所依赖的所有bean，直到某个bean没有依赖其他bean，此时就会将该实例返回，然后反递归的将获取到的bean设置为各个上层bean的属性的。\n\n\n**三级缓存**\n\n\n> 如何解决循环依赖，Spring主要的思路就是依据三级缓存，在实例化A时调用doGetBean，发现A依赖的B的实例，此时调用doGetBean去实例B，实例化的B的时候发现又依赖A，如果不解决这个循环依赖的话此时的doGetBean将会无限循环下去，导致内存溢出，程序奔溃。spring引用了一个早期对象，并且把这个\"早期引用\"并将其注入到容器中，让B先完成实例化，此时A就获取B的引用，完成实例化。\n\n\nSpring能够轻松的解决属性的循环依赖正式用到了三级缓存，在AbstractBeanFactory中有详细的注释。\n\n\n![](https://blog-file.hehouhui.cn/267f9e2f07082838a8dc0415ac9cc8074d08f1ee.jpeg)\n\n\n一级缓存：singletonObjects，存放完全实例化属性赋值完成的Bean，直接可以使用。二级缓存：earlySingletonObjects，存放早期Bean的引用，尚未属性装配的Bean三级缓存：singletonFactories，三级缓存，存放实例化完成的Bean工厂。\n\n\n根据以上的分析，大概清楚了Spring是如何解决循环依赖的。假设A依赖B，B依赖A（注意：这里是set属性依赖）分以下步骤执行：A依次执行**doGetBean**、查询缓存、**createBean**创建实例，实例化完成放入三级缓存singletonFactories中，接着执行**populateBean**方法装配属性，但是发现有一个属性是B的对象。因此再次调用doGetBean方法创建B的实例，依次执行doGetBean、查询缓存、createBean创建实例，实例化完成之后放入三级缓存singletonFactories中，执行populateBean装配属性，但是此时发现有一个属性是A对象。因此再次调用doGetBean创建A的实例，但是执行到getSingleton查询缓存的时候，从三级缓存中查询到了A的实例(早期引用，未完成属性装配)，此时直接返回A，不用执行后续的流程创建A了，那么B就完成了属性装配，此时是一个完整的对象放入到一级缓存singletonObjects中。B创建完成了，则A自然完成了属性装配，也创建完成放入了一级缓存singletonObjects中。Spring三级缓存的应用完美的解决了循环依赖的问题，下面是循环依赖的解决流程图。\n\n\n![](https://blog-file.hehouhui.cn/3b292df5e0fe9925237cd9d720ad3fd98cb171d4.jpeg)\n\n\n## SPI（自动装配)\n\n\n1）SPI思想\n\n- SPI的全名为Service Provider Interface.这个是针对厂商或者插件的。\n- SPI的思想：系统里抽象的各个模块，往往有很多不同的实现方案，比如日志模块的方案，xml解析模块、jdbc模块的方案等。面向的对象的设计里，我们一般推荐模块之间基于接口编程，模块之间不对实现类进行硬编码。一旦代码里涉及具体的实现类，就违反了可拔插的原则，如果需要替换一种实现，就需要修改代码。为了实现在模块装配的时候能不在程序里动态指明，这就需要一种服务发现机制。**java spi就是提供这样的一个机制：为某个接口寻找服务实现的机制**\n\n（2）SPI约定\n\n- 当服务的提供者，提供了服务接口的一种实现之后，在jar包的META-INF/services/目录里同时创建一个以服务接口命名的文件。该文件里就是实现该服务接口的具体实现类。而当外部程序装配这个模块的时候，就能通过该jar包META-INF/services/里的配置文件找到具体的实现类名，并装载实例化，完成模块的注入。通过这个约定，就不需要把服务放在代码中了，通过模块被装配的时候就可以发现服务类了。\n\n> springboot最重要的特性就是自动配置，许多功能不需要手动开启，会自动帮助程序员开启，如果想扩展某些\n\n\n\t第三方组件的功能，例如mybatis，只需要配置依赖，就可以了，这其中都是需要SPI支持实现的，下面我们从源码\n\n\n\t层面看看springboot如何通过spi机制实现自动配置的。\n\n\n### spring.factories\n\n\n`加载所有工程中META-INF/spring.factories文件中的配置`\n\n\n![](https://blog-file.hehouhui.cn/822721-20200710172626394-125453546.png)\n\n\n创建SpringApplication对象：\n\n\n![](https://blog-file.hehouhui.cn/822721-20200710172809055-563196645.png)\n\n\n![](https://blog-file.hehouhui.cn/822721-20200710174643727-898888706.png)\n\n\n![](https://blog-file.hehouhui.cn/822721-20200710174806671-2008924729.png)\n\n\n![](https://blog-file.hehouhui.cn/822721-20200710174910758-1821038900.png)\n\n\n![](https://blog-file.hehouhui.cn/822721-20200710175407768-329869967.png)\n\n\n![](https://blog-file.hehouhui.cn/822721-20200710175505244-1271936908.png)\n\n\n我们随便看一个工程的目录：spring-boot-autoconfigure 工程下META-INF 目录下的spring.factories文件的内容， 配置类似map ，key为某一项，value为实现集合\n\n\n![](https://blog-file.hehouhui.cn/822721-20200710175629357-1831178468.png)\n\n\n这就是Spi的加载机制，可以通过配置的方式实现和业务代码的解耦，需要增加时直接配置到文件内。\n\n\n这一步是在容器启动的时候，加载所有的factoryname的值到缓存，包括自动配置的。\n\n\n### 如何寻找并注册\n\n\n看一下这个注解@SpringBootApplication\n\n\n![](https://blog-file.hehouhui.cn/822721-20200710182840439-1132246697.png)\n\n\n![](https://blog-file.hehouhui.cn/822721-20200710183026874-1157551085.png)\n\n\n![](https://blog-file.hehouhui.cn/822721-20200710183110415-288019503.png)\n\n\n看一下这个类的内容：类里面有获取自动配置的方法getAutoConfigrationEntry\n\n\n![](https://blog-file.hehouhui.cn/822721-20200710183217611-972487277.png)\n\n\n![](https://blog-file.hehouhui.cn/822721-20200710183307950-573299482-20211227103446740.png)\n\n\n![](https://blog-file.hehouhui.cn/822721-20200710183320075-132538192.png)\n\n\n就是从上一步缓存result中查询所有的EnableAutoConfiguration的value集合，\n\n\n这一节主要分析spi机制加载spring.factories的配置项，下一节我们来分析自动配置的加载流程\n\n",
      "properties": {
        "password": "",
        "icon": "",
        "date": "2021-12-27",
        "type": "Post",
        "category": "技术分享",
        "urlname": "24",
        "catalog": [
          "archives"
        ],
        "tags": [
          "Spring",
          "Java"
        ],
        "summary": "springBoot 三板斧AOPaop是一种面向切面编程 能够将那些与业务无关，却为业务模块所共同调用的逻辑或责任（缓存，锁） 封装起来，便于减少系统的重复代码，降低模块间的耦合度，并有利于未来的可",
        "sort": "",
        "title": "springBoot三剑客",
        "status": "Published",
        "updated": "2023-10-08 14:42:00"
      },
      "catalog": [
        {
          "title": "archives",
          "doc_id": "807ec69b-8970-4659-8443-d224934a8e7f"
        }
      ],
      "body": "",
      "realName": "springBoot三剑客",
      "relativePath": "/archives/springBoot三剑客.md"
    },
    {
      "id": "f81c2940-d22e-4782-9d95-d16b57f6d3f4",
      "doc_id": "f81c2940-d22e-4782-9d95-d16b57f6d3f4",
      "title": "f81c2940-d22e-4782-9d95-d16b57f6d3f4",
      "updated": 1696747320000,
      "body_original": "\n## 一致性hash\n\n\n> 在Redis 集群模式Cluster中，Redis采用的是分片Sharding的方式，也就是将数据采用一定的分区策略，分发到相应的集群节点中。但是我们使用上述HASH算法进行缓存时，会出现一些缺陷，主要体现在服务器数量变动的时候，所有缓存的位置都要发生改变！具体来讲就是说第一当缓存服务器数量发生变化时，会引起缓存的雪崩，可能会引起整体系统压力过大而崩溃（大量缓存同一时间失效）。第二当缓存服务器数量发生变化时，几乎所有缓存的位置都会发生改变。\n\n\n一致性Hash算法也是使用取模的方法，只是，刚才描述的取模法是对服务器的数量进行取模，而一致性Hash算法是对232取模，什么意思呢？简单来说，一致性Hash算法将整个哈希值空间组织成一个虚拟的圆环，如假设某哈希函数H的值空间为0-232-1（即哈希值是一个32位无符号整形），整个哈希环如下：\n\n\n整个空间按顺时针方向组织，圆环的正上方的点代表0，0点右侧的第一个点代表1，以此类推，2、3、4、5、6……直到232-1，也就是说0点左侧的第一个点代表232-1， 0和232-1在零点中方向重合，我们把这个由232个点组成的圆环称为Hash环。\n\n\n那么，一致性哈希算法与上图中的圆环有什么关系呢？我们继续聊，仍然以之前描述的场景为例，假设我们有4台缓存服务器，服务器A、服务器B、服务器C，服务器D，那么，在生产环境中，这4台服务器肯定有自己的IP地址或主机名，我们使用它们各自的IP地址或主机名作为关键字进行哈希计算，使用哈希后的结果对2^32取模，可以使用如下公式示意：\n\n\nhash（服务器A的IP地址） %  2^32\n\n\n通过上述公式算出的结果一定是一个0到232-1之间的一个整数，我们就用算出的这个整数，代表服务器A，既然这个整数肯定处于0到232-1之间，那么，上图中的hash环上必定有一个点与这个整数对应，而我们刚才已经说明，使用这个整数代表服务器A，那么，服务器A就可以映射到这个环上。\n\n\n以此类推，下一步将各个服务器使用类似的Hash算式进行一个哈希，这样每台机器就能确定其在哈希环上的位置，这里假设将上文中四台服务器使用IP地址哈希后在环空间的位置如下：\n\n\n接下来使用如下算法定位数据访问到相应服务器： 将数据key使用相同的函数Hash计算出哈希值，并确定此数据在环上的位置，从此位置沿环顺时针“行走”，第一台遇到的服务器就是其应该定位到的服务器！\n\n\n### Hash算法的容错性和可扩展性\n\n\n现假设Node C不幸宕机，可以看到此时对象A、B、D不会受到影响，只有C对象被重定位到Node D。一般的，在一致性Hash算法中，如果一台服务器不可用，则受影响的数据仅仅是此服务器到其环空间中前一台服务器（即沿着逆时针方向行走遇到的第一台服务器）之间数据，其它不会受到影响，如下所示：\n\n\n![image-20210228225729174](/Users/hehui/Library/Application Support/typora-user-images/image-20210228225729174.png)\n\n\n下面考虑另外一种情况，如果在系统中增加一台服务器Node X，如下图所示：\n\n\n![image-20210228225716137](/Users/hehui/Library/Application Support/typora-user-images/image-20210228225716137.png)\n\n\n此时对象Object A、B、D不受影响，只有对象C需要重定位到新的Node X ！一般的，在一致性Hash算法中，如果增加一台服务器，则受影响的数据仅仅是新服务器到其环空间中前一台服务器（即沿着逆时针方向行走遇到的第一台服务器）之间数据，其它数据也不会受到影响。\n\n\n综上所述，一致性Hash算法对于节点的增减都只需重定位环空间中的一小部分数据，具有较好的容错性和可扩展性。\n\n\n### 数据倾斜问题\n\n\n一致性Hash算法在服务节点太少时，容易因为节点分部不均匀而造成数据倾斜（被缓存的对象大部分集中缓存在某一台服务器上）问题，例如系统中只有两台服务器，其环分布如下：\n\n\n![image-20210228225828563](/Users/hehui/Library/Application Support/typora-user-images/image-20210228225828563.png)\n\n\n此时必然造成大量数据集中到Node A上，而只有极少量会定位到Node B上，从而出现hash环偏斜的情况，当hash环偏斜以后，缓存往往会极度不均衡的分布在各服务器上，如果想要均衡的将缓存分布到2台服务器上，最好能让这2台服务器尽量多的、均匀的出现在hash环上，但是，真实的服务器资源只有2台，我们怎样凭空的让它们多起来呢，没错，就是凭空的让服务器节点多起来，既然没有多余的真正的物理服务器节点，我们就只能将现有的物理节点通过虚拟的方法复制出来。\n\n\n这些由实际节点虚拟复制而来的节点被称为\"虚拟节点\"，即对每一个服务节点计算多个哈希，每个计算结果位置都放置一个此服务节点，称为虚拟节点。具体做法可以在服务器IP或主机名的后面增加编号来实现。\n\n\n例如上面的情况，可以为每台服务器计算三个虚拟节点，于是可以分别计算 “Node A#1”、“Node A#2”、“Node A#3”、“Node B#1”、“Node B#2”、“Node B#3”的哈希值，于是形成六个虚拟节点：\n\n\n![image-20210228225807543](/Users/hehui/Library/Application Support/typora-user-images/image-20210228225807543.png)\n\n\n同时数据定位算法不变，只是多了一步虚拟节点到实际节点的映射，例如定位到“Node A#1”、“Node A#2”、“Node A#3”三个虚拟节点的数据均定位到Node A上。这样就解决了服务节点少时数据倾斜的问题。在实际应用中，通常将虚拟节点数设置为32甚至更大，因此即使很少的服务节点也能做到相对均匀的数据分布。\n\n\n## 集群情况下什么时候不可用\n\n- 如果集群任意_master_挂掉*,_且当前_master_没有_slave._集群进入_fail*状态\n- 有A,B,C三个节点的集群，在没有复制模型的情况下，如果节点B失败了，那么整个集群就会以为缺少5501-11000这个范围的槽而不可用。\n- 如果集群超过半数以上_master_挂掉，无论是否有_slave_集群进入_fail_状态\n- 集群某一节点的主从全数宕机 （与2相似）\n\n## 故障的处理过程\n\n1. 查看业务日志（微服务）\n2. 首先查看redis集群状态。\n3. 继续查看redis集群节点的状态。\n\n\t**处理过程**\n\n\n1、先停止所有redis节点。\n2、删除每个节点的缓存文件，包括node-6380.conf dump.rdp等文件。\n3、重启每个redis节点。\n4、重新创建redis集群。\n\n\n## 集群\n\n\n> 发现我们当前项目用的redis是主从，但是跟单点其实没有什么区别，因为我们在应用层面没有做读写分离，所以其实从服务器只是做了一个主从复制的工作，其他的什么都没有做。\n\n\n\t那么如果我们的系统升级，用户量上升，那么一主一从可能扛不住那么大的压力，可能需要一主多从做备机，那么假如主服务器宕机了，选举哪台从服务器做主呢？这就是一个问题，需要一个第三个人来解决，所以我查了一下，哨兵模式可以解决这个问题。哨兵模式的细节下面会讲到。\n\n\n\t然后我又想了，那如果单台服务器无法承受100%的存储压力，那就应该将存储压力分散开来，所以集群就可以解决这个问题 了，比如我们用6台服务器做集群，3主3从，那么每台服务器只需要存储1/3即可。好，那么我们就来详细看一下这些具体怎么做的。\n\n\n### 单点主从\n\n\n基本上就是一主一从，我们应用层主要使用的是主节点，从节点的主要工作是从主节点做主从复制。关键时刻，如果主服务器挂掉，可以手动启动从服务器，然后更改应用层的redis的ip即可\n\n\n> 实现主从复制（Master-Slave Replication）的工作原理：Slave从节点服务启动并连接到Master之后，它将主动发送一个SYNC命令。Master服务主节点收到同步命令后将启动后台存盘进程，同时收集所有接收到的用于修改数据集的命令，在后台进程执行完毕后，Master将传送整个数据库文件到Slave，以完成一次完全同步。而Slave从节点服务在接收到数据库文件数据之后将其存盘并加载到内存中。此后，Master主节点继续将所有已经收集到的修改命令，和新的修改命令依次传送给Slaves，Slave将在本次执行这些数据修改命令，从而达到最终的数据同步。\n\n\n如果Master和Slave之间的链接出现断连现象，Slave可以自动重连Master，但是在连接成功之后，一次完全同步将被自动执行。\n\n\n**主从复制配置**\n\n\n修改从节点的配置文件：slaveof masterip masterport\n如果设置了密码，就要设置：masterauth master-password\n\n\n主从模式的优缺点\n\n\n**优点：**\n\n- 同一个Master可以同步多个Slaves。\n- Slave同样可以接受其它Slaves的连接和同步请求，这样可以有效的分载Master的同步压力。因此我们可以将Redis的Replication架构视为图结构。\n- Master Server是以非阻塞的方式为Slaves提供服务。所以在Master-Slave同步期间，客户端仍然可以提交查询或修改请求。\n- Slave Server同样是以非阻塞的方式完成数据同步。在同步期间，如果有客户端提交查询请求，Redis则返回同步之前的数据\n- 为了分载Master的读操作压力，Slave服务器可以为客户端提供只读操作的服务，写服务仍然必须由Master来完成。即便如此，系统的伸缩性还是得到了很大的提高。\n- Master可以将数据保存操作交给Slaves完成，从而避免了在Master中要有独立的进程来完成此操作。\n支持主从复制，主机会自动将数据同步到从机，可以进行读写分离。\n\n**缺点：**\n\n- Redis不具备自动容错和恢复功能，主机从机的宕机都会导致前端部分读写请求失败，需要等待机器重启或者手动切换前端的IP才能恢复。\n- 主机宕机，宕机前有部分数据未能及时同步到从机，切换IP后还会引入数据不一致的问题，降低了系统的可用性。\n- Redis的主从复制采用全量复制，复制过程中主机会fork出一个子进程对内存做一份快照，并将子进程的内存快照保存为文件发送给从机，这一过程需要确保主机有足够多的空余内存。若快照文件较大，对集群的服务能力会产生较大的影响，而且复制过程是在从机新加入集群或者从机和主机网络断开重连时都会进行，也就是网络波动都会造成主机和从机间的一次全量的数据复制，这对实际的系统运营造成了不小的麻烦。\n- Redis较难支持在线扩容，在集群容量达到上限时在线扩容会变得很复杂。为避免这一问题，运维人员在系统上线时必须确保有足够的空间，这对资源造成了很大的浪费。\n- 其实redis的主从模式很简单，在实际的生产环境中是很少使用的，我也不建议在实际的生产环境中使用主从模式来提供系统的高可用性，之所以不建议使用都是由它的缺点造成的，在数据量非常大的情况，或者对系统的高可用性要求很高的情况下，主从模式也是不稳定的。\n\n### 读写分离\n\n\n> 对于读占比较高的场景，可以通过把一部分流量分摊导出从节点(salve) 来减轻主节点（master）压力，同时需要主要只对主节点执行写操作。\n\n\n常见的应用场景下我觉得redis没必要进行读写分离。\n\n\n先来讨论一下为什么要读写分离：\n\n\n> 读写分离使用于大量读请求的情况，通过多个slave分摊了读的压力，从而增加了读的性能。过多的select会阻塞住数据库，使你增删改不能执行，而且到并发量过大时，数据库会拒绝服务。\n\n\n\t因而通过读写分离，从而增加性能，避免拒绝服务的发生。\n\n\n\t我认为需要读写分离的应用场景是：写请求在可接受范围内，但读请求要远大于写请求的场景。\n\n\n再来讨论一下redis常见的应用场景：\n\n1. 缓存\n2. 排名型的应用，访问计数型应用\n3. 实时消息系统\n\n首先说一下缓存集群，这也是非常常见的应用场景：\n\n1. 缓存主要解决的是用户访问时，怎么以更快的速度得到数据。\n2. 单机的内存资源是很有限的，所以缓存集群会通过某种算法将不同的数据放入到不同的机器中。\n3. 不同持久化数据库，一般来说，内存数据库单机可以支持大量的增删查改。\n4. 如果一台机器支持不住，可以用主从复制，进行缓存的方法解决。\n5. 综上，在这个场景下应用redis 进行读写分离，完全就失去了读写分离的意义。\n\n> 当然，也有可能考虑不到的地方需要读写分离，毕竟“存在即合理”嘛，那么就来介绍一下这个读写分离吧。\n\n\n`当使用从节点响应读请求时，业务端可能会遇到以下问题`\n\n- 复制数据延迟\n- 读到过期数据\n- 从节点故障\n\n**数据延迟**\n\n\n> Redis 复制数的延迟由于异步复制特性是无法避免的，延迟取决于网络带宽和命令阻塞情况，对于无法容忍大量延迟场景，可以编写外部监控程序监听主从节点的复制偏移量，当延迟较大时触发报警或者通知客户端避免读取延迟过高的从节点，实现逻辑如下：\n\n1. 监控程序(monitor) 定期检查主从节点的偏移量，主节点偏移量在info replication 的master_repl_offset 指标记录，从节点 偏移量可以查询主节点的slave0 字段的offset指标，它们的差值就是主从节点延迟的字节 量。\n2. 当延迟字节量过高时，比如超过10M。监控程序触发报警并通知客户端从节点延迟过高。可以采用Zookeeper的监听回调机制实现客户端通知。\n3. 客户端接到具体的从节点高延迟通知后，修改读命令路由到其他从节点或主节点上。当延迟回复后，再次通知客户端，回复从节点的读命令请求。\n\n这种方案成本较高，需要单独修改适配Redis的客户端类库。\n\n\n**读到过期数据**\n\n\n当主节点存储大量设置超时的数据时，如缓存数据，Redis内部需要维护过期数据删除策略，删除策略主要有两种：`惰性删除和定时删除`。\n\n\n`惰性删除`：主节点每次处理读取命令时，都会检查键是否超时，如果超时则执行del命令删除键对象那个，之后del命令也会异步 发送给 从节点\n\n\n> 需要注意的是为了保证复制的一致性，从节点自身永远不会主动删除超时数据，\n\n\n`定时删除`：\n\n\nRedis主节点在内部定时任务会循环采样一定数量的键，当发现采样的键过期就执行del命令，之后再同步给从节点\n\n\n> 如果此时 数据的大量超时，主节点采样速度跟不上过期速度且主节点没有读取过期键的操作，那么从节点将无法收到del命令，这时在从节点 上可以读取到已经超时的数据。Redis在3.2 版本解决了这个问题，从节点 读取数据之前会检查键的过期时间来决定是否返回数据，可以升级到3.2版本来规避这个问题。\n\n\n### **哨兵模式**\n\n\n> 该模式是从Redis的2.6版本开始提供的，但是当时这个版本的模式是不稳定的，直到Redis的2.8版本以后，这个哨兵模式才稳定下来，无论是主从模式，还是哨兵模式，这两个模式都有一个问题，不能水平扩容，并且这两个模式的高可用特性都会受到Master主节点内存的限制。\n\n\nSentinel(哨兵)进程是用于监控redis集群中Master主服务器工作的状态，在Master主服务器发生故障的时候，可以实现Master和Slave服务器的切换，保证系统的高可用。\n\n\n**Sentinel（哨兵）进程的作用**\n\n- 监控(Monitoring): 哨兵(sentinel) 会不断地检查你的Master和Slave是否运作正常。\n- 提醒(Notification)：当被监控的某个Redis节点出现问题时, 哨兵(sentinel) 可以通过 API 向管理员或者其他应用程序发送通知。\n- 自动故障迁移(Automatic failover)：\n\n\t> 当一个Master不能正常工作时，哨兵(sentinel) 会开始一次自动故障迁移操作，它会将失效Master的其中一个Slave升级为新的Master, 并让失效Master的其他Slave改为复制新的Master；当客户端试图连接失效的Master时，集群也会向客户端返回新Master的地址，使得集群可以使用现在的Master替换失效Master。\n\n\n\t\tMaster和Slave服务器切换后，Master的redis.conf、Slave的redis.conf和sentinel.conf的配置文件的内容都会发生相应的改变，即，Master主服务器的redis.conf配置文件中会多一行slaveof的配置，sentinel.conf的监控目标会随之调换。\n\n\n**Sentinel（哨兵）进程的工作方式**\n\n- 每个Sentinel（哨兵）进程以每秒钟一次的频率向整个集群中的Master主服务器，Slave从服务器以及其他Sentinel（哨兵）进程发送一个 PING 命令。\n- 如果一个实例（instance）距离最后一次有效回复 PING 命令的时间超过 down-after-milliseconds 选项所指定的值， 则这个实例会被 Sentinel（哨兵）进程标记为主观下线（SDOWN）\n- 如果一个Master主服务器被标记为主观下线（SDOWN），则正在监视这个Master主服务器的所有 Sentinel（哨兵）进程要以每秒一次的频率确认Master主服务器的确进入了主观下线状态\n- 当有足够数量的 Sentinel（哨兵）进程（大于等于配置文件指定的值）在指定的时间范围内确认Master主服务器进入了主观下线状态（SDOWN）， 则Master主服务器会被标记为客观下线（ODOWN）\n- 在一般情况下， 每个 Sentinel（哨兵）进程会以每 10 秒一次的频率向集群中的所有Master主服务器、Slave从服务器发送 INFO 命令。\n- 当Master主服务器被 Sentinel（哨兵）进程标记为客观下线（ODOWN）时，Sentinel（哨兵）进程向下线的 Master主服务器的所有 Slave从服务器发送 INFO 命令的频率会从 10 秒一次改为每秒一次。\n- 若没有足够数量的 Sentinel（哨兵）进程同意 Master主服务器下线， Master主服务器的客观下线状态就会被移除。若 Master主服务器重新向 Sentinel（哨兵）进程发送 PING 命令返回有效回复，Master主服务器的主观下线状态就会被移除。\n哨兵模式的优缺点\n\n**优点:**\n\n- 哨兵集群模式是基于主从模式的，所有主从的优点，哨兵模式同样具有。\n- 主从可以切换，故障可以转移，系统可用性更好。\n- 哨兵模式是主从模式的升级，系统更健壮，可用性更高。\n\n**缺点:**\n\n- Redis较难支持在线扩容，在集群容量达到上限时在线扩容会变得很复杂。为避免这一问题，运维人员在系统上线时必须确保有足够的空间，这对资源造成了很大的浪费。\n- 配置复杂\n\n    `工作原理:`\n\n1. 用户链接时先通过哨兵获取主机Master的信息\n2. 获取Master的链接后实现redis的操作(set/get)\n3. 当master出现宕机时,哨兵的心跳检测发现主机长时间没有响应.这时哨兵会进行推选.推选出新的主机完成任务.\n4. 当新的主机出现时,其余的全部机器都充当该主机的从机\n\n> 这就有一个问题，就是添加哨兵以后，所有的请求都会经过哨兵询问当前的主服务器是谁，所以如果哨兵部在主服务器上面的话可能会增加服务器的压力，所以最好是将哨兵单独放在一个服务器上面。以分解压力。\n\n\n\t然后可能还有人担心哨兵服务器宕机了怎么办啊，首先哨兵服务器宕机的可能性很小，然后是如果哨兵服务器宕机了，使用人工干预重启即可，就会导致主从服务器监控的暂时不可用，不影响主从服务器的正常运行。\n\n\n先配置服务器（本地）哨兵模式，直接从redis官网下载安装或者解压版，安装后的目录结构\n\n\n![](https://images2018.cnblogs.com/blog/764785/201806/764785-20180629092443832-206225549.png)\n\n\n然后配置哨兵模式\n\n\n测试采用3个哨兵，1个主redis，2个从redis。\n\n\n复制6份redis.windows.conf文件并重命名如下（开发者可根据自己的开发习惯进行重命名）\n\n\n![](https://images2018.cnblogs.com/blog/764785/201806/764785-20180629092420294-1751487154.png)\n\n\n**配置master.6378.conf**\n\n\n```text\nport:6379\n\n\n\n#设置连接密码\n\n\n\nrequirepass:grs\n\n\n\n#连接密码\n\n\n\nmasterauth:grs\n\n```\n\n\n**slave.6380.conf配置**\n\n\n```text\nport:6380\n\n\n\ndbfilename dump6380.rdb\n\n\n\n#配置master\n\n\n\nslaveof 127.0.0.1 6379\n\n```\n\n\n**slave.6381.conf配置**\n\n\n```text\nport 6381\n\n\n\nslaveof 127.0.0.1 6379\n\n\n\ndbfilename \"dump.rdb\"\n\n```\n\n\n**配置哨兵sentinel.63791.conf（其他两个哨兵配置文件一致，只修改端口号码即可）**\n\n\n```text\nport 63791\n\n\n\n#主master，2个sentinel选举成功后才有效，这里的master-1是名称，在整合的时候需要一致，这里可以随便更改\n\n\n\nsentinel monitor master-1 127.0.0.1 6379 2\n\n\n\n#判断主master的挂机时间（毫秒），超时未返回正确信息后标记为sdown状态\n\n\n\nsentinel down-after-milliseconds master-1 5000\n\n\n\n#若sentinel在该配置值内未能完成failover操作（即故障时master/slave自动切换），则认为本次failover失败。\n\n\n\nsentinel failover-timeout master-1 18000\n\n\n\n#选项指定了在执行故障转移时， 最多可以有多少个从服务器同时对新的主服务器进行同步，这个数字越小，完成故障转移所需的时间就越长\n\n\n\nsentinel config-epoch master-1 2\n\n```\n\n\n**需要注意的地方**\n\n\n1、若通过redis-cli -h 127.0.0.1 -p 6379连接，无需改变配置文件，配置文件默认配置为bind 127.0.0.1(只允许127.0.0.1连接访问）若通过redis-cli -h 192.168.180.78 -p 6379连接，需改变配置文件，配置信息为bind 127.0.0.1 192.168.180.78（只允许127.0.0.1和192.168.180.78访问）或者将bind 127.0.0.1注释掉（允许所有远程访问）\n\n\n2、masterauth为所要连接的master服务器的requirepass,如果一个redis集群中有一个master服务器，两个slave服务器，当master服务器挂掉时，sentinel哨兵会随机选择一个slave服务器充当master服务器，鉴于这种机制，解决办法是将所有的主从服务器的requirepass和masterauth都设置为一样。\n\n\n3、sentinel monitor master-1 127.0.0.1 6379 2 行尾最后的一个2代表什么意思呢？我们知道，网络是不可靠的，有时候一个sentinel会因为网络堵塞而误以为一个master redis已经死掉了，当sentinel集群式，解决这个问题的方法就变得很简单，只需要多个sentinel互相沟通来确认某个master是否真的死了，这个2代表，当集群中有2个sentinel认为master死了时，才能真正认为该master已经不可用了。（sentinel集群中各个sentinel也有互相通信，通过gossip协议）。\n\n\n**依次启动redis**\n\n\nredis-server master.6379.conf\n\n\n![](https://images2018.cnblogs.com/blog/764785/201806/764785-20180629094553158-1599779576.png)\n\n\nredis-server slave.6380.conf\n\n\n![](https://images2018.cnblogs.com/blog/764785/201806/764785-20180629094725047-214223016.png)\n\n\nredis-server slave.6381.conf\n\n\n![](https://images2018.cnblogs.com/blog/764785/201806/764785-20180629094815575-184254002.png)\n\n\nredis-server sentinel.63791.conf --sentinel（linux:redis-sentinel sentinel.63791.conf）**其他两个哨兵也这样启动**\n\n\n![](https://images2018.cnblogs.com/blog/764785/201806/764785-20180629095126662-1063517024.png)\n\n\n使用客户端查看一下master状态\n\n\n![](https://images2018.cnblogs.com/blog/764785/201806/764785-20180629095541491-1901809546.png)\n\n\n查看一下哨兵状态\n\n\n![](https://images2018.cnblogs.com/blog/764785/201806/764785-20180629095632770-302235804.png)\n\n\n现在就可以在master插入数据，所有的redis服务都可以获取到，slave只能读\n\n\n**整合spring，导入依赖**\n\n\n```text\n<dependency>\n        <groupId>redis.clients</groupId>\n        <artifactId>jedis</artifactId>\n        <version>2.8.0</version>\n    </dependency>\n    <!-- spring-redis -->\n    <dependency>\n        <groupId>org.springframework.data</groupId>\n        <artifactId>spring-data-redis</artifactId>\n        <version>1.6.4.RELEASE</version>\n    </dependency>\n<dependency>\n    <groupId>org.apache.commons</groupId>\n    <artifactId>commons-pool2</artifactId>\n    <version>2.4.2</version>\n</dependency>\n\n\n```\n\n\n**redis.properties**\n\n\n```text\n#redis中心\nredis.host=127.0.0.1\n#redis.host=10.75.202.11\nredis.port=6379\nredis.password=\n#redis.password=123456\nredis.maxTotal=200\nredis.maxIdle=100\nredis.minIdle=8\nredis.maxWaitMillis=100000\nredis.maxActive=300\nredis.testOnBorrow=true\nredis.testOnReturn=true\n#Idle时进行连接扫描\nredis.testWhileIdle=true\n#表示idle object evitor两次扫描之间要sleep的毫秒数\nredis.timeBetweenEvictionRunsMillis=30000\n#表示idle object evitor每次扫描的最多的对象数\nredis.numTestsPerEvictionRun=10\n#表示一个对象至少停留在idle状态的最短时间，然后才能被idle object evitor扫描并驱逐；这一项只有在timeBetweenEvictionRunsMillis大于0时才有意义\nredis.minEvictableIdleTimeMillis=60000\nredis.timeout=100000\n\n```\n\n\n### Cluster\n\n\n> Redis3.0版本之后支持Cluster.\n\n\n**redis cluster的现状**\n\n\n目前redis支持的cluster特性：\n\n\n1):节点自动发现\n\n\n2):slave->master 选举,集群容错\n\n\n3):Hot resharding:在线分片\n\n\n4):进群管理:cluster xxx\n\n\n5):基于配置(nodes-port.conf)的集群管理\n\n\n6):ASK 转向/MOVED 转向机制.\n\n\n**redis cluster 架构**\n\n\n1)redis-cluster架构图\n\n\n![](https://images2015.cnblogs.com/blog/17405/201607/17405-20160729120110388-883077606.jpg)\n\n\n在这个图中，每一个蓝色的圈都代表着一个redis的服务器节点。它们任何两个节点之间都是相互连通的。客户端可以与任何一个节点相连接，然后就可以访问集群中的任何一个节点。对其进行存取和其他操作。\n\n\n**架构细节:**\n\n- 所有的redis节点彼此互联(PING-PONG机制),内部使用二进制协议优化传输速度和带宽.\n- 节点的fail是通过集群中超过半数的节点检测失效时才生效.\n- 客户端与redis节点直连,不需要中间proxy层.客户端不需要连接集群所有节点,连接集群中任何一个可用节点即可\n- redis-cluster把所有的物理节点映射到[0-16383]slot上,cluster 负责维护node<->slot<->value\n\n\t**redis-cluster选举:容错**\n\n\n![](https://images2015.cnblogs.com/blog/17405/201607/17405-20160729120154169-1347608301.jpg)\n\n- 领着选举过程是集群中所有master参与,如果半数以上master节点与master节点通信超过(cluster-node-timeout),认为当前master节点挂掉.\n- 什么时候整个集群不可用(cluster_state:fail),当集群不可用时,所有对集群的操作做都不可用，收到((error) CLUSTERDOWN The cluster is down)错误\n- 如果集群任意master挂掉,且当前master没有slave.集群进入fail状态,也可以理解成进群的slot映射[0-16383]不完成时进入fail状态.\n- 如果进群超过半数以上master挂掉，无论是否有slave集群进入fail状态.\n\n> 它们之间通过互相的ping-pong判断是否节点可以连接上。如果有一半以上的节点去ping一个节点的时候没有回应，集群就认为这个节点宕机了，然后去连接它的备用节点。\n\n\n\t如果某个节点和所有从节点全部挂掉，我们集群就进入faill状态。还有就是如果有一半以上的主节点宕机，那么我们集群同样进入发力了状态。这就是我们的redis的投票机制\n\n\n**Redis 3.0的集群方案有以下两个问题。**\n\n\n`一个Redis实例具备了“数据存储”和“路由重定向”，完全去中心化的设计。`\n\n\n> 这带来的好处是部署非常简单，直接部署Redis就行，不像Codis有那么多的组件和依赖。但带来的问题是很难对业务进行无痛的升级，如果哪天Redis集群出了什么严重的Bug，就只能回滚整个Redis集群。  \n> 对协议进行了较大的修改，对应的Redis客户端也需要升级。升级Redis客户端后谁能确保没有Bug？而且对于线上已经大规模运行的业务，升级代码中的Redis客户端也是一个很麻烦的事情。  \n> Redis Cluster是Redis 3.0以后才正式推出，时间较晚，目前能证明在大规模生产环境下成功的案例还不是很多，需要时间检验。\n\n\n### Jedis sharding\n\n\n> Redis Sharding可以说是在Redis cluster出来之前业界普遍的采用方式，其主要思想是采用hash算法将存储数据的key进行hash散列，这样特定的key会被定为到特定的节点上。\n\n\n**庆幸的是，Java Redis客户端驱动Jedis已支持Redis Sharding功能，即ShardedJedis以及结合缓存池的ShardedJedisPool**\n\n\n`Jedis的Redis Sharding实现具有如下特点：`\n\n1. 采用一致性哈希算法，将key和节点name同时hashing，然后进行映射匹配，采用的算法是MURMUR_HASH。采用一致性哈希而不是采用简单类似哈希求模映射的主要原因是当增加或减少节点时，不会产生由于重新匹配造成的rehashing。一致性哈希只影响相邻节点key分配，影响量小。\n2. 为了避免一致性哈希只影响相邻节点造成节点分配压力，ShardedJedis会对每个Redis节点根据名字(没有，Jedis会赋予缺省名字)会虚拟化出160个虚拟节点进行散列。根据权重weight，也可虚拟化出160倍数的虚拟节点。用虚拟节点做映射匹配，可以在增加或减少Redis节点时，key在各Redis节点移动再分配更均匀，而不是只有相邻节点受影响。\n3. ShardedJedis支持keyTagPattern模式，即抽取key的一部分keyTag做sharding，这样通过合理命名key，可以将一组相关联的key放入同一个Redis节点，这在避免跨节点访问相关数据时很重要。\n\n> 当然，Redis Sharding这种轻量灵活方式必然在集群其它能力方面做出妥协。比如扩容，当想要增加Redis节点时，尽管采用一致性哈希，毕竟还是会有key匹配不到而丢失，这时需要键值迁移。  \n> 作为轻量级客户端sharding，处理Redis键值迁移是不现实的，这就要求应用层面允许Redis中数据丢失或从后端数据库重新加载数据。但有些时候，击穿缓存层，直接访问数据库层，会对系统访问造成很大压力。\n\n\n### 利用中间件代理\n\n\n中间件的作用是将我们需要存入redis中的数据的key通过一套算法计算得出一个值。然后根据这个值找到对应的redis节点，将这些数据存在这个redis的节点中。\n\n\n常用的中间件有这几种\n\n- Twemproxy\n- Codis\n- nginx\n\n具体用法就不赘述了，可以自行百度。\n\n\n### 总结\n\n1. 客户端分片（sharding）需要客户端维护分片算法，这是一种静态的分片方案，需要增加或者减少Redis实例的数量，需要手工调整分片的程序。\n2. 利用中间件的情况则会影响到redis的性能，具体看中间件而定，毕竟所有请求都要经过中间件一层过滤\n3. 官方提供方案 （Cluster），现时点成功案例不多。\n\n## Redis分片\n\n\nRedis 的分片承担着两个主要目标：\n\n1. 允许使用很多电脑的内存总和来支持更大的数据库。没有分片，你就被局限于单机能支持的内存容量\n2. 允许伸缩计算能力到多核或多服务器，伸缩网络带宽到多服务器或多网络适配器\n范围分片的替代方案是哈希分片(hash partitioning)。这种模式适用于任何键\n哈希槽设置\nkey-->hashcode-->16384\n\n> 在redis的每一个节点上，都有这么两个东西\n\n\n\t     一个是插槽（slot）可以理解为是一个可以存储两个数值的一个变量这个变量的取值范围是：0-16383。\n\n\n\t    还有一个就是cluster我个人把这个cluster理解为是一个集群管理的插件。\n\n\n\t当我们的存取的key到达的时候，redis会根据crc16的算法得出一个结果，然后把结果对 16384 求余数，这样每个 key 都会对应一个编号在 0-16383 之间的哈希槽，通过这个值，去找到对应的插槽所对应的节点，然后直接自动跳转到这个对应的节点上进行存取操作。\n\n\n\t还有就是因为如果集群的话，是有好多个redis一起工作的，那么，就需要这个集群不是那么容易挂掉，所以呢，理论上就应该给集群中的每个节点至少一个备用的redis服务。这个备用的redis称为从节点（slave）。那么这个集群是如何判断是否有某个节点挂掉了呢？\n\n\n首先要说的是，每一个节点都存有这个集群所有主节点以及从节点的信息。\n\n\n## **Redis持久化**\n\n\n### RDB\n\n\n> Redis Database，就是快照snapshots。缺省情况情况下，Redis把数据快照存放在磁盘上的二进制文件中，文件名为dump.rdb。可以配置Redis的持久化策略，例如数据集中每N秒钟有超过M次更新，就将数据写入磁盘；或者你可以手工调用命令SAVE或BGSAVE。\n\n\nRedis是使用fork函数复制一份当前进程(父进程)的副本(子进程) \t子进程开始将数据写到临时RDB文件中 \t当子进程完成写RDB文件，用新文件替换老文件 \t这种方式可以使Redis使用copy-on-write技术。\n\n\n### AOF\n\n\n> Append Only File。快照模式并不十分健壮，当系统停止或者无意中Redis被kill掉，最后写入Redis的数据就会丢失。这对某些应用也许不是大问题，但对于要求高可靠性的应用来说，Redis就不是一个合适的选择。Append-only文件模式是另一种选择。可以在配置文件中打开AOF模式  \n> Redis中提供了3中同步策略，即每秒同步、每修改同步和不同步。事实上每秒同步也是异步完成的，其效率也是非常高的，所差的是一旦系统出现宕机现象，那么这一秒钟之内修改的数据将会丢失。\n\n1. appendfsync always每次有数据修改发生时都会写入AOF文件\n2. appendfsync everysec 每秒钟同步一次，该策略为AOF的缺省策略。在性能和持久化方面作了很好的折中\n3. appendfsync no从不同步。高效但是数据不会被持久化。\n\n### 虚拟内存方式\n\n\n> 当key很小而value很大时，使用VM的效果会比较好，因为这样节约的内存比较大。当key不小时可以考虑使用一些非常方法将很大的key变成很大的value，如可以考虑将key/value组合成一个新value.  \n> vm-max-threads这个参数,可以设置访问swap文件的线程数,设置最好不要超过机器的核数,如果设置为0,那么所有对swap文件的操作都是串行的.可能会造成比较长时间的延迟,但是对数据完整性有很好的保证.用虚拟内存性能也不错。如果数据量很大，可以考虑分布式或者其他数据库\n\n\nredis.windows.conf\ndaemonize no默认情况下redis不是作为守护进程运行的，如果想让它在后台运行，就把它改成yes。当redis作为守护进程运行的时候，它会写一个pid到/var/run/redis.pid文件里面\n\n\n### 建议\n\n- 更新频繁: 一致性要求比较高，AOF策略为主\n- 更新不频繁: 可以容忍少量数据丢失或错误，snapshot（快照）策略为主\n\n## **Redis事务**\n\n\nredis事务是通过MULTI，EXEC，DISCARD和WATCH四个原语实现的。\n\n\n```text\nMULTI命令用于开启一个事务，它总是返回OK。\n\t\tMULTI执行之后，客户端可以继续向服务器发送任意多条命令，这些命令不会立即被执行，而是被放到一个队列中，当EXEC命令被调用时，所有队列中的命令才会被执行。\n\t\t另一方面，通过调用DISCARD，客户端可以清空事务队列，并放弃执行事务。\n\nredis事务范围\n\t从multi命令开始，到exec或者discard为止，整个操作过程是原子性的，不能打乱顺序，也不能插入操作\n\t但是出错之前的操作会正常提交\n\nWATCH 命令可以为 Redis 事务提供 check-and-set （CAS）行为。\n\t被 WATCH 的键会被监视，并会发觉这些键是否被改动过了。 如果有至少一个被监视的键在 EXEC 执行之前\n被修改了， 那么整个事务都会被取消， EXEC 返回空多条批量回复（null multi-bulk reply）来表示事务已\n经失败。\n\n```\n\n\n### **使用Redis实现分布式锁**\n\n\n```text\n1、向Redis中存放固定key的值，如果key不存在则实现存放并获取锁；如果key已经存在则不能获取锁\n\t\t\t（依靠Redis中的原子操作进行CAS比对，实现锁的互斥）\n2、获取key所对应的时间，时间是锁预期的实效时间，如果已经实效，则存储新值，并获取锁\n3、否则获取锁失败\n\n解锁：\n\t删除指定key的redis列\n\n\n```\n\n\n抢购、秒杀是如今很常见的一个应用场景，主要需要解决的问题有两个：\n\n1. 高并发对数据库产生的压力\n2. 竞争状态下如何解决库存的正确减少（\"超卖\"问题）\n\n对于第一个问题，已经很容易想到用缓存来处理抢购，避免直接操作数据库，例如使用Redis。\n\n\n### **Redis使用watch完成秒杀抢购功能：**\n\n\n使用redis中两个key完成秒杀抢购功能，mywatchkey用于存储抢购数量和mywatchlist用户存储抢购列表。\n\n\n```text\n优点：\n\t1、首先选用内存数据库来抢购速度极快\n\t2、速度快并发自然没不是问题\n\t3、使用悲观锁，会迅速增加系统资源\n\t4、比队列强的多，队列会使内存数据库资源瞬间爆棚\n\t5、使用乐观锁，达到综合需求\n\n```\n\n\n## 与关系型数据库的区别\n\n\n### 数据bai存储方式不同。\n\n\n关系型和非关系型数据库的主要差异是数据存储的方式。关系型数据天然就是表格式的，因此存储在数据表的行和列中。数据表可以彼此关联协作存储，也很容易提取数据。\n\n\n与其相反，非关系型数据不适合存储在数据表的行和列中，而是大块组合在一起。非关系型数据通常存储在数据集中，就像文档、键值对或者图结构。你的数据及其特性是选择数据存储和提取方式的首要影响因素。\n\n\n### 扩展方式不同。\n\n\nSQL和NoSQL数据库最大的差别可能是在扩展方式上，要支持日益增长的需求当然要扩展。\n\n\n要支持更多并发量，SQL数据库是纵向扩展，也就是说提高处理能力，使用速度更快速的计算机，这样处理相同的数据集就更快了。\n\n\n因为数据存储在关系表中，操作的性能瓶颈可能涉及很多个表，这都需要通过提高计算机性能来客服。虽然SQL数据库有很大扩展空间，但最终肯定会达到纵向扩展的上限。而NoSQL数据库是横向扩展的。\n\n\n而非关系型数据存储天然就是分布式的，NoSQL数据库的扩展可以通过给资源池添加更多普通的数据库服务器(节点)来分担负载。\n\n\n### 对事务性的支持不同。\n\n\n如果数据操作需要高事务性或者复杂数据查询需要控制执行计划，那么传统的SQL数据库从性能和稳定性方面考虑是你的最佳选择。SQL数据库支持对事务原子性细粒度控制，并且易于回滚事务。\n\n\n虽然NoSQL数据库也可以使用事务操作，但稳定性方面没法和关系型数据库比较，所以它们真正闪亮的价值是在操作的扩展性和大数据量处理方面。\n\n\n### 关系型\n\n\n**优点：**\n\n1. 易于维护：都是使用表结构，格式一致；\n2. 使用方便：SQL语言通用，可用于复杂查询；\n3. 复杂操作：支持SQL，可用于一个表以及多个表之间非常复杂的查询。\n\n**缺点：**\n\n1. 读写性能比较差，尤其是海量数据的高效率读写；\n2. 固定的表结构，灵活度稍欠；\n3. 高并发读写需求，传统关系型数据库来说，硬盘I/O是一个很大的瓶颈。\n\n### 非关系型\n\n\n非关系型数据库严格上不是一种数据库，应该是一种数据结构化存储方法的集合，可以是文档或者键值对等。\n\n\n**优点：**\n\n1. 格式灵活：存储数据的格式可以是key,value形式、文档形式、图片形式等等，文档形式、图片形式等等，使用灵活，应用场景广泛，而关系型数据库则只支持基础类型。\n2. 速度快：nosql可以使用硬盘或者随机存储器作为载体，而关系型数据库只能使用硬盘；\n3. 高扩展性；\n4. 成本低：nosql数据库部署简单，基本都是开源软件。\n\n**缺点：**\n\n1. 不提供sql支持，学习和使用成本较高；\n2. 无事务处理；\n3. 数据结构相对复杂，复杂查询方面稍欠。\n",
      "properties": {
        "password": "",
        "icon": "",
        "date": "2021-12-18",
        "type": "Post",
        "category": "技术分享",
        "urlname": "23",
        "catalog": [
          "archives"
        ],
        "tags": [
          "微服务",
          "分布式",
          "Redis"
        ],
        "summary": "一致性hash在Redis 集群模式Cluster中，Redis采用的是分片Sharding的方式，也就是将数据采用一定的分区策略，分发到相应的集群节点中。但是我们使用上述HASH算法进行缓存时，会出",
        "sort": "",
        "title": "Redis集群与特性",
        "status": "Published",
        "updated": "2023-10-08 14:42:00"
      },
      "catalog": [
        {
          "title": "archives",
          "doc_id": "f81c2940-d22e-4782-9d95-d16b57f6d3f4"
        }
      ],
      "body": "",
      "realName": "Redis集群与特性",
      "relativePath": "/archives/Redis集群与特性.md"
    },
    {
      "id": "44263dbd-e5d6-4365-a72e-b0ff92bd0220",
      "doc_id": "44263dbd-e5d6-4365-a72e-b0ff92bd0220",
      "title": "44263dbd-e5d6-4365-a72e-b0ff92bd0220",
      "updated": 1696747320000,
      "body_original": "\n# RestTemplate\n\n\n在`SpringCloud`体系中，我们知道服务之间的调用是通过`http`协议进行调用的。而注册中心的主要目的就是维护这些服务的服务列表。我们知道，在`Spring`中，提供了`RestTemplate`。`RestTemplate`是`Spring`提供的用于访问Rest服务的客户端。而在`SpringCloud`中也是使用此服务进行服务调用的。\n\n\n同时在微服务中，一般上服务都不会进行单点部署的，都会至少部署2台及以上的。现在我们有了注册中心进行服务列表的维护，就需要一个客户端负载均衡来进行动态服务的调用。\n\n\n所以开始示例前，我们先来大致了解下关于`负载均衡`和`RestTemplate`的相关知识点。其实后面实例的`Ribbon`和`Feign`最后的调用都是基于`RestTemplate`的。使用比较简单~\n\n\n### 何为负载均衡\n\n\n> 负载均衡(Load Balance)是分布式系统架构设计中必须考虑的因素之一，它通常是指，将请求/数据**【均匀】分摊**到多个操作单元上执行，负载均衡的关键在于【均匀】。\n\n\n### 实现的方式\n\n\n实现负载均衡的方式有很多种，这里简单介绍下几种方式，并未过多深入。\n\n\n**注意：以下部分内容转至**[**几种负载均衡技术的实现**](https://blog.csdn.net/mengdonghui123456/article/details/53981976)**。**\n\n\n1.HTTP重定向负载均衡\n\n\n> 根据用户的http请求计算出一个真实的web服务器地址，并将该web服务器地址写入http重定向响应中返回给浏览器，由浏览器重新进行访问\n\n\n![](http://qiniu.xds123.cn/18-9-20/60217159.jpg)\n\n\n**优缺点：实现起来很简单，而缺点也显而易见了：请求两次才能完成一次访问；性能差;重定向服务器会成为瓶颈**\n\n\n2.DNS域名解析负载均衡\n\n\n> 在DNS服务器上配置多个域名对应IP的记录。例如一个域名www.baidu.com对应一组web服务器IP地址，域名解析时经过DNS服务器的算法将一个域名请求分配到合适的真实服务器上。\n\n\n![](http://qiniu.xds123.cn/18-9-20/60831825.jpg)\n\n\n**优缺点：加快访问速度,改善性能。同时由于DNS解析是多级解析，每一级DNS都可能化缓存记录A，当某一服务器下线后，该服务器对应的DNS记录A可能仍然存在，导致分配到该服务器的用户访问失败，而且DNS负载均衡采用的是简单的轮询算法，不能区分服务器之间的差异，不能反映服务器当前运行状态。**\n\n\n3.反向代理负载均衡\n\n\n> 反向代理处于web服务器这边，反向代理服务器提供负载均衡的功能，同时管理一组web服务器，它根据负载均衡算法将请求的浏览器访问转发到不同的web服务器处理，处理结果经过反向服务器返回给浏览器。\n\n\n![](http://qiniu.xds123.cn/18-9-20/44882339.jpg)\n\n\n**优缺点：实现简单，可利用反向代理缓存资源(这是最常用的了)及改善网站性能。同时因为是所有请求和响应的中转站，所以反向代理服务器可能成为瓶颈。**\n\n\n**以上仅仅是部分实现方式，还有比如****`IP负载均衡`****、****`数据链路层负载均衡`****等等，这些可能涉及到相关网络方面的知识点了，不是很了解，大家有兴趣可以自行搜索下吧。**\n\n\n### 客户端和服务端的负载均衡\n\n\n实现负载均衡也又区分客户端和服务端之分，`Ribbon`就是基于客户端的负载均衡。\n客户端负载均衡：\n\n\n![](http://qiniu.xds123.cn/18-9-20/49529849.jpg)\n\n\n服务端负载均衡：\n\n\n![](http://qiniu.xds123.cn/18-9-20/19525388.jpg)\n\n\n服务端实现负载均衡方式有很多，比如：`硬件F5`、`Nginx`、`HA Proxy`等等，这些应该实施相关人员应该比较熟悉了，本人可能也就对`Nginx`了解下，⊙﹏⊙‖∣\n\n\n### RestTemplate简单介绍\n\n\n> RestTemplate是Spring提供的用于访问Rest服务的客户端，RestTemplate提供了多种便捷访问远程Http服务的方法，能够大大提高客户端的编写效率。\n\n\n![](http://qiniu.xds123.cn/18-9-20/9663891.jpg)\n\n\n简单来说，`RestTemplate`采用了`模版设计`的设计模式，将过程中与特定实现相关的部分委托给接口,而这个接口的不同实现定义了接口的不同行为,所以可以很容易的使用不同的第三方http服务，如`okHttp`、`httpclient`等。\n\n\n`RestTemplate`定义了很多的与REST资源交互，这里简单介绍下一些常用的请求方式的使用。\n\n\n### exchange\n\n\n在URL上执行特定的HTTP方法，返回包含对象的`ResponseEntity`。其他的如`GET`、`POST`等方法底层都是基于此方法的。\n\n\n![](http://qiniu.xds123.cn/18-9-20/91101426.jpg)\n\n\n如：\n\n- get请求\n\n```text\nRequestEntity requestEntity = RequestEntity.get(new URI(uri)).build();\nResponseEntity<User> responseEntity2 = this.restTemplate.exchange(requestEntity, User.class);\n\n```\n\n- post请求\n\n```text\nRequestEntity<User> requestEntity = RequestEntity.post(new URI(uri)).body(user);\nResponseEntity<User> responseEntity2 = this.restTemplate.exchange(requestEntity, User.class);\n\n```\n\n\n### GET请求\n\n\n> get请求可以分为两类：getForEntity() 和 getForObject().\n\n\n![](http://qiniu.xds123.cn/18-9-20/50274746.jpg)\n\n\n```text\n// 1-getForObject()\nUser user1 = this.restTemplate.getForObject(uri, User.class);\n\n// 2-getForEntity()\nResponseEntity<User> responseEntity1 = this.restTemplate.getForEntity(uri, User.class);\nHttpStatus statusCode = responseEntity1.getStatusCode();\nHttpHeaders header = responseEntity1.getHeaders();\nUser user2 = responseEntity1.getBody();\n\n```\n\n\n其他的方法都大同小异了，可以根据实际的业务需求进行调用。\n\n\n### POST请求\n\n\n![](http://qiniu.xds123.cn/18-9-20/94233042.jpg)\n\n\n简单示例：\n\n\n```text\n// 1-postForObject()\nUser user1 = this.restTemplate.postForObject(uri, user, User.class);\n\n// 2-postForEntity()\nResponseEntity<User> responseEntity1 = this.restTemplate.postForEntity(uri, user, User.class);\n\n```\n\n\n**关于****`postForLocation()`****，用的比较少，作用是返回新创建资源的URI，前面介绍的两者是返回资源本身，也就是结果集了。**\n\n\n关于其他的请求类型相关用法，这里就不详细阐述了，都是类似的。可以查看下此文章：[详解 RestTemplate 操作](https://blog.csdn.net/itguangit/article/details/78825505)，讲的蛮详细了。\n\n- *特别说明：系列教程为了方便，github上分别创建了一个单体的`Eureka`注册中心和高可用的`Eureka`注册中心，无特殊说明，都是使用单体的`Eureka`注册中心进行服务注册与发现的，工程名为：`spring-cloud-eureka-server`，端口号为：1000。服务提供方工程名为：`spring-cloud-eureka-client`,应用名称为：`eureka-client`,端口号为：2000，提供了一个接口：[http://127.0.0.1:2000/hello**](http://127.0.0.1:2000/hello**)\n\n![](http://qiniu.xds123.cn/18-9-20/67366013.jpg)\n\n\nspring-cloud-eureka-server示例：[spring-cloud-eureka-server](https://github.com/xie19900123/spring-cloud-learning/tree/master/spring-cloud-eureka-server)\n\n\nspring-cloud-eureka-client示例：[spring-cloud-eureka-client](https://github.com/xie19900123/spring-cloud-learning/tree/master/spring-cloud-eureka-client)\n\n\n## LoadBalancerClient实例\n\n\n此类是实现客户端负载均衡的关键。本身它是个接口类，位于`spring-cloud-commons`包下，此包包含了大量的服务治理相关的抽象接口，比如已经介绍过的`DiscoveryClient`、`ServiceRegistry`以及`LoadBalancerClient实例`等等。\n\n\n![](http://qiniu.xds123.cn/18-9-20/74317604.jpg)\n\n\n首先，我们使用最原生的方式去获取调用服务接口。\n\n\n**创建个工程:****`spring-cloud-eureka-consumer`**\n\n\n0.引入pom文件依赖。\n\n\n```text\n<!-- 客户端依赖 -->\n    <dependency>\n        <groupId>org.springframework.cloud</groupId>\n        <artifactId>spring-cloud-starter-netflix-eureka-client</artifactId>\n    </dependency>\n    <dependency>\n        <groupId>org.springframework.boot</groupId>\n        <artifactId>spring-boot-starter-web</artifactId>\n    </dependency>\n\n```\n\n\n1.配置文件添加相关注册中心等信息。\n\n\n```text\n## 服务名称\nspring.application.name=eureka-consumer\n## 端口号\nserver.port=8008\n\n#指定注册中心地址\neureka.client.service-url.defaultZone=http://127.0.0.1:1000/eureka\n# 启用ip配置 这样在注册中心列表中看见的是以ip+端口呈现的\neureka.instance.prefer-ip-address=true\n# 实例名称  最后呈现地址：ip:2000\neureka.instance.instance-id=${spring.cloud.client.ip-address}:${server.port}\n\n```\n\n\n2.编写启动类，加入`@EnableDiscoveryClient`,申明为一个客户端应用,同时申明一个`RestTemplate`，最后是使用`RestTemplate`来完成rest服务调用的。\n\n\n```text\n@SpringBootApplication\n@EnableDiscoveryClient\n@Slf4j\npublic class EurekaConsumerApplication {\n\n    public static void main(String[] args) throws Exception {\n        SpringApplication.run(EurekaConsumerApplication.class, args);\n        log.info(\"spring-cloud-eureka-consumer启动!\");\n    }\n\n    @Bean\n    public RestTemplate restTemplate() {\n        return new RestTemplate();\n    }\n}\n\n```\n\n\n3.编写一个调用类，调用`spring-cloud-eureka-client`服务提供者提供的服务。\n\n\n```text\n/**\n * 访问客户端示例\n * @author Hehui\n *\n */\n@RestController\n@Slf4j\npublic class DemoController {\n\n    @Autowired\n    LoadBalancerClient loadBalancerClient;\n\n    @Autowired\n    RestTemplate restTemplate;\n\n    @GetMapping(\"/hello\")\n    public String hello(String name) {\n        ServiceInstance serviceInstance = loadBalancerClient.choose(\"eureka-client\");\n        String url = \"http://\" + serviceInstance.getHost() + \":\" + serviceInstance.getPort() + \"/hello?name=\" + name;\n        log.info(\"url地址为：{}\", url);\n        return restTemplate.getForObject(url, String.class);\n    }\n}\n\n```\n\n\n4.启动应用，访问：[http://127.0.0.1:8008/hell0?name=oKong](http://127.0.0.1:8008/hell0?name=oKong) ，可以看见控制台输出了利用`LoadBalancerClient`的`choose`方法，获取到了对应`eureka-client`服务ID的服务地址。\n\n\n![](http://qiniu.xds123.cn/18-9-20/2558852.jpg)\n\n\n最后通过范围对应的http地址进行服务请求：\n\n\n![](http://qiniu.xds123.cn/18-9-20/15565028.jpg)\n\n\n最后浏览器上可以看见，进行了正确的访问了：\n\n\n![](http://qiniu.xds123.cn/18-9-20/65199797.jpg)\n\n\n此时，切换到服务提供者\n\n\n```text\nspring-cloud-eureka-client\n```\n\n\n控制台，可以看见日志输出：\n\n\n![](http://qiniu.xds123.cn/18-9-20/70647514.jpg)\n\n\n此时我们已经调用成功了，通过`LoadBalancerClient`获取到了服务提供者实际服务地址，最后进行调用。\n\n\n大家可以创建多个的`spring-cloud-eureka-client`服务提供者，再去调用下，可以看见会调用不同的服务地址的。\n\n\n## 客户端负载均衡Ribbon实例\n\n\n> Spring Cloud Ribbon是一个基于Http和TCP的客服端负载均衡工具，它是基于Netflix Ribbon实现的。与Eureka配合使用时，Ribbon可自动从Eureka Server (注册中心)获取服务提供者地址列表，并基于负载均衡算法，通过在客户端中配置ribbonServerList来设置服务端列表去轮询访问以达到均衡负载的作用。\n\n\n![](http://qiniu.xds123.cn/18-9-20/86401854.jpg)\n\n\n上小节，简单的使用`LoadBalancerClient`进行了服务实例获取最后调用，也说了其实`LoadBalancerClient`是个接口类。而`Ribbon`实现了此接口，对应实现类为：`RibbonLoadBalancerClient`.\n\n\n![](http://qiniu.xds123.cn/18-9-20/40970081.jpg)\n\n\n### Ribbon实例\n\n\n现在我们来看下，使用`Ribbon`的方式如何进行更加优雅的方式进行服务调用。\n\n\n**创建一个工程：****`spring-cloud-eureka-consumer-ribbon`**\n(其实这个工程和`spring-cloud-eureka-consumer`是差不多的，只是有些许不同。)\n\n\n0.加入pom依赖\n\n\n```text\n<!-- 客户端依赖 -->\n    <dependency>\n        <groupId>org.springframework.cloud</groupId>\n        <artifactId>spring-cloud-starter-netflix-eureka-client</artifactId>\n    </dependency>\n    <dependency>\n        <groupId>org.springframework.boot</groupId>\n        <artifactId>spring-boot-starter-web</artifactId>\n    </dependency>\n\n```\n\n\n1.配置文件修改，添加注册中心等相关信息。\n\n\n```text\nspring.application.name=eureka-consumer-ribbon\nserver.port=8018\n\n#指定注册中心地址\neureka.client.service-url.defaultZone=http://127.0.0.1:1000/eureka\n# 启用ip配置 这样在注册中心列表中看见的是以ip+端口呈现的\neureka.instance.prefer-ip-address=true\n# 实例名称  最后呈现地址：ip:2000\neureka.instance.instance-id=${spring.cloud.client.ip-address}:${server.port}\n\n```\n\n\n2.编写启动类，加入`@EnableDiscoveryClient`，同时申明一个`RestTemplate`，**这里和原先不同，就在于加入了****`@LoadBalanced`****注解进行修饰****`RestTemplate`****类，稍后会大致讲解下是如何进行实现的。**\n\n\n```text\n@SpringBootApplication\n@EnableDiscoveryClient\n@Slf4j\npublic class EurekaConsumerRibbonApplication {\n\n    public static void main(String[] args) throws Exception {\n        SpringApplication.run(EurekaConsumerRibbonApplication.class, args);\n        log.info(\"spring-cloud-eureka-consumer-ribbon启动!\");\n    }\n\n    //添加 @LoadBalanced 使其具备了使用LoadBalancerClient 进行负载均衡的能力\n    @Bean\n    @LoadBalanced\n    public RestTemplate restTemplage() {\n        return new RestTemplate();\n    }\n}\n\n```\n\n\n3.编写测试类，进行服务调用。\n\n\n```text\n/**\n * ribbon访问客户端示例\n * @author Hehui\n *\n */\n@RestController\n@Slf4j\npublic class DemoController {\n\n    @Autowired\n    RestTemplate restTemplate;\n\n    @GetMapping(\"/hello\")\n    public String hello(String name) {\n        //直接使用服务名进行访问\n        log.info(\"请求参数name:{}\", name);\n        return restTemplate.getForObject(\"<http://eureka-client/hello?name=>\" + name, String.class);\n    }\n}\n\n```\n\n\n可以看见，可以直接注入`RestTemplate`，通过服务名直接调用.\n\n\n4.启动应用，访问:[http://127.0.0.1:8018/hello?name=oKong](http://127.0.0.1:8018/hello?name=oKong) ,可以看见调用成功：\n\n\n![](http://qiniu.xds123.cn/18-9-20/20652450.jpg)\n\n\n控制台输出：\n\n\n![](http://qiniu.xds123.cn/18-9-20/75877117.jpg)\n\n\n### 简单聊聊LoadBalanced注解\n\n\n> 可以从以上示例中，可以看出，我们就加了一个@LoadBalanced注解修饰RestTemplatebean类，就实现了服务的调用。现在来简单看看具体是如何实现的。\n\n\n首先，我们看看此注解的代码说明：\n\n\n![](http://qiniu.xds123.cn/18-9-20/55952682.jpg)\n\n\n从注释可以看出，该注解用来给RestTemplate做标记，以使用负载均衡的客户端`LoadBalancerClient`。\n\n\n现在来看一眼相同包下的类的情况，可以看到有个`LoadBalancerAutoConfiguration`,字面意思可以知道这是一个自动配置类，此类就是我们要找的关键类了。\n\n\n![](http://qiniu.xds123.cn/18-9-20/33505933.jpg)\n\n\n`LoadBalancerAutoConfiguration`,此类不长，一百来行，这里就不贴了。\n\n\n![](http://qiniu.xds123.cn/18-9-20/6039671.jpg)\n\n\n简单说明下：\n首先，此类生效的条件是\n\n\n```text\n@ConditionalOnClass(RestTemplate.class)\n@ConditionalOnBean(LoadBalancerClient.class)\n\n```\n\n- `RestTemplate`类必须存在于当前工程的环境中。\n- 在Spring的Bean工程中有必须有`LoadBalancerClient`的实现Bean。\n\n该自动化配置类中，主要做了几件事情：\n\n- 维护了一个被@LoadBalanced注解修饰的RestTemplate对象列表\n\n```text\n@LoadBalanced\n    @Autowired(required = false)\n    private List<RestTemplate> restTemplates = Collections.emptyList();\n\n```\n\n\n同时为其每个对象通过调用`RestTemplateCustomizer`添加了一个`LoadBalancerInterceptor`和`RetryLoadBalancerInterceptor`拦截器(有生效条件)，其为`ClientHttpRequestInterceptor`接口的实现类，`ClientHttpRequestInterceptor`是`RestTemplate`的请求拦截器\n\n\n![](http://qiniu.xds123.cn/18-9-20/75808703.jpg)\n\n\n**RetryLoadBalancerInterceptor拦截器**\n\n\n![](http://qiniu.xds123.cn/18-9-20/63370995.jpg)\n\n\n**LoadBalancerInterceptor拦截器**\n\n\n![](http://qiniu.xds123.cn/18-9-20/84887976.jpg)\n\n\n我们主要看下`LoadBalancerInterceptor`：\n\n\n![](http://qiniu.xds123.cn/18-9-21/73056978.jpg)\n\n\n可以看见，最后是实现了`ClientHttpRequestInterceptor`接口的实现类执行`execute`方法进行.\n\n\n![](http://qiniu.xds123.cn/18-9-21/82979252.jpg)\n\n\n从继承关系里，此实现类就是`RibbonLoadBalancerClient`类了。\n\n\n![](http://qiniu.xds123.cn/18-9-21/59541189.jpg)\n\n\n```text\nRibbonLoadBalancerClient\n```\n\n\n类：\n\n\n![](http://qiniu.xds123.cn/18-9-21/13645724.jpg)\n\n\n简单来说：最后还是通过`loadBalancerClient.choose()`获取到服务实例，最通过拼凑http地址来进行最后的服务调用。\n\n\n**总体来说，就是通过为加入****`@LoadBalanced`****注解的****`RestTemplate`****添加一个请求拦截器，在请求前通过拦截器获取真正的请求地址，最后进行服务调用。**\n\n\n里面的细节就不阐述了，毕竟源码分析不是很在行呀，大家可以跟踪进去一探究竟吧。\n\n\n**友情提醒：若被****`@LoadBalanced`****注解的****`RestTemplate`****访问正常的服务地址，如****`http://127.0.0.1:8080/hello`****时，是会提示无法找到此服务的。**\n\n\n具体原因：`serverid`必须是我们访问的`服务名称` ，当我们直接输入`ip`的时候获取的`server`是`null`，就会抛出异常。\n\n\n![](http://qiniu.xds123.cn/18-9-21/95656166.jpg)\n\n\n此时，**若是需要调用非注册中心的服务，可以创建一个不被****`@LoadBalanced`****注解的****`RestTemplate`****,同时指定bean的名称，使用时，使用****`@Qualifier`****指定name注入此****`RestTemplate`****。**\n\n\n```text\n    @Bean(\"normalRestTemplage\")\n    public RestTemplate normalRestTemplage() {\n        return new RestTemplate();\n    }\n\n    //使用\n    @Autowired\n    @Qualifier(\"normalRestTemplage\")\n    RestTemplate normalRestTemplate;\n\n     @GetMapping(\"/ip\")\n    public String ip(String name) {\n        //直接使用服务名进行访问\n        log.info(\"使用ip请求，请求参数name:{}\", name);\n        return normalRestTemplate.getForObject(\"<http://127.0.0.1:2000/hello?name=>\" + name, String.class);\n    }\n\n```\n\n\n### 负载均衡器\n\n\n目前还未进行过自定义负载均衡，这里就简单的举例下，上次整理ppt时有讲过一些，但未深入了解过⊙﹏⊙‖∣，\n\n\n![](http://qiniu.xds123.cn/18-9-21/6181280.jpg)\n\n\n可以从继承关系看出，是通过继承`IRule`来实现的。\n\n\n![](http://qiniu.xds123.cn/18-9-21/39759685.jpg)\n\n\n**可继承ClientConfigEnabledRoundRobinRule，来实现自己负载均衡策略。**\n\n\n## 声明式服务Feign实例\n\n\n从上一章节，我们知道，当我们要调用一个服务时，需要知道服务名和api地址，这样才能进行服务调用，服务少时，这样写觉得没有什么问题，但当服务一多，接口参数很多时，上面的写法就显得不够优雅了。所以，接下来，来说说一种更好更优雅的调用服务的方式：**Feign**。\n\n\n> Feign是Netflix开发的声明式、模块化的HTTP客户端。Feign可帮助我们更好更快的便捷、优雅地调用HTTP API。\n\n\n在`Spring Cloud`中，使用`Feign`非常简单——创建一个接口，并在接口上添加一些注解。`Feign`支持多种注释，例如Feign自带的注解或者JAX-RS注解等\nSpring Cloud对Feign进行了增强，使Feign支持了Spring MVC注解，并整合了Ribbon和 Eureka,从而让Feign 的使用更加方便。**只需要通过创建接口并用注解来配置它既可完成对Web服务接口的绑定。**\n\n\n### Feign实例\n\n\n**创建个****`spring-cloud-eureka-consumer-ribbon`****工程项目。**\n\n\n0.加入`feigin`依赖\n\n\n```text\n        <!-- feign -->\n        <dependency>\n            <groupId>org.springframework.cloud</groupId>\n            <artifactId>spring-cloud-starter-openfeign</artifactId>\n        </dependency>\n        <!-- eureka客户端依赖 -->\n        <dependency>\n            <groupId>org.springframework.cloud</groupId>\n            <artifactId>spring-cloud-starter-netflix-eureka-client</artifactId>\n        </dependency>\n        <!-- rest api -->\n        <dependency>\n            <groupId>org.springframework.boot</groupId>\n            <artifactId>spring-boot-starter-web</artifactId>\n        </dependency>\n\n```\n\n\n1.配置文件\n\n\n```text\nspring.application.name=eureka-consumer-feign\nserver.port=8028\n\n#指定注册中心地址\neureka.client.service-url.defaultZone=http://127.0.0.1:1000/eureka\n# 启用ip配置 这样在注册中心列表中看见的是以ip+端口呈现的\neureka.instance.prefer-ip-address=true\n# 实例名称  最后呈现地址：ip:2000\neureka.instance.instance-id=${spring.cloud.client.ip-address}:${server.port}\n\n```\n\n\n2.创建启动类，加入注解`@EnableFeignClients`，开启`feign`支持。\n\n\n```text\n@SpringBootApplication\n@EnableFeignClients\n@Slf4j\npublic class EurekaConsumerFeignApplication {\n\n    public static void main(String[] args) throws Exception {\n        SpringApplication.run(EurekaConsumerFeignApplication.class, args);\n        log.info(\"spring-cloud-eureka-consumer-feign启动\");\n    }\n\n}\n\n```\n\n\n3.创建一个接口类`IHelloClient`,加入注解`@FeignClient`来指定这个接口所要调用的服务名称。\n\n\n```text\n@FeignClient(name=\"eureka-client\")\npublic interface IHelloClient {\n\n    /**\n     * 定义接口\n     * @param name\n     * @return\n     */\n    @RequestMapping(value=\"/hello\", method=RequestMethod.GET)\n    public String hello(@RequestParam(\"name\") String name);\n}\n\n```\n\n\n4.创建一个demo控制层，引入此接口类。\n\n\n```text\n/**\n * feign 示例\n * @author Hehui\n *\n */\n@RestController\n@Slf4j\npublic class DemoController {\n\n    @Autowired\n    IHelloClient helloClient;\n\n    @GetMapping(\"/hello\")\n    public String hello(String name) {\n        log.info(\"使用feign调用服务，参数name:{}\", name);\n        return helloClient.hello(name);\n    }\n}\n\n```\n\n\n5.启动应用，访问：[http://127.0.0.1:8028/hello?name=Hehui-feign](http://127.0.0.1:8028/hello?name=Hehui-feign)\n\n\n![](http://qiniu.xds123.cn/18-9-21/99970618.jpg)\n\n\n**是不是很简单，和调用本地服务是一样的了！**\n\n\n### Feign继承特性\n\n\n> Feign支持继承，但不支持多继承。使用继承，可将一些公共操作分组到一些父类接口中，从而简化Feign的开发。\n\n\n**所以在实际开发中，调用服务接口时，可直接按接口类和实现类进行编写，调用方引入接口依赖，继承一个本地接口，这样接口方法默认都是定义好的，也少了很多编码量。用起来就更爽了，就是有点依赖性，对方服务修改后需要同步更新下，但这个团队内部约定下问题不大的**\n\n\n这里简单实例下，创建一个`spring-cloud-eureka-client-api`工程。\n\n\n0.加入依赖，注意此依赖的作用范围：\n\n\n```text\n    <!--api接口依赖-->\n        <dependency>\n            <groupId>org.springframework.boot</groupId>\n            <artifactId>spring-boot-starter-web</artifactId>\n            <scope>provided</scope>\n        </dependency>\n\n```\n\n\n1.编写一个接口类`IHellpApi`：\n\n\n```text\npublic interface IHelloApi {\n    //定义提供者服务名\n    public static final String SERVICE_NAME = \"eureka-client\";\n\n    /**\n     * 定义接口\n     * @param name\n     * @return\n     */\n    @RequestMapping(value=\"/hello\", method=RequestMethod.GET)\n    public String hello(@RequestParam(\"name\") String name);\n}\n\n```\n\n\n**修改****`spring-cloud-eureka-client`****工程**\n\n\n0.引入api依赖\n\n\n```text\n    <!-- 导入接口依赖 -->\n    <dependency>\n       <groupId>cn.lqdev.learning</groupId>\n       <artifactId>spring-cloud-eureka-client-api</artifactId>\n       <version>0.0.1-SNAPSHOT</version>\n    </dependency>\n\n```\n\n\n1.创建一个`HelloApiImpl`类，实现`IHelloApi`:\n\n\n```text\n/**\n * 使用接口方式进行接口编写\n * @author Hehui\n *\n */\n@RestController\n@Slf4j\npublic class HelloApiImpl implements IHelloApi {\n\n    @Override\n    public String helloApi(@RequestParam(\"name\") String name) {\n        log.info(\"[spring-cloud-eureka-client]服务[helloApi]被调用，参数name值为：{}\", name);\n        return name + \",helloApi调用!\";\n    }\n}\n\n```\n\n\n**此时，****`HelloApiImpl`****是个控制层也是个接口实现类了。**\n\n\n**修改****`spring-cloud-eureka-consumer-feign`****工程。**\n0.引入api依赖\n\n\n```text\n    <!-- 导入接口依赖 -->\n    <dependency>\n       <groupId>cn.lqdev.learning</groupId>\n       <artifactId>spring-cloud-eureka-client-api</artifactId>\n       <version>0.0.1-SNAPSHOT</version>\n    </dependency>\n\n```\n\n\n1.同样创建一个接口，使其继承`IHelloApi`:\n\n\n```text\n/**\n * 直接继承接口\n * @author Hehui\n *\n */\n@FeignClient(name = IHelloApi.SERVICE_NAME)\npublic interface HelloApi extends IHelloApi{\n\n}\n\n```\n\n\n**小技巧：可以在****`IHelloApi`****定义一个服务名变量，如：SERVICE_NAME，这样让提供者进行变量的赋值，可以避免一些不必要的交流成本的，若有变化，服务调用方也无需关心的。一切都是约定编程！**\n\n\n2.修改下`DemoController`类，注入`HelloApi`：\n\n\n```text\n    @Autowired\n    HelloApi helloApi;\n\n    @GetMapping(\"hello2\")\n    public String hello2(String name) {\n        log.info(\"使用feign继承方式调用服务，参数name:{}\", name);\n        return helloApi.helloApi(name);\n    }\n\n```\n\n\n3.分别启动各服务，访问：[http://127.0.0.1:8028/hello2?name=oKong-api](http://127.0.0.1:8028/hello2?name=oKong-api)\n\n\n![](http://qiniu.xds123.cn/18-9-21/10151632.jpg)\n\n\n使用起来没啥差别的，一样的调用，**但对于调用方而言，可以无需去理会具体细节了，照着接口方法去传参就好了。**\n\n\n**这种方式，和原来的****`dubbo`****调用的方式是类似的，简单方便。大家可以把接口和实体放入一个包中，调用者和提供者都进行依赖即可。**\n\n\n### 注意事项\n\n\n在使用`Feign`时，会碰见一些问题，为了避免不必要的错误，以下这些需要额外注意下。\n\n- GET请求多个参数时，需要使用@RequestParam\n- GET请求参数为实体时，会自动转换成POST请求\n- POST请求使用@RequestBody注解参数\n- 不建议直接将@RequestMapping注解在类上，直接写在方法上\n\n## 参考资料\n\n1. [https://blog.csdn.net/mengdonghui123456/article/details/53981976](https://blog.csdn.net/mengdonghui123456/article/details/53981976)\n2. [https://cloud.spring.io/spring-cloud-static/Finchley.SR1/single/spring-cloud.html#_spring_cloud_openfeign](https://cloud.spring.io/spring-cloud-static/Finchley.SR1/single/spring-cloud.html#_spring_cloud_openfeign)\n\n## 总结\n\n\n> 本章节主要讲解了下服务消费者如何利用原生、ribbon、fegin三种方式进行服务调用的，其实每种调用方式都是使用ribbon来进行调用的，只是有些进行了增强，是的使用起来更简单高效而已。对于其原理的实现，本文未进行详细阐述，大家可以谷歌想相关知识，跟踪下源码了解下，本人也尚未深入研究过，还是停留在使用阶段，之后有时间了看一看，有啥心得再来分享吧。此时若服务上线下线，调用者调用可能会出现短暂的调用异常，最常见的就是找不到服务，此时服务容错保护就排上用场了，所以下一章节，就来说说关于服务容错保护相关知识点~\n\n\n# @LoadBalanced注解与RestTemplate\n\n\n在`Spring Cloud`微服务应用体系中，远程调用都应负载均衡。我们在使用`RestTemplate`作为远程调用客户端的时候，开启负载均衡极其简单：**一个****`@LoadBalanced`****注解就搞定了**。\n相信大家大都使用过`Ribbon`做**Client端**的负载均衡，也许你有和我一样的感受：**Ribbon虽强大但不是特别的好用**。我研究了一番，其实根源还是我们对它内部的原理不够了解，导致对一些现象无法给出合理解释，同时也影响了我们对它的**定制和扩展**。本文就针对此做出梳理，希望大家通过本文也能够对`Ribbon`有一个较为清晰的理解（本文只解释它`@LoadBalanced`这一小块内容）。\n\n\n开启客户端负载均衡只需要一个注解即可，形如这样：\n\n\n```text\n@LoadBalanced // 标注此注解后，RestTemplate就具有了客户端负载均衡能力\n@Bean\npublic RestTemplate restTemplate(){\n    return new RestTemplate();\n}\n\n```\n\n\n说`Spring`是Java界最优秀、最杰出的重复发明轮子作品一点都不为过。本文就代领你一探究竟，为何开启`RestTemplate`的负载均衡如此简单。\n\n\n> 说明：本文建立在你已经熟练使用RestTemplate，并且了解RestTemplate它相关组件的原理的基础上分析。若对这部分还比较模糊，强行推荐你先参看我前面这篇文章：RestTemplate的使用和原理你都烂熟于胸了吗？【享学Spring MVC】\n\n\n## RibbonAutoConfiguration\n\n\n这是`Spring Boot/Cloud`启动`Ribbon`的入口自动配置类，需要先有个大概的了解：\n\n\n```text\n@Configuration\n// 类路径存在com.netflix.client.IClient、RestTemplate等时生效\n@Conditional(RibbonAutoConfiguration.RibbonClassesConditions.class)\n// // 允许在单个类中使用多个@RibbonClient\n@RibbonClients\n// 若有Eureka，那就在Eureka配置好后再配置它~~~（如果是别的注册中心呢，ribbon还能玩吗？）\n@AutoConfigureAfter(name = \"org.springframework.cloud.netflix.eureka.EurekaClientAutoConfiguration\")\n@AutoConfigureBefore({ LoadBalancerAutoConfiguration.class, AsyncLoadBalancerAutoConfiguration.class })\n// 加载配置：ribbon.eager-load --> true的话，那么项目启动的时候就会把Client初始化好，避免第一次惩罚\n@EnableConfigurationProperties({ RibbonEagerLoadProperties.class, ServerIntrospectorProperties.class })\npublic class RibbonAutoConfiguration {\n\n    @Autowired\n    private RibbonEagerLoadProperties ribbonEagerLoadProperties;\n    // Ribbon的配置文件们~~~~~~~（复杂且重要）\n    @Autowired(required = false)\n    private List<RibbonClientSpecification> configurations = new ArrayList<>();\n\n    // 特征，FeaturesEndpoint这个端点(`/actuator/features`)会使用它org.springframework.cloud.client.actuator.HasFeatures\n    @Bean\n    public HasFeatures ribbonFeature() {\n        return HasFeatures.namedFeature(\"Ribbon\", Ribbon.class);\n    }\n\n\n    // 它是最为重要的，是一个org.springframework.cloud.context.named.NamedContextFactory  此工厂用于创建命名的Spring容器\n    // 这里传入配置文件，每个不同命名空间就会创建一个新的容器（和Feign特别像） 设置当前容器为父容器\n    @Bean\n    public SpringClientFactory springClientFactory() {\n        SpringClientFactory factory = new SpringClientFactory();\n        factory.setConfigurations(this.configurations);\n        return factory;\n    }\n\n    // 这个Bean是关键，若你没定义，就用系统默认提供的Client了~~~\n    // 内部使用和持有了SpringClientFactory。。。\n    @Bean\n    @ConditionalOnMissingBean(LoadBalancerClient.class)\n    public LoadBalancerClient loadBalancerClient() {\n        return new RibbonLoadBalancerClient(springClientFactory());\n    }\n    ...\n}\n\n```\n\n\n这个配置类最重要的是完成了`Ribbon`相关组件的自动配置，有了`LoadBalancerClient`才能做负载均衡（这里使用的是它的唯一实现类`RibbonLoadBalancerClient`）\n\n\n---\n\n\n## @LoadBalanced\n\n\n注解本身及其简单（一个属性都木有）：\n\n\n```text\n// 所在包是org.springframework.cloud.client.loadbalancer\n// 能标注在字段、方法参数、方法上\n// JavaDoc上说得很清楚：它只能标注在RestTemplate上才有效\n@Target({ ElementType.FIELD, ElementType.PARAMETER, ElementType.METHOD })\n@Retention(RetentionPolicy.RUNTIME)\n@Documented\n@Inherited\n@Qualifier\npublic @interface LoadBalanced {\n}\n\n```\n\n\n它最大的特点：头上标注有`@Qualifier`注解，这是它生效的最重要因素之一，本文后半啦我花了大篇幅介绍它的生效时机。\n关于`@LoadBalanced`自动生效的配置，我们需要来到这个自动配置类：`LoadBalancerAutoConfiguration`\n\n\n### LoadBalancerAutoConfiguration\n\n\n```text\n// Auto-configuration for Ribbon (client-side load balancing).\n// 它的负载均衡技术依赖于的是Ribbon组件~\n// 它所在的包是：org.springframework.cloud.client.loadbalancer\n@Configuration\n@ConditionalOnClass(RestTemplate.class) //可见它只对RestTemplate生效\n@ConditionalOnBean(LoadBalancerClient.class) // Spring容器内必须存在这个接口的Bean才会生效（参见：RibbonAutoConfiguration）\n@EnableConfigurationProperties(LoadBalancerRetryProperties.class) // retry的配置文件\npublic class LoadBalancerAutoConfiguration {\n\n    // 拿到容器内所有的标注有@LoadBalanced注解的Bean们\n    // 注意：必须标注有@LoadBalanced注解的才行\n    @LoadBalanced\n    @Autowired(required = false)\n    private List<RestTemplate> restTemplates = Collections.emptyList();\n    // LoadBalancerRequestTransformer接口：允许使用者把request + ServiceInstance --> 改造一下\n    // Spring内部默认是没有提供任何实现类的（匿名的都木有）\n    @Autowired(required = false)\n    private List<LoadBalancerRequestTransformer> transformers = Collections.emptyList();\n\n    // 配置一个匿名的SmartInitializingSingleton 此接口我们应该是熟悉的\n    // 它的afterSingletonsInstantiated()方法会在所有的单例Bean初始化完成之后，再调用一个一个的处理BeanName~\n    // 本处：使用配置好的所有的RestTemplateCustomizer定制器们，对所有的`RestTemplate`定制处理\n    // RestTemplateCustomizer下面有个lambda的实现。若调用者有需要可以书写然后扔进容器里既生效\n    // 这种定制器：若你项目中有多个RestTempalte，需要统一处理的话。写一个定制器是个不错的选择\n    // （比如统一要放置一个请求拦截器：输出日志之类的）\n    @Bean\n    public SmartInitializingSingleton loadBalancedRestTemplateInitializerDeprecated(final ObjectProvider<List<RestTemplateCustomizer>> restTemplateCustomizers) {\n        return () -> restTemplateCustomizers.ifAvailable(customizers -> {\n            for (RestTemplate restTemplate : LoadBalancerAutoConfiguration.this.restTemplates) {\n                for (RestTemplateCustomizer customizer : customizers) {\n                    customizer.customize(restTemplate);\n                }\n            }\n        });\n    }\n\n    // 这个工厂用于createRequest()创建出一个LoadBalancerRequest\n    // 这个请求里面是包含LoadBalancerClient以及HttpRequest request的\n    @Bean\n    @ConditionalOnMissingBean\n    public LoadBalancerRequestFactory loadBalancerRequestFactory(LoadBalancerClient loadBalancerClient) {\n        return new LoadBalancerRequestFactory(loadBalancerClient, this.transformers);\n    }\n\n    // =========到目前为止还和负载均衡没啥关系==========\n    // =========接下来的配置才和负载均衡有关（当然上面是基础项）==========\n\n    // 若有Retry的包，就是另外一份配置，和这差不多~~\n    @Configuration\n    @ConditionalOnMissingClass(\"org.springframework.retry.support.RetryTemplate\")\n    static class LoadBalancerInterceptorConfig {、\n\n        // 这个Bean的名称叫`loadBalancerClient`，我个人觉得叫`loadBalancerInterceptor`更合适吧（虽然ribbon是唯一实现）\n        // 这里直接使用的是requestFactory和Client构建一个拦截器对象\n        // LoadBalancerInterceptor可是`ClientHttpRequestInterceptor`，它会介入到http.client里面去\n        // LoadBalancerInterceptor也是实现负载均衡的入口，下面详解\n        // Tips:这里可没有@ConditionalOnMissingBean哦~~~~\n        @Bean\n        public LoadBalancerInterceptor ribbonInterceptor(LoadBalancerClient loadBalancerClient, LoadBalancerRequestFactory requestFactory) {\n            return new LoadBalancerInterceptor(loadBalancerClient, requestFactory);\n        }\n\n\n        // 向容器内放入一个RestTemplateCustomizer 定制器\n        // 这个定制器的作用上面已经说了：在RestTemplate初始化完成后，应用此定制化器在**所有的实例上**\n        // 这个匿名实现的逻辑超级简单：向所有的RestTemplate都塞入一个loadBalancerInterceptor 让其具备有负载均衡的能力\n\n        // Tips：此处有注解@ConditionalOnMissingBean。也就是说如果调用者自己定义过RestTemplateCustomizer类型的Bean，此处是不会执行的\n        // 请务必注意这点：容易让你的负载均衡不生效哦~~~~\n        @Bean\n        @ConditionalOnMissingBean\n        public RestTemplateCustomizer restTemplateCustomizer(final LoadBalancerInterceptor loadBalancerInterceptor) {\n            return restTemplate -> {\n                List<ClientHttpRequestInterceptor> list = new ArrayList<>(restTemplate.getInterceptors());\n                list.add(loadBalancerInterceptor);\n                restTemplate.setInterceptors(list);\n            };\n        }\n    }\n    ...\n}\n\n```\n\n\n这段配置代码稍微有点长，我把流程总结为如下几步：\n\n1. `LoadBalancerAutoConfiguration`要想生效类路径必须有`RestTemplate`，以及Spring容器内必须有`LoadBalancerClient`的实现Bean\n\\1. `LoadBalancerClient`的唯一实现类是：`org.springframework.cloud.netflix.ribbon.RibbonLoadBalancerClient`\n2. `LoadBalancerInterceptor`是个`ClientHttpRequestInterceptor`客户端请求拦截器。它的作用是在客户端发起请求之前拦截，**进而实现客户端的负载均衡**\n3. `restTemplateCustomizer()`返回的匿名定制器`RestTemplateCustomizer`它用来给所有的`RestTemplate`加上负载均衡拦截器（需要注意它的`@ConditionalOnMissingBean`注解~）\n\n不难发现，负载均衡实现的核心就是一个拦截器，就是这个拦截器让一个普通的`RestTemplate`逆袭成为了一个具有负载均衡功能的请求器\n\n\n### `LoadBalancerInterceptor`\n\n\n该类唯一被使用的地方就是`LoadBalancerAutoConfiguration`里配置上去~\n\n\n```text\npublic class LoadBalancerInterceptor implements ClientHttpRequestInterceptor {\n\n    // 这个命名都不叫Client了，而叫loadBalancer~~~\n    private LoadBalancerClient loadBalancer;\n    // 用于构建出一个Request\n    private LoadBalancerRequestFactory requestFactory;\n    ... // 省略构造函数（给这两个属性赋值）\n\n    @Override\n    public ClientHttpResponse intercept(final HttpRequest request, final byte[] body, final ClientHttpRequestExecution execution) throws IOException {\n        final URI originalUri = request.getURI();\n        String serviceName = originalUri.getHost();\n        Assert.state(serviceName != null, \"Request URI does not contain a valid hostname: \" + originalUri);\n        return this.loadBalancer.execute(serviceName, this.requestFactory.createRequest(request, body, execution));\n    }\n}\n\n```\n\n\n此拦截器拦截请求后把它的`serviceName`委托给了`LoadBalancerClient`去执行，根据`ServiceName`可能对应N多个实际的`Server`，因此就可以从众多的Server中运用均衡算法，挑选出一个最为合适的`Server`做最终的请求（它持有真正的请求执行器`ClientHttpRequestExecution`）。\n\n\n---\n\n\n### LoadBalancerClient\n\n\n请求被拦截后，最终都是委托给了`LoadBalancerClient`处理。\n\n\n```text\n// 由使用负载平衡器选择要向其发送请求的服务器的类实现\npublic interface ServiceInstanceChooser {\n\n    // 从负载平衡器中为指定的服务选择Service服务实例。\n    // 也就是根据调用者传入的serviceId，负载均衡的选择出一个具体的实例出来\n    ServiceInstance choose(String serviceId);\n}\n\n// 它自己定义了三个方法\npublic interface LoadBalancerClient extends ServiceInstanceChooser {\n\n    // 执行请求\n    <T> T execute(String serviceId, LoadBalancerRequest<T> request) throws IOException;\n    <T> T execute(String serviceId, ServiceInstance serviceInstance, LoadBalancerRequest<T> request) throws IOException;\n\n    // 重新构造url：把url中原来写的服务名 换掉 换成实际的\n    URI reconstructURI(ServiceInstance instance, URI original);\n}\n\n```\n\n\n它只有一个实现类`RibbonLoadBalancerClient`（`ServiceInstanceChooser`是有多个实现类的~）。\n\n\n### `RibbonLoadBalancerClient`\n\n\n首先我们应当关注它的`choose()`方法：\n\n\n```text\npublic class RibbonLoadBalancerClient implements LoadBalancerClient {\n\n    @Override\n    public ServiceInstance choose(String serviceId) {\n        return choose(serviceId, null);\n    }\n    // hint：你可以理解成分组。若指定了，只会在这个偏好的分组里面去均衡选择\n    // 得到一个Server后，使用RibbonServer把server适配起来~~~\n    // 这样一个实例就选好了~~~真正请求会落在这个实例上~\n    public ServiceInstance choose(String serviceId, Object hint) {\n        Server server = getServer(getLoadBalancer(serviceId), hint);\n        if (server == null) {\n            return null;\n        }\n        return new RibbonServer(serviceId, server, isSecure(server, serviceId),\n                serverIntrospector(serviceId).getMetadata(server));\n    }\n\n    // 根据ServiceId去找到一个属于它的负载均衡器\n    protected ILoadBalancer getLoadBalancer(String serviceId) {\n        return this.clientFactory.getLoadBalancer(serviceId);\n    }\n\n}\n\n```\n\n\n`choose方法`：传入serviceId，然后通过`SpringClientFactory`获取负载均衡器`com.netflix.loadbalancer.ILoadBalancer`，最终委托给它的`chooseServer()`方法选取到一个`com.netflix.loadbalancer.Server`实例，也就是说真正完成`Server`选取的是`ILoadBalancer`。\n\n\n> ILoadBalancer以及它相关的类是一个较为庞大的体系，本文不做更多的展开，而是只聚焦在我们的流程上\n\n\n`LoadBalancerInterceptor`执行的时候是直接委托执行的`loadBalancer.execute()`这个方法：\n\n\n```text\nRibbonLoadBalancerClient：\n\n    // hint此处传值为null：一视同仁\n    // 说明：LoadBalancerRequest是通过LoadBalancerRequestFactory.createRequest(request, body, execution)创建出来的\n    // 它实现LoadBalancerRequest接口是用的一个匿名内部类，泛型类型是ClientHttpResponse\n    // 因为最终执行的显然还是执行器：ClientHttpRequestExecution.execute()\n    @Override\n    public <T> T execute(String serviceId, LoadBalancerRequest<T> request) throws IOException {\n        return execute(serviceId, request, null);\n    }\n    // public方法（非接口方法）\n    public <T> T execute(String serviceId, LoadBalancerRequest<T> request, Object hint) throws IOException {\n        // 同上：拿到负载均衡器，然后拿到一个serverInstance实例\n        ILoadBalancer loadBalancer = getLoadBalancer(serviceId);\n        Server server = getServer(loadBalancer, hint);\n        if (server == null) { // 若没找到就直接抛出异常。这里使用的是IllegalStateException这个异常\n            throw new IllegalStateException(\"No instances available for \" + serviceId);\n        }\n\n        // 把Server适配为RibbonServer  isSecure：客户端是否安全\n        // serverIntrospector内省  参考配置文件：ServerIntrospectorProperties\n        RibbonServer ribbonServer = new RibbonServer(serviceId, server,\n                isSecure(server, serviceId), serverIntrospector(serviceId).getMetadata(server));\n\n        //调用本类的重载接口方法~~~~~\n        return execute(serviceId, ribbonServer, request);\n    }\n\n    // 接口方法：它的参数是ServiceInstance --> 已经确定了唯一的Server实例~~~\n    @Override\n    public <T> T execute(String serviceId, ServiceInstance serviceInstance, LoadBalancerRequest<T> request) throws IOException {\n\n        // 拿到Server）（说白了，RibbonServer是execute时的唯一实现）\n        Server server = null;\n        if (serviceInstance instanceof RibbonServer) {\n            server = ((RibbonServer) serviceInstance).getServer();\n        }\n        if (server == null) {\n            throw new IllegalStateException(\"No instances available for \" + serviceId);\n        }\n\n        // 说明：执行的上下文是和serviceId绑定的\n        RibbonLoadBalancerContext context = this.clientFactory.getLoadBalancerContext(serviceId);\n        ...\n        // 真正的向server发送请求，得到返回值\n        // 因为有拦截器，所以这里肯定说执行的是InterceptingRequestExecution.execute()方法\n        // so会调用ServiceRequestWrapper.getURI()，从而就会调用reconstructURI()方法\n            T returnVal = request.apply(serviceInstance);\n            return returnVal;\n        ... // 异常处理\n    }\n\n```\n\n\n`returnVal`是一个`ClientHttpResponse`，最后交给`handleResponse()`方法来处理异常情况（若存在的话），若无异常就交给提取器提值：`responseExtractor.extractData(response)`，这样整个请求就算全部完成了。\n\n\n### 使用细节\n\n\n针对`@LoadBalanced`下的`RestTemplate`的使用，我总结如下细节供以参考：\n\n1. 传入的`String`类型的url必须是绝对路径（`http://...`），否则抛出异常：`java.lang.IllegalArgumentException: URI is not absolute`\n2. `serviceId`不区分大小写（`http://user/...效果同http://USER/...`）\n3. `serviceId`后请不要跟port端口号了~~~\n\n最后，需要特别指出的是：标注有`@LoadBalanced`的`RestTemplate`只能书写`serviceId`而不能再写`IP地址/域名`去发送请求了。若你的项目中两种case都有需要，请定义多个`RestTemplate`分别应对不同的使用场景~\n\n\n### 本地测试\n\n\n了解了它的执行流程后，若需要本地测试（不依赖于注册中心），可以这么来做：\n\n\n```text\n// 因为自动配置头上有@ConditionalOnMissingBean注解，所以自定义一个覆盖它的行为即可\n// 此处复写它的getServer()方法，返回一个固定的（访问百度首页）即可，方便测试\n@Bean\npublic LoadBalancerClient loadBalancerClient(SpringClientFactory factory) {\n    return new RibbonLoadBalancerClient(factory) {\n        @Override\n        protected Server getServer(ILoadBalancer loadBalancer, Object hint) {\n            return new Server(\"www.baidu.com\", 80);\n        }\n    };\n}\n\n```\n\n\n这么一来，下面这个访问结果就是百度首页的html内容喽。\n\n\n```text\n@Test\npublic void contextLoads() {\n    String obj = restTemplate.getForObject(\"<http://my-serviceId>\", String.class);\n    System.out.println(obj);\n}\n\n```\n\n\n> 此处my-serviceId肯定是不存在的，但得益于我上面自定义配置的LoadBalancerClient\n\n\n什么，写死`return`一个`Server`实例不优雅？确实，总不能每次上线前还把这部分代码给注释掉吧，若有多个实例呢？还得自己写负载均衡算法吗？很显然`Spring Cloud`早早就为我们考虑到了这一点：**脱离Eureka使用配置listOfServers进行客户端负载均衡调度（****`<clientName>.<nameSpace>.listOfServers=<comma delimited hostname:port strings>`****）**\n\n\n对于上例我只需要在主配置文件里这么配置一下：\n\n\n```text\n# ribbon.eureka.enabled=false # 若没用euraka，此配置可省略。否则不可以\nmy-serviceId.ribbon.listOfServers=www.baidu.com # 若有多个实例请用逗号分隔\n\n```\n\n\n效果完全同上。\n\n\n> Tips：这种配置法不需要是完整的绝对路径，http://是可以省略的（new Server()方式亦可）\n\n\n### 自己添加一个记录请求日志的拦截器可行吗？\n\n\n显然是可行的，我给出示例如下：\n\n\n```text\n@LoadBalanced\n@Bean\npublic RestTemplate restTemplate() {\n    RestTemplate restTemplate = new RestTemplate();\n    List<ClientHttpRequestInterceptor> list = new ArrayList<>();\n    list.add((request, body, execution) -> {\n        System.out.println(\"当前请求的URL是：\" + request.getURI().toString());\n        return execution.execute(request, body);\n    });\n    restTemplate.setInterceptors(list);\n    return restTemplate;\n}\n\n```\n\n\n这样每次客户端的请求都会打印这句话：`当前请求的URI是：<http://my-serviceId`>，一般情况（缺省情况）自定义的拦截器都会在负载均衡拦截器前面执行（因为它要执行最终的请求）。若你有必要定义多个拦截器且要控制顺序，可通过`Ordered`系列接口来实现~\n\n\n---\n\n\n**最后的最后，我抛出一个非常非常重要的问题：**\n\n\n```text\n    @LoadBalanced\n    @Autowired(required = false)\n    private List<RestTemplate> restTemplates = Collections.emptyList();\n\n```\n\n\n`@Autowired` + `@LoadBalanced`能把你配置的`RestTemplate`自动注入进来拿来定制呢？？？核心原理是什么？\n\n\n## > 提示：本原理内容属于`Spring Framwork`核心技术，建议深入思考而不囫囵吞枣。有疑问的可以给我留言，我也将会在下篇文章给出详细解答（建议先思考）\n\n\n### 推荐阅读\n\n\n[RestTemplate的使用和原理你都烂熟于胸了吗？【享学Spring MVC】](https://fangshixiang.blog.csdn.net/article/details/100753981)[@Qualifier高级应用---按类别批量依赖注入【享学Spring】](https://fangshixiang.blog.csdn.net/article/details/100890879)\n\n\n\\------------------------------------------------------\n\n\n# Ribbon是如何通过一个@LoadBalanced注解就实现负载均衡的\n\n\n原创[绅士jiejie](https://me.csdn.net/weixin_38106322) 最后发布于2019-11-08 15:09:04 阅读数 94 收藏\n\n\n发布于2019-11-06 16:14:45\n\n\n分类专栏： [Spring Cloud](https://blog.csdn.net/weixin_38106322/category_9431347.html)\n\n\n版权声明：本文为博主原创文章，遵循[ CC 4.0 BY-SA ](http://creativecommons.org/licenses/by-sa/4.0/)版权协议，转载请附上原文出处链接和本声明。\n\n\n本文链接：[https://blog.csdn.net/weixin_38106322/article/details/102937313](https://blog.csdn.net/weixin_38106322/article/details/102937313)\n\n\n展开\n\n\n一.介绍下测试用到的服务\n\n\n![](https://img-blog.csdnimg.cn/20191106161350811.png)\n\n\n从Eureka注册中心中可以可以看出有EUREKA-CLIENT和RIBBON-CLIENT的服务，其中EUREKA-CLIENT有两个节点作为服务提供者，而RIBBON-CLIENT则是服务消费者，通过RestTemplate来消费EUREKA-CLIENT的服务。\n\n\n下面代码就是简单实现Ribbon负载均衡的配置类：\n\n\n```text\n@Configuration\npublic class RibbonConfig {\n\n    @Bean\n    @LoadBalanced\n    RestTemplate getRestTemlate() {\n        return new RestTemplate();\n    }\n}\n\n```\n\n\n这样简单的通过一个@LoadBalanced注解在RestTemplate上 ，在RestTemplate 远程调用的时候，就会出现负载均衡的效果。\n\n\n二.一步一步理清Ribbon负载均衡的逻辑\n\n1. 首先全局搜索@LoadBalanced这个注解，发现在LoadBalancerAutoConfiguration类有用到该注解：\n\n```text\n@Configuration\n@ConditionalOnClass(RestTemplate.class)\n@ConditionalOnBean(LoadBalancerClient.class)\n@EnableConfigurationProperties(LoadBalancerRetryProperties.class)\npublic class LoadBalancerAutoConfiguration {\n\n    /**\n    *  这段代码的作用是将有用@LoadBalanced注解的RestTemplate注入\n    */\n\t@LoadBalanced\n\t@Autowired(required = false)\n\tprivate List<RestTemplate> restTemplates = Collections.emptyList();\n}\n\n```\n\n\n分析以上代码：\n\n- 通过@Configuration表明这是一个配置类\n- 通过@ConditionalOnClass(RestTemplate.class)可以知道RestTemplate类要在类路径上存在才会实例化LoadBalancerAutoConfiguration\n- 通过@ConditionalOnBean(LoadBalancerClient.class)可以知道LoadBalancerClient类要存在才会实例化LoadBalancerAutoConfiguration\n- @EnableConfigurationProperties(LoadBalancerRetryProperties.class)是用来使用@ConfigurationProperties注解的类LoadBalancerRetryProperties生效，贴上部分LoadBalancerRetryProperties类的代码，会更清晰：\n\n```text\n@ConfigurationProperties(\"spring.cloud.loadbalancer.retry\")\npublic class LoadBalancerRetryProperties {\n\n\tprivate boolean enabled = true;\n\n\t/**\n\t * Returns true if the load balancer should retry failed requests.\n\t * @return True if the load balancer should retry failed requests; false otherwise.\n\t */\n\tpublic boolean isEnabled() {\n\t\treturn this.enabled;\n\t}\n\n```\n\n1. 所以重启下RIBBON-CLIENT服务，Debug继续看LoadBalancerAutoConfiguration 类的代码，发现在启动时会先进入LoadBalancerAutoConfiguration 的loadBalancerRequestFactory方法，实例化出LoadBalancerRequestFactory\n\n```text\n    @Bean\n\t@ConditionalOnMissingBean\n\tpublic LoadBalancerRequestFactory loadBalancerRequestFactory(\n\t\t\tLoadBalancerClient loadBalancerClient) {\n\t\treturn new LoadBalancerRequestFactory(loadBalancerClient, this.transformers);\n\t}\n\n```\n\n\n接下去断点进入LoadBalancerAutoConfiguration 类中的静态内部类LoadBalancerInterceptorConfig的ribbonInterceptor方法，可以看出这是为了实例化出LoadBalancerInterceptor 拦截器\n\n\n```text\n    @Configuration\n\t@ConditionalOnMissingClass(\"org.springframework.retry.support.RetryTemplate\")\n\tstatic class LoadBalancerInterceptorConfig {\n\n\t\t@Bean\n\t\tpublic LoadBalancerInterceptor ribbonInterceptor(\n\t\t\t\tLoadBalancerClient loadBalancerClient,\n\t\t\t\tLoadBalancerRequestFactory requestFactory) {\n\t\t\treturn new LoadBalancerInterceptor(loadBalancerClient, requestFactory);\n\t\t}\n\n```\n\n\n继续跟断点，进入了loadBalancedRestTemplateInitializerDeprecated方法，可以看出这个方法里主要的逻辑代码是customizer.customize(restTemplate)\n\n\n```text\n    @Bean\n\tpublic SmartInitializingSingleton loadBalancedRestTemplateInitializerDeprecated(\n\t\t\tfinal ObjectProvider<List<RestTemplateCustomizer>> restTemplateCustomizers) {\n\t\treturn () -> restTemplateCustomizers.ifAvailable(customizers -> {\n\t\t\tfor (RestTemplate restTemplate : LoadBalancerAutoConfiguration.this.restTemplates) {\n\t\t\t\tfor (RestTemplateCustomizer customizer : customizers) {\n\t\t\t\t\tcustomizer.customize(restTemplate);\n\t\t\t\t}\n\t\t\t}\n\t\t});\n\t}\n\n```\n\n\n继续Debug,断点进入LoadBalancerAutoConfiguration类中的静态内部类LoadBalancerInterceptorConfig：\n\n\n```text\n@Configuration\n@ConditionalOnMissingClass(\"org.springframework.retry.support.RetryTemplate\")\n\tstatic class LoadBalancerInterceptorConfig {\n\t\t@Bean\n\t\t@ConditionalOnMissingBean\n\t\tpublic RestTemplateCustomizer restTemplateCustomizer(\n\t\t\t\tfinal LoadBalancerInterceptor loadBalancerInterceptor) {\n\t\t\treturn restTemplate -> {\n\t\t\t\tList<ClientHttpRequestInterceptor> list = new ArrayList<>(\n\t\t\t\t\t\trestTemplate.getInterceptors());\n\t\t\t\tlist.add(loadBalancerInterceptor);\n\t\t\t\trestTemplate.setInterceptors(list);\n\t\t\t};\n\t\t}\n\t}\n\n```\n\n\n通过 list.add(loadBalancerInterceptor)和restTemplate.setInterceptors(list)两段代码可以看出，这是要给restTemplate加上loadBalancerInterceptor拦截器。\n\n\n那么接下来看看loadBalancerInterceptor拦截器里做了什么,通过页面发起一个http请求,断点进入到LoadBalancerInterceptor类的intercept方法，\n\n\n```text\n@Override\n\tpublic ClientHttpResponse intercept(final HttpRequest request, final byte[] body,\n\t\t\tfinal ClientHttpRequestExecution execution) throws IOException {\n\t\tfinal URI originalUri = request.getURI();\n\t\tString serviceName = originalUri.getHost();\n\t\tAssert.state(serviceName != null,\n\t\t\t\t\"Request URI does not contain a valid hostname: \" + originalUri);\n\t\treturn this.loadBalancer.execute(serviceName,\n\t\t\t\tthis.requestFactory.createRequest(request, body, execution));\n\t}\n\n```\n\n\n截图看下信息：\n\n\n![](https://img-blog.csdnimg.cn/20191107162815756.png)\n\n\n可以看到该方法取得了request里的url和servicName，然后将这些参数交给loadBalancer.execute去执行方法。而loadBalancer是LoadBalancerClient类的实例。\n看下LoadBalancerClient的类图，可以看到LoadBalancerClient继承了ServiceInstanceChooser，LoadBalancerClient的实现类是RibbonLoadBalancerClient\n\n\n![](https://img-blog.csdnimg.cn/20191107163802485.png)\n\n\n逻辑继续，断点进入了RibbonLoadBalancerClient的execute方法\n\n\n```text\npublic <T> T execute(String serviceId, LoadBalancerRequest<T> request, Object hint)\n\t\t\tthrows IOException {\n\t\tILoadBalancer loadBalancer = getLoadBalancer(serviceId);\n\t\tServer server = getServer(loadBalancer, hint);\n\t\tif (server == null) {\n\t\t\tthrow new IllegalStateException(\"No instances available for \" + serviceId);\n\t\t}\n\t\tRibbonServer ribbonServer = new RibbonServer(serviceId, server,\n\t\t\t\tisSecure(server, serviceId),\n\t\t\t\tserverIntrospector(serviceId).getMetadata(server));\n\n\t\treturn execute(serviceId, ribbonServer, request);\n\t}\n\n```\n\n\n跟着断点一步一步看方法：\n\n- ILoadBalancer loadBalancer = getLoadBalancer(serviceId);\n\n经过这个方法，得到loadBalancer，从截图里可以看到，loadBalancer里有个allServerList集合，里面有两个对象，端口号分别是8763和8762，这就是我们提供的服务节点。\n\n\t![](https://img-blog.csdnimg.cn/20191107172116799.png)\n\n- Server server = getServer(loadBalancer, hint)\n\n从图里可以看出，通过这个getServer方法，会返回给我们一个当前可调用的服务节点，而至于怎么返回服务节点，会再写一篇分析，写完后会更新链接到该篇。\n\n\t![](https://img-blog.csdnimg.cn/20191107172537248.png)\n\n- 生成RibbonServer 作为参数传入execute方法\n- 运行execute方法\n\n接着跟进execute方法\n\n\n![](https://img-blog.csdnimg.cn/20191108111558612.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl8zODEwNjMyMg==,size_16,color_FFFFFF,t_70)\n\n\n可以看该方法里的关键执行方法是：\nT returnVal = request.apply(serviceInstance);\n接着看apply方法，发现它是LoadBalancerRequest接口的方法，该接口却没有具体的实现类：\n\n\n```text\npublic interface LoadBalancerRequest<T> {\n\n\tT apply(ServiceInstance instance) throws Exception;\n\n}\n\n```\n\n\n思路回溯，是request对象调用的apply方法，而request其实是execute方法传进来的参数，追溯到源头，发现是LoadBalancerInterceptor类的intercept方法里this.requestFactory.createRequest(request, body, execution)生成了LoadBalancerRequest，然后作为参数传入，之后再调用了apply方法\n\n\n```text\n@Override\n\tpublic ClientHttpResponse intercept(final HttpRequest request, final byte[] body,\n\t\t\tfinal ClientHttpRequestExecution execution) throws IOException {\n\t\tfinal URI originalUri = request.getURI();\n\t\tString serviceName = originalUri.getHost();\n\t\tAssert.state(serviceName != null,\n\t\t\t\t\"Request URI does not contain a valid hostname: \" + originalUri);\n\t\treturn this.loadBalancer.execute(serviceName,\n\t\t\t\tthis.requestFactory.createRequest(request, body, execution));\n\t}\n\n```\n\n\n跟进createRequest方法里：\n\n\n![](https://img-blog.csdnimg.cn/20191108143112594.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl8zODEwNjMyMg==,size_16,color_FFFFFF,t_70)\n\n\n可以从图中看到，经过一些操作后，生成的serviceRequest对象里的serviceId是eureka-client，也就是我们的服务节点名，而server是localhost:8763，这是具体的服务节点ip，之后作为参数调用org.springframework.http.client包下的InterceptingClientHttpRequest类中的execute方法断点进入该方法：\n\n\n![](https://img-blog.csdnimg.cn/20191108145133749.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl8zODEwNjMyMg==,size_16,color_FFFFFF,t_70)\n\n\n可以看出通过requestFactory.createRequest(request.getURI(), method)方法生成了ClientHttpRequest类的实例delegate，它的url就是我们最后真正要请求的，最后正常调用delegate.execute()方法取得返回ClientHttpResponse就好了。\n\n\n而这里产生了一个疑问，url是怎么产生的？重新发起请求断点试下\n发现关键在LoadBalancerRequestFactory类中的createRequest方法中的这句：\n\n\n```text\nHttpRequest serviceRequest = new ServiceRequestWrapper(request, instance,his.loadBalancer);\n\n```\n\n\n跟进ServiceRequestWrapper类中，发现它继承了HttpRequestWrapper 类，同时重写了getURI方法\n\n\n```text\npublic class ServiceRequestWrapper extends HttpRequestWrapper {\n\n\tprivate final ServiceInstance instance;\n\n\tprivate final LoadBalancerClient loadBalancer;\n\n\tpublic ServiceRequestWrapper(HttpRequest request, ServiceInstance instance,\n\t\t\tLoadBalancerClient loadBalancer) {\n\t\tsuper(request);\n\t\tthis.instance = instance;\n\t\tthis.loadBalancer = loadBalancer;\n\t}\n\n\t@Override\n\tpublic URI getURI() {\n\t\tURI uri = this.loadBalancer.reconstructURI(this.instance, getRequest().getURI());\n\t\treturn uri;\n\t}\n\n}\n\n```\n\n\n断点打在getURI方法里：\n\n\n![](https://img-blog.csdnimg.cn/201911081504477.png)\n\n\n可以看到该方法返回了我们最后需要的url。\n\n\n最后，关于Ribbon是如何通过一个@LoadBalanced注解就实现负载均衡的分析就到这了，还是有很多疏漏的地方，但是大致的逻辑就是这样的了，还有一些更深层的比如如何根据策略选出当前提供服务的节点等，留待后续补充，来日方长~\n\n",
      "properties": {
        "password": "",
        "icon": "",
        "date": "2021-08-27",
        "type": "Post",
        "category": "技术分享",
        "urlname": "20",
        "catalog": [
          "archives"
        ],
        "tags": [
          "Spring",
          "微服务",
          "分布式",
          "Redis"
        ],
        "summary": "在SpringCloud体系中，我们知道服务之间的调用是通过http协议进行调用的。而注册中心的主要目的就是维护这些服务的服务列表。我们知道，在Spring中，提供了RestTemplate。RestTemplate是Spring提供的用于访问Rest服务的客户端。而在SpringCloud中也是使用此服务进行服务调用的。",
        "sort": "",
        "title": "RestTemplate与OpenFeign",
        "status": "Published",
        "updated": "2023-10-08 14:42:00"
      },
      "catalog": [
        {
          "title": "archives",
          "doc_id": "44263dbd-e5d6-4365-a72e-b0ff92bd0220"
        }
      ],
      "body": "",
      "realName": "RestTemplate与OpenFeign",
      "relativePath": "/archives/RestTemplate与OpenFeign.md"
    },
    {
      "id": "747841e4-eeac-4349-88b0-63fc6421cf0c",
      "doc_id": "747841e4-eeac-4349-88b0-63fc6421cf0c",
      "title": "747841e4-eeac-4349-88b0-63fc6421cf0c",
      "updated": 1696747320000,
      "body_original": "\n### synchronized\n\n\n**`synchronized`** **关键字解决的是多个线程之间访问资源的同步性，****`synchronized`****关键字可以保证被它修饰的方法或者代码块在任意时刻只能有一个线程执行。**\n\n\n另外，在 Java 早期版本中，`synchronized` 属于 **重量级锁**，效率低下。\n\n\n**为什么呢？**\n\n\n因为监视器锁（monitor）是依赖于底层的操作系统的 `Mutex Lock` 来实现的，Java 的线程是映射到操作系统的原生线程之上的。如果要挂起或者唤醒一个线程，都需要操作系统帮忙完成，而操作系统实现线程之间的切换时需要从用户态转换到内核态，这个状态之间的转换需要相对比较长的时间，时间成本相对较高。\n\n\n庆幸的是在 Java 6 之后 Java 官方对从 JVM 层面对 `synchronized` 较大优化，所以现在的 `synchronized` 锁效率也优化得很不错了。JDK1.6 对锁的实现引入了大量的优化，如自旋锁、适应性自旋锁、锁消除、锁粗化、偏向锁、轻量级锁等技术来减少锁操作的开销。\n\n\n所以，你会发现目前的话，不论是各种开源框架还是 JDK 源码都大量使用了 `synchronized` 关键字。\n\n\n### 使用方式\n\n\n**synchronized 关键字最主要的三种使用方式：**\n\n\n**1.修饰实例方法:** 作用于当前对象实例加锁，进入同步代码前要获得 **当前对象实例的锁**\n\n\n```text\nsynchronized void method() {\n  //业务代码\n}Copy to clipboardErrorCopied\n\n```\n\n\n**2.修饰静态方法:** 也就是给当前类加锁，会作用于类的所有对象实例 ，进入同步代码前要获得 **当前 class 的锁**。因为静态成员不属于任何一个实例对象，是类成员（ _static 表明这是该类的一个静态资源，不管 new 了多少个对象，只有一份_）。所以，如果一个线程 A 调用一个实例对象的非静态 `synchronized` 方法，而线程 B 需要调用这个实例对象所属类的静态 `synchronized` 方法，是允许的，不会发生互斥现象，**因为访问静态** **`synchronized`** **方法占用的锁是当前类的锁，而访问非静态** **`synchronized`** **方法占用的锁是当前实例对象锁**。\n\n\n```text\nsynchronized void staic method() {\n  //业务代码\n}Copy to clipboardErrorCopied\n\n```\n\n\n**3.修饰代码块** ：指定加锁对象，对给定对象/类加锁。`synchronized(this|object)` 表示进入同步代码库前要获得**给定对象的锁**。`synchronized(类.class)` 表示进入同步代码前要获得 **当前 class 的锁**\n\n\n```text\nsynchronized(this) {\n  //业务代码\n}Copy to clipboardErrorCopied\n\n```\n\n\n**总结：**\n\n- `synchronized` 关键字加到 `static` 静态方法和 `synchronized(class)` 代码块上都是是给 Class 类上锁。\n- `synchronized` 关键字加到实例方法上是给对象实例上锁。\n- 尽量不要使用 `synchronized(String a)` 因为 JVM 中，字符串常量池具有缓存功能！\n\n下面我以一个常见的面试题为例讲解一下 `synchronized` 关键字的具体使用。\n\n\n面试中面试官经常会说：“单例模式了解吗？来给我手写一下！给我解释一下双重检验锁方式实现单例模式的原理呗！”\n\n\n**双重校验锁实现对象单例（线程安全）**\n\n\n```text\npublic class Singleton {\n\n    private volatile static Singleton uniqueInstance;\n\n    private Singleton() {\n    }\n\n    public  static Singleton getUniqueInstance() {\n       //先判断对象是否已经实例过，没有实例化过才进入加锁代码\n        if (uniqueInstance == null) {\n            //类对象加锁\n            synchronized (Singleton.class) {\n                if (uniqueInstance == null) {\n                    uniqueInstance = new Singleton();\n                }\n            }\n        }\n        return uniqueInstance;\n    }\n}Copy to clipboardErrorCopied\n\n```\n\n\n另外，需要注意 `uniqueInstance` 采用 `volatile` 关键字修饰也是很有必要。\n\n\n`uniqueInstance` 采用 `volatile` 关键字修饰也是很有必要的， `uniqueInstance = new Singleton();` 这段代码其实是分为三步执行：\n\n1. 为 `uniqueInstance` 分配内存空间\n2. 初始化 `uniqueInstance`\n3. 将 `uniqueInstance` 指向分配的内存地址\n\n但是由于 JVM 具有指令重排的特性，执行顺序有可能变成 1->3->2。指令重排在单线程环境下不会出现问题，但是在多线程环境下会导致一个线程获得还没有初始化的实例。例如，线程 T1 执行了 1 和 3，此时 T2 调用 `getUniqueInstance`() 后发现 `uniqueInstance` 不为空，因此返回 `uniqueInstance`，但此时 `uniqueInstance` 还未被初始化。\n\n\n使用 `volatile` 可以禁止 JVM 的指令重排，保证在多线程环境下也能正常运行。\n\n\n### 底层原理\n\n\n**synchronized 关键字底层原理属于 JVM 层面。**\n\n\n**`代码块`**\n\n\n```text\npublic class SynchronizedDemo {\n    public void method() {\n        synchronized (this) {\n            System.out.println(\"synchronized 代码块\");\n        }\n    }\n}\n\n```\n\n\n通过 JDK 自带的 `javap` 命令查看 `SynchronizedDemo` 类的相关字节码信息：首先切换到类的对应目录执行 `javac SynchronizedDemo.java` 命令生成编译后的 .class 文件，然后执行`javap -c -s -v -l SynchronizedDemo.class`。\n\n\n![](https://blog-file.hehouhui.cn/202203222203508.png)\n\n\n从上面我们可以看出：\n\n\n**`synchronized`** **同步语句块的实现使用的是** **`monitorenter`** **和** **`monitorexit`** **指令，其中** **`monitorenter`** **指令指向同步代码块的开始位置，****`monitorexit`** **指令则指明同步代码块的结束位置。**\n\n\n当执行 `monitorenter` 指令时，线程试图获取锁也就是获取 **对象监视器** **`monitor`** 的持有权。\n\n\n> 在 Java 虚拟机(HotSpot)中，Monitor 是基于 C++实现的，由ObjectMonitor实现的。每个对象中都内置了一个 ObjectMonitor对象。\n\n\n\t另外，**`wait/notify`****等方法也依赖于****`monitor`****对象，这就是为什么只有在同步的块或者方法中才能调用****`wait/notify`****等方法，否则会抛出****`java.lang.IllegalMonitorStateException`****的异常的原因。**\n\n\n在执行`monitorenter`时，会尝试获取对象的锁，如果锁的计数器为 0 则表示锁可以被获取，获取后将锁计数器设为 1 也就是加 1。\n\n\n在执行 `monitorexit` 指令后，将锁计数器设为 0，表明锁被释放。如果获取对象锁失败，那当前线程就要阻塞等待，直到锁被另外一个线程释放为止。\n\n\n**修饰方法**\n\n\n```text\npublic class SynchronizedDemo2 {\n    public synchronized void method() {\n        System.out.println(\"synchronized 方法\");\n    }\n}\nCopy to clipboardErrorCopied\n\n```\n\n\n![](https://blog-file.hehouhui.cn/202203222128189.png)\n\n\n`synchronized` 修饰的方法并没有 `monitorenter` 指令和 `monitorexit` 指令，取得代之的确实是 `ACC_SYNCHRONIZED` 标识，该标识指明了该方法是一个同步方法。JVM 通过该 `ACC_SYNCHRONIZED` 访问标志来辨别一个方法是否声明为同步方法，从而执行相应的同步调用。\n\n\n### 锁\n\n\n### 乐观锁和悲观锁\n\n\n> 乐观锁对应于生活中乐观的人总是想着事情往好的方向发展，悲观锁对应于生活中悲观的人总是想着事情往坏的方向发展。这两种人各有优缺点，不能不以场景而定说一种人好于另外一种人。\n\n\n**悲观锁**\n\n\n> 总是假设最坏的情况，每次去拿数据的时候都认为别人会修改，所以每次在拿数据的时候都会上锁，这样别人想拿这个数据就会阻塞直到它拿到锁（共享资源每次只给一个线程使用，其它线程阻塞，用完后再把资源转让给其它线程）。传统的关系型数据库里边就用到了很多这种锁机制，比如行锁，表锁等，读锁，写锁等，都是在做操作之前先上锁。Java中synchronized和ReentrantLock等独占锁就是悲观锁思想的实现。\n\n\n**乐观锁**\n\n\n> 总是假设最好的情况，每次去拿数据的时候都认为别人不会修改，所以不会上锁，但是在更新的时候会判断一下在此期间别人有没有去更新这个数据，可以使用版本号机制和CAS算法实现。乐观锁适用于多读的应用类型，这样可以提高吞吐量，像数据库提供的类似于write_condition机制，其实都是提供的乐观锁。在Java中java.util.concurrent.atomic包下面的原子变量类就是使用了乐观锁的一种实现方式CAS实现的。\n\n\n### 使用场景\n\n\n从上面对两种锁的介绍，我们知道两种锁各有优缺点，不可认为一种好于另一种，像**乐观锁适用于写比较少的情况下（多读场景）**，即冲突真的很少发生的时候，这样可以省去了锁的开销，加大了系统的整个吞吐量。但如果是多写的情况，一般会经常产生冲突，这就会导致上层应用会不断的进行retry，这样反倒是降低了性能，所以**一般多写的场景下用悲观锁就比较合适。**\n\n\n### 乐观锁\n\n\n### 版本号机制\n\n\n一般是在数据表中加上一个数据版本号version字段，表示数据被修改的次数，当数据被修改时，version值会加一。当线程A要更新数据值时，在读取数据的同时也会读取version值，在提交更新时，若刚才读取到的version值为当前数据库中的version值相等时才更新，否则重试更新操作，直到更新成功。\n\n\n**举一个简单的例子：** 假设数据库中帐户信息表中有一个 version 字段，当前值为 1 ；而当前帐户余额字段（ balance ）为 $100 。\n\n1. 操作员 A 此时将其读出（ version=1 ），并从其帐户余额中扣除 $50（ $100-$50 ）。\n2. 在操作员 A 操作的过程中，操作员B 也读入此用户信息（ version=1 ），并从其帐户余额中扣除 $20 （ $100-$20 ）。\n3. 操作员 A 完成了修改工作，将数据版本号（ version=1 ），连同帐户扣除后余额（ balance=$50 ），提交至数据库更新，此时由于提交数据版本等于数据库记录当前版本，数据被更新，数据库记录 version 更新为 2 。\n4. 操作员 B 完成了操作，也将版本号（ version=1 ）试图向数据库提交数据（ balance=$80 ），但此时比对数据库记录版本时发现，操作员 B 提交的数据版本号为 1 ，数据库记录当前版本也为 2 ，不满足 “ 提交版本必须等于当前版本才能执行更新 “ 的乐观锁策略，因此，操作员 B 的提交被驳回。\n\n这样，就避免了操作员 B 用基于 version=1 的旧数据修改的结果覆盖操作员A 的操作结果的可能\n\n\n### CAS算法\n\n\n即**compare and swap（比较与交换）**，是一种有名的**无锁算法**。无锁编程，即不使用锁的情况下实现多线程之间的变量同步，也就是在没有线程被阻塞的情况下实现变量的同步，所以也叫非阻塞同步（Non-blocking Synchronization）。**CAS算法**涉及到三个操作数\n\n- 需要读写的内存值 V\n- 进行比较的值 A\n- 拟写入的新值 B\n\n当且仅当 V 的值等于 A时，CAS通过原子方式用新值B来更新V的值，否则不会执行任何操作（比较和替换是一个原子操作）。一般情况下是一个**自旋操作**，即**不断的重试**。\n\n\n关于自旋锁，大家可以看一下这篇文章，非常不错：[《 面试必备之深入理解自旋锁》](https://blog.csdn.net/qq_34337272/article/details/81252853)\n\n\n### 缺点\n\n\n> ABA 问题是乐观锁一个常见的问题\n\n\n### ABA 问题\n\n\n如果一个变量V初次读取的时候是A值，并且在准备赋值的时候检查到它仍然是A值，那我们就能说明它的值没有被其他线程修改过了吗？很明显是不能的，因为在这段时间它的值可能被改为其他值，然后又改回A，那CAS操作就会误认为它从来没有被修改过。这个问题被称为CAS操作的 **\"ABA\"问题。**\n\n\nJDK 1.5 以后的 `AtomicStampedReference 类`就提供了此种能力，其中的 `compareAndSet 方法`就是首先检查当前引用是否等于预期引用，并且当前标志是否等于预期标志，如果全部相等，则以原子方式将该引用和该标志的值设置为给定的更新值。\n\n\n### 开销大\n\n\n**自旋CAS（也就是不成功就一直循环执行直到成功）如果长时间不成功，会给CPU带来非常大的执行开销。** 如果JVM能支持处理器提供的pause指令那么效率会有一定的提升，pause指令有两个作用，第一它可以延迟流水线执行指令（de-pipeline）,使CPU不会消耗过多的执行资源，延迟的时间取决于具体实现的版本，在一些处理器上延迟时间是零。第二它可以避免在退出循环的时候因内存顺序冲突（memory order violation）而引起CPU流水线被清空（CPU pipeline flush），从而提高CPU的执行效率。\n\n\n### 只能一个变量原子操作\n\n\nCAS 只对单个共享变量有效，当操作涉及跨多个共享变量时 CAS 无效。但是从 JDK 1.5开始，提供了`AtomicReference类`来保证引用对象之间的原子性，你可以把多个变量放在一个对象里来进行 CAS 操作.所以我们可以使用锁或者利用`AtomicReference类`把多个共享变量合并成一个共享变量来操作。\n\n\nCAS与`synchronized`的使用情景\n\n\n> 简单的来说CAS适用于写比较少的情况下（多读场景，冲突一般较少），synchronized适用于写比较多的情况下（多写场景，冲突一般较多）\n\n1. 对于资源竞争较少（线程冲突较轻）的情况，使用`synchronized`同步锁进行线程阻塞和唤醒切换以及用户态内核态间的切换操作额外浪费消耗cpu资源；而CAS基于硬件实现，不需要进入内核，不需要切换线程，操作自旋几率较少，因此可以获得更高的性能。\n2. 对于资源竞争严重（线程冲突严重）的情况，CAS自旋的概率会比较大，从而浪费更多的CPU资源，效率低于synchronized。\n\n补充： Java并发编程这个领域中`synchronized`关键字一直都是元老级的角色，很久之前很多人都会称它为 **“重量级锁”** 。但是，在JavaSE 1.6之后进行了主要包括为了减少获得锁和释放锁带来的性能消耗而引入的 **偏向锁** 和 **轻量级锁** 以及其它**各种优化**之后变得在某些情况下并不是那么重了。`synchronized`的底层实现主要依靠 **Lock-Free** 的队列，基本思路是 **自旋后阻塞**，**竞争切换后继续竞争锁**，**稍微牺牲了公平性，但获得了高吞吐量**。在线程冲突较少的情况下，可以获得和CAS类似的性能；而线程冲突严重的情况下，性能远高于CAS。\n\n\n### 死锁\n\n\n线程死锁描述的是这样一种情况：多个线程同时被阻塞，它们中的一个或者全部都在等待某个资源被释放。由于线程被无限期地阻塞，因此程序不可能正常终止。\n\n\n如下图所示，线程 A 持有资源 2，线程 B 持有资源 1，他们同时都想申请对方的资源，所以这两个线程就会互相等待而进入死锁状态。\n\n\n![](https://blog-file.hehouhui.cn/202203222203695.png)\n\n\n线程 A 通过 synchronized (resource1) 获得 resource1 的监视器锁，然后通过`Thread.sleep(1000);`让线程 A 休眠 1s 为的是让线程 B 得到执行然后获取到 resource2 的监视器锁。线程 A 和线程 B 休眠结束了都开始企图请求获取对方的资源，然后这两个线程就会陷入互相等待的状态，这也就产生了死锁。上面的例子符合产生死锁的四个必要条件。\n\n\n学过操作系统的朋友都知道产生死锁必须具备以下`四个条件`：\n\n1. 互斥条件：该资源任意一个时刻只由一个线程占用。\n2. 请求与保持条件：一个进程因请求资源而阻塞时，对已获得的资源保持不放。\n3. 不剥夺条件:线程已获得的资源在未使用完之前不能被其他线程强行剥夺，只有自己使用完毕后才释放资源。\n4. 循环等待条件:若干进程之间形成一种头尾相接的循环等待资源关系。\n\n### 如何避免死锁\n\n\n我上面说了产生死锁的四个必要条件，为了避免死锁，我们只要破坏产生死锁的四个条件中的其中一个就可以了。现在我们来挨个分析一下：\n\n1. **破坏互斥条件** ：这个条件我们没有办法破坏，因为我们用锁本来就是想让他们互斥的（临界资源需要互斥访问）。\n2. **破坏请求与保持条件** ：一次性申请所有的资源。\n3. **破坏不剥夺条件** ：占用部分资源的线程进一步申请其他资源时，如果申请不到，可以主动释放它占有的资源。\n4. **破坏循环等待条件** ：靠按序申请资源来预防。按某一顺序申请资源，释放资源则反序释放。破坏循环等待条件。\n\n### AQS\n\n\n> AQS ，AbstractQueuedSynchronizer ，即队列同步器。它是构建锁或者其他同步组件的基础框架（如 ReentrantLock、ReentrantReadWriteLock、Semaphore 等），J.U.C 并发包的作者（Doug Lea）期望它能够成为实现大部分同步需求的基础。\n\n\n> 它是 J.U.C 并发包中的核心基础组件。  \n> 在介绍 Lock 之前，我们需要先熟悉一个非常重要的组件，掌握了该组件 J.U.C 包下面很多问题都不在是问题了。该组件就是 AQS 。\n\n\n**AQS 核心思想是，如果被请求的共享资源空闲，则将当前请求资源的线程设置为有效的工作线程，并且将共享资源设置为锁定状态。如果被请求的共享资源被占用，那么就需要一套线程阻塞等待以及被唤醒时锁分配的机制，这个机制 AQS 是用 CLH 队列锁实现的，即将暂时获取不到锁的线程加入到队列中。**\n\n\n> CLH(Craig,Landin,and Hagersten)队列是一个虚拟的双向队列（虚拟的双向队列即不存在队列实例，仅存在结点之间的关联关系）。AQS 是将每条请求共享资源的线程封装成一个 CLH 锁队列的一个结点（Node）来实现锁的分配。\n\n\n看个 AQS(AbstractQueuedSynchronizer)原理图：\n\n\n![](https://blog-file.hehouhui.cn/image-1648381455274.png)\n\n\nAQS 使用一个 int 成员变量来表示同步状态，通过内置的 FIFO 队列来完成获取资源线程的排队工作。AQS 使用 CAS 对该同步状态进行原子操作实现对其值的修改。\n\n\n```text\nprivate volatile int state;//共享变量，使用volatile修饰保证线程可见性Copy to clipboardErrorCopied\n\n```\n\n\n状态信息通过 protected 类型的`getState`，`setState`，`compareAndSetState`进行操作\n\n\n```text\n//返回同步状态的当前值\nprotected final int getState() {\n        return state;\n}\n // 设置同步状态的值\nprotected final void setState(int newState) {\n        state = newState;\n}\n//原子地（CAS操作）将同步状态值设置为给定值update如果当前同步状态的值等于expect（期望值）\nprotected final boolean compareAndSetState(int expect, int update) {\n        return unsafe.compareAndSwapInt(this, stateOffset, expect, update);\n}\n\n```\n\n\n### 优势\n\n\n> AQS 解决了在实现同步器时涉及当的大量细节问题，例如获取同步状态、FIFO 同步队列。基于 AQS 来构建同步器可以带来很多好处。它不仅能够极大地减少实现工作，而且也不必处理在多个位置上发生的竞争问题。\n\n\n在基于 AQS 构建的同步器中，只能在一个时刻发生阻塞，从而降低上下文切换的开销，提高了吞吐量。同时在设计 AQS 时充分考虑了可伸缩性，因此 J.U.C 中，所有基于 AQS 构建的同步器均可以获得这个优势。\n\n\n### 同步状态\n\n\n> AQS 的主要使用方式是继承，子类通过继承同步器，并实现它的抽象方法来管理同步状态。\n\n\nAQS 使用一个 int 类型的成员变量 state 来表示同步状态：\n\n\n当 state > 0 时，表示已经获取了锁。\n当 state = 0 时，表示释放了锁。\n它提供了三个方法，来对同步状态 state 进行操作，并且 AQS 可以确保对 state 的操作是安全的：\n\n\ngetState()\nsetState(int newState)\ncompareAndSetState(int expect, int update)\n\n\n### 同步队列\n\n\nAQS 通过内置的 FIFO 同步队列来完成资源获取线程的排队工作：\n\n\n如果当前线程获取同步状态失败（锁）时，AQS 则会将当前线程以及等待状态等信息构造成一个节点（Node）并将其加入同步队列，同时会阻塞当前线程\n当同步状态释放时，则会把节点中的线程唤醒，使其再次尝试获取同步状态。如图：\n\n\n它维护了一个volatile int state（代表共享资源）和一个FIFO线程等待队列（多线程争用资源被阻塞时会进入此队列）。这里volatile是核心关键词，具体volatile的语义，在此不述。state的访问方式有三种（也就 第3点 的三个方法）\n\n\ngetState()\nsetState(int newState)\ncompareAndSetState(int expect, int update)\n\n\n### 主要内置方法\n\n\nAQS 主要提供了如下方法：\n\n\ngetState()：返回同步状态的当前值。\nsetState(int newState)：设置当前同步状态。\ncompareAndSetState(int expect, int update)：使用 CAS 设置当前状态，该方法能够保证状态设置的原子性。\n【可重写】#tryAcquire(int arg)：独占式获取同步状态，获取同步状态成功后，其他线程需要等待该线程释放同步状态才能获取同步状态。\n【可重写】#tryRelease(int arg)：独占式释放同步状态。\n-【可重写】#tryAcquireShared(int arg)：共享式获取同步状态，返回值大于等于 0 ，则表示获取成功；否则，获取失败。\n【可重写】#tryReleaseShared(int arg)：共享式释放同步状态。\n【可重写】#isHeldExclusively()：当前同步器是否在独占式模式下被线程占用，一般该方法表示是否被当前线程所独占。\nacquire(int arg)：独占式获取同步状态。如果当前线程获取同步状态成功，则由该方法返回；否则，将会进入同步队列等待。该方法将会调用可重写的 #tryAcquire(int arg) 方法；\nacquireInterruptibly(int arg)：与 #acquire(int arg) 相同，但是该方法响应中断。当前线程为获取到同步状态而进入到同步队列中，如果当前线程被中断，则该方法会抛出InterruptedException 异常并返回。\ntryAcquireNanos(int arg, long nanos)：超时获取同步状态。如果当前线程在 nanos 时间内没有获取到同步状态，那么将会返回 false ，已经获取则返回 true 。\nacquireShared(int arg)：共享式获取同步状态，如果当前线程未获取到同步状态，将会进入同步队列等待，与独占式的主要区别是在同一时刻可以有多个线程获取到同步状态；\nacquireSharedInterruptibly(int arg)：共享式获取同步状态，响应中断。\ntryAcquireSharedNanos(int arg, long nanosTimeout)：共享式获取同步状态，增加超时限制。\nrelease(int arg)：独占式释放同步状态，该方法会在释放同步状态之后，将同步队列中第一个节点包含的线程唤醒。\nreleaseShared(int arg)：共享式释放同步状态。\n\n\n**从上面的方法看下来，基本上可以分成 3 类：**\n\n1. 独占式获取与释放同步状态\n2. 共享式获取与释放同步状态\n3. 查询同步队列中的等待线程情况\n\n### 资源的共享方式\n\n\n**AQS 定义两种资源共享方式**\n\n\n**1)Exclusive**（独占）\n\n\n只有一个线程能执行，如 ReentrantLock。又可分为公平锁和非公平锁,ReentrantLock 同时支持两种锁,下面以 ReentrantLock 对这两种锁的定义做介绍：\n\n- 公平锁：按照线程在队列中的排队顺序，先到者先拿到锁\n- 非公平锁：当线程要获取锁时，先通过两次 CAS 操作去抢锁，如果没抢到，当前线程再加入到队列中等待唤醒。\n\n> 说明：下面这部分关于 ReentrantLock 源代码内容节选自：https://www.javadoop.com/post/AbstractQueuedSynchronizer-2 ，这是一篇很不错文章，推荐阅读。\n\n\n**下面来看 ReentrantLock 中相关的源代码：**\n\n\nReentrantLock 默认采用非公平锁，因为考虑获得更好的性能，通过 boolean 来决定是否用公平锁（传入 true 用公平锁）。\n\n\n```text\n/** Synchronizer providing all implementation mechanics */\nprivate final Sync sync;\npublic ReentrantLock() {\n    // 默认非公平锁\n    sync = new NonfairSync();\n}\npublic ReentrantLock(boolean fair) {\n    sync = fair ? new FairSync() : new NonfairSync();\n}Copy to clipboardErrorCopied\n\n```\n\n\nReentrantLock 中公平锁的 `lock` 方法\n\n\n```text\nstatic final class FairSync extends Sync {\n    final void lock() {\n        acquire(1);\n    }\n    // AbstractQueuedSynchronizer.acquire(int arg)\n    public final void acquire(int arg) {\n        if (!tryAcquire(arg) &&\n            acquireQueued(addWaiter(Node.EXCLUSIVE), arg))\n            selfInterrupt();\n    }\n    protected final boolean tryAcquire(int acquires) {\n        final Thread current = Thread.currentThread();\n        int c = getState();\n        if (c == 0) {\n            // 1. 和非公平锁相比，这里多了一个判断：是否有线程在等待\n            if (!hasQueuedPredecessors() &&\n                compareAndSetState(0, acquires)) {\n                setExclusiveOwnerThread(current);\n                return true;\n            }\n        }\n        else if (current == getExclusiveOwnerThread()) {\n            int nextc = c + acquires;\n            if (nextc < 0)\n                throw new Error(\"Maximum lock count exceeded\");\n            setState(nextc);\n            return true;\n        }\n        return false;\n    }\n}Copy to clipboardErrorCopied\n\n```\n\n\n非公平锁的 lock 方法：\n\n\n```text\nstatic final class NonfairSync extends Sync {\n    final void lock() {\n        // 2. 和公平锁相比，这里会直接先进行一次CAS，成功就返回了\n        if (compareAndSetState(0, 1))\n            setExclusiveOwnerThread(Thread.currentThread());\n        else\n            acquire(1);\n    }\n    // AbstractQueuedSynchronizer.acquire(int arg)\n    public final void acquire(int arg) {\n        if (!tryAcquire(arg) &&\n            acquireQueued(addWaiter(Node.EXCLUSIVE), arg))\n            selfInterrupt();\n    }\n    protected final boolean tryAcquire(int acquires) {\n        return nonfairTryAcquire(acquires);\n    }\n}\n/**\n * Performs non-fair tryLock.  tryAcquire is implemented in\n * subclasses, but both need nonfair try for trylock method.\n */\nfinal boolean nonfairTryAcquire(int acquires) {\n    final Thread current = Thread.currentThread();\n    int c = getState();\n    if (c == 0) {\n        // 这里没有对阻塞队列进行判断\n        if (compareAndSetState(0, acquires)) {\n            setExclusiveOwnerThread(current);\n            return true;\n        }\n    }\n    else if (current == getExclusiveOwnerThread()) {\n        int nextc = c + acquires;\n        if (nextc < 0) // overflow\n            throw new Error(\"Maximum lock count exceeded\");\n        setState(nextc);\n        return true;\n    }\n    return false;\n}Copy to clipboardErrorCopied\n\n```\n\n\n总结：公平锁和非公平锁只有两处不同：\n\n1. 非公平锁在调用 lock 后，首先就会调用 CAS 进行一次抢锁，如果这个时候恰巧锁没有被占用，那么直接就获取到锁返回了。\n2. 非公平锁在 CAS 失败后，和公平锁一样都会进入到 tryAcquire 方法，在 tryAcquire 方法中，如果发现锁这个时候被释放了（state == 0），非公平锁会直接 CAS 抢锁，但是公平锁会判断等待队列是否有线程处于等待状态，如果有则不去抢锁，乖乖排到后面。\n\n公平锁和非公平锁就这两点区别，如果这两次 CAS 都不成功，那么后面非公平锁和公平锁是一样的，都要进入到阻塞队列等待唤醒。\n\n\n相对来说，非公平锁会有更好的性能，因为它的吞吐量比较大。当然，非公平锁让获取锁的时间变得更加不确定，可能会导致在阻塞队列中的线程长期处于饥饿状态。\n\n\n**2)Share**（共享）\n\n\n多个线程可同时执行，如 Semaphore/CountDownLatch。Semaphore、CountDownLatCh、 CyclicBarrier、ReadWriteLock 我们都会在后面讲到。\n\n\nReentrantReadWriteLock 可以看成是组合式，因为 ReentrantReadWriteLock 也就是读写锁允许多个线程同时对某一资源进行读。\n\n\n不同的自定义同步器争用共享资源的方式也不同。自定义同步器在实现时只需要实现共享资源 state 的获取与释放方式即可，至于具体线程等待队列的维护（如获取资源失败入队/唤醒出队等），AQS 已经在上层已经帮我们实现好了。\n\n\n### 底层模版方法模式\n\n\n同步器的设计是基于模板方法模式的，如果需要自定义同步器一般的方式是这样（模板方法模式很经典的一个应用）：\n\n1. 使用者继承 AbstractQueuedSynchronizer 并重写指定的方法。（这些重写方法很简单，无非是对于共享资源 state 的获取和释放）\n2. 将 AQS 组合在自定义同步组件的实现中，并调用其模板方法，而这些模板方法会调用使用者重写的方法。\n\n这和我们以往通过实现接口的方式有很大区别，这是模板方法模式很经典的一个运用，下面简单的给大家介绍一下模板方法模式，模板方法模式是一个很容易理解的设计模式之一。\n\n\n> 模板方法模式是基于”继承“的，主要是为了在不改变模板结构的前提下在子类中重新定义模板中的内容以实现复用代码。举个很简单的例子假如我们要去一个地方的步骤是：购票buyTicket()->安检securityCheck()->乘坐某某工具回家ride()->到达目的地arrive()。我们可能乘坐不同的交通工具回家比如飞机或者火车，所以除了ride()方法，其他方法的实现几乎相同。我们可以定义一个包含了这些方法的抽象类，然后用户根据自己的需要继承该抽象类然后修改 ride()方法。\n\n\n**AQS 使用了模板方法模式，自定义同步器时需要重写下面几个 AQS 提供的模板方法：**\n\n\n```text\nisHeldExclusively()//该线程是否正在独占资源。只有用到condition才需要去实现它。\ntryAcquire(int)//独占方式。尝试获取资源，成功则返回true，失败则返回false。\ntryRelease(int)//独占方式。尝试释放资源，成功则返回true，失败则返回false。\ntryAcquireShared(int)//共享方式。尝试获取资源。负数表示失败；0表示成功，但没有剩余可用资源；正数表示成功，且有剩余资源。\ntryReleaseShared(int)//共享方式。尝试释放资源，成功则返回true，失败则返回false。\nCopy to clipboardErrorCopied\n\n```\n\n\n默认情况下，每个方法都抛出 `UnsupportedOperationException`。 这些方法的实现必须是内部线程安全的，并且通常应该简短而不是阻塞。AQS 类中的其他方法都是 final ，所以无法被其他类使用，只有这几个方法可以被其他类使用。\n\n\n以 ReentrantLock 为例，state 初始化为 0，表示未锁定状态。A 线程 lock()时，会调用 tryAcquire()独占该锁并将 state+1。此后，其他线程再 tryAcquire()时就会失败，直到 A 线程 unlock()到 state=0（即释放锁）为止，其它线程才有机会获取该锁。当然，释放锁之前，A 线程自己是可以重复获取此锁的（state 会累加），这就是可重入的概念。但要注意，获取多少次就要释放多么次，这样才能保证 state 是能回到零态的。\n\n\n再以 CountDownLatch 以例，任务分为 N 个子线程去执行，state 也初始化为 N（注意 N 要与线程个数一致）。这 N 个子线程是并行执行的，每个子线程执行完后 countDown()一次，state 会 CAS(Compare and Swap)减 1。等到所有子线程都执行完后(即 state=0)，会 unpark()主调用线程，然后主调用线程就会从 await()函数返回，继续后余动作。\n\n\n一般来说，自定义同步器要么是独占方法，要么是共享方式，他们也只需实现`tryAcquire-tryRelease`、`tryAcquireShared-tryReleaseShared`中的一种即可。但 AQS 也支持自定义同步器同时实现独占和共享两种方式，如`ReentrantReadWriteLock`。\n\n\n推荐两篇 AQS 原理和相关源码分析的文章：\n\n- [http://www.cnblogs.com/waterystone/p/4920797.html](http://www.cnblogs.com/waterystone/p/4920797.html)\n- [https://www.cnblogs.com/chengxiao/archive/2017/07/24/7141160.html](https://www.cnblogs.com/chengxiao/archive/2017/07/24/7141160.html)\n\n### 信号量（允许多个线程同时访问）\n\n\n**synchronized 和 ReentrantLock 都是一次只允许一个线程访问某个资源，Semaphore(信号量)可以指定多个线程同时访问某个资源。**\n\n\n示例代码如下：\n\n\n```text\n/**\n *\n * @author Snailclimb\n * @date 2018年9月30日\n * @Description: 需要一次性拿一个许可的情况\n */\npublic class SemaphoreExample1 {\n  // 请求的数量\n  private static final int threadCount = 550;\n\n  public static void main(String[] args) throws InterruptedException {\n    // 创建一个具有固定线程数量的线程池对象（如果这里线程池的线程数量给太少的话你会发现执行的很慢）\n    ExecutorService threadPool = Executors.newFixedThreadPool(300);\n    // 一次只能允许执行的线程数量。\n    final Semaphore semaphore = new Semaphore(20);\n\n    for (int i = 0; i < threadCount; i++) {\n      final int threadnum = i;\n      threadPool.execute(() -> {// Lambda 表达式的运用\n        try {\n          semaphore.acquire();// 获取一个许可，所以可运行线程数量为20/1=20\n          test(threadnum);\n          semaphore.release();// 释放一个许可\n        } catch (InterruptedException e) {\n          // TODO Auto-generated catch block\n          e.printStackTrace();\n        }\n\n      });\n    }\n    threadPool.shutdown();\n    System.out.println(\"finish\");\n  }\n\n  public static void test(int threadnum) throws InterruptedException {\n    Thread.sleep(1000);// 模拟请求的耗时操作\n    System.out.println(\"threadnum:\" + threadnum);\n    Thread.sleep(1000);// 模拟请求的耗时操作\n  }\n}Copy to clipboardErrorCopied\n\n```\n\n\n执行 `acquire` 方法阻塞，直到有一个许可证可以获得然后拿走一个许可证；每个 `release` 方法增加一个许可证，这可能会释放一个阻塞的 acquire 方法。然而，其实并没有实际的许可证这个对象，Semaphore 只是维持了一个可获得许可证的数量。 Semaphore 经常用于限制获取某种资源的线程数量。\n\n\n当然一次也可以一次拿取和释放多个许可，不过一般没有必要这样做：\n\n\n```text\nsemaphore.acquire(5);// 获取5个许可，所以可运行线程数量为20/5=4\ntest(threadnum);\nsemaphore.release(5);// 获取5个许可，所以可运行线程数量为20/5=4Copy to clipboardErrorCopied\n\n```\n\n\n除了 `acquire`方法之外，另一个比较常用的与之对应的方法是`tryAcquire`方法，该方法如果获取不到许可就立即返回 false。\n\n\nSemaphore 有两种模式，公平模式和非公平模式。\n\n- **公平模式：** 调用 acquire 的顺序就是获取许可证的顺序，遵循 FIFO；\n- **非公平模式：** 抢占式的。\n\n**Semaphore 对应的两个构造方法如下：**\n\n\n```text\n   public Semaphore(int permits) {\n        sync = new NonfairSync(permits);\n    }\n\n    public Semaphore(int permits, boolean fair) {\n        sync = fair ? new FairSync(permits) : new NonfairSync(permits);\n    }Copy to clipboardErrorCopied\n\n```\n\n\n**这两个构造方法，都必须提供许可的数量，第二个构造方法可以指定是公平模式还是非公平模式，默认非公平模式。**\n\n\n[issue645 补充内容](https://github.com/Snailclimb/JavaGuide/issues/645) ：Semaphore 与 CountDownLatch 一样，也是共享锁的一种实现。它默认构造 AQS 的 state 为 permits。当执行任务的线程数量超出 permits,那么多余的线程将会被放入阻塞队列 Park,并自旋判断 state 是否大于 0。只有当 state 大于 0 的时候，阻塞的线程才能继续执行,此时先前执行任务的线程继续执行 release 方法，release 方法使得 state 的变量会加 1，那么自旋的线程便会判断成功。 如此，每次只有最多不超过 permits 数量的线程能自旋成功，便限制了执行任务线程的数量。\n\n\n由于篇幅问题，如果对 Semaphore 源码感兴趣的朋友可以看下这篇文章：[https://juejin.im/post/5ae755366fb9a07ab508adc6](https://juejin.im/post/5ae755366fb9a07ab508adc6)\n\n\n### CountDownLatch 倒计时器\n\n\n`CountDownLatch` 允许 `count` 个线程阻塞在一个地方，直至所有线程的任务都执行完毕。\n\n\n`CountDownLatch` 是共享锁的一种实现,它默认构造 AQS 的 `state` 值为 `count`。当线程使用 `countDown()` 方法时,其实使用了`tryReleaseShared`方法以 CAS 的操作来减少 `state`,直至 `state` 为 0 。当调用 `await()` 方法的时候，如果 `state` 不为 0，那就证明任务还没有执行完毕，`await()` 方法就会一直阻塞，也就是说 `await()` 方法之后的语句不会被执行。然后，`CountDownLatch` 会自旋 CAS 判断 `state == 0`，如果 `state == 0` 的话，就会释放所有等待的线程，`await()` 方法之后的语句\n\n\n**`CountDownLatch`****两种经典算法**\n\n1. 某一线程在开始运行前等待 n 个线程执行完毕。将 CountDownLatch 的计数器初始化为 n ：`new CountDownLatch(n)`，每当一个任务线程执行完毕，就将计数器减 1 `countdownlatch.countDown()`，当计数器的值变为 0 时，在`CountDownLatch上 await()` 的线程就会被唤醒。一个典型应用场景就是启动一个服务时，主线程需要等待多个组件加载完毕，之后再继续执行。\n2. 实现多个线程开始执行任务的最大并行性。注意是并行性，不是并发，强调的是多个线程在某一时刻同时开始执行。类似于赛跑，将多个线程放到起点，等待发令枪响，然后同时开跑。做法是初始化一个共享的 `CountDownLatch` 对象，将其计数器初始化为 1 ：`new CountDownLatch(1)`，多个线程在开始执行任务前首先 `coundownlatch.await()`，当主线程调用 countDown() 时，计数器变为 0，多个线程同时被唤醒。\n\n**使用示范**\n\n\n```text\n/**\n *\n * @author SnailClimb\n * @date 2018年10月1日\n * @Description: CountDownLatch 使用方法示例\n */\npublic class CountDownLatchExample1 {\n  // 请求的数量\n  private static final int threadCount = 550;\n\n  public static void main(String[] args) throws InterruptedException {\n    // 创建一个具有固定线程数量的线程池对象（如果这里线程池的线程数量给太少的话你会发现执行的很慢）\n    ExecutorService threadPool = Executors.newFixedThreadPool(300);\n    final CountDownLatch countDownLatch = new CountDownLatch(threadCount);\n    for (int i = 0; i < threadCount; i++) {\n      final int threadnum = i;\n      threadPool.execute(() -> {// Lambda 表达式的运用\n        try {\n          test(threadnum);\n        } catch (InterruptedException e) {\n          // TODO Auto-generated catch block\n          e.printStackTrace();\n        } finally {\n          countDownLatch.countDown();// 表示一个请求已经被完成\n        }\n\n      });\n    }\n    countDownLatch.await();\n    threadPool.shutdown();\n    System.out.println(\"finish\");\n  }\n\n  public static void test(int threadnum) throws InterruptedException {\n    Thread.sleep(1000);// 模拟请求的耗时操作\n    System.out.println(\"threadnum:\" + threadnum);\n    Thread.sleep(1000);// 模拟请求的耗时操作\n  }\n}\nCopy to clipboardErrorCopied\n\n```\n\n\n上面的代码中，我们定义了请求的数量为 550，当这 550 个请求被处理完成之后，才会执行`System.out.println(\"finish\");`。\n\n\n与 CountDownLatch 的第一次交互是主线程等待其他线程。主线程必须在启动其他线程后立即调用 `CountDownLatch.await()` 方法。这样主线程的操作就会在这个方法上阻塞，直到其他线程完成各自的任务。\n\n\n其他 N 个线程必须引用闭锁对象，因为他们需要通知 `CountDownLatch` 对象，他们已经完成了各自的任务。这种通知机制是通过 `CountDownLatch.countDown()`方法来完成的；每调用一次这个方法，在构造函数中初始化的 count 值就减 1。所以当 N 个线程都调 用了这个方法，count 的值等于 0，然后主线程就能通过 `await()`方法，恢复执行自己的任务。\n\n\n再插一嘴：`CountDownLatch` 的 `await()` 方法使用不当很容易产生死锁，比如我们上面代码中的 for 循环改为：\n\n\n```text\nfor (int i = 0; i < threadCount-1; i++) {\n.......\n}Copy to clipboardErrorCopied\n\n```\n\n\n这样就导致 `count` 的值没办法等于 0，然后就会导致一直等待。\n\n\n如果对 CountDownLatch 源码感兴趣的朋友，可以查看： [【JUC】JDK1.8 源码分析之 CountDownLatch（五）](https://www.cnblogs.com/leesf456/p/5406191.html)\n\n",
      "properties": {
        "password": "",
        "icon": "",
        "date": "2022-02-08",
        "type": "Post",
        "category": "技术分享",
        "urlname": "37",
        "catalog": [
          "archives"
        ],
        "tags": [
          "Java"
        ],
        "summary": "synchronized 关键字解决的是多个线程之间访问资源的同步性，synchronized关键字可以保证被它修饰的方法或者代码块在任意时刻只能有一个线程执行。另外，在 Ja",
        "sort": "",
        "title": "Java基础-锁",
        "status": "Published",
        "updated": "2023-10-08 14:42:00"
      },
      "catalog": [
        {
          "title": "archives",
          "doc_id": "747841e4-eeac-4349-88b0-63fc6421cf0c"
        }
      ],
      "body": "",
      "realName": "Java基础-锁",
      "relativePath": "/archives/Java基础-锁.md"
    },
    {
      "id": "07c3aa88-08b7-4093-9ac6-0a495a187163",
      "doc_id": "07c3aa88-08b7-4093-9ac6-0a495a187163",
      "title": "07c3aa88-08b7-4093-9ac6-0a495a187163",
      "updated": 1696747320000,
      "body_original": "\n# wkhtmltopdf精讲\n\n\n([原文](https://www.jianshu.com/p/4d65857ffe5e)) 作者：JSON_NULL\n\n\n## 术语定义\n\n\n### 文档对象\n\n\n“文档对象”是指PDF文档中的文档对象，共有三种类型的“文档对象”，他们分别是“页面对象”，“封面对象”和“目录对象”。\n\n\n### 页面对象\n\n\n“页面对象”是指以页面的形式在PDF文档中呈现的对象，这个是相对于“封面对象”和“目录对象”来讲的。此类对象会成为PDF文档中内容。\n\n\n### 封面对象\n\n\n“封面对象”是指以封面的形式在PDF文档中呈现的对象。这类对象会成为PDF文档中的封面。\n\n\n### 目录对象\n\n\n“目录对象”是以目录的形式在PDF文档中呈现的对象，又叫“TOC对象”。这类对象会成为PDF文档中的目录。\n\n\n### 大纲\n\n\n“大纲”是指阅读PDF文档时的书签导航。\n\n\n### 外链\n\n\n“外链”是指所有在这个页面中且不指向它自身页面中锚点的超链接。\n\n\n### 内链\n\n\n“内链”是指在这个页面中且指向的目标页面是这个页面本身中的一个锚点的超链接。\n\n\n## 命令格式\n\n\n```text\nwkhtmltopdf [GLOBAL OPTION]... [OBJECT]... <output file>\n\n```\n\n\n上述代码就是 wkhtmltopdf 的命令行格式，看似简单，其实在 `[GLOBAL OPTION]` 和 `[OBJECT]` 中还别有洞天。预知详情，且听我慢慢道来。\n\n\n## 文档对象简介\n\n\nwkhtmltopdf 能够把多个“对象”合并生成一个pdf文档，这些“对象”可以是“页面对象”、“封面对象”、或是“目录对象”。这些对象在pdf文档中的顺序可以通过命令行参数来指定。命令行参数包括两部分，一种是针对某一特定“对象”的命令行参数，另一种是全局的命令行参数。并且全局的命令行参数只能放在全局参数区([GLOBAL OPTION])中指定。\n\n\n### 页面对象简介\n\n\n“页面对象”作用是用来把一个网页作为内容输出到PDF文档中。\n\n\n```text\n(page)? <input url/file name> [PAGE OPTION]...\n\n```\n\n\n“页面对象”的参数可以放在“全局参数域([GLOBAL OPTIONS])”和“页面参数域([PAGE OPTIONS])”。程序会根据实际情况在所有参数中找到合适的参数应用到页面、页眉和页脚。\n\n\n### 封面对象简介\n\n\n“封面对象”用来把一个网页作为封面输出到PDF文档中，输出的页面不会在TOC中出现，并且不会包含页眉和页脚。\n\n\n```text\ncover <input url/file name> [PAGE OPTION]...\n\n```\n\n\n所有能够在“页面对象”中使用的参数都可以用到“封面对象”\n\n\n### 目录对象简介\n\n\n“目录对象”的作用是输出一个目录到PDF文件中。\n\n\n```text\ntoc [TOC OPTION]...\n\n```\n\n\n所有能够在“页面对象”中使用的参数都可以用到“TOC对象”，并且还有许多的针对“TOC对象”的参数可以应用到“TOC对象”中。目录是通过 XSLT 生成的，这就意味着它可以被定义成任何你想看到的样子。你可以通过命令行参数 `--dump-default-toc-xsl` 输出默认的 XSLT 文档，通过 `--dump-outline` 命令行参数 可指定以XML格式输出当前处理文档的目录到指定文件。更多详细内容请查看后面介绍的 **目录对象参数**\n\n\n## 命令参数\n\n\n命令参数包含五部分，分别是“全局参数”，“大纲参数选项”，“页面对象参数”，“页眉和页脚参数选项”和“目录对象参数”。\n\n\n### 全局参数\n\n\n```text\n    --collate             当输出多个副本时进行校验(这是默认设置)\n    --no-collate          当输出多个副本时不进行校验\n    --cookie-jar <path>   从提供的JAR文件中读写cookie数据\n    --copies <number>     设置输出副本的数量(默认主1)，其实为1就够了\n-d, --dpi <dpi>           指定一个要分辨率(这在 X11 系统中并没有什么卵用)\n-H, --extended-help       相对 -h 参数，显示更详细的说明文档\n-g, --grayscale           指定以灰度图生成PDF文档。占用的空间更小\n-h, --help                显示帮助信息\n    --htmldoc             输出程序的html帮助文档\n    --image-dpi <integer> 当页面中有内嵌的图片时，\n                          会下载此命令行参数指定尺寸的图片(默认值是 600)\n    --image-quality <interger> 当使用 jpeg 算法压缩图片时使用这个参数指定的质量(默认为 94)\n    --license             输出授权信息并退出\n-l, --lowquality          生成低质量的 PDF/PS ,能够很好的节约最终生成文档所占存储空间\n    --manpage             输出程序的手册页\n-B, --margin-bottom <unitreal> 设置页面的 底边距\n-L, --margin-left <unitreal>   设置页面的 左边距 (默认是 10mm)\n-R, --margin-right <unitreal>  设置页面的 右边距 (默认是 10mm)\n-T, --margin-top <unitreal>    设置页面的 上边距\n-O, --orientation <orientation> 设置为“风景(Landscape)”或“肖像(Portrait)”模式,\n                                默认是肖像模块(Portrait)\n    --page-height <unitreal>   页面高度\n-s, --page-size <Size>         设置页面的尺寸，如：A4,Letter等，默认是：A4\n    --page-width <unitreal>    页面宽度\n    --no-pdf-compression       不对PDF对象使用丢失少量信息的压缩算法，不建议使用些参数，\n                               因为生成的PDF文件会非常大。\n-q, --quiet                    静态模式，不在标准输出中打印任何信息\n    --read-args-from-stdin     从标准输入中读取命令行参数，后续会有针对此指令的详细介绍，\n                               请参见 **从标准输入获取参数**\n    --readme                   输出程序的 readme 文档\n    --title <text>             生成的PDF文档的标题，如果不指定则使用第一个文档的标题\n-V, --version                  输出版本信息后退出\n\n```\n\n\n上述代码区是所有全局参数及注释，下面简单说一下个别参数的意义及用法。\n\n\n### -copies N\n\n\nN 是一个正整数。\n\n\n这个选项可以先不用关心了，因为你这辈子可能都用不到。他的作用是在生成的PDF文档中，把内容重复输出 N 份。也就是说，你将得到一个PDF文档，这个文档中的大小、内容量都将是不使用此参数时的 N 倍。然而重复的内容对你来说并没有什么卵用。\n\n\n如果不使用 `--copies` 参数，那么 `--collate` 和 `--no-collate` 参数就不用了解了，因为他们只在 `--copies` 参数中的 N 大于 1 时才有意义。\n\n\n### g, --grayscale\n\n\n这个参数非常有用，使用这个参数可以有效压缩生成的PDF所占用的存储空间。当然这个压缩是要付出一定代价的，那就是最终生成的PDF文档将是灰度的，没有任何色彩。如果你能接受灰度PDF文档，并不影响实际使用，那就请使用这个参数吧。生成的PDF文档越大，使用此参数获得的惊喜就越大。\n\n\n### l, --lowquality\n\n\n这个参数与 `-g` 参数有异曲同工之妙， `-l` 参数也会大大压缩PDF文档所占用的存储空间。只是它是通过降低PDF文档的质量来完成这一任务的。这个参数也值得推荐，你最好先尝试一下，看看使用此参数后生成的PDF文档与不使用此参数的区别再做决定。我可以告诉你的是，在纯文字的情况下他们的差别不大，此参数只是降低了PDF文档的质量，看上去是糙了一些，但不会影响阅读。如果你是一个追求感官享受，或是你生成的PDF文档中有大量图片，那就不要使用此参数了。\n\n\n### -no-pdf-compression\n\n\n这个参数强烈建议不要使用，最好这辈子都不要去了解他的好，因为对于你来说肯定用不到。它的作用就是在输出PDF文档时，不使用任何的压缩。这将会导致输出的PDF文档特别的大，质量是无损的，但是对于人类来说从感观上根本察觉不到压缩前后的质量变化的。如果你的感观超乎于常人，压缩之后的体验对你来说无法接受，那我收回前面的话，你就尽情使用此参数吧。\n\n\n### q, --quiet\n\n\n使用这个参数后，你将得到一个干净的命令行输出，就连程序处理的进度和状态都没有。这个参数会抑制所有命令行输出，在程序的工作过程中，你看不到任何输出。建议不会使用此参数，因为程序输出一些进度和状态信息还是非常有用的。万一程序工作到某处死了呢(嘿嘿)，在 `-q` 模式下你是无法分辨是否程序死掉了的。\n\n\n### 大纲参数选项\n\n\n```text\n--dump-default-toc-xsl     输出默认的 TOC xsl 样式表到标准输出\n--dump-outline <file>      输出“大纲”到指定的文件(文件内容为xml)\n--outline                  在生成的PDF文档中输出“大纲”(这是默认设置)\n--no-outline               不在pdf文档中输出大纲\n--outline-depth <level>    设置生成大纲的深度(默认为 4)\n\n```\n\n\n大纲参数中唯一需要特别说一下的是 `--outline-depth` ，其他参数默认就好了。\n\n\n### 何为大纲\n\n\n![](https:////upload-images.jianshu.io/upload_images/3060014-05914583ee52091d.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1140/format/webp)\n\n\n大纲\n\n\n如上图所示，其实我更喜欢称之为目录或导航。大纲是根据你HTML中的标题(Hn标签)自动生成的。\n\n\n### -outline-depth\n\n- `-outline-depth` 用来指定生成的大纲的深度。默认值为 4。你可以指定一个大一些的数字，以保证所有在HTML中指定的H标签都能在大纲中生成对应的项，方便阅读时快速跳转。\n\n当指定了 `--no-outline` 参数时， 将不会输出大纲到PDF文档，所以再指定 `--outline-depth` 也就没有意义了。\n\n\n### 页面对象参数\n\n\n```text\n    --allow <path>                指定加载HTML中相对路径文件的目录(可重复使用此参数指定多个\n                                  目录)，这个参数会在后面进行更详细的讲解\n    --background                  输出页面背景到PDF文档(这是默认设置)\n    --no-background               不输出页面背景到PDF文档\n    --cache-dir <path>            网页的缓存目录\n    --checkbox-checked-svg <path> 使用指定的SVG文件渲染选中的复选框\n    --checkbox-svg <path>         使用指定的SVG文件渲染未选中的筛选框\n    --cookie <name> <value>       设置访问网页时的cookie,value 需要进行url编码\n                                  (可重复使用此参数指定多个cookie)\n    --custom-header <name> <value> 设置访问网页时的HTTP头(可重复使用此参数指定多个HTTP头)\n    --custom-header-propagation   为每个要加载的资源添加由 --custom-header 指定的HTTP头\n    --no-custom-header-propagation 不为每个要加载的资源添加由 --custom-header 指定的HTTP头\n    --debug-javascript            显示javascript调试输出的信息\n    --no-debug-javascript         不显示javascript调试输出的信息(这是默认设置)\n    --default-header              添加一个默认的“头”，在页面的左头显示页面的名字，\n                                  在页面的右头显示页码，这相对于进行了如下设置：\n                                  --header-left='[webpage]'\n                                  --header-right='[page]/[toPage]'\n                                  --top 2cm\n                                  --header-line\n    --encoding <encoding>         为输入的文本设置默认的编码方式\n    --disable-external-links      禁止页面中的外链生成超链接\n    --enable-external-links       允许页面中的外链生成超链接(这是默认设置)\n    --disable-forms               不转换HTML表单为PDF表单(这是默认设置)\n    --enable-forms                转换HTML表单为PDF表单\n    --images                      加载图片并输出到PDF文档(这是默认设置)\n    --no-images                   在生成的PDF文档中过滤掉图片\n    --disable-internal-links      禁止页面中的内链生成超链接\n    --enable-internal-links       允许页面中的内链生成超连接(这是默认设置)\n-n, --disable-javascript          禁止WEB页面执行 javascript\n    --enable-javascript           允许WEB页面执行 javascript(这是默认设置)\n    --javascript-delay <msec>     延迟一定的毫秒等待javascript 执行完成(默认值是200)\n    --load-error-handling <handler> 指定当页面加载失败后的动作，可以指定为：abort(中止)、\n                                    ignore(忽略)、skip(跳过)；(默认值是：abort)\n    --load-media-error-handling <handler> 指定当媒体文件加载失败后的动作，可以指定为：\n                                          abort(中止)、ignore(忽略)、skip(跳过)；\n                                          (默认值是：ignore)\n    --disable-local-file-access   不允许一个本地文件加载其他的本地文件，使用命令行参数\n                                   `--allow` 指定的目录除外。\n    --enable-local-file-access    允许本地文件加载其他的本地文件(这是默认设置)\n    --minimum-font-size <int>     设置最小的字号，除非必要不推荐使用该参数\n    --exclude-from-outline        拒绝加载当前页面到PDF文档的目录和大纲中\n    --include-in-outline          加载当前页面到PDF文档的目录和大纲中(这是默认设置)\n    --page-offset <offset>        设置页码的起始值(默认值为0)\n    --password <password>         HTTP身份认证的密码\n    --disable-plugins             禁止使用插件(这是默认设置)\n    --enable-plugins              允许使用插件，但插件可能并不工作\n    --post <name> <value>         添加一个POST字段，可以重复使用该参数添加多个POST字段。\n    --post-file <name> <value>    添加一个POST文件，可以重复使用该参数添加多个文件。\n    --print-media-type            用显示媒体类型代替屏幕\n    --no-print-media-type         不用显示媒体类型代替屏幕\n-p, --proxy <proxy>               使用代理\n--radiobutton-checked-svg <path>  使用指定的SVG文件渲染选中的单选框\n--radiobutton-svg <path>          使用指定的SVG文件渲染未选中的单选框\n--run-sript <js>                  页面加载完成后执行一个附加的JS文件，可以重复使用此参数指定\n                                  多个要在页面加载完成后要执行的JS文件。\n--disable-smart-shrinking         不使用智能收缩策略\n--enable-smart-shrinking          使用智能收缩策略(这是默认设置)\n--stop-slow-scripts               停止运行缓慢的javascript代码(这是默认设置)\n--no-stop-slow-scripts            不停止运行缓慢的javascript代码\n--disable-toc-back-links          禁止从标题链接到目录(这是默认设置)\n--enable-toc-back-links           允许从标题链接到目录\n--user-style-sheet <url>          设置一个在每个页面都加载的用户自定义样式表\n--username <username>             HTTP身谁的用户名\n--viewport-size <>                设置窗口大小,需要你自定义滚动条或css属性来自适应窗口大小。\n--window-status <windowStatus>    Wait until window.status is equal to\n                                  this string before rendering page\n--zoom <float>                    设置转换成PDF时页面的缩放比例(默认为1)\n\n```\n\n\n上面代码段中只是对所有 **页面对象参数** 做了个大概的说明，下面针对个别主要参数做更详细的讲解。\n\n\n### -allow\n\n\n这个参数只在“页面对象”是一个文件时有效，在“页面对象”是一个url时此参数无效。\n\n\n这个参数的作用是为HTML页面中使用相对路径引用的文件指定一个加载文件的基目录。也就是说HTML文件中所有以相对路径指定的文件都会从 `--allow` 参数指定的目录进行加载。其实在HTML中指定 `base` 标签可以达到同样的目的。如果两者(`--allow`参数和`base`标签)都没有指定，则使用当前处理的HTML文件所在的目录作为基目录加载当前处理的HTML中相对路径指定的文件。\n\n\n### -background AND --no-background\n\n\n这两个参数是一对，用来指定是否在生成的PDF中应用网页的背景，默认 `--background` 参数是开启的，也就是说默认生成的PDF文档中是带有HTML页面的背景图片或背景色的。如果开启 `--no-backgroupd` 参数，则生成的PDF文档中不会有HTML页面中的背景图片和背景色。\n\n\n### -debug-javascript ADN --no-debug-javascript\n\n\n这两个参数用来指定是否在标准输出中输出javascript的调试信息，默认 `--no-debug-javasript` 参数是开启的，也就是说默认不会输出javascript的调试信息。下图是打开 `--debug-javascript` 参数的演示。\n\n\n![](https:////upload-images.jianshu.io/upload_images/3060014-c9a2fdee76719829.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/861/format/webp)\n\n- -debug-javascript\n\n### -disable-external-links AND --enable-external-links\n\n\n这两个参数是用来设置在页面中的外链是否以超链接的形式出现在PDF文档中。关于“外链”的定义请移架 **术语定义** 。默认 `--enable-external-links` 参数被打开，所以默认情况是页面中的外链是以超链接的形式出现的PDF文档中的，点击可以打开指定的网页。\n\n\n### -exclude-from-outline AND --include-in-outline\n\n\n这两个参数用来设置当前页面对象是否包含到目录和大纲中。\n默认情况下 `--include-in-outline` 参数是打开的。也就是说默认情况下生成的PDF文档目录和大纲中是包含当前页面的，如果你不想让当前页面加到目录和大纲中可以打开 `--exclude-from-outline` 参数。\n\n\n### -post AND --post-file\n\n\n当目标页面需要接受POST表单才能正确得到响应时，可以用这两个参数。这两个参数都是可以重复使用的。\n\n\n还有一个应用场景是，用于自动化的WEB应用测试中。可以得到PDF文档作为测试报告。\n\n- `-post-file` 也可以用于自动批量上传文件的场景。\n\n### -run-sript\n\n\n当需要对页面进行一定的预处理后再生成PDF文档的场景，使用该参数再合适不过了。这个参数可以重复使用指定多个需要在页面加载完成后执行的JS代码。你可以在这些JS中对页面的结构和内容进处理，JS执行完成后才会把对应的页面生成PDF文档。\n\n\n### -disable-internal-links AND --enable-internal-links\n\n\n这两个参数是用来设置在页面中的内链是否以超链接的形式出现在PDF文档中。关于“内链”的定义请移架 **术语定义** 。默认 `--enable-internal-links` 参数被打开，所以默认情况是页面中的内链是以超链接的形式出现的PDF文档中的，点击在当前PDF中跳转到指定锚点。\n\n\n### -enable-toc-back-links AND --disable-toc-back-links\n\n\n这组参数用来设置，是否在PDF内容中的H标签处生成超链接。生成的超链接点击后会跳转到目录和大纲中该H标签对应的锚点位置。默认情况下 `--disable-toc-back-links` 参数被打开，不会在PDF文档的H标签处生成超链接。\n\n\n如果你需要在阅读PDF文档的内容时快速回到目录，你可以打开 `--enable-toc-back-links` 参数。\n\n\n### -user-style-sheet\n\n\n这个参数用来加载一个用户自定义的样式表，用来改变HTML页面原有的样式。需要高度自定义页面新式的同学可以尝试使用这个参数达到目的。\n\n\n### 页眉和页脚参数选项\n\n\n```text\n    --footer-center <text>        在页脚的居中部分显示页脚文本 <text>\n    --footer-font-name <name>     设置页脚的字体 (默认为 Arial)\n    --footer-font-size <size>     设置页脚的字体大小 (默认为 12)\n    --footer-html <url>           添加一个html作为页脚\n    --footer-left <text>          在页脚的居左部分显示页脚文本 <text>\n    --footer-line                 在页脚上方显示一条直线分隔正文\n    --no-footer-line              不使用直线分隔页脚与正文(这是默认设置)\n    --footer-right <text>         在页脚的居右部分显示页脚文本 <text>\n    --footer-spacing <real>       页脚与正文之间的距离(默认为零)\n\n\n    --header-center <text>        在页眉的居中部分显示页眉文本 <text>\n    --header-font-name <name>     设置页眉的字体 (默认为 Arial)\n    --header-font-size <size>     设置页眉的字体大小 (默认为 12)\n    --header-html <url>           添加一个html作为页眉\n    --header-left <text>          在页眉的居左部分显示页眉文本 <text>\n    --header-line                 在页眉下方显示一条直线分隔正文\n    --no-header-line              不使用直线分隔页眉与正文(这是默认设置)\n    --header-right <text>         在页眉的居右部分显示页眉文本 <text>\n    --header-spacing <real>       页眉与正文之间的距离(默认为零)\n\n```\n\n\n页眉页脚的设置比较简单，看上述代码段中的解释已经非常明了，所以不再赘述。后面还有针对页眉与页脚的其他相关介绍。\n\n\n### 目录对象参数\n\n\n```text\n    --disable-dotted-lines        在目录中不使用虚线\n    --toc-header-text <text>      设置目录的页眉文本\n    --toc-level-indentation <width> 第级标题在目录中的缩进宽度(默认为1em)\n    --disable-toc-links           在目录中不生成指向内容锚点的超链接\n    --toc-text-size-shrink <real> 在目录中每级标题的缩放比例(默认为0.8)\n    --xsl-style-sheet <file>      使用自定义的 XSL 样式表显示目录内容\n\n```\n\n\n“目录对象”我们一般用不到，上述代码段中的讲解也不难懂，所以不针对每一个具体参数再做详细的讲解。\n\n\n## 关于页面尺寸说明\n\n\n默认的页面尺寸是 A4，你可以使用 `--page-size` 参数指定你想要的页面尺寸，如：A3，Letter 和 Legal等。想要查看本程序支持的所有页面尺寸，请访问 [http://qt-project.org/doc/qt-4.8/qprinter.html#PaperSize-enum](http://qt-project.org/doc/qt-4.8/qprinter.html#PaperSize-enum)\n\n\n你还可以使用 `--page-height` 和 `--page-width` 对页面尺寸进行更精细的控制。\n\n\n## 从标准输入获取参数\n\n\n如果你需要对许多页面进行批量的处理，并且感觉 wkhtmltopdf 开启比较慢，你可以尝试使用 `--read-args-from-stdin` 参数。\n\n\nwkhtmltopdf 命令会为 `--read-args-from-stdin` 参数发送过来的每一行进行一次单独命令调用。也就是说此参数每读取一行都会执行一次 wkhtmltopdf 命令。而最终执行的命令中的参数是命令行中参数与此参数读取的标准输入流中参数的结合。\n\n\n下面的代码段是一个例子:\n\n\n```text\necho \"<http://qt-project.org/doc/qt-4.8/qapplication.html> qapplication.pdf\" >> cmds\necho \"cover google.com <http://en.wikipedia.org/wiki/Qt_(software)> qt.pdf\" >> cmds\nwkhtmltopdf --read-args-from-stdin --book < cmds\n\n```\n\n\n## 指令一个代理\n\n\n默认情况下代理信息将读取环境变量:proxy、all_proxy 和 http_proxy,代理选项还可以通过指定 `-p` 参数开启。\n\n\n使用 BNF 对代理的定义如下：\n\n\n```text\n<type> := \"http://\" | \"socks5://\"\n<serif> := <username> (\":\" <password>)? \"@\"\n<proxy> := \"None\" | <type>? <serif>? <host> (\":\" <port>)?\n\n```\n\n\n如果你不熟悉 BNF 的话，下面的代码段中是三个例子:\n\n\n```text\n<http://user:password@myproxyserver:8080>\nsocks5://myproxyserver\nNone\n\n```\n\n\n## 页眉和页脚\n\n\n页眉和页脚可以使用参数 `--header-*` 和 `--footer-*` 添加到文件中。有些参数(如 `--footer-left`)需要提供一个字符串`text`作为参数值。你可以在 `text`中插入下述变量，他们将会被替换成对应的值。\n\n\n```text\n[page]       当前正在被输出页面的页码\n[frompage]   第一页在文档中的页码\n[topage]     最后一面在文档中的页码\n[webpage]    当前正在被输出页面的URL\n[section]    当前正在被输出的章节的名字\n[subsection] 当前正在被输出的小节的名字\n[date]       本地系统格式的当前日期\n[isodate]    ISO 8601 格式的当前日期\n[time]       本地系统格式的当前时间\n[title]      当前对象的标题\n[doctitle]   输出文档的标题\n[sitepage]   当前正在处理的对象中当前页面的页码\n[sitepages]  当前正在处理的对象中的总页数\n\n```\n\n\n举一个例子来说明吧，`--header-right \"Page [page] of [toPage]\"`, 会在页面的右上角生成一个类似 `Page x of y` 的字符串，其中 `x` 是当前页面的页码， `y` 是当前文档最后一页的页码。\n\n\n页眉和页脚也可以通过 HTML文档来提供。 同样举一个例子，使用命令行参数 `--header-html header.html` 来生成页眉，而 header.html 的内容如下:\n\n\n```text\n<html><head><script>\nfunction subst() {\n  var vars={};\n  var x=window.location.search.substring(1).split('&');\n  for (var i in x) {var z=x[i].split('=',2);vars[z[0]] = unescape(z[1]);}\n  var x=['frompage','topage','page','webpage','section','subsection','subsubsection'];\n  for (var i in x) {\n    var y = document.getElementsByClassName(x[i]);\n    for (var j=0; j<y.length; ++j) y[j].textContent = vars[x[i]];\n  }\n}\n</script></head><body style=\"border:0; margin: 0;\" onload=\"subst()\">\n<table style=\"border-bottom: 1px solid black; width: 100%\">\n  <tr>\n    <td class=\"section\"></td>\n    <td style=\"text-align:right\">\n      Page <span class=\"page\"></span> of <span class=\"topage\"></span>\n    </td>\n  </tr>\n</table>\n</body></html>\n\n```\n\n\n## 大纲(Outlines)\n\n\nwkhtmltopdf 可以使用 `--outline` 命令行参数来指定在PDF就要中输出像书本中目录一样的“大纲”，“大纲”是基本HTML文档中H标签生成的，具体的大纲的层级和尝试请移步 **目录**\n\n\n如果HTML文档中的H标签等级比较多，就可以生成深层级树形结构的“大纲”，而生成“大纲”的真实深度是通过 `--outline-depth` 参数来控制。\n\n\n## 目录\n\n\n通过在命令行中添加 TOC对象 可以把一个目录添加到生成的PDF文档中，例如下面的代码段：\n\n\n```text\nwkhtmltopdf toc <http://qt-project.org/doc/qt-4.8/qstring.html> qstring.pdf\n\n```\n\n\n生成的目录也是基于HTML文档的H标签。过程是首先生成一个XML文档,然后使用XSLT转换为HTML。\n生成的 XML 文档可以通过 `--dump-outline` 参数查看。\n\n\n```text\nwkhtmltopdf --dump-outline toc.xml <http://qt-project.org/doc/qt-4.8/qstring.html> qstring.pdf\n\n```\n\n\n你如果想要使用自定义的XSLT文档可以通过 `--xsl-style-sheet` 参数指定\n\n\n```text\nwkhtmltopdf toc --xsl-style-sheet my.xsl <http://qt-project.org/doc/qt-4.8/qstring.html> qstring.pdf\n\n```\n\n\n你可以使用 `--dump-default-toc-xsl` 参数把默认的 XSLT 文档打印到标准输出，然后基于它创建你的自定义 XSLT 文档。\n\n\n```text\nwkhtmltopdf --dump-default-toc-xsl\n\n```\n\n",
      "properties": {
        "password": "",
        "icon": "",
        "date": "2022-03-15",
        "type": "Post",
        "category": "技术分享",
        "urlname": "39",
        "catalog": [
          "archives"
        ],
        "tags": [
          "图像处理"
        ],
        "summary": "wkhtmltopdf精讲(原文) 作者：JSON_NULL术语定义文档对象“文档对象”是指PDF文档中的文档对象，共有三种类型的“文档对象”，他们分别是“页面对象”，“封面对象”和“目录对象”。页面",
        "sort": "",
        "title": "wkhtmltopdf详细使用",
        "status": "Published",
        "updated": "2023-10-08 14:42:00"
      },
      "catalog": [
        {
          "title": "archives",
          "doc_id": "07c3aa88-08b7-4093-9ac6-0a495a187163"
        }
      ],
      "body": "",
      "realName": "wkhtmltopdf详细使用",
      "relativePath": "/archives/wkhtmltopdf详细使用.md"
    },
    {
      "id": "a60e468f-c55d-4add-a043-970fc03927d0",
      "doc_id": "a60e468f-c55d-4add-a043-970fc03927d0",
      "title": "a60e468f-c55d-4add-a043-970fc03927d0",
      "updated": 1696747320000,
      "body_original": "\n# wkhtmltopdf\n\n\n> “wkhtmltopdf\"，是一个能够把网页/文件转换成PDF的工具。工具全名叫 \"wkhtmltopdf\" ;  是一个使用 Qt WebKit 引擎做渲染的，能够把html 文档转换成 pdf 文档 或 图片(image) 的**“命令行工具”**。\n\n\n\t支持多个平台，可在win，linux，os x 等系统下运行。\n\n\n## 安装\n\n\n        通过文章底部的下载连接选择适合的版本**注意系统版本** 此处按照liunx版本来讲解\n\n\n       liunx版本的安装相对比较简单直接解压即可 解压后目录为use，重命名为wkhtmltopdf（强迫症）。 目录结构\n\n\n`/wkhtmltopdf/local/bin/`\n\n\n![](https://blog-file.hehouhui.cn/202203152219619.png)\n\n- wkhtmltopdf 把一个文件或网页转成pdf\n- wkhtmltoimage 把一个文件或网页转成图片\n\n> 妥妥的见名知意\n\n\n## 使用\n\n\n### 把一个html文件转换成PDF\n\n\n命令格式 ：`wkhtmltopdf xxx.html xxx.pdf`\n\n\n```text\nwkhtmltopdf 1.html 1.pdf\nLoading pages (1/6)\nCounting pages (2/6)\nResolving links (4/6)\nLoading headers and footers (5/6)\nPrinting pages (6/6)\nDone\n\n```\n\n\n当你看到类似上面的内容时，说转换已经完成了，去打开转换好的pdf文档慢慢研究吧。\n\n\n### 把一个 url 指向的网页转换成PDF\n\n\n命令格式 ： `wkhtmltopdf url xxx.pdf`\n\n\n```text\nwkhtmltopdf www.yioks.com yioks.pdf\nLoading pages (1/6)\nQFont::setPixelSize: Pixel size <= 0 (0)                     ] 55%\nCounting pages (2/6)\nQFont::setPixelSize: Pixel size <= 0 (0)=====================] Object 1 of 1\nResolving links (4/6)\nLoading headers and footers (5/6)\nPrinting pages (6/6)\nDone\n\n```\n\n\n当你看到如上信息时代表转换成功，是不是很酷。\n\n\n### 把html文件 和 url指向的网页 转换成图片\n\n\n命令格式 ：\n\n\n```text\nwkhtmltoimage xxx.html xxx.jpg\nwkhtmltoimage url xxx.jpg\n\n```\n\n\n其实和转pdf时的参数是一样的，只是命令和输出文件的扩展名变了。上的命令格式中，我是把图片保存成了 jpg 格式，当然，如果你愿意也可以保存成其他图片格式(如：png)，但文件可能会变大很多倍。在我的测试中，jpg格式文件是最小的。\n\n\n## 使用过程中的问题\n\n- liunx环境下中文乱码\n- 部分liunx服务器缺少依赖lib\n- *中文乱码 **\n\nliunx中缺少中文（宋体）字体文件，\n\n\n([下载字体文件](https://blog-file.hehouhui.cn/202203152226460.zip))\n\n\n放置服务器中 `/usr/share/fonts`目录下\n\n\n![](https://blog-file.hehouhui.cn/202203152228291.png)\n\n\n**缺少lib**\n\n\nlibjpeg.so.62 的解决方案\n\n\n![](https://blog-file.hehouhui.cn/202203152229619.png)\n\n\n执行如下命令下载依赖\n\n\n`yum install fontconfig freetype libpng libjpeg libX11 libXext libXrender xorg-x11-fonts-Type1 xorg-x11-fonts-75dpi`\n\n\n## 其他的html转pdf方案\n\n- itextpdf.html2pdf 可转换样式简单的html使用简单速度快，对CSS样式支持不是很好。在contract中的实现为 **PdfGeneratorIText**\n- lowagie.itext 可转换较复杂的html，但对格式极其严苛。速度快\n\n> 也有其他几款转换工具但都有或多或少的缺陷，wkhtmltopdf 虽说慢至少样式还行\n\n\n## 下载链接\n\n\n[【官网】](http://wkhtmltopdf.org/):[【](http://wkhtmltopdf.org/)[http://wkhtmltopdf.org/】](http://wkhtmltopdf.org/%E3%80%91)\n\n\n[【下载列表】](http://wkhtmltopdf.org/downloads.html)\n\n\n([详细使用](https://blog.hehouhui.cn/archives/39))\n\n",
      "properties": {
        "password": "",
        "icon": "",
        "date": "2022-03-15",
        "type": "Post",
        "category": "技术分享",
        "urlname": "40",
        "catalog": [
          "archives"
        ],
        "tags": [
          "图像处理"
        ],
        "summary": "“wkhtmltopdf\"，是一个能够把网页/文件转换成PDF的工具。工具全名叫 \"wkhtmltopdf\" ; 是一个使用 Qt WebKit 引擎做渲染的，能够把html 文档转换成 pdf 文档 或 图片(image) 的**“命令行工具”**。\n支持多个平台，可在win，linux，os x 等系统下运行。",
        "sort": "",
        "title": "wkhtmltopdf 安装",
        "status": "Published",
        "updated": "2023-10-08 14:42:00"
      },
      "catalog": [
        {
          "title": "archives",
          "doc_id": "a60e468f-c55d-4add-a043-970fc03927d0"
        }
      ],
      "body": "",
      "realName": "wkhtmltopdf 安装",
      "relativePath": "/archives/wkhtmltopdf 安装.md"
    },
    {
      "id": "6a6aca31-1871-49f3-a600-5062cb673260",
      "doc_id": "6a6aca31-1871-49f3-a600-5062cb673260",
      "title": "6a6aca31-1871-49f3-a600-5062cb673260",
      "updated": 1696747320000,
      "body_original": "\n# \n\n\nSILK v3编码是Skype向第三方开发人员和硬件制造商提供免版税认证(RF)的Silk宽带音频编码器，Skype后来将其开源。具体可见Wikipedia。\n\n\n> 之前一直使用ffmpeg来进行格式转换，但是将微信的amr转为mp3后语音质量不理想（也可能是我参数没有调正确🤪）。  \n> 于是就继续想解决办法，后来在github瞎逛时看到可以使用silk-v3-decoder来做这件事情。虽然本质上还是使用的ffmpeg来转的，只是封装了一下。\n\n\n([前往silk-v3-decoder](https://github.com/kn007/silk-v3-decoder))\n\n\n++环境要求gcc和ffmpeg，所以还是得要安装ffmpeg，gcc是拿来编译silk-v3-decoder源码，ffmpeg是拿来转换格式的。++\n\n\n## 安装gcc\n\n\n```text\nyum -y install gcc\nyum -y install gcc-c++\n\n```\n\n\n## ffmpeg 安装\n\n\n打开官网地址，进入下载页：[https://ffmpeg.org/download.html#build-linux](https://ffmpeg.org/download.html#build-linux)\n\n\n![](https://blog-file.hehouhui.cn/202203172321911.png)\n\n\n选择Linux Static Builds下的构建选项，进入详情页\n\n\n![](https://blog-file.hehouhui.cn/202203172323866.png)\n\n\n在列表中选择适合自己的版本，鼠标右键，复制链接地址\n\n\n```text\n# 下载文件\nwget <https://johnvansickle.com/ffmpeg/builds/ffmpeg-git-amd64-static.tar.xz>\n\n# 解压\nxz -d ffmpeg-git-amd64-static.tar.xz\n\n# 再次解压\ntar -xvf ffmpeg-git-amd64-static.tar\n\n\n```\n\n\n得到目录\n\n\n![](https://blog-file.hehouhui.cn/202203172329558.png)\n\n\nffmpeg 和 ffprobe 都在这里\n\n\n> 如果想要ffmpeg命令全局可用，可以在bin目录加个链接。比如，分别执行如下命令，即可在:/usr/bin目录下创建ffmpeg和ffprobe软链接。\n\n\n```text\ncd /usr/bin\nln -s 解压目录/ffmpeg ffmpeg\nln -s 解压目录/ffprobe ffprobe\n\n```\n\n\n## 下载silk-v3-decoder源码\n\n\n```text\n  <https://ghproxy.com/https://github.com/kn007/silk-v3-decoder/archive/refs/heads/master.zip>\n\n```\n\n\n![](https://blog-file.hehouhui.cn/202203172333398.png)\n\n\n给脚步赋执行权限\n\n\n```text\n   chmod +x converter.sh\n   chmod +x converter_beta.sh\n\n```\n\n\n## 使用\n\n\n```text\n   silk-v3-decoder目录/converter.sh silk音频文件路径 mp3\n\n```\n\n\n> 第一个为执行脚本 第二个为silk音频如amr文件路径  第三个为需要转换为的音频格式\n\n",
      "properties": {
        "password": "",
        "icon": "",
        "date": "2022-03-17",
        "type": "Post",
        "category": "技术分享",
        "urlname": "41",
        "catalog": [
          "archives"
        ],
        "tags": [
          "Python",
          "健康"
        ],
        "summary": "之前一直使用ffmpeg来进行格式转换，但是将微信的amr转为mp3后语音质量不理想（也可能是我参数没有调正确🤪）。\n于是就继续想解决办法，后来在github瞎逛时看到可以使用silk-v3-decoder来做这件事情。虽然本质上还是使用的ffmpeg来转的，只是封装了一下。",
        "sort": "",
        "title": "silk-v3-decoder 一款微信音频转码的工具",
        "status": "Published",
        "updated": "2023-10-08 14:42:00"
      },
      "catalog": [
        {
          "title": "archives",
          "doc_id": "6a6aca31-1871-49f3-a600-5062cb673260"
        }
      ],
      "body": "",
      "realName": "silk-v3-decoder 一款微信音频转码的工具",
      "relativePath": "/archives/silk-v3-decoder 一款微信音频转码的工具.md"
    },
    {
      "id": "0d2fc9f8-00d3-40d8-b7bb-cc4dbbb10251",
      "doc_id": "0d2fc9f8-00d3-40d8-b7bb-cc4dbbb10251",
      "title": "0d2fc9f8-00d3-40d8-b7bb-cc4dbbb10251",
      "updated": 1696747320000,
      "body_original": "\n# Gateway\n\n\n## **什么是服务网关**\n\n\n`API Gateway`，顾名思义，是出现在系统边界上的一个面向 API 的、串行集中式的强管控服务，这里的边界是系统服务的边界，可以理解为`服务防火墙`，主要起到`隔离外部访问与内部系统的作用`。在微服务概念的流行之前，API 网关就已经诞生了，例如银行、证券等领域常见的前置机系统，它也是解决访问认证、报文转换、访问统计等问题的。\n\n\nAPI 网关的流行，源于近几年来移动应用与企业间互联需求的兴起。移动应用、企业互联，使得后台服务支持的对象，从以前单一的Web应用，扩展到多种使用场景，且每种使用场景对后台服务的要求都不尽相同。这不仅增加了后台服务的响应量，还增加了后台服务的复杂性。`随着微服务架构概念的提出，API网关成为了微服务架构的一个标配组件`。\n\n\nAPI 网关是一个服务器，是系统对外的唯一入口。API 网关封装了系统内部架构，为每个客户端提供定制的 API。所有的客户端和消费端都通过统一的网关接入微服务，在网关层处理所有非业务功能。API 网关并不是微服务场景中必须的组件，如下图，不管有没有 API 网关，后端微服务都可以通过 API 很好地支持客户端的访问。\n\n\n![](https://blog-file.hehouhui.cn/1280X1280.png)\n\n\nAPI 网关出现的原因是微服务架构的出现，不同的微服务一般会有不同的网络地址，而外部客户端可能需要调用多个服务的接口才能完成一个业务需求，如果让客户端直接与各个微服务通信，会有以下的问题：\n\n- 客户端会多次请求不同的微服务，增加了客户端的复杂性。\n- 存在跨域请求，在一定场景下处理相对复杂。\n- 认证复杂，每个服务都需要独立认证。\n- 难以重构，随着项目的迭代，可能需要重新划分微服务。例如，可能将多个服务合并成一个或者将一个服务拆分成多个。如果客户端直接与微服务通信，那么重构将会很难实施。\n- 某些微服务可能使用了防火墙 / 浏览器不友好的协议，直接访问会有一定的困难。\n\n**API网关方式的核心要点是，所有的客户端和消费端都通过统一的网关接入微服务，** 在网关层处理所有的非业务功能。通常，网关也是提供REST/HTTP的访问API。\n\n\n## **网关解决了什么问题**\n\n\n但对于服务数量众多、复杂度比较高、规模比较大的业务来说，引入 API 网关也有一系列的好处：\n\n- 聚合接口使得服务对调用者透明，客户端与后端的耦合度降低\n- 聚合后台服务，节省流量，提高性能，提升用户体验\n- 提供安全、流控、过滤、缓存、计费、监控等 API 管理功能\n\n![](https://blog-file.hehouhui.cn/clip_image003.png)\n\n\n网关应当具备以下功能\n\n- 性能：API 高可用，负载均衡，容错机制。\n- 安全：权限身份认证、脱敏，流量清洗分发，后端签名（保证全链路可信调用），黑名单（非法调用的限制）。\n- 日志：日志记录，一旦涉及分布式，全链路跟踪必不可少。\n- 缓存：数据缓存。\n- 监控：记录请求响应数据，API 耗时分析，性能监控。\n- 限流：流量控制，错峰流控，可以定义多种限流规则。\n- 灰度：线上灰度部署，可以减小风险。\n- 路由：动态路由规则。\n\n主要功能\n\n- 数据平面主要功能是接入用户的HTTP请求和微服务被拆分后的聚合。使用微服务网关统一对外暴露后端服务的API和契约，路由和过滤功能正是网关的核心能力模块。另外，微服务网关可以实现拦截机制和专注跨横切面的功能，包括协议转换、安全认证、熔断限流、灰度发布、日志管理、流量监控等。\n- 控制平面主要功能是对后端服务做统一的管控和配置管理。例如，可以控制网关的弹性伸缩；可以统一下发配置；可以对网关服务添加标签；\n\n# **Spring Cloud Gateway**\n\n\n> Spring Cloud Gateway 是基于 Spring 生态系统之上构建的 API 网关，包括：Spring 5，Spring Boot 2 和 Project Reactor。Spring Cloud Gateway 旨在提供一种简单而有效的方法来路由到 API，并为它们提供跨领域的关注点，例如：安全性，监视/指标，限流等。\n\n\n\t由于 Spring 5.0 支持 Netty，Http2，而 Spring Boot 2.0 支持 Spring 5.0，因此 Spring Cloud Gateway 天然支持 Netty 和 Http2。\n\n\nSpring Cloud Gateway 建立在[Spring Boot 2.x](https://spring.io/projects/spring-boot#learn)、[Spring WebFlux](https://docs.spring.io/spring/docs/current/spring-framework-reference/web-reactive.html)和[Project Reactor](https://projectreactor.io/docs)之上（基于⾼性能的Reactor模式响应式通信框架Netty，异步⾮阻塞模型）。因此当使用 Spring Cloud Gateway 时，许多熟悉的同步库（例如 Spring Data 和 Spring Security）和模式可能并不适用。\n\n\nSpring Cloud Gateway 需要 Spring Boot 和 `Spring Webflux` 提供的 `Netty` 运行时。它不适用于传统的 Servlet 容器或构建为 WAR 时。\n\n\n## **核心概念**\n\n- **Route:** The basic building block of the gateway. It is defined by an ID, a destination URI, a collection of predicates, and a collection of filters. A route is matched if the aggregate predicate is true.\n\t- 路由是构建网关的基本模块，它由ID，目标URI，一系列的断言和过滤器组成，如果断言为true则匹配该路由\n- **Predicate**: This is a [Java 8 Function Predicate](https://docs.oracle.com/javase/8/docs/api/java/util/function/Predicate.html). The input type is a [Spring Framework ServerWebExchange](https://docs.spring.io/spring/docs/5.0.x/javadoc-api/org/springframework/web/server/ServerWebExchange.html). This lets you match on anything from the HTTP request, such as headers or parameters.\n\t- 参考的是java8的java.util.function.Predicate开发人员可以匹配HTTP请求中的所有内容（例如请求头或请求参数），如果请求与断言相匹配则进行路由\n- **Filter**: These are instances of `GatewayFilter` that have been constructed with a specific factory. Here, you can modify requests and responses before or after sending the downstream request.\n\t- 指的是Spring框架中GatewayFilter的实例，使用过滤器，可以在请求被路由前或者之后对请求进行修改。\n\n工作流程\n\n\n![](https://blog-file.hehouhui.cn/clip_image005.png)\n\n\n如上图所示，客户端向 `Spring Cloud Gateway` 发出请求。再由网关处理程序 `Gateway Handler Mapping` 映射确定与请求相匹配的路由，将其发送到网关 Web 处理程序 `Gateway Web Handler`。该处理程序通过指定的过滤器链将请求发送到我们实际的服务执行业务逻辑，然后返回。过滤器由虚线分隔的原因是，过滤器可以在发送代理请求之前和之后运行逻辑。所有 `pre` 过滤器逻辑均被执行。然后发出代理请求。发出代理请求后，将运行 `post` 过滤器逻辑。\n\n\n## **Predicate 断言**\n\n\nSpringCloud Gateway 包括许多内置的Route Predicate工厂。所有这些Predicate都与HTTP请求的不同属性匹配。多个Route Predicate工厂可以进行组合。\n\n\nSpringCloud Gateway 创建Route对象时，使用RoutePredicateFactory创建Predicate对象，Predicate对象可以赋值给Route。SpringCloud Gateway 包含许多内置的RoutePredicateFactories。\n\n\n所有这些断言都匹配HTTP请求的不同属性。多种断言工厂可以组合，并通过逻辑and。\n\n\n> SpringCloud Gateway 将路由匹配作为Spring WebFlux HandlerMapping基础架构的一部分。\n\n\n```text\nserver:\n  port: 11001\nspring:\n  application:\n    name: gateway\n  cloud:\n    gateway:\n      discovery:\n        locator:\n          enabled: true  #开启从注册中心动态创建路由的功能，利用微服务名进行路由\n      routes:\n        - id: blog-admin #路由的ID，没有固定规则但要求唯一，建议配合服务名\n          #uri: <http://localhost:8001>   #匹配后提供服务的路由地址\n          uri: lb://blog-service\n          predicates:\n            - Path=/api/admin/**   #断言,路径相匹配的进行路由\n   - id: blog\n      #uri: <http://localhost:8001>   #匹配后提供服务的路由地址\n      uri: lb://blog-service\n      predicates:\n        - Path=/archives/**   #断言,路径相匹配的进行路由\n        #- After=2020-03-08T10:59:34.102+08:00[Asia/Shanghai]\n        #- Cookie=username,zhangshuai #并且Cookie是username=zhangshuai才能访问\n        #- Header=X-Request-Id, \\\\d+ #请求头中要有X-Request-Id属性并且值为整数的正则表达式\n        #- Host=**.atguigu.com\n        #- Method=GET\n        #- Query=username, \\\\d+ #要有参数名称并且是正整数才能路由\n\n```\n\n\n### **内置谓词断言**\n\n- **After Route Predicate Factory** 匹配在指定日期时间之后发生的请求\n\t- 参数：datetime\n\t- 注意时间格式带时区 ZonedDateTime\n\t- `spring: cloud: gateway: routes: - id: after_route_test uri: <https://www.hehouhui.cn> predicates: - After=2022-07-10T17:42:00.789-07:00[Asia/Shanghai]`\n- **Before Route Predicate Factory** 匹配在指定日期时间之前发生的请求\n\n\t```text\n\t参数：datetime\n\t\n\t```\n\n\n注意时间格式带时区 ZonedDateTime\n\n\n```text\n spring:\n  cloud:\n    gateway:\n      routes:\n      - id: before_route_test\n        uri: <https://www.hehouhui.cn>\n        predicates:\n        - Before=2022-07-10T17:43:48.789-07:00[Asia/Shanghai]\n\n```\n\n- **Between Route Predicate Factory** 匹配在指定范围日期时间发生的请求\n\t- 参数：datetime1 开始\n\t- datetime2 结束\n\t- 注意时间格式带时区 ZonedDateTime\n\t- `spring: cloud: gateway: routes: - id: between_route_test uri: <https://www.hehouhui.cn> predicates: - Between=2022-07-10T17:42:30.789-07:00[Asia/Shanghai],2022-07-10T17:42:59.789-07:00[Asia/Shanghai]`\n- **Cookie Route Predicate Factory** 匹配在指定包含指定cookie 值的请求\n\n参数：name cookie名称\n\n- regexp 匹配值的正则\n- `spring: cloud: gateway: routes: - id: cookie_route_test uri: <https://www.hehouhui.cn> predicates: - Cookie=token,allow_account`\n- **Header Route Predicate Factory** 匹配在指定包含指定请求头的请求\n\n参数：name 请求头的名称\n\n\n```text\n               regexp 匹配值的正则\n\n```\n\n\n```text\n spring:\n  cloud:\n    gateway:\n      routes:\n      - id: handler_route_test\n        uri: <https://www.hehouhui.cn>\n        predicates:\n        - Handler=User-Agent,*Mac OS*\n\n```\n\n- **Host Route Predicate Factory** 请求主机地址匹配\n\n\t```text\n\t参数： patterns 正则集合\n\t\n\t```\n\n\n```text\n spring:\n  cloud:\n    gateway:\n      routes:\n      - id: host_route_test\n        uri: <https://www.hehouhui.cn>\n        predicates:\n        - Host=blog.hehouhui.cn\n\n```\n\n- **Method Route Predicate Factory** 请求方法的匹配\n\n\t```text\n\t参数： methods 集合\n\t\n\t```\n\n\n```text\n spring:\n  cloud:\n    gateway:\n      routes:\n      - id: method_route_test\n        uri: <https://www.hehouhui.cn>\n        predicates:\n        - Method=Get,Post\n\n```\n\n- **Path Route Predicate Factory** 请求路由规则匹配\n\n\t```text\n\t参数：patterns 匹配集合\n\t\n\t```\n\n\n```text\nspring:\n  cloud:\n    gateway:\n      routes:\n      - id: path_route_test\n        uri: <https://www.hehouhui.cn>\n        predicates:\n        - Path=/test/**\n\n```\n\n- **Query Route Predicate Factory** 请求参数 url ？后的参数\n\n\t```text\n\t参数  param 参数名\n\t\n\t          regexp 参数匹配值，如果为空的情况下表示任何值\n\t\n\t```\n\n\n```text\nspring:\n  cloud:\n    gateway:\n      routes:\n      - id: query_route_test\n        uri: <https://www.hehouhui.cn>\n        predicates:\n        - Query=name,laiweihua\n\n```\n\n- **RemoteAddr Route Predicate Factory** 客户端请求IP匹配\n\n\t```text\n\t参数：sources 注意此处为cidr格式 CIDR 表示法（IPv4 或 IPv6）字符串\n\t\n\t```\n\n\n```text\nspring:\n  cloud:\n    gateway:\n      routes:\n      - id: remoteAddr_route_test\n        uri: <https://www.hehouhui.cn>\n        predicates:\n        - RemoteAddr=192.168.1.1/24\n\n```\n\n\n默认情况下，RemoteAddr 路由谓词工厂使用来自传入请求的远程地址。如果 Spring Cloud Gateway 位于代理层后面，这可能与实际客户端 IP 地址不匹配。\n\n\n可以通过设置自定义来自定义解析远程地址的方式`RemoteAddressResolver`。Spring Cloud Gateway 带有一个基于[X-Forwarded-For 标头](https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/X-Forwarded-For)的非默认远程地址解析器，`XForwardedRemoteAddressResolver`.\n\n\n`XForwardedRemoteAddressResolver`有两个静态构造方法，它们采用不同的安全方法：\n\n- `XForwardedRemoteAddressResolver::trustAll`返回`RemoteAddressResolver`始终采用在`X-Forwarded-For`标头中找到的第一个 IP 地址的 a。这种方法容易受到欺骗，因为恶意客户端可以为 设置初始值，`X-Forwarded-For`解析器会接受该值。\n- `XForwardedRemoteAddressResolver::maxTrustedIndex`采用与 Spring Cloud Gateway 前运行的受信任基础架构数量相关的索引。例如，如果 Spring Cloud Gateway 只能通过 HAProxy 访问，则应使用值 1。如果在访问 Spring Cloud Gateway 之前需要两跳可信基础架构，则应使用值 2。\n- **Weight Route Predicate Factory** 权重路由匹配\n\n\t参数： group 自定义分组名称\n\n\n\t```text\n\t        weight 权重\n\t\n\t```\n\n\n```text\nspring:\n  cloud:\n    gateway:\n      routes:\n      - id: weight_high\n        uri: <https://www.hehouhui.cn>\n        predicates:\n        - Weight=group, 8\n      - id: weight_low\n        uri: <https://blog.hehouhui.cn>\n        predicates:\n        - Weight=group, 2\n\n```\n\n\n```text\n  该路由会将约 80% 的流量转发到 <https://www.hehouhui.cn>，将约 20% 的流量转发到https://blog.hehouhui.cn\n\n```\n\n- **XForwarded Remote Addr Route Predicate Factory** 根据 X-Forwarded-For请求头匹配\n\n\t参数：sources  注意此处为cidr格式\n\n\n```text\nspring:\n  cloud:\n    gateway:\n      routes:\n      - id: xforwarded_route\n        uri: <https://www.hehouhui.cn>\n        predicates:\n        - XForwardedRemoteAddr=192.168.1.1/24\n\n```\n\n\n## **Filter过滤器**\n\n\n### **GlobaFilter 全局过滤器**\n\n\n> GlobaFilter 全局过滤器，不需要在配置文件中配置，系统初始化时加载，并作用在每个路由上。 SpringCloud Gateway 核心的功能也是通过内置的全局过滤器来完成\n\n\n| 类名                               | 作用                                                                                                       |\n| -------------------------------- | -------------------------------------------------------------------------------------------------------- |\n| CustomGlobalFilter               | 组合所有过滤器的排序,包含GlobaFilter和GatewayFilter。Gateway 区分过滤器逻辑执行的“前”和“后”阶段，具有最高优先级的过滤器是“前”阶段的第一个和“后”阶段的最后一个 -阶段。 |\n| ForwardRoutingFilter             | forward://开头的url协议将会被转发                                                                                  |\n| ReactiveLoadBalancerClientFilter | 将以lb://开头的url 经过 ReactorLoadBalancer 路由加载转换                                                              |\n| NettyRoutingFilter               | http或者https协议 由Netty 过滤器运行。它使用 NettyHttpClient发出下游代理请求                                                   |\n| NettyWriteResponseFilter         | 并将代理响应写回网关客户端响应                                                                                          |\n| RouteToRequestUrlFilter          | 将 Route对象转为URL,并存储原Route信息                                                                               |\n| WebsocketRoutingFilter           | 将以ws,wss或请求头Upgrade = WebSocket的http,https 以We bsocket协议执行                                               |\n| GatewayMetricsFilter             | 路由指标数据收集用于集成Grafana仪表板                                                                                   |\n|                                  |                                                                                                          |\n\n\n### **AbstractGatewayFilterFactory 局部过滤器工厂**\n\n\n> GatewayFilter局部过滤器，是针对单个路由的过滤器。 在SpringCloud Gateway组件中提供了大量内置的局部过滤器，对请求和响应坐过滤操作。\n\n\n> 遵循约定大于配置的思想，只需要在配置文件配置局部过滤器名称，并为其制定对应的值。就可以让其生效。\n\n\n| 名称                            | 作用                                                        | 参数                                                                                                                                                                            |\n| ----------------------------- | --------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| AddRequestHeader              | 为路由添加请求头                                                  | name,value                                                                                                                                                                    |\n| AddRequestParameter           | 为路由添加请求query参数(拼接在url后)                                   | name,value                                                                                                                                                                    |\n| AddResponseHeader             | 为路由添加响应头                                                  | name,value                                                                                                                                                                    |\n| DedupeResponseHeader          | 响应头去重,默认取响应头的首个value，最主要是用来处理自2.2.5版本后网关和下游服务都处理了CORS的bug | name,strategy                                                                                                                                                                 |\n| CircuitBreaker                | 路由断路器，在服务异常或者返回指定状态码时转发路由至指定 fallbackUri                  | name,fallbackUri,routeId,statusCodes                                                                                                                                          |\n| FallbackHeaders               | 在执行请求转发时添加指定的请求头信息                                        |                                                                                                                                                                               |\n| MapRequestHeader              | copy请求头中指定值到新的请求头中                                        | fromHeader,toHeader                                                                                                                                                           |\n| PrefixPath                    | 为路由URL添加统一前缀                                              | prefix                                                                                                                                                                        |\n| PreserveHostHeader            | 无特殊逻辑，只是标记一下路由为原始请求                                       |                                                                                                                                                                               |\n| RequestRateLimiter            | 路由限流，由KeyResolver解析出路由细粒度，RateLimiter进行限流处理               | keyResolver, **rate-limiter.replenish-rate ,rate-limiter.burst-capacity,rate-limiter.requested-tokens**。requestedTokens每次请求消耗多少令牌，burstCapacity令牌桶总容量，replenishRate每多少秒生产一枚令牌 |\n| RedirectTo                    | 请求转发                                                      |                                                                                                                                                                               |\n| RemoveRequestHeader           | 删除请求头                                                     |                                                                                                                                                                               |\n| RemoveResponseHeader          | 删除响应头                                                     |                                                                                                                                                                               |\n| RemoveRequestParameter        | 删除请求参数                                                    |                                                                                                                                                                               |\n| RequestHeaderSize             | 累计请求头value bytes大小不超过最大值                                  |                                                                                                                                                                               |\n| RewritePath                   | 重写Path                                                    |                                                                                                                                                                               |\n| RewriteLocationResponseHeader | 修改Location的值                                              |                                                                                                                                                                               |\n| RewriteResponseHeader         | 重写响应头值                                                    |                                                                                                                                                                               |\n| SaveSession                   | 保存session                                                 |                                                                                                                                                                               |\n| SecureHeaders                 | 安全生命请求头                                                   |                                                                                                                                                                               |\n| SetPath                       | 设置新Path                                                   |                                                                                                                                                                               |\n| SetRequestHeader              | 设置请求头                                                     |                                                                                                                                                                               |\n| SetResponseHeader             | 设置响应头                                                     |                                                                                                                                                                               |\n| SetStatus                     | 设置状态码                                                     |                                                                                                                                                                               |\n| StripPrefix                   | 路由摘除以/分割                                                  | parts                                                                                                                                                                         |\n| Retry                         | 路由重试                                                      | retries：重试次数,statuses: 判断可重试的状态吗默认5xx,methods: 支持重试的请求方法,exceptions: 重试的抛出异常列表                                                                                                |\n| RequestSize                   | 通过Content-length判断是否超过最大请求大小                              |                                                                                                                                                                               |\n| SetRequestHostHeader          | 清除原请求头Host，并添加新值                                          |                                                                                                                                                                               |\n| ModifyRequestBody             | 修改请求体                                                     |                                                                                                                                                                               |\n| ModifyResponseBody            | 修改响应体                                                     |                                                                                                                                                                               |\n| TokenRelay                    | OAuth2标准认证服务，执行授权认证后并将信息传递下游服务                            |                                                                                                                                                                               |\n| CacheRequestBody              | 缓存请求体                                                     |                                                                                                                                                                               |\n\n\n### RouteLocator\n\n\n    路由配置加载\n\n\n### ConfigurationService\n\n\n路由&谓词&过滤器工程等动态配置等实现\n\n",
      "properties": {
        "password": "",
        "icon": "",
        "date": "2022-03-05",
        "type": "Post",
        "category": "技术分享",
        "urlname": "46",
        "catalog": [
          "archives"
        ],
        "tags": [
          "Spring",
          "微服务",
          "分布式"
        ],
        "summary": "API 网关是一个服务器，是系统对外的唯一入口。API 网关封装了系统内部架构，为每个客户端提供定制的 API。所有的客户端和消费端都通过统一的网关接入微服务，在网关层处理所有非业务功能。API 网关并不是微服务场景中必须的组件，如下图，不管有没有 API 网关，后端微服务都可以通过 API 很好地支持客户端的访问",
        "sort": "",
        "title": "API网关之Gateway",
        "status": "Published",
        "updated": "2023-10-08 14:42:00"
      },
      "catalog": [
        {
          "title": "archives",
          "doc_id": "0d2fc9f8-00d3-40d8-b7bb-cc4dbbb10251"
        }
      ],
      "body": "",
      "realName": "API网关之Gateway",
      "relativePath": "/archives/API网关之Gateway.md"
    },
    {
      "id": "397d5281-9f11-4c6f-8430-399176cc403d",
      "doc_id": "397d5281-9f11-4c6f-8430-399176cc403d",
      "title": "397d5281-9f11-4c6f-8430-399176cc403d",
      "updated": 1696747320000,
      "body_original": "\n# Mysql数据库\n\n\n## 存储引擎\n\n\n### MyISAM引擎\n\n\n    不支持事务\n\n\n支持表级锁（MySql支持两种表级锁，表共享读锁和表独占写锁），但不支持行级锁\n\n\n存储表的总行数\n\n\n一个MyISAM表有三个文件：索引文件（.MYI），表结构文件(.frm)，数据文件(.MYD)\n\n\n采用非聚集索引：即索引文件和数据文件是分开的，索引文件的数据域存储指向数据文件的指针\n\n\n跨平台应用更方便（表保存为文件形式）\n\n\n支持三种不同的存储格式：\n\n\n（1）静态表：存储迅速，容易缓存，出现故障容易恢复；但是占用内存多，因为会按列宽度补足空格\n\n\n（2）动态表：占用空间少，但是频繁更新和删除容易产生碎片，需要定期整理（OPTIMIZE TABLE），并且出现故障难以恢复。\n\n\n（3）压缩表：占据的磁盘空间非常小（每个记录被单独压缩，所以访问开支很小）。\n\n\n### InnoDb引擎\n\n\n支持事务\n\n\n支持行级锁（仅在条件语句中包括主键索引时）\n\n\n内存使用率低\n\n\n查询效率和写的效率更低\n\n\n采用聚集索引，索引和数据存在一起，叶子结点直接存的是数据。\n\n\n支持外键\n\n\n> 注：MyISAM在查询时的性能比InnoDB高，因为它采用的辅索引和主键索引类似，所以通过辅索引查找数据时只需要通过辅索引树就可以查找到，而InnoDB需要先通过辅索引查找到主索引，再通过主索引树查找到数据。\n\n\n### MyISAM和InnoDB区别\n\n\nMyISAM是MySQL的默认数据库引擎（5.5版之前）。虽然性能极佳，而且提供了大量的特性，包括全文索引、压缩、空间函数等，但MyISAM不支持事务和行级锁，而且最大的缺陷就是崩溃后无法安全恢复。不过，5.5版本之后，MySQL引入了InnoDB（事务性数据库引擎），MySQL 5.5版本后默认的存储引擎为InnoDB。\n\n\n大多数时候我们使用的都是 InnoDB 存储引擎，但是在某些情况下使用 MyISAM 也是合适的比如读密集的情况下。（如果你不介意 MyISAM 崩溃恢复问题的话）。\n\n\n**两者的对比：**\n\n1. **是否支持行级锁** : MyISAM 只有表级锁(table-level locking)，而InnoDB 支持行级锁(row-level locking)和表级锁,默认为行级锁。\n2. **是否支持事务和崩溃后的安全恢复： MyISAM** 强调的是性能，每次查询具有原子性,其执行速度比InnoDB类型更快，但是不提供事务支持。但是**InnoDB** 提供事务支持，外部键等高级数据库功能。 具有事务(commit)、回滚(rollback)和崩溃修复能力(crash recovery capabilities)的事务安全(transaction-safe (ACID compliant))型表。\n3. **是否支持外键：** MyISAM不支持，而InnoDB支持。\n4. **是否支持MVCC** ：仅 InnoDB 支持。应对高并发事务, MVCC比单纯的加锁更高效;MVCC只在 `READ COMMITTED` 和 `REPEATABLE READ` 两个隔离级别下工作;MVCC可以使用 乐观(optimistic)锁 和 悲观(pessimistic)锁来实现;各数据库中MVCC实现并不统一。推荐阅读：[MySQL-InnoDB-MVCC多版本并发控制](https://segmentfault.com/a/1190000012650596)\n5. ......\n\n《MySQL高性能》上面有一句话这样写到:\n\n\n> 不要轻易相信“MyISAM比InnoDB快”之类的经验之谈，这个结论往往不是绝对的。在很多我们已知场景中，InnoDB的速度都可以让MyISAM望尘莫及，尤其是用到了聚簇索引，或者需要访问的数据都可以放入内存的应用。\n\n\n一般情况下我们选择 InnoDB 都是没有问题的，但是某些情况下你并不在乎可扩展能力和并发能力，也不需要事务支持，也不在乎崩溃后的安全恢复问题的话，选择MyISAM也是一个不错的选择。但是一般情况下，我们都是需要考虑到这些问题的。\n\n\n### MEMORY\n\n\n只对应一个磁盘文件.frm，用来存储表结构\n\n\n访问速度非常快，因为他的数据存在内存中，但是一旦服务关闭，数据就会丢失\n\n\n可以指定Hash索引或BTREE索引\n\n\n默认存储数据大小不超过16MB，但可以调整\n\n\n应用场景：比如作为统计操作的的中间结果表，便于高效地对中间结果分析并得到最终结果。\n\n\n### MERGE\n\n\n一组MyISAM表的组合，这些MyISAM表结构必须完全相同\n\n\n对MERGE表的操作实际上是对其子表进行的\n\n\n可以通过指定INSERT_METHOD=LAST来制定插入数据的表（这里指定为最后一个表）\n\n\n### 参考\n\n\n[1] 《深入浅出MySQL》\n\n\n[2] 平衡查找树之B树：[http://www.cnblogs.com/yangecnu/p/Introduce-B-Tree-and-B-Plus-Tree.html](http://www.cnblogs.com/yangecnu/p/Introduce-B-Tree-and-B-Plus-Tree.html)\n\n\n[3] B树，B+树，B*树：[https://www.jianshu.com/p/db226e0196b4](https://www.jianshu.com/p/db226e0196b4)\n\n\n## 数据结构\n\n\n### B树（B-Tree）\n\n\nB树是2-3树的一种扩展，对于M阶(M就是树的高度，比如下图为一个四阶的树)的B树来说：\n\n\n（1）根节点至少有两个子节点\n\n\n（2）每个节点至多有M-1个key，以升序排列，以及Nk+1个指针，其中Nk代表key的数量。\n\n\n（3）对于一个key1来说，它左侧的指针指向的子节点的key值<=key1,右侧子指针指向的子节点的key值>key1（详见下图）\n\n\n（4）其他节点至少有M/2个子节点\n\n\n![](https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy8xNDQ2MDg3LWJjMDIzZTQ3YmM3NGNmYTEuanBn?x-oss-process=image/format,png)\n\n\n他的插入过程和红黑树很相似，总结一下就是：\n\n\n（1）当要插入一个新值时，首先根据第三条原则找到他在叶子节点的位置并插入\n\n\n（2）如果当前叶子节点的key数目等于M，那么就要拆分，拆分的过程就是把所有key通过一个中间值（如M=4取第二个数，M=5取第三个数），分成相同的两份（如果M是偶数会相差一），然后中间数为父节点，两边的树作为左右子节点，但要注意中间数是插入到他们的父节点中的，而不是新生成一棵树，这也就意味着如果这时候父节点的key树等于M，那么就要通过相同的变换把key值接着向上传递，直到key数<M。\n\n\n**B树由来**\n\n\n> 定义：B-树是一类树，包括B-树、B+树、B*树等，是一棵自平衡的搜索树，它类似普通的平衡二叉树，不同的一点是B-树允许每个节点有更多的子节点。  \n> B-树是专门为外部存储器设计的，如磁盘，它对于读取和写入大块数据有良好的性能，所以一般被用在文件系统及数据库中。\n\n\n定义只需要知道B-树允许每个节点有更多的子节点即可（多叉树）。子节点数量一般在上千，具体数量依赖外部存储器的特性。\n\n\n先来看看为什么会出现B-树这类数据结构。\n\n\n> 传统用来搜索的平衡二叉树有很多，如 AVL 树，红黑树等。这些树在一般情况下查询性能非常好，但当数据非常大的时候它们就无能为力了。\n\n\n\t原因当数据量非常大时，内存不够用，大部分数据只能存放在磁盘上，只有需要的数据才加载到内存中。一般而言内存访问的时间约为 50 ns，而磁盘在 10 ms 左右。速度相差了近 5 个数量级，磁盘读取时间远远超过了数据在内存中比较的时间。\n\n\n\t这说明程序大部分时间会阻塞在磁盘 IO 上。那么我们如何提高程序性能？减少磁盘 IO 次数，像 AVL 树，红黑树这类平衡二叉树从设计上无法“迎合”磁盘。\n\n\n![](https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy8xNDQ2MDg3LTI0Njk5ZTFmZTNmYzlhZGY?x-oss-process=image/format,png)\n\n\n上图是一颗简单的平衡二叉树，平衡二叉树是通过旋转来保持平衡的，而旋转是对整棵树的操作，若部分加载到内存中则无法完成旋转操作。其次平衡二叉树的高度相对较大为 log n（底数为2），这样逻辑上很近的节点实际可能非常远，无法很好的利用磁盘预读（局部性原理），所以这类平衡二叉树在数据库和文件系统上的选择就被 pass 了。\n\n\n> 空间局部性原理：如果一个存储器的某个位置被访问，那么将它附近的位置也会被访问。\n\n\n我们从“迎合”磁盘的角度来看看B-树的设计。\n\n\n**索引的效率依赖**与**磁盘 IO** 的次数，**快速索引**需要有效的减少磁盘 IO 次数，如何快速索引呢？\n\n\n> 索引的原理其实是不断的缩小查找范围，就如我们平时用字典查单词一样，先找首字母缩小范围，再第二个字母等等。\n\n\n\t平衡二叉树是每次将范围分割为两个区间。为了更快，B-树每次将范围分割为多个区间，区间越多，定位数据越快越精确。那么如果节点为区间范围，每个节点就较大了。所以新建节点时，直接申请页大小的空间（磁盘存储单位是按 block 分的，一般为 512 Byte。\n\n\n\t磁盘 IO 一次读取若干个 block，我们称为一页，具体大小和操作系统有关，一般为 4 k，8 k或 16 k），计算机内存分配是按页对齐的，这样就实现了一个节点只需要一次 IO。\n\n\n![](https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy8xNDQ2MDg3LWI2ZDQ3NTQzMDRmYTg5NTU?x-oss-process=image/format,png)\n\n\n上图是一棵简化的B-树，多叉的好处非常明显，有效的降低了B-树的高度，为底数很大的 log n，底数大小与节点的子节点数目有关，一般一棵B-树的高度在 3 层左右。\n\n\n层数低，每个节点区确定的范围更精确，范围缩小的速度越快（比二叉树深层次的搜索肯定快很多）。上面说了一个节点需要进行一次 IO，那么总 IO 的次数就缩减为了 log n 次。B-树的每个节点是 n 个有序的序列(a1,a2,a3…an)，并将该节点的子节点分割成 n+1 个区间来进行索引(X1< a1, a2 < X2 < a3, … , an+1 < Xn < anXn+1 > an)。\n\n\n> 点评：B树的每个节点，都是存多个值的，不像二叉树那样，一个节点就一个值，B树把每个节点都给了一点的范围区间，区间更多的情况下，搜索也就更快了，比如：有1-100个数，二叉树一次只能分两个范围，0-50和51-100，而B树，分成4个范围 1-25， 25-50，51-75，76-100一次就能筛选走四分之三的数据。所以作为多叉树的B树是更快的\n\n\n### B+Tree（树）\n\n\n存储引擎常用的一种数据结构，一种多叉平衡查找树，特点（对于M阶的B+树）：\n\n\n（1）除叶子结点外所有节点都有M个键以及M个指向子节点的指针。\n\n\n（2）所有叶子节点都在同一层\n\n\n（3）非叶子结点的子树指针与关键字（Key）个数相同；\n\n\n（4）非叶子结点的子树指针P[i]，指向关键字值属于[K[i], K[i+1])的子树（B-树是开区间）；\n\n\n（5）为所有叶子结点增加一个链指针；\n\n\n（6）所有关键字（key）都在叶子结点出现；，因此所有查找只会在叶子结点命中\n\n\n结构如下图所示：\n\n\n![](https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy8xNDQ2MDg3LTMwYjcwYWFhMjg0MDM4MDMuanBn?x-oss-process=image/format,png)\n\n\n![](https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy8xNDQ2MDg3LTJhYTFjMTdkMTY4OGI3OTk?x-oss-process=image/format,png)\n\n\n**因为内节点并不存储 data，所以一般B+树的叶节点和内节点大小不同，而B-树的每个节点大小一般是相同的，为一页。**\n\n\n为了增加 **区间访问性**，一般会对B+树做一些优化。\n如下图带顺序访问的B+树。\n\n\n![](https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy8xNDQ2MDg3LTg5MTkwOTQzNzk0M2Q4ODg?x-oss-process=image/format,png)\n\n\n**相比B树的优点：**\n\n\n（1）支持范围查找\n\n\n（2）遍历更方便\n\n\n（3）节省空间：因为B+树只有叶子节点才存数据，因此内部节点不需要只想关键字具体信息的指针。\n\n\n（4）所有查询操作都需要命中子节点，所以是相同的。\n\n\n> PS: B*树就是在B+树基础上，为非叶子结点也增加链表指针\n\n\n### B树与B+Tree的区别\n\n\n1 .  **B+树内节点不存储数据，所有 data 存储在叶节点导致查询时间复杂度固定为 log n。而B-树查询时间复杂度不固定，与 key 在树中的位置有关，最好为O(1)。**\n\n\n如下所示B-树/B+树查询节点 key 为 50 的 data。\n\n\nB-树：\n\n\n![](https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy8xNDQ2MDg3LWQ2MmVjNmU4M2VlYmUwNmY?x-oss-process=image/format,png)\n\n\n> 从上图可以看出，key 为 50 的节点就在第一层，B-树只需要一次磁盘 IO 即可完成查找。所以说B-树的查询最好时间复杂度是 O(1)。\n\n\nB+树：\n\n\n![](https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy8xNDQ2MDg3LWQ2OTBmYWJlOGM5MTkyODE?x-oss-process=image/format,png)\n\n\n**由于B+树所有的 data 域都在根节点，所以查询 key 为 50的节点必须从根节点索引到叶节点，时间复杂度固定为 O(log n)。**\n\n\n> 点评：B树的由于每个节点都有key和data，所以查询的时候可能不需要O(logn)的复杂度，甚至最好的情况是O(1)就可以找到数据，而B+树由于只有叶子节点保存了data，所以必须经历O(logn)复杂度才能找到数据\n\n1. **B+树叶节点两两相连可大大增加区间访问性，可使用在范围查询等，而B-树每个节点 key 和 data 在一起，则无法区间查找。**\n\n`根据空间局部性原理`：如果一个存储器的某个位置被访问，那么将它附近的位置也会被访问。\n\n\n> B+树可以很好的利用局部性原理，若我们访问节点 key为 50，则 key 为 55、60、62 的节点将来也可能被访问，我们可以利用磁盘预读原理提前将这些数据读入内存，减少了磁盘 IO 的次数。  \n> 当然B+树也能够很好的完成范围查询。比如查询 key 值在 50-70 之间的节点。\n\n\n> 点评：由于B+树的叶子节点的数据都是使用链表连接起来的，而且他们在磁盘里是顺序存储的，所以当读到某个值的时候，磁盘预读原理就会提前把这些数据都读进内存，使得范围查询和排序都很快\n\n1. **B+树更适合外部存储。由于内节点无 data 域，每个节点能索引的范围更大更精确**\n\n这个很好理解，由于B-树节点内部每个 key 都带着 data 域，而B+树节点只存储 key 的副本，真实的 key 和 data 域都在叶子节点存储。\n\n\n      前面说过磁盘是分 block 的，一次磁盘 IO 会读取若干个 block，具体和操作系统有关，那么由于磁盘 IO 数据大小是固定的，在一次 IO 中，单个元素越小，量就越大。\n\n\n这就意味着B+树单次磁盘 IO 的信息量大于B-树，从这点来看B+树相对B-树磁盘 IO 次数少。\n\n\n> 点评：由于B树的节点都存了key和data，而B+树只有叶子节点存data，非叶子节点都只是索引值，没有实际的数据，这就时B+树在一次IO里面，能读出的索引值更多。从而减少查询时候需要的IO次数！\n\n\n![](https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy8xNDQ2MDg3LTBmNmUxZjdiODkyMWI2ZjM?x-oss-process=image/format,png)\n\n\n> 从上图可以看出相同大小的区域，B-树仅有 2 个 key，而B+树有 3 个 key。\n\n\n### 为什么使用B-Tree（B+Tree）\n\n\n      `上文说过，红黑树等数据结构也可以用来实现索引，但是文件系统及数据库系统普遍采用B-/+Tree作为索引结构，这一节将结合计算机组成原理相关知识讨论B-/+Tree作为索引的理论基础。`\n\n\n> 一般来说，索引本身也很大，不可能全部存储在内存中，因此索引往往以索引文件的形式存储的磁盘上。这样的话，索引查找过程中就要产生磁盘I/O消耗，相对于内存存取，I/O存取的消耗要高几个数量级，所以评价一个数据结构作为索引的优劣最重要的指标就是在查找过程中磁盘I/O操作次数的渐进复杂度。\n\n\n\t换句话说，索引的结构组织要尽量减少查找过程中磁盘I/O的存取次数。下面先介绍内存和磁盘存取原理，然后再结合这些原理分析B-/+Tree作为索引的效率。\n\n\n### 存储数据最小单元\n\n\n我们都知道计算机在存储数据的时候，有最小存储单元，这就好比我们今天进行现金的流通最小单位是一毛。\n\n\n在计算机中磁盘存储数据最小单元是扇区，一个扇区的大小是512字节，而文件系统（例如XFS/EXT4）他的最小单元是块，一个块的大小是4k\n\n\n而对于我们的InnoDB存储引擎也有自己的最小储存单元——页（Page），一个页的大小是16K。\n\n\n下面几张图可以帮你理解最小存储单元：\n\n\n文件系统中一个文件大小只有1个字节，但不得不占磁盘上4KB的空间。\n\n\n磁盘扇区、文件系统、InnoDB存储引擎都有各自的最小存储单元。\n\n\n![](https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy8xNDQ2MDg3LTAyMzJiODIzZTNmMWViM2E?x-oss-process=image/format,png)\n\n\n在MySQL中我们的InnoDB页的大小默认是16k，当然也可以通过参数设置：\n\n\n![](https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy8xNDQ2MDg3LWVkNzFhNWIzZWVmNDMxZGM?x-oss-process=image/format,png)\n\n\n> 数据表中的数据都是存储在页中的，所以一个页中能存储多少行数据呢？假设一行数据的大小是1k，那么一个页可以存放16行这样的数据。\n\n\n### 主存存取原理\n\n\n目前计算机使用的主存基本都是随机读写存储器（RAM），现代RAM的结构和存取原理比较复杂，这里本文抛却具体差别，抽象出一个十分简单的存取模型来说明RAM的工作原理。\n\n\n![](https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy8xNDQ2MDg3LTBmODlkNzBlYTM0YTI1OTkucG5n?x-oss-process=image/format,png)\n\n\n`从抽象角度看`，主存是一系列的存储单元组成的矩阵，每个存储单元存储固定大小的数据。每个存储单元有唯一的地址，现代主存的编址规则比较复杂\n\n\n    这里将其简化成一个`二维地址`：通过一个行地址和一个列地址可以唯一定位到一个存储单元。图5展示了一个4 x 4的主存模型。\n\n\n**主存的存取过程如下：**\n\n\n> 当系统需要读取主存时，则将地址信号放到地址总线上传给主存，主存读到地址信号后，解析信号并定位到指定存储单元，然后将此存储单元数据放到数据总线上，供其它部件读取。\n\n\n> 写主存的过程类似，系统将要写入单元地址和数据分别放在地址总线和数据总线上，主存读取两个总线的内容，做相应的写操作。\n\n\n这里可以看出，主存存取的时间仅与存取次数呈线性关系，因为不存在机械操作，两次存取的数据的“距离”不会对时间有任何影响，例如，先取A0再取A1和先取A0再取D3的时间消耗是一样的。\n\n\n### 磁盘存取原理\n\n\n上文说过，索引一般以文件形式存储在磁盘上，索引检索需要磁盘I/O操作。与主存不同，磁盘I/O存在机械运动耗费，因此磁盘I/O的时间消耗是巨大的。\n\n\n图6是磁盘的整体结构示意图。\n\n\n![](https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy8xNDQ2MDg3LTA3Njg5ZDJjNTM5YzY1MGIucG5n?x-oss-process=image/format,png)\n\n\n一个磁盘由大小相同且同轴的圆形盘片组成，磁盘可以转动（各个磁盘必须同步转动）。在磁盘的一侧有磁头支架，磁头支架固定了一组磁头，每个磁头负责存取一个磁盘的内容。磁头不能转动，但是可以沿磁盘半径方向运动（实际是斜切向运动），每个磁头同一时刻也必须是同轴的，即从正上方向下看，所有磁头任何时候都是重叠的（不过目前已经有多磁头独立技术，可不受此限制）。\n\n\n图7是磁盘结构的示意图。\n\n\n![](https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy8xNDQ2MDg3LWZiZDUyMTlkMmVkNWQ3YzUucG5n?x-oss-process=image/format,png)\n\n\n盘片被划分成一系列同心环，圆心是盘片中心，每个同心环叫做一个磁道，所有半径相同的磁道组成一个柱面。磁道被沿半径线划分成一个个小的段，**每个段叫做一个扇区，每个扇区是磁盘的最小存储单元**。为了简单起见，我们下面假设磁盘只有一个盘片和一个磁头。\n\n\n> 当需要从磁盘读取数据时，系统会将数据逻辑地址传给磁盘，磁盘的控制电路按照寻址逻辑将逻辑地址翻译成物理地址，即确定要读的数据在哪个磁道，哪个扇区。\n\n\n\t    为了读取这个扇区的数据，需要将磁头放到这个扇区上方，为了实现这一点，磁头需要移动对准相应磁道，这个过程叫做寻道，所耗费时间叫做寻道时间，然后磁盘旋转将目标扇区旋转到磁头下，这个过程耗费的时间叫做旋转时间。\n\n\n### 局部性原理与磁盘预读\n\n\n> 由于存储介质的特性，磁盘本身存取就比主存慢很多，再加上机械运动耗费，磁盘的存取速度往往是主存的几百分分之一，因此为了提高效率，要尽量减少磁盘I/O。为了达到这个目的，磁盘往往不是严格按需读取，而是每次都会预读，即使只需要一个字节，磁盘也会从这个位置开始，顺序向后读取一定长度的数据放入内存。这样做的理论依据是计算机科学中著名的局部性原理：\n\n\n当一个数据被用到时，其附近的数据也通常会马上被使用。\n\n\n程序运行期间所需要的数据通常比较集中。\n\n\n由于磁盘顺序读取的效率很高（不需要寻道时间，只需很少的旋转时间），因此对于具有局部性的程序来说，预读可以提高I/O效率。\n\n\n预读的长度一般为页（page）的整倍数。页是计算机管理存储器的逻辑块，硬件及操作系统往往将主存和磁盘存储区分割为连续的大小相等的块，每个存储块称为一页（在许多操作系统中，页得大小通常为4k），主存和磁盘以页为单位交换数据。当程序要读取的数据不在主存中时，会触发一个缺页异常，此时系统会向磁盘发出读盘信号，磁盘会找到数据的起始位置并向后连续读取一页或几页载入内存中，然后异常返回，程序继续运行。\n\n\n**所以IO一次就是读一页的大小**\n\n\n## 索引\n\n\nMySQL索引使用的数据结构主要有**BTree索引** 和 **哈希索引** 。对于哈希索引来说，底层的数据结构就是哈希表，因此在绝大多数需求为单条记录查询的时候，可以选择哈希索引，查询性能最快；其余大部分场景，建议选择BTree索引。\n\n\nMySQL的BTree索引使用的是B树中的B+Tree，但对于主要的两种存储引擎的实现方式是不同的。\n\n- **MyISAM:** B+Tree叶节点的data域存放的是数据记录的地址。在索引检索的时候，首先按照B+Tree搜索算法搜索索引，如果指定的Key存在，则取出其 data 域的值，然后以 data 域的值为地址读取相应的数据记录。这被称为“非聚簇索引”。\n- **InnoDB:** 其数据文件本身就是索引文件。相比MyISAM，索引文件和数据文件是分离的，其表数据文件本身就是按B+Tree组织的一个索引结构，树的叶节点data域保存了完整的数据记录。这个索引的key是数据表的主键，因此InnoDB表数据文件本身就是主索引。这被称为“聚簇索引（或聚集索引）”。而其余的索引都作为辅助索引，辅助索引的data域存储相应记录主键的值而不是地址，这也是和MyISAM不同的地方。**在根据主索引搜索时，直接找到key所在的节点即可取出数据；在根据辅助索引查找时，则需要先取出主键的值，再走一遍主索引。** **因此，在设计表的时候，不建议使用过长的字段作为主键，也不建议使用非单调的字段作为主键，这样会造成主索引频繁分裂。** PS：整理自《Java工程师修炼之道》\n\n### Hash索引\n\n\n> Hash 索引结构的特殊性，其检索效率非常高，索引的检索可以一次定位，不像B-Tree 索引需要从根节点到枝节点，最后才能访问到页节点这样多次的IO访问，所以 Hash 索引的查询效率要远高于 B-Tree 索引。\n\n\n可 能很多人又有疑问了，既然 Hash 索引的效率要比 B-Tree 高很多，为什么大家不都用 Hash 索引而还要使用 B-Tree 索引呢？任何事物都是有两面性的，Hash 索引也一样，虽然 Hash 索引效率高，但是 Hash 索引本身由于其特殊性也带来了很多限制和弊端，主要有以下这些。\n\n\n（1）Hash 索引仅仅能满足\"=\",\"IN\"和\"<=>\"查询，不能使用范围查询。\n\n\n> 由于 Hash 索引比较的是进行 Hash 运算之后的 Hash 值，所以它只能用于等值的过滤，不能用于基于范围的过滤，因为经过相应的 Hash 算法处理之后的 Hash 值的大小关系，并不能保证和Hash运算前完全一样。\n\n\n（2）Hash 索引无法被用来避免数据的排序操作。\n\n\n> 由于 Hash 索引中存放的是经过 Hash 计算之后的 Hash 值，而且Hash值的大小关系并不一定和 Hash 运算前的键值完全一样，所以数据库无法利用索引的数据来避免任何排序运算；\n\n\n（3）Hash 索引不能利用部分索引键查询。\n\n\n> 对于组合索引，Hash 索引在计算 Hash 值的时候是组合索引键合并后再一起计算 Hash 值，而不是单独计算 Hash 值，所以通过组合索引的前面一个或几个索引键进行查询的时候，Hash 索引也无法被利用。\n\n\n（4）Hash 索引在任何时候都不能避免表扫描。\n\n\n> 前面已经知道，Hash 索引是将索引键通过 Hash 运算之后，将 Hash运算结果的 Hash 值和所对应的行指针信息存放于一个 Hash 表中，由于不同索引键存在相同 Hash 值，所以即使取满足某个 Hash 键值的数据的记录条数，也无法从 Hash 索引中直接完成查询，还是要通过访问表中的实际数据进行相应的比较，并得到相应的结果。\n\n\n（5）Hash 索引遇到大量Hash值相等的情况后性能并不一定就会比B-Tree索引高。\n\n\n> 对于选择性比较低的索引键，如果创建 Hash 索引，那么将会存在大量记录指针信息存于同一个 Hash 值相关联。这样要定位某一条记录时就会非常麻烦，会浪费多次表数据的访问，而造成整体性能低下。\n\n\n(6)  hash索引查找数据基本上能一次定位数据，当然有大量碰撞的话性能也会下降。而btree索引就得在节点上挨着查找了，很明显在数据精确查找方面hash索引的效率是要高于btree的；\n\n\n(7)  那么不精确查找呢，也很明显，因为hash算法是基于等值计算的，所以对于“like”等范围查找hash索引无效，不支持；\n\n\n(8)  对于btree支持的联合索引的最优前缀，hash也是无法支持的，联合索引中的字段要么全用要么全不用。提起最优前缀居然都泛起迷糊了，看来有时候放空得太厉害；\n\n\n(9)  hash不支持索引排序，索引值和计算出来的hash值大小并不一定一致。\n\n\n### B-Tree 索引\n\n\nB-tree 索引可以用于使用 =, >, >=, <, <= 或者 BETWEEN 运算符的列比较。如果 LIKE 的参数是一个没有以通配符起始的常量字符串的话也可以使用这种索引。比如，以下 SELECT 语句就使用索引：\n\n\n```text\n1.  SELECT * FROM tbl_name WHERE key_col LIKE 'Patrick%';\n2.  SELECT * FROM tbl_name WHERE key_col LIKE 'Pat%_ck%';\n\n```\n\n\n在第一个句子中，只会考虑 'Patrick' <= key_col < 'Patricl' 的记录。第二句中，则只会考虑 'Pat' <= key_col < 'Pau' 的记录。\n以下 SELECT 语句不使用索引：\n\n\n```text\n1.  SELECT * FROM tbl_name WHERE key_col LIKE '%Patrick%';\n2.  SELECT * FROM tbl_name WHERE key_col LIKE other_col;\n\n```\n\n\n第一句里面，LIKE 的值起始于一个通配符。在第二句里，LIKE 的值不是一个常量。如果你这样使用： ... LIKE '%string%'，其中的 string 不大于三个字符，MySql 将使用 Turbo Boyer-Moore 算法来对该字符串表达式进行初始化，并使用这种表达式来让查询更加迅速。如果 col_name 列创建了索引，那么一个使用了 col_name IS NULL 的查询是可以使用该索引的。任何没有涵盖 WHERE 从句中所有 AND 级别的条件的索引将不会被使用。换句话讲，要想使用索引，该索引的前导列必须在每一个 AND 组合中使用到。\n以下 WHERE 从句使用索引：\n\n\n```text\n1.  ... WHERE index_part1=1 AND index_part2=2 AND other_column=3\n\n4.  /* index = 1 OR index = 2 */\n5.  ... WHERE index=1 OR A=10 AND index=2\n\n8.  /* optimized like \"index_part1='hello'\" */\n9.  ... WHERE index_part1='hello' AND index_part3=5\n\n12.  /* Can use index on index1 but not on index2 or index3 */\n13.  ... WHERE index1=1 AND index2=2 OR index1=3 AND index3=3;\n\n```\n\n\n这些 WHERE 从句不使用索引：\n\n\n```text\n1.  /* index_part1 is not used */\n2.  WHERE index_part2=1 AND index_part3=2\n\n5.  /*  Index is not used in both parts of the WHERE clause  */\n6.  WHERE index=1 OR A=10\n\n9.  /* No index spans all rows  */\n10.  WHERE index_part1=1 OR index_part2=10\n\n```\n\n\n有时，即使有索引可以使用，[MySQL](http://lib.csdn.net/base/mysql) 也不使用任何索引。发生这种情况的场景之一就是优化器估算出使用该索引将要求 MySql 去访问这张表的绝大部分记录。这种情况下，一个表扫描可能更快，因为它要求更少量的查询。但是，如果这样的一个查询使用了 LIMIT 来检索只是少量的记录时，MySql 还是会使用索引，因为它能够更快地找到这点记录并将其返回。\n\n\n## 事务\n\n\n**事务是逻辑上的一组操作，要么都执行，要么都不执行。**\n\n\n事务最经典也经常被拿出来说例子就是转账了。假如小明要给小红转账1000元，这个转账会涉及到两个关键操作就是：将小明的余额减少1000元，将小红的余额增加1000元。万一在这两个操作之间突然出现错误比如银行系统崩溃，导致小明余额减少而小红的余额没有增加，这样就不对了。事务就是保证这两个关键操作要么都成功，要么都要失败。\n\n\n### [事务的四大特性(ACID)](https://snailclimb.gitee.io/javaguide/#/docs/database/MySQL?id=%E4%BA%8B%E5%8A%A1%E7%9A%84%E5%9B%9B%E5%A4%A7%E7%89%B9%E6%80%A7acid)\n\n\n![](https://blog-file.hehouhui.cn/image.png)\n\n\n> 事务处理可以确保除非事务性单元内的所有操作都成功完成，否则不会永久更新面向数据的资源。通过将一组相关操作组合为一个要么全部成功要么全部失败的单元，可以简化错误恢复并使应用程序更加可靠。一个逻辑工作单元要成为事务，必须满足所谓的ACID(原子性、一致性、隔离性和持久性)属性：\n\n1. **原子性(Atomicity)**\n\n\t事务是最小的执行单位，不允许分割。事务的原子性确保动作要么全部完成，要么完全不起作用；\n\n\n\t> 事务必须是原子工作单元；对于其数据修改，要么全都执行，要么全都不执行。通常，与某个事务关联的操作具有共同的目标，并且是相互依赖的。如果系统只执行这些操作的一个子集，则可能会破坏事务的总体目标。原子性消除了系统处理操作子集的可能性。\n\n2. **一致性(Consistency)**\n\n\t执行事务后，数据库从一个正确的状态变化到另一个正确的状态；\n\n\n\t> 事务在完成时，必须使所有的数据都保持一致状态。在相关数据库中，所有规则都必须应用于事务的修改，以保持所有数据的完整性。事务结束时，所有的内部数据结构（如 B 树索引或双向链表）都必须是正确的。某些维护一致性的责任由应用程序开发人员承担，他们必须确保应用程序已强制所有已知的完整性约束。例如，当开发用于转帐的应用程序时，应避免在转帐过程中任意移动小数点。\n\n3. **隔离性(Isolation)**\n\n\t并发访问数据库时，一个用户的事务不被其他事务所干扰，各并发事务之间数据库是独立的；\n\n\n> 由并发事务所作的修改必须与任何其它并发事务所作的修改隔离。事务查看数据时数据所处>的状态，要么是另一并发事务修改它之前的状态，要么是另一事务修改它之后的状态，事务不会查看中间状态的数据。这称为可串行性，因为它能够重新装载起始数据，并且重播一系列事务，以使数据结束时的状态与原始事务执行的状态相同。当事务可序列化时将获得最高的隔离级别。在此级别上，从一组可并行执行的事务获得的结果与通过连续运行每个事务所获得的结果相同。由于高度隔离会限制可并行执行的事务数，所以一些应用程序降低隔离级别以换取更大的吞吐量。\n\n1. · **持久性(Durability）**\n\n\t一个事务被提交之后。它对数据库中数据的改变是持久的，即使数据库发生故障也不应该对其有任何影响。\n\n\n\t> 事务完成之后，它对于系统的影响是永久性的。该修改即使出现致命的系统故障也将一直保持。\n\n\n### [并发事务带来哪些问题?](https://snailclimb.gitee.io/javaguide/#/docs/database/MySQL?id=%E5%B9%B6%E5%8F%91%E4%BA%8B%E5%8A%A1%E5%B8%A6%E6%9D%A5%E5%93%AA%E4%BA%9B%E9%97%AE%E9%A2%98)\n\n\n在典型的应用程序中，多个事务并发运行，经常会操作相同的数据来完成各自的任务（多个用户对同一数据进行操作）。并发虽然是必须的，但可能会导致以下的问题。\n\n- **脏读（Dirty read）:** 当一个事务正在访问数据并且对数据进行了修改，而这种修改还没有提交到数据库中，这时另外一个事务也访问了这个数据，然后使用了这个数据。因为这个数据是还没有提交的数据，那么另外一个事务读到的这个数据是“脏数据”，依据“脏数据”所做的操作可能是不正确的。\n- **丢失修改（Lost to modify）:** 指在一个事务读取一个数据时，另外一个事务也访问了该数据，那么在第一个事务中修改了这个数据后，第二个事务也修改了这个数据。这样第一个事务内的修改结果就被丢失，因此称为丢失修改。 例如：事务1读取某表中的数据A=20，事务2也读取A=20，事务1修改A=A-1，事务2也修改A=A-1，最终结果A=19，事务1的修改被丢失。\n- **不可重复读（Unrepeatableread）:** 指在一个事务内多次读同一数据。在这个事务还没有结束时，另一个事务也访问该数据。那么，在第一个事务中的两次读数据之间，由于第二个事务的修改导致第一个事务两次读取的数据可能不太一样。这就发生了在一个事务内两次读到的数据是不一样的情况，因此称为不可重复读。\n- **幻读（Phantom read）:** 幻读与不可重复读类似。它发生在一个事务（T1）读取了几行数据，接着另一个并发事务（T2）插入了一些数据时。在随后的查询中，第一个事务（T1）就会发现多了一些原本不存在的记录，就好像发生了幻觉一样，所以称为幻读。\n\n**不可重复读和幻读区别：**\n\n\n不可重复读的重点是修改比如多次读取一条记录发现其中某些列的值被修改，幻读的重点在于新增或者删除比如多次读取一条记录发现记录增多或减少了。\n\n\n### [事务隔离级别有哪些?MySQL的默认隔离级别是?](https://snailclimb.gitee.io/javaguide/#/docs/database/MySQL?id=%E4%BA%8B%E5%8A%A1%E9%9A%94%E7%A6%BB%E7%BA%A7%E5%88%AB%E6%9C%89%E5%93%AA%E4%BA%9Bmysql%E7%9A%84%E9%BB%98%E8%AE%A4%E9%9A%94%E7%A6%BB%E7%BA%A7%E5%88%AB%E6%98%AF)\n\n\n**SQL 标准定义了四个隔离级别：**\n\n- **READ-UNCOMMITTED(读取未提交)：** 最低的隔离级别，允许读取尚未提交的数据变更，**可能会导致脏读、幻读或不可重复读**。\n- **READ-COMMITTED(读取已提交)：** 允许读取并发事务已经提交的数据，**可以阻止脏读，但是幻读或不可重复读仍有可能发生**。\n- **REPEATABLE-READ(可重复读)：** 对同一字段的多次读取结果都是一致的，除非数据是被本身事务自己所修改，**可以阻止脏读和不可重复读，但幻读仍有可能发生**。\n- **SERIALIZABLE(可串行化)：** 最高的隔离级别，完全服从ACID的隔离级别。所有的事务依次逐个执行，这样事务之间就完全不可能产生干扰，也就是说，**该级别可以防止脏读、不可重复读以及幻读**。\n\n---\n\n\n| 隔离级别             | 脏读 | 不可重复读 | 幻影读 |\n| ---------------- | -- | ----- | --- |\n| READ-UNCOMMITTED | √  | √     | √   |\n| READ-COMMITTED   | ×  | √     | √   |\n| REPEATABLE-READ  | ×  | ×     | √   |\n| SERIALIZABLE     | ×  | ×     | ×   |\n\n\nMySQL InnoDB 存储引擎的默认支持的隔离级别是 **REPEATABLE-READ（可重读）**。我们可以通过`SELECT @@tx_isolation;`命令来查看，MySQL 8.0 该命令改为`SELECT @@transaction_isolation;`\n\n\n### **事务保存点(SavePoint)**\n\n\n> 有一个数据库管理员，在早上的时候，设置了一个保存点a1，然后工作（对数据库进行增删改查），到了中午他又设置了一个保存点a2，到了下午他又继续工作（还是对数据库进行增删改查），然后到了晚上。突然之间，他执行了一个很愚蠢的操作，把数据库里的数据全部给删除了！！！！\n\n\n那这个时候怎么办呢？好在他在早上和中午的时候设置了保存点，然后只要回到保存点就可以了。\n\n\nrollback to xxxSavePoint\n\n\n当事务commit之后，事务中设置的SavePoint就取消了\n\n\n### **事务并发**\n\n\n两个并发事务同时访问数据库表相同的行时，可能存在以下三个问题：\n\n\n1、`幻想读`：事务T1读取一条指定where条件的语句，返回结果集。此时事务T2插入一行新记录，恰好满足T1的where条件。然后T1使用相同的条件再次查询，结果集中可以看到T2插入的记录，这条新纪录就是幻想。\n\n\n2、`不可重复读取`：事务T1读取一行记录，紧接着事务T2修改了T1刚刚读取的记录，然后T1再次查询，发现与第一次读取的记录不同，这称为不可重复读。\n\n\n3、`脏读`：事务T1更新了一行记录，还未提交所做的修改，这个T2读取了更新后的数据，然后T1执行回滚操作，取消刚才的修改，所以T2所读取的行就无效，也就是脏数据。\n\n\n### 事务实现原理\n\n\n**数据库系统是通过并发控制技术和日志恢复技术来避免这种情况发生的。**\n\n\n> 并发控制技术保证了事务的隔离性,使数据库的一致性状态不会因为并发执行的操作被破坏。  \n> 日志恢复技术保证了事务的原子性,使一致性状态不会因事务或系统故障被破坏。同时使已提交的对数据库的修改不会因系统崩溃而丢失,保证了事务的持久性。\n\n\n![](https://blog-file.hehouhui.cn/20220221101406.png)\n\n\n### 并发异常\n\n\n> 在讲解并发控制技术前,先简单介绍下数据库常见的并发异常。\n\n\n### 脏写\n\n\n脏写是指事务回滚了其他事务对数据项的已提交修改,比如下面这种情况\n\n\n![](https://blog-file.hehouhui.cn/20220221101453.png)\n\n\n### 丢失更新\n\n- 丢失更新是指事务覆盖了其他事务对数据的已提交修改,导致这些修改好像丢失了一样。\n\n\t![](https://blog-file.hehouhui.cn/20220221101626.png)\n\n\n事务1和事务2读取A的值都为10,事务2先将A加上10并提交修改,之后事务2将A减少10并提交修改,A的值最后为,导致事务2对A的修改好像丢失了一样\n\n\n### 脏读\n\n- 脏读是指一个事务读取了另一个事务未提交的数据\n\n\t![](https://blog-file.hehouhui.cn/20220221101652.png)\n\n\n在事务1对A的处理过程中,事务2读取了A的值,但之后事务1回滚,导致事务2读取的A是未提交的脏数据。\n\n\n### 不可重复读\n\n- 不可重复读是指一个事务对同一数据的读取结果前后不一致。脏读和不可重复读的区别在于:前者读取的是事务未提交的脏数据,后者读取的是事务已经提交的数据,只不过因为数据被其他事务修改过导致前后两次读取的结果不一样,比如下面这种情况\n\n\t![](https://blog-file.hehouhui.cn/1422237-20181122103147288-996731595.png)\n\n\n由于事务2对A的已提交修改,事务1前后两次读取的结果不一致。\n\n\n### 幻读\n\n- 幻读是指事务读取某个范围的数据时，因为其他事务的操作导致前后两次读取的结果不一致。幻读和不可重复读的区别在于,不可重复读是针对确定的某一行数据而言,而幻读是针对不确定的多行数据。因而幻读通常出现在带有查询条件的范围查询中,比如下面这种情况:\n\n\t![](https://blog-file.hehouhui.cn/1422237-20181122103158043-533492513.png)\n\n\n事务1查询A<5的数据,由于事务2插入了一条A=4的数据,导致事务1两次查询得到的结果不一样\n\n\n### 并发控制\n\n\n> 并发控制技术是实现事务隔离性以及不同隔离级别的关键,实现方式有很多,按照其对可能冲突的操作采取的不同策略可以分为乐观并发控制和悲观并发控制两大类。\n\n- 乐观并发控制:对于并发执行可能冲突的操作,假定其不会真的冲突,允许并发执行,直到真正发生冲突时才去解决冲突,比如让事务回滚。\n- 悲观并发控制:对于并发执行可能冲突的操作,假定其必定发生冲突,通过让事务等待(锁)或者中止(时间戳排序)的方式使并行的操作串行执行。\n\n### 基于封锁的并发控制\n\n\n核心思想:对于并发可能冲突的操作,比如读-写,写-读,写-写,通过锁使它们互斥执行。\n锁通常分为共享锁和排他锁两种类型\n\n- 1.共享锁(S):事务T对数据A加共享锁,其他事务只能对A加共享锁但不能加排他锁。\n- 2.排他锁(X):事务T对数据A加排他锁,其他事务对A既不能加共享锁也不能加排他锁\n\n基于锁的并发控制流程:\n\n1. 事务根据自己对数据项进行的操作类型申请相应的锁(读申请共享锁,写申请排他锁)\n2. 申请锁的请求被发送给锁管理器。锁管理器根据当前数据项是否已经有锁以及申请的和持有的锁是否冲突决定是否为该请求授予锁。\n3. 若锁被授予,则申请锁的事务可以继续执行;若被拒绝,则申请锁的事务将进行等待,直到锁被其他事务释放。\n\n可能出现的问题:\n\n- 死锁:多个事务持有锁并互相循环等待其他事务的锁导致所有事务都无法继续执行。\n- 饥饿:数据项A一直被加共享锁,导致事务一直无法获取A的排他锁。\n\n对于可能发生冲突的并发操作,锁使它们由并行变为串行执行,是一种悲观的并发控制。\n\n\n### 基于时间戳的并发控制\n\n\n核心思想:对于并发可能冲突的操作,基于时间戳排序规则选定某事务继续执行,其他事务回滚。\n\n\n系统会在每个事务开始时赋予其一个时间戳,这个时间戳可以是系统时钟也可以是一个不断累加的计数器值,当事务回滚时会为其赋予一个新的时间戳,先开始的事务时间戳小于后开始事务的时间戳。\n\n\n每一个数据项Q有两个时间戳相关的字段:\nW-timestamp(Q):成功执行write(Q)的所有事务的最大时间戳\nR-timestamp(Q):成功执行read(Q)的所有事务的最大时间戳\n\n\n时间戳排序规则如下:\n\n1. 假设事务T发出read(Q),T的时间戳为TS\na.若TS(T)<W-timestamp(Q),则T需要读入的Q已被覆盖。此\nread操作将被拒绝,T回滚。\nb.若TS(T)>=W-timestamp(Q),则执行read操作,同时把\nR-timestamp(Q)设置为TS(T)与R-timestamp(Q)中的最大值\n2. 假设事务T发出write(Q)\na.若TS(T)<R-timestamp(Q),write操作被拒绝,T回滚。\nb.若TS(T)<W-timestamp(Q),则write操作被拒绝,T回滚。\nc.其他情况:系统执行write操作,将W-timestamp(Q)设置\n为TS(T)。\n\n基于时间戳排序和基于锁实现的本质一样:对于可能冲突的并发操作,以串行的方式取代并发执行,因而它也是一种悲观并发控制。它们的区别主要有两点:\n\n- 基于锁是让冲突的事务进行等待,而基于时间戳排序是让冲突的事务回滚。\n- 基于锁冲突事务的执行次序是根据它们申请锁的顺序,先申请的先执行;而基于时间戳排序是根据特定的时间戳排序规则。\n\n### 基于有效性检查的并发控制\n\n\n核心思想:事务对数据的更新首先在自己的工作空间进行,等到要写回数据库时才进行有效性检查,对不符合要求的事务进行回滚。\n\n\n基于有效性检查的事务执行过程会被分为三个阶段:\n\n1. 读阶段:数据项被读入并保存在事务的局部变量中。所有write操作都是对局部变量进行,并不对数据库进行真正的更新。\n2. 有效性检查阶段:对事务进行有效性检查,判断是否可以执行write操作而不违反可串行性。如果失败,则回滚该事务。\n3. 写阶段:事务已通过有效性检查,则将临时变量中的结果更新到数据库中。\n\n有效性检查通常也是通过对事务的时间戳进行比较完成的,不过和基于时间戳排序的规则不一样。\n\n\n该方法允许可能冲突的操作并发执行,因为每个事务操作的都是自己工作空间的局部变量,直到有效性检查阶段发现了冲突才回滚。因而这是一种乐观的并发策略。\n\n\n### 基于快照隔离的并发控制\n\n\n快照隔离是多版本并发控制(mvcc)的一种实现方式。\n\n\n其核心思想是:数据库为每个数据项维护多个版本(快照),每个事务只对属于自己的私有快照进行更新,在事务真正提交前进行有效性检查,使得事务正常提交更新或者失败回滚。\n\n\n由于快照隔离导致事务看不到其他事务对数据项的更新,为了避免出现丢失更新问题,可以采用以下两种方案避免：\n\n- 先提交者获胜:对于执行该检查的事务T,判断是否有其他事务已经将更新写入数据库,是则T回滚否则T正常提交。\n- 先更新者获胜:通过锁机制保证第一个获得锁的事务提交其更新,之后试图更新的事务中止。\n\n事务间可能冲突的操作通过数据项的不同版本的快照相互隔离,到真正要写入数据库时才进行冲突检测。因而这也是一种乐观并发控制。\n\n\n### 事务的执行过程以及可能产生的问题\n\n\n![](https://blog-file.hehouhui.cn/1422237-20181122103254113-1342077140.png)\n\n\n事务的执行过程可以简化如下:\n\n1. 系统会为每个事务开辟一个私有工作区\n2. 事务读操作将从磁盘中拷贝数据项到工作区中,在执行写操作前所有的更新都作用于工作区中的拷贝.\n3. 事务的写操作将把数据输出到内存的缓冲区中,等到合适的时间再由缓冲区管理器将数据写入到磁盘。\n\n由于数据库存在立即修改和延迟修改,所以在事务执行过程中可能存在以下情况:\n\n- 在事务提交前出现故障,但是事务对数据库的部分修改已经写入磁盘数据库中。这导致了事务的原子性被破坏。\n- 在系统崩溃前事务已经提交,但数据还在内存缓冲区中,没有写入磁盘。系统恢复时将丢失此次已提交的修改。这是对事务持久性的破坏。\n\n### 日志的种类和格式\n\n- ``:描述一次数据库写操作,T是执行写操作的事务的唯一标识,X是要写的数据项,V1是数据项的旧值,V2是数据项的新值。\n- ``:对数据库写操作的撤销操作,将事务T的X数据项恢复为旧值V1。在事务恢复阶段插入。\n- ``: 事务T开始\n- ``: 事务T提交\n- ``: 事务T中止\n\n关于日志,有以下两条规则\n\n- 1.系统在对数据库进行修改前会在日志文件末尾追加相应的日志记录。\n- 2.当一个事务的commit日志记录写入到磁盘成功后,称这个事务已提交,但事务所做的修改可能并未写入磁盘\n\n### 日志恢复的核心思想\n\n- 撤销事务undo:将事务更新的所有数据项恢复为日志中的旧值,事务撤销完毕时将插入一条``记录。\n- 重做事务redo:将事务更新的所有数据项恢复为日志中的新值。\n\n事务正常回滚/因事务故障中止将进行redo\n系统从崩溃中恢复时将先进行redo再进行undo。\n\n\n以下事务将进行undo:日志中只包括`记录,但既不包括`记录也不包括``记录.\n\n\n以下事务将进行redo:日志中包括`记录,也包括`记录或``记录。\n\n\n假设系统从崩溃中恢复时日志记录如下\n\n\n```text\n<T0 start>\n<T0,A,1000,950>\n<T0,B,2000,2050>\n<T0 commit>\n<T1 start>\n<T1,C,700,600>\n\n```\n\n\n由于T0既有start记录又有commit记录,将会对事务T0进行重做,执行相应的redo操作。\n由于T1只有start记录,将会对T1进行撤销,执行相应的undo操作,撤销完毕将写入一条abort记录。\n\n\n### 事务故障中止/正常回滚的恢复流程\n\n1. 从后往前扫描日志,对于事务T的每个形如``的记录,将旧值V1写入数据项X中。\n2. 往日志中写一个特殊的只读记录``,表示将数据项恢复成旧值V1,\n这是一个只读的补偿记录,不需要根据它进行undo。\n3. 一旦发现了`日志记录,就停止继续扫描,并往日志中写一个` 日志记录。\n\n\t![](https://blog-file.hehouhui.cn/1422237-20181122103326981-1784817664.png)\n\n\n### 系统崩溃时的恢复过程(带检查点)\n\n\n检查点是形如``的特殊的日志记录,L是写入检查点记录时还未提交的事务的集合,系统保证在检查点之前已经提交的事务对数据库的修改已经写入磁盘,不需要进行redo。检查点可以加快恢复的过程。\n\n\n系统奔溃时的恢复过程分为两个阶段:重做阶段和撤销阶段。\n\n\n重做阶段:\n\n1. 系统从最后一个检查点开始正向的扫描日志,将要重做的事务的列表undo-list设置为检查点日志记录中的L列表。\n2. 发现`的更新记录或`的补偿撤销记录,就重做该操作。\n3. 发现``记录,就把T加入到undo-list中。\n4. 发现`或`记录,就把T从undo-list中去除。\n\n撤销阶段:\n\n1. 系统从尾部开始反向扫描日志\n2. 发现属于undo-list中的事务的日志记录,就执行undo操作\n3. 发现undo-list中事务的T的`记录,就写入一条`记录,\n并把T从undo-list中去除。\n\n4.undo-list为空,则撤销阶段结束\n\n\n总结:先将日志记录中所有事务的更新按顺序重做一遍,在针对需要撤销的事务按相反的顺序执行其更新操作的撤销操作。\n\n\n### 总结\n\n\n事务是数据库系统进行并发控制的基本单位,是数据库系统进行故障恢复的基本单位,从而也是保持数据库状态一致性的基本单位。ACID是事务的基本特性,数据库系统是通过并发控制技术和日志恢复技术来对事务的ACID进行保证的,从而可以得到如下的关于数据库事务的概念体系结构。\n\n\n![](https://blog-file.hehouhui.cn/1422237-20181122103402725-726722235.png)\n\n\n## 故障与故障恢复技术\n\n\n数据库运行过程中可能会出现故障,这些故障包括事务故障和系统故障两大类\n\n- 事务故障:比如非法输入,系统出现死锁,导致事务无法继续执行。\n- 系统故障:比如由于软件漏洞或硬件错误导致系统崩溃或中止。\n\n这些故障可能会对事务和数据库状态造成破坏,因而必须提供一种技术来对各种故障进行恢复,保证数据库一致性,事务的原子性以及持久性。数据库通常以日志的方式记录数据库的操作从而在故障时进行恢复,因而可以称之为日志恢复技术。\n\n\n## [锁机制与InnoDB锁算法](https://snailclimb.gitee.io/javaguide/#/docs/database/MySQL?id=%E9%94%81%E6%9C%BA%E5%88%B6%E4%B8%8Einnodb%E9%94%81%E7%AE%97%E6%B3%95)\n\n\n**MyISAM和InnoDB存储引擎使用的锁：**\n\n- MyISAM采用表级锁(table-level locking)。\n- InnoDB支持行级锁(row-level locking)和表级锁,默认为行级锁\n\n**表级锁和行级锁对比：**\n\n- **表级锁：** MySQL中锁定 **粒度最大** 的一种锁，对当前操作的整张表加锁，实现简单，资源消耗也比较少，加锁快，不会出现死锁。其锁定粒度最大，触发锁冲突的概率最高，并发度最低，MyISAM和 InnoDB引擎都支持表级锁。\n- **行级锁：** MySQL中锁定 **粒度最小** 的一种锁，只针对当前操作的行进行加锁。 行级锁能大大减少数据库操作的冲突。其加锁粒度最小，并发度高，但加锁的开销也最大，加锁慢，会出现死锁。\n\n详细内容可以参考： MySQL锁机制简单了解一下：[https://blog.csdn.net/qq_34337272/article/details/80611486](https://blog.csdn.net/qq_34337272/article/details/80611486)\n\n\n### 行锁\n\n\n    A recordlockisalockonanindexrecord.Recordlocksalwayslockindexrecords, evenifatableisdefinedwithnoindexes.Forsuch cases,InnoDBcreates a hidden clusteredindexanduses thisindexforrecordlocking.\n\n\n上文出自MySQL的官方文档，从这里我们可以看出行锁是作用在索引上的，哪怕你在建表的时候没有定义一个索引，InnoDB也会创建一个聚簇索引并将其作为锁作用的索引。\n\n\n这里还是讲一下InnoDB中的聚簇索引。每一个InnoDB表都需要一个聚簇索引，有且只有一个。如果你为该表表定义一个主键，那么MySQL将使用主键作为聚簇索引；如果你不为定义一个主键，那么MySQL将会把第一个唯一索引(而且要求NOT NULL)作为聚簇索引；如果上诉两种情况都GG，那么MySQL将自动创建一个名字为 的隐藏聚簇索引。\n\n\n因为是聚簇索引，所以B+树上的叶子节点都存储了数据行，那么如果现在是二级索引呢？InnoDB中的二级索引的叶节点存储的是主键值(或者说聚簇索引的值)，所以通过二级索引查询数据时，还需要将对应的主键去聚簇索引中再次进行查询。\n\n\n关于索引的问题就到这，我们用一张直观的图来表示行锁：\n\n\n![](https://img-blog.csdnimg.cn/img_convert/63ebdaf075f09b127f9ca322af07fcab.png)\n\n\n接下来以两条SQL的执行为例，讲解一下InnoDB对于单行数据的加锁原理：\n\n\nupdateusersetage =10whereid=49;\n\n\nupdateusersetage =10wherename='Tom';\n\n\n第一条SQL使用主键查询，只需要在 id = 49 这个主键索引上加上锁。第二条 SQL 使用二级索引来查询，那么首先在 name = Tom 这个索引上加写锁，然后由于使用 InnoDB 二级索引还需再次根据主键索引查询，所以还需要在 id = 49 这个主键索引上加锁。\n\n\n也就是说使用主键索引需要加一把锁，使用二级索引需要在二级索引和主键索引上各加一把锁。\n\n\n根据索引对单行数据进行更新的加锁原理了解了，那如果更新操作涉及多个行呢，比如下面 SQL 的执行场景。\n\n\nupdateusersetage =10whereid>49;\n\n\n上述 SQL 的执行过程如下图所示。MySQL Server 会根据 WHERE 条件读取第一条满足条件的记录，然后 InnoDB 引擎会将第一条记录返回并加锁，接着 MySQL Server 发起更新改行记录的 UPDATE 请求，更新这条记录。一条记录操作完成，再读取下一条记录，直至没有匹配的记录为止。\n\n\n![](https://img-blog.csdnimg.cn/img_convert/39bea16e38602978f21fb5f6f50e92d4.png)\n\n\n**InnoDB存储引擎的锁的算法有三种：**\n\n- Record lock：单个行记录上的锁\n- Gap lock：间隙锁，锁定一个范围，不包括记录本身\n- Next-key lock：record+gap 锁定一个范围，包含记录本身\n\n**相关知识点：**\n\n1. innodb对于行的查询使用next-key lock\n2. Next-locking keying为了解决Phantom Problem幻读问题\n3. 当查询的索引含有唯一属性时，将next-key lock降级为record key\n4. Gap锁设计的目的是为了阻止多个事务将记录插入到同一范围内，而这会导致幻读问题的产生\n5. 有两种方式显式关闭gap锁：（除了外键约束和唯一性检查外，其余情况仅使用record lock） A. 将事务隔离级别设置为RC B. 将参数innodb_locks_unsafe_for_binlog设置为1\n\n### 表锁\n\n\n上面我们讲解行锁的时候，操作语句中的条件判断列都是有建立索引的，那么如果现在的判断列不存在索引呢？InnoDB既支持行锁，也支持表锁，当没有查询列没有索引时，InnoDB就不会去搞什么行锁了，毕竟行锁一定要有索引，所以它现在搞表锁，把整张表给锁住了。那么具体啥是表锁？还有其他什么情况下也会进行锁表呢？\n\n\n表锁使用的是一次性锁技术，也就是说，在会话开始的地方使用 lock 命令将后续需要用到的表都加上锁，在表释放前，只能访问这些加锁的表，不能访问其他表，直到最后通过 unlock tables 释放所有表锁。\n\n\n除了使用 unlock tables 显示释放锁之外，会话持有其他表锁时执行lock table 语句会释放会话之前持有的锁；会话持有其他表锁时执行 start transaction 或者 begin 开启事务时，也会释放之前持有的锁。\n\n\n![](https://img-blog.csdnimg.cn/img_convert/c0db3b3234e3875aaf94d6add3b14269.png)\n\n\n表锁由 MySQL Server 实现，行锁则是存储引擎实现，不同的引擎实现的不同。在 MySQL 的常用引擎中 InnoDB 支持行锁，而 MyISAM 则只能使用 MySQL Server 提供的表锁。\n\n\n## 日志\n\n\nMySQL中有以下日志文件，分别是：\n\n\n1：**重做日志（redo log）**\n\n\n2：**回滚日志（undo log）**\n\n\n3：**二进制日志（binlog）**\n\n\n4：**错误日志（errorlog）**\n\n\n5：**慢查询日志（slow query log）**\n\n\n6：**一般查询日志（general log）**\n\n\n7：**中继日志（relay log）。**\n\n\n其中重做日志和回滚日志与事务操作息息相关，二进制日志也与事务操作有一定的关系，这三种日志，对理解MySQL中的事务操作有着重要的意义。\n\n\n### 重做日志（redo log）\n\n\n**作用：**\n\n\n确保事务的持久性。redo日志记录事务执行后的状态，用来恢复未写入data file的已成功事务更新的数据。防止在发生故障的时间点，尚有脏页未写入磁盘，在重启[mysql](https://www.2cto.com/database/MySQL/)服务的时候，根据redo log进行重做，从而达到事务的持久性这一特性。\n\n\n**内容：**\n\n\n物理格式的日志，记录的是物理数据页面的修改的信息，其redo log是顺序写入redo log file的物理文件中去的。\n\n\n**什么时候产生：**\n\n\n事务开始之后就产生redo log，redo log的落盘并不是随着事务的提交才写入的，而是在事务的执行过程中，便开始写入redo log文件中。\n\n\n**什么时候释放：**\n\n\n当对应事务的脏页写入到磁盘之后，redo log的使命也就完成了，重做日志占用的空间就可以重用（被覆盖）。\n\n\n**对应的物理文件：**\n\n\n默认情况下，对应的物理文件位于[数据库](https://www.2cto.com/database/)的data目录下的ib_logfile1&ib_logfile2\n\n\ninnodb_log_group_home_dir 指定日志文件组所在的路径，默认./ ，表示在数据库的数据目录下。\n\n\ninnodb_log_files_in_group 指定重做日志文件组中文件的数量，默认2\n\n\n**关于文件的大小和数量，由以下两个参数配置：**\n\n\ninnodb_log_file_size 重做日志文件的大小。\n\n\ninnodb_mirrored_log_groups 指定了日志镜像文件组的数量，默认1\n\n\n**其他：**\n\n\n很重要一点，redo log是什么时候写盘的？前面说了是在事物开始之后逐步写盘的。\n\n\n之所以说重做日志是在事务开始之后逐步写入重做日志文件，而不一定是事务提交才写入重做日志缓存，原因就是，重做日志有一个缓存区Innodb_log_buffer，Innodb_log_buffer的默认大小为8M(这里设置的16M),Innodb存储引擎先将重做日志写入innodb_log_buffer中。\n\n\n![](https://www.2cto.com/uploadfile/Collfiles/20180313/20180313092754205.png)\n\n\n然后会通过以下三种方式将innodb日志缓冲区的日志刷新到磁盘\n\n\nMaster Thread 每秒一次执行刷新Innodb_log_buffer到重做日志文件。\n\n\n每个事务提交时会将重做日志刷新到重做日志文件。\n\n\n当重做日志缓存可用空间 少于一半时，重做日志缓存被刷新到重做日志文件\n\n\n由此可以看出，重做日志通过不止一种方式写入到磁盘，尤其是对于第一种方式，Innodb_log_buffer到重做日志文件是Master Thread线程的定时任务。\n\n\n因此重做日志的写盘，并不一定是随着事务的提交才写入重做日志文件的，而是随着事务的开始，逐步开始的。\n\n\n另外引用《MySQL技术内幕 Innodb 存储引擎》（page37）上的原话：\n\n\n即使某个事务还没有提交，Innodb存储引擎仍然每秒会将重做日志缓存刷新到重做日志文件。\n\n\n这一点是必须要知道的，因为这可以很好地解释再大的事务的提交（commit）的时间也是很短暂的。\n\n\n### 回滚日志（undo log）\n\n\n**作用：**\n\n\n保证数据的原子性，保存了事务发生之前的数据的一个版本，可以用于回滚，同时可以提供多版本并发控制下的读（MVCC），也即非锁定读\n\n\n**内容：**\n\n\n逻辑格式的日志，在执行undo的时候，仅仅是将数据从逻辑上恢复至事务之前的状态，而不是从物理页面上操作实现的，这一点是不同于redo log的。\n\n\n**什么时候产生：**\n\n\n事务开始之前，将当前是的版本生成undo log，undo 也会产生 redo 来保证undo log的可靠性\n\n\n**什么时候释放：**\n\n\n当事务提交之后，undo log并不能立马被删除，而是放入待清理的链表，由purge线程判断是否由其他事务在使用undo段中表的上一个事务之前的版本信息，决定是否可以清理undo log的日志空间。\n\n\n**对应的物理文件：**\n\n\nMySQL5.6之前，undo表空间位于共享表空间的回滚段中，共享表空间的默认的名称是ibdata，位于数据文件目录中。\n\n\nMySQL5.6之后，undo表空间可以配置成独立的文件，但是提前需要在配置文件中配置，完成数据库初始化后生效且不可改变undo log文件的个数\n\n\n如果初始化数据库之前没有进行相关配置，那么就无法配置成独立的表空间了。\n\n\n**关于MySQL5.7之后的独立undo 表空间配置参数如下：**\n\n\ninnodb_undo_directory = /data/un[dos](https://www.2cto.com/os/dos/)pace/ –undo独立表空间的存放目录 innodb_undo_logs = 128 –回滚段为128KB innodb_undo_tablespaces = 4 –指定有4个undo log文件\n\n\n如果undo使用的共享表空间，这个共享表空间中又不仅仅是存储了undo的信息，共享表空间的默认为与MySQL的数据目录下面，其属性由参数innodb_data_file_path配置。\n\n\n![](https://www.2cto.com/uploadfile/Collfiles/20180313/20180313092754206.png)\n\n\n**其他：**\n\n\nundo是在事务开始之前保存的被修改数据的一个版本，产生undo日志的时候，同样会伴随类似于保护事务持久化机制的redolog的产生。\n\n\n默认情况下undo文件是保持在共享表空间的，也即ibdatafile文件中，当数据库中发生一些大的事务性操作的时候，要生成大量的undo信息，全部保存在共享表空间中的。\n\n\n因此共享表空间可能会变的很大，默认情况下，也就是undo 日志使用共享表空间的时候，被“撑大”的共享表空间是不会也不能自动收缩的。\n\n\n因此，mysql5.7之后的“独立undo 表空间”的配置就显得很有必要了。\n\n\n### 二进制日志（binlog）\n\n\n**作用：**\n\n\n用于复制，在主从复制中，从库利用主库上的binlog进行重播，实现主从同步。\n\n\n用于数据库的基于时间点的还原。\n\n\n**内容：**\n\n\n逻辑格式的日志，可以简单认为就是执行过的事务中的sql语句。\n\n\n但又不完全是sql语句这么简单，而是包括了执行的sql语句（增删改）反向的信息，也就意味着delete对应着delete本身和其反向的insert；update对应着update执行前后的版本的信息；insert对应着delete和insert本身的信息。\n\n\n在使用mysqlbinlog解析binlog之后一些都会真相大白。\n\n\n因此可以基于binlog做到类似于oracle的闪回功能，其实都是依赖于binlog中的日志记录。\n\n\n**什么时候产生：**\n\n\n事务提交的时候，一次性将事务中的sql语句（一个事物可能对应多个sql语句）按照一定的格式记录到binlog中。\n\n\n这里与redo log很明显的差异就是redo log并不一定是在事务提交的时候刷新到磁盘，redo log是在事务开始之后就开始逐步写入磁盘。\n\n\n因此对于事务的提交，即便是较大的事务，提交（commit）都是很快的，但是在开启了bin_log的情况下，对于较大事务的提交，可能会变得比较慢一些。\n\n\n这是因为binlog是在事务提交的时候一次性写入的造成的，这些可以通过测试验证。\n\n\n**什么时候释放：**\n\n\nbinlog的默认是保持时间由参数expire_logs_days配置，也就是说对于非活动的日志文件，在生成时间超过expire_logs_days配置的天数之后，会被自动删除。\n\n\n![](https://www.2cto.com/uploadfile/Collfiles/20180313/20180313092754207.png)\n\n\n**对应的物理文件：**\n\n\n配置文件的路径为log_bin_basename，binlog日志文件按照指定大小，当日志文件达到指定的最大的大小之后，进行滚动更新，生成新的日志文件。\n\n\n对于每个binlog日志文件，通过一个统一的index文件来组织。\n\n\n![](https://www.2cto.com/uploadfile/Collfiles/20180313/20180313092754208.png)\n\n\n**其他：**\n\n\n二进制日志的作用之一是还原数据库的，这与redo log很类似，很多人混淆过，但是两者有本质的不同\n\n\n**作用不同**：redo log是保证事务的持久性的，是事务层面的，binlog作为还原的功能，是数据库层面的（当然也可以精确到事务层面的），虽然都有还原的意思，但是其保护数据的层次是不一样的。\n\n\n**内容不同**：redo log是物理日志，是数据页面的修改之后的物理记录，binlog是逻辑日志，可以简单认为记录的就是sql语句\n\n\n另外，两者日志产生的时间，可以释放的时间，在可释放的情况下清理机制，都是完全不同的。\n\n\n恢复数据时候的效率，基于物理日志的redo log恢复数据的效率要高于语句逻辑日志的binlog\n\n\n关于事务提交时，redo log和binlog的写入顺序，为了保证主从复制时候的主从一致（当然也包括使用binlog进行基于时间点还原的情况），是要严格一致的，MySQL通过两阶段提交过程来完成事务的一致性的，也即redo log和binlog的一致性的，理论上是先写redo log，再写binlog，两个日志都提交成功（刷入磁盘），事务才算真正的完成。\n\n\n### **错误日志**\n\n\n错误日志记录着mysqld启动和停止,以及服务器在运行过程中发生的错误的相关信息。在默认情况下，系统记录错误日志的功能是关闭的，错误信息被输出到标准错误输出。\n　　指定日志路径两种方法:\n　　　　编辑my.cnf 写入 log-error=[path]\n　　　　通过命令参数错误日志 mysqld_safe –user=mysql –log-error=[path] &\n\n\n显示错误日志的命令（如下图所示）\n\n\n![](https://img2018.cnblogs.com/blog/885859/201904/885859-20190418111428261-484993255.png)\n\n\n### **普通查询日志**\n\n\n记录了服务器接收到的每一个查询或是命令，无论这些查询或是命令是否正确甚至是否包含语法错误，general log 都会将其记录下来 ，记录的格式为 {Time ，Id ，Command，Argument }。也正因为mysql服务器需要不断地记录日志，开启General log会产生不小的系统开销。 因此，Mysql默认是把General log关闭的。\n\n\n查看日志的存放方式：show variables like ‘log_output’;\n\n\n![](https://blog-file.hehouhui.cn/885859-20190418111501042-772781517.png)\n\n\n如果设置mysql> set global log_output=’table’ 的话，则日志结果会记录到名为gengera_log的表中，这表的默认引擎都是CSV\n　　如果设置表数据到文件set global log_output=file;\n　　设置general log的日志文件路径：\n　　　　set global general_log_file=’/tmp/general.log’;\n　　　　开启general log： set global general_log=on;\n　　　　关闭general log： set global general_log=off;\n\n\n![](https://blog-file.hehouhui.cn/885859-20190418111501042-772781517-1645409187723.png)\n\n\n然后在用：show global variables like ‘general_log’\n\n\n![](https://blog-file.hehouhui.cn/885859-20190418111551171-1738333923.png)\n\n\n### 慢查询日志\n\n\n慢日志记录执行时间过长和没有使用索引的查询语句，报错select、update、delete以及insert语句，慢日志只会记录执行成功的语句。\n　　1. 查看慢查询时间：\n　　show variables like “long_query_time”;默认10s\n\n\n![](9)\n\n\n2. 查看慢查询配置情况：\n　　show status like “%slow_queries%”;\n\n\n![](https://blog-file.hehouhui.cn/885859-20190418111656450-521011638.png)\n\n\n3. 查看慢查询日志路径：\n　　show variables like “%slow%”;\n\n\n![](https://blog-file.hehouhui.cn/885859-20190418111712973-766266117.png)\n\n\n4. 开启慢日志\n\n\n![](https://blog-file.hehouhui.cn/885859-20190418111737882-1420825238.png)\n\n\n查看已经开启：\n\n\n![](https://blog-file.hehouhui.cn/885859-20190418111753391-1884429309.png)\n\n\n## 优化\n\n\n### 大表优化\n\n\n> 当MySQL单表记录数过大时，数据库的CRUD性能会明显下降，一些常见的优化措施如下：\n\n\n### [1. 限定数据的范围](https://snailclimb.gitee.io/javaguide/#/docs/database/MySQL?id=_1-%E9%99%90%E5%AE%9A%E6%95%B0%E6%8D%AE%E7%9A%84%E8%8C%83%E5%9B%B4)\n\n\n务必禁止不带任何限制数据范围条件的查询语句。比如：我们当用户在查询订单历史的时候，我们可以控制在一个月的范围内；\n\n\n### [2. 读/写分离](https://snailclimb.gitee.io/javaguide/#/docs/database/MySQL?id=_2-%E8%AF%BB%E5%86%99%E5%88%86%E7%A6%BB)\n\n\n经典的数据库拆分方案，主库负责写，从库负责读；\n\n\n### [3. 垂直分区](https://snailclimb.gitee.io/javaguide/#/docs/database/MySQL?id=_3-%E5%9E%82%E7%9B%B4%E5%88%86%E5%8C%BA)\n\n\n**根据数据库里面数据表的相关性进行拆分。** 例如，用户表中既有用户的登录信息又有用户的基本信息，可以将用户表拆分成两个单独的表，甚至放到单独的库做分库。\n\n\n**简单来说垂直拆分是指数据表列的拆分，把一张列比较多的表拆分为多张表。**\n\n\n如下图所示，这样来说大家应该就更容易理解了。\n\n\n![](https://my-blog-to-use.oss-cn-beijing.aliyuncs.com/2019-6/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%9E%82%E7%9B%B4%E5%88%86%E5%8C%BA.png)\n\n- **垂直拆分的优点：** 可以使得列数据变小，在查询时减少读取的Block数，减少I/O次数。此外，垂直分区可以简化表的结构，易于维护。\n- **垂直拆分的缺点：** 主键会出现冗余，需要管理冗余列，并会引起Join操作，可以通过在应用层进行Join来解决。此外，垂直分区会让事务变得更加复杂；\n\n### [4. 水平分区](https://snailclimb.gitee.io/javaguide/#/docs/database/MySQL?id=_4-%E6%B0%B4%E5%B9%B3%E5%88%86%E5%8C%BA)\n\n\n**保持数据表结构不变，通过某种策略存储数据分片。这样每一片数据分散到不同的表或者库中，达到了分布式的目的。 水平拆分可以支撑非常大的数据量。**\n\n\n水平拆分是指数据表行的拆分，表的行数超过200万行时，就会变慢，这时可以把一张的表的数据拆成多张表来存放。举个例子：我们可以将用户信息表拆分成多个用户信息表，这样就可以避免单一表数据量过大对性能造成影响。\n\n\n![](https://blog-file.hehouhui.cn/20220221101205.png)\n\n\n水平拆分可以支持非常大的数据量。需要注意的一点是：分表仅仅是解决了单一表数据过大的问题，但由于表的数据还是在同一台机器上，其实对于提升MySQL并发能力没有什么意义，所以 **水平拆分最好分库** 。\n\n\n水平拆分能够 **支持非常大的数据量存储，应用端改造也少**，但 **分片事务难以解决** ，跨节点Join性能较差，逻辑复杂。《Java工程师修炼之道》的作者推荐 **尽量不要对数据进行分片，因为拆分会带来逻辑、部署、运维的各种复杂度** ，一般的数据表在优化得当的情况下支撑千万以下的数据量是没有太大问题的。如果实在要分片，尽量选择客户端分片架构，这样可以减少一次和中间件的网络I/O。\n\n\n**补充一下数据库分片的两种常见方案：**\n\n- **客户端代理：** **分片逻辑在应用端，封装在jar包中，通过修改或者封装JDBC层来实现。** 当当网的 **Sharding-JDBC** 、阿里的TDDL是两种比较常用的实现。\n- **中间件代理：** **在应用和数据中间加了一个代理层。分片逻辑统一维护在中间件服务中。** 我们现在谈的 **Mycat** 、360的Atlas、网易的DDB等等都是这种架构的实现。\n",
      "properties": {
        "password": "",
        "icon": "",
        "date": "2022-02-21",
        "type": "Post",
        "category": "技术分享",
        "urlname": "38",
        "catalog": [
          "archives"
        ],
        "tags": [
          "mysql"
        ],
        "summary": "Mysql数据库存储引擎MyISAM引擎 不支持事务支持表级锁（MySql支持两种表级锁，表共享读锁和表独占写锁），但不支持行级锁存储表的总行数一个MyISAM表有三个文件：索引文件（.MYI",
        "sort": "",
        "title": "Mysql数据结构&锁",
        "status": "Published",
        "updated": "2023-10-08 14:42:00"
      },
      "catalog": [
        {
          "title": "archives",
          "doc_id": "397d5281-9f11-4c6f-8430-399176cc403d"
        }
      ],
      "body": "",
      "realName": "Mysql数据结构&锁",
      "relativePath": "/archives/Mysql数据结构&锁.md"
    }
  ],
  "catalog": [
    {
      "object": "page",
      "id": "f8b19b63-1f95-492f-afb0-70b7d69ca735",
      "created_time": "2023-09-15T07:15:00.000Z",
      "last_edited_time": "2023-10-08T06:40:00.000Z",
      "created_by": {
        "object": "user",
        "id": "a62f8c60-4c10-40c8-aa57-ef88b58f25a4"
      },
      "last_edited_by": {
        "object": "user",
        "id": "a62f8c60-4c10-40c8-aa57-ef88b58f25a4"
      },
      "cover": {
        "type": "external",
        "external": {
          "url": "https://cdn.jsdelivr.net/gh/listener-He/images@default/202309151532992.png"
        }
      },
      "icon": null,
      "parent": {
        "type": "database_id",
        "database_id": "e2aef2eb-e7a1-4fc0-94f9-a186b6be6efa"
      },
      "archived": false,
      "properties": {
        "password": "",
        "icon": "",
        "date": "2023-09-15",
        "type": "Post",
        "category": "技术分享",
        "urlname": "spring-boot-tenant-202309",
        "catalog": [
          "archives"
        ],
        "tags": [
          "Spring",
          "微服务",
          "SAAS"
        ],
        "summary": "常见的技术选择包括：\n• 数据库级多租户：使用多个数据库实例分别存储各个租户的数据，每个租户对应一个独立的数据库。\n• 模式级多租户：使用同一个数据库实例，但是为每个租户创建独立的数据库模式，实现数据隔离。\n• 表级多租户：在同一个数据库模式下，使用不同的表来存储各个租户的数据，实现数据隔离。\n• 应用级多租户：在应用程序中实现租户隔离，例如使用Spring Cloud等微服务框架。\n• 容器级多租户：使用容器技术，为每个租户创建独立的容器，实现资源隔离和控制。\n4 实现多租户架构的注意事项\n• 保证租户数据的隔离性和安全性。\n• 设计合理的租户数据结构和关系模型。\n• 统一管理租户的配置和权限。\n• 保证系统的可扩展性和可伸缩性，支持水平扩展。\n• 保证系统的高可用性和容错性，避免单点故障。\n• 对租户数据进行备份和恢复，保证数据的可靠性和完整性。",
        "sort": "",
        "title": "Spring Boot 实现多租户架构：支持应用多租户部署和管理",
        "status": "Published",
        "updated": "2023-10-08 14:40:00"
      },
      "url": "https://www.notion.so/Spring-Boot-f8b19b631f95492fafb070b7d69ca735",
      "public_url": "https://honesty-blog.notion.site/Spring-Boot-f8b19b631f95492fafb070b7d69ca735"
    },
    {
      "object": "page",
      "id": "3a5d9238-0fa8-4154-a6d5-92340f2ad733",
      "created_time": "2023-09-15T04:10:00.000Z",
      "last_edited_time": "2023-10-08T06:42:00.000Z",
      "created_by": {
        "object": "user",
        "id": "a62f8c60-4c10-40c8-aa57-ef88b58f25a4"
      },
      "last_edited_by": {
        "object": "user",
        "id": "a62f8c60-4c10-40c8-aa57-ef88b58f25a4"
      },
      "cover": {
        "type": "external",
        "external": {
          "url": "https://cdn.jsdelivr.net/gh/listener-He/images@default/202309151218999.png"
        }
      },
      "icon": null,
      "parent": {
        "type": "database_id",
        "database_id": "e2aef2eb-e7a1-4fc0-94f9-a186b6be6efa"
      },
      "archived": false,
      "properties": {
        "password": "",
        "icon": "",
        "date": "2023-09-15",
        "type": "Post",
        "category": "技术分享",
        "urlname": "springcloud-data-202309",
        "catalog": [
          "archives"
        ],
        "tags": [
          "Spring",
          "微服务"
        ],
        "summary": "微服务，顾名思义，就是将我们程序拆分为最小化单元来提供服务。在一体化系统中，各个微服务也是不可能独立存在的，那么微服务之间涉及到的数据依赖问题，应该怎么处理呢？我们从场景入手来分析考虑此类问题。",
        "sort": "",
        "title": "微服务之间的数据依赖问题，该如何解决？",
        "status": "Published",
        "updated": "2023-10-08 14:42:00"
      },
      "url": "https://www.notion.so/3a5d92380fa84154a6d592340f2ad733",
      "public_url": "https://honesty-blog.notion.site/3a5d92380fa84154a6d592340f2ad733"
    },
    {
      "object": "page",
      "id": "28f5e6a0-2d06-439d-b947-b7121480c146",
      "created_time": "2023-09-12T07:52:00.000Z",
      "last_edited_time": "2023-10-08T06:42:00.000Z",
      "created_by": {
        "object": "user",
        "id": "a62f8c60-4c10-40c8-aa57-ef88b58f25a4"
      },
      "last_edited_by": {
        "object": "user",
        "id": "a62f8c60-4c10-40c8-aa57-ef88b58f25a4"
      },
      "cover": {
        "type": "external",
        "external": {
          "url": "https://cdn.jsdelivr.net/gh/listener-He/images@default/202309121610208.png"
        }
      },
      "icon": null,
      "parent": {
        "type": "database_id",
        "database_id": "e2aef2eb-e7a1-4fc0-94f9-a186b6be6efa"
      },
      "archived": false,
      "properties": {
        "password": "",
        "icon": "",
        "date": "2023-09-12",
        "type": "Post",
        "category": "技术分享",
        "urlname": "redis-key-202309",
        "catalog": [
          "archives"
        ],
        "tags": [
          "Redis"
        ],
        "summary": "做一些C端业务，不可避免的要引入一级缓存来代替数据库的压力并且减少业务响应时间，其实每次引入一个中间件来解决问题的同时，必然会带来很多新的问题需要注意，比如缓存一致性问题。\n那么其实还会有一些其他问题比如使用Redis作为一级缓存时可能带来的热key、大key等问题，本文我们就热key（hot key）问题来讨论，如何合理的解决热key问题。",
        "sort": "",
        "title": "Redis 热key是什么问题，如何导致的？有什么解决方案？",
        "status": "Published",
        "updated": "2023-10-08 14:42:00"
      },
      "url": "https://www.notion.so/Redis-key-28f5e6a02d06439db947b7121480c146",
      "public_url": "https://honesty-blog.notion.site/Redis-key-28f5e6a02d06439db947b7121480c146"
    },
    {
      "object": "page",
      "id": "121dcfb7-7028-42d1-998e-b81fe6e0d870",
      "created_time": "2023-09-06T09:02:00.000Z",
      "last_edited_time": "2023-10-08T06:42:00.000Z",
      "created_by": {
        "object": "user",
        "id": "a62f8c60-4c10-40c8-aa57-ef88b58f25a4"
      },
      "last_edited_by": {
        "object": "user",
        "id": "a62f8c60-4c10-40c8-aa57-ef88b58f25a4"
      },
      "cover": {
        "type": "external",
        "external": {
          "url": "https://source.unsplash.com/random/720x480/?encryption"
        }
      },
      "icon": {
        "type": "emoji",
        "emoji": "™️"
      },
      "parent": {
        "type": "database_id",
        "database_id": "e2aef2eb-e7a1-4fc0-94f9-a186b6be6efa"
      },
      "archived": false,
      "properties": {
        "password": "",
        "icon": "",
        "date": "2023-09-06",
        "type": "Post",
        "category": "技术分享",
        "urlname": "spring-restclient-2023",
        "catalog": [
          "archives"
        ],
        "tags": [
          "Spring",
          "Java"
        ],
        "summary": "Spring 框架一直提供了两种不同的客户端来执行 http 请求:\n• RestTemplate: 它在 Spring 3 中被引入，提供同步的阻塞式通信。\n• WebClient: 它在 Spring 5 的 Spring WebFlux 库中作为一部分被发布。它提供了流式 API,遵循响应式模型。\nRestTemplate 的方法暴露了太多的 HTTP 特性,导致了大量重载的方法，使用成本较高。WebClient 是 RestTemplate 的替代品,支持同步和异步调用。它是 Spring Web Reactive 项目的一部分。\n现在 Spring 6.1 M1 版本引入了 RestClient。一个新的同步 http 客户端,其工作方式与 WebClient 类似,使用与 RestTemplate 相同的基础设施。",
        "sort": "",
        "title": "HttpClient? RestTemplate？WebClient? 不~是 RestClient",
        "status": "Published",
        "updated": "2023-10-08 14:42:00"
      },
      "url": "https://www.notion.so/HttpClient-RestTemplate-WebClient-RestClient-121dcfb7702842d1998eb81fe6e0d870",
      "public_url": "https://honesty-blog.notion.site/HttpClient-RestTemplate-WebClient-RestClient-121dcfb7702842d1998eb81fe6e0d870"
    },
    {
      "object": "page",
      "id": "613b560e-75d7-4f45-88e9-b487310cfe82",
      "created_time": "2023-06-02T07:11:00.000Z",
      "last_edited_time": "2023-10-08T06:42:00.000Z",
      "created_by": {
        "object": "user",
        "id": "a62f8c60-4c10-40c8-aa57-ef88b58f25a4"
      },
      "last_edited_by": {
        "object": "user",
        "id": "a62f8c60-4c10-40c8-aa57-ef88b58f25a4"
      },
      "cover": {
        "type": "external",
        "external": {
          "url": "https://cdn.jsdelivr.net/gh/listener-He/images@default/202306021513432.png"
        }
      },
      "icon": {
        "type": "emoji",
        "emoji": "☕"
      },
      "parent": {
        "type": "database_id",
        "database_id": "e2aef2eb-e7a1-4fc0-94f9-a186b6be6efa"
      },
      "archived": false,
      "properties": {
        "password": "",
        "icon": "",
        "date": "2023-06-02",
        "type": "Post",
        "category": "碎片杂文",
        "urlname": "lkcoffee-shanghai-coffee-activities-20230602",
        "catalog": [
          "archives"
        ],
        "tags": [
          "咖啡",
          "整活",
          "技术流"
        ],
        "summary": "最近瑞幸在搞活动，每天免费送10000份咖啡，我是个狂热喝咖啡的人儿，今天最后一天来整个活儿，点开瑞幸咖啡小程序主页，banner 栏轮播图中有一张海报入口，操作一通下来，果然，没抢到。\n手速不够快不是主要原因，手指操作延迟 + 系统页面跳转耗时加起来到 http 发出就已经耽误了1 -2 秒钟了，这个时间才是关键，本文从技术角度探讨下怎么在最小成本比如几分钟内，实现一个小工具，来解决这个问题。",
        "sort": "",
        "title": "上海咖啡文化周之薅瑞幸羊毛",
        "status": "Published",
        "updated": "2023-10-08 14:42:00"
      },
      "url": "https://www.notion.so/613b560e75d74f4588e9b487310cfe82",
      "public_url": "https://honesty-blog.notion.site/613b560e75d74f4588e9b487310cfe82"
    },
    {
      "object": "page",
      "id": "d8051be9-7bd7-4b59-a482-f05b9fb15699",
      "created_time": "2023-05-23T04:03:00.000Z",
      "last_edited_time": "2023-10-08T06:42:00.000Z",
      "created_by": {
        "object": "user",
        "id": "a62f8c60-4c10-40c8-aa57-ef88b58f25a4"
      },
      "last_edited_by": {
        "object": "user",
        "id": "a62f8c60-4c10-40c8-aa57-ef88b58f25a4"
      },
      "cover": {
        "type": "external",
        "external": {
          "url": "https://plus.hutool.cn/images/logo.jpg"
        }
      },
      "icon": null,
      "parent": {
        "type": "database_id",
        "database_id": "e2aef2eb-e7a1-4fc0-94f9-a186b6be6efa"
      },
      "archived": false,
      "properties": {
        "password": "",
        "icon": "",
        "date": "2023-05-23",
        "type": "Post",
        "category": "技术分享",
        "urlname": "hutool-beanutil-error",
        "catalog": [
          "archives"
        ],
        "tags": [
          "BUG",
          "Java",
          "hutool"
        ],
        "summary": "Hutool是一个小而全的Java工具类库，通过静态方法封装，降低相关API的学习成本，提高工作效率，使Java拥有函数式语言般的优雅，让Java语言也可以“甜甜的”。 JavaBean是一个拥有对属性进行set和get方法的类。它可以被简单地定义为包含setXXX和getXXX方法的对象。在Hutool中，判定Bean的方法为：是否存在只有一个参数的setXXX方法。Bean工具类主要操作setXXX和getXXX方法，如将Bean对象转为Map等。",
        "sort": "",
        "title": "Hutool 5.8.8  BeanUtil.copyProperties 致命异常",
        "status": "Published",
        "updated": "2023-10-08 14:42:00"
      },
      "url": "https://www.notion.so/Hutool-5-8-8-BeanUtil-copyProperties-d8051be97bd74b59a482f05b9fb15699",
      "public_url": "https://honesty-blog.notion.site/Hutool-5-8-8-BeanUtil-copyProperties-d8051be97bd74b59a482f05b9fb15699"
    },
    {
      "object": "page",
      "id": "50c530fc-8d3c-4f6b-9956-3be2602e1be8",
      "created_time": "2023-04-28T05:50:00.000Z",
      "last_edited_time": "2023-10-08T06:42:00.000Z",
      "created_by": {
        "object": "user",
        "id": "a62f8c60-4c10-40c8-aa57-ef88b58f25a4"
      },
      "last_edited_by": {
        "object": "user",
        "id": "a62f8c60-4c10-40c8-aa57-ef88b58f25a4"
      },
      "cover": {
        "type": "external",
        "external": {
          "url": "https://static.effie.co/blog/2021/05/20210524053542357-1024x796.png?x-oss-process=image/auto-orient,1/quality,q_90/format,webp"
        }
      },
      "icon": {
        "type": "emoji",
        "emoji": "✍️"
      },
      "parent": {
        "type": "database_id",
        "database_id": "e2aef2eb-e7a1-4fc0-94f9-a186b6be6efa"
      },
      "archived": false,
      "properties": {
        "password": "",
        "icon": "",
        "date": "2023-04-21",
        "type": "Post",
        "category": "学习思考",
        "urlname": "efficient-learning-methods-pq4r",
        "catalog": [
          "archives"
        ],
        "tags": [
          "文字",
          "思考",
          "学习"
        ],
        "summary": "在 21 世纪的今天，我们每天都要面临大量的知识和信息。我们每个人都需要不断学习新知识、新思想和进行新的实践。\n这意味着，学习不再是学生阶段才需要做的事情，终身教育应该成为我们工作和生活的有机组成部分。\n在自我学习的过程中，很多人都会尝试寻找高效的学习方法，以便提升自己的学习能力和学习效率。\n接下来我分享的学习方法，不仅是我自己切身实践有效，也有科学依据作为支撑。",
        "sort": "",
        "title": "你有什么值得分享的高效学习方法？",
        "status": "Published",
        "updated": "2023-10-08 14:42:00"
      },
      "url": "https://www.notion.so/50c530fc8d3c4f6b99563be2602e1be8",
      "public_url": "https://honesty-blog.notion.site/50c530fc8d3c4f6b99563be2602e1be8"
    },
    {
      "object": "page",
      "id": "8900fb84-5f49-4f66-a5fa-fd34b7fd832a",
      "created_time": "2023-04-28T05:45:00.000Z",
      "last_edited_time": "2023-10-08T06:42:00.000Z",
      "created_by": {
        "object": "user",
        "id": "a62f8c60-4c10-40c8-aa57-ef88b58f25a4"
      },
      "last_edited_by": {
        "object": "user",
        "id": "a62f8c60-4c10-40c8-aa57-ef88b58f25a4"
      },
      "cover": {
        "type": "external",
        "external": {
          "url": "https://static.effie.co/blog/2021/08/20210816143032448-1024x512.png?x-oss-process=image/auto-orient,1/quality,q_90/format,webp"
        }
      },
      "icon": {
        "type": "emoji",
        "emoji": "📖"
      },
      "parent": {
        "type": "database_id",
        "database_id": "e2aef2eb-e7a1-4fc0-94f9-a186b6be6efa"
      },
      "archived": false,
      "properties": {
        "password": "",
        "icon": "",
        "date": "2023-04-25",
        "type": "Post",
        "category": "学习思考",
        "urlname": "active-reading-becoming-better-reader",
        "catalog": [
          "archives"
        ],
        "tags": [
          "文字",
          "思考",
          "学习"
        ],
        "summary": "主动阅读是一种积极、深入的阅读方式，它需要读者在阅读过程中利用多种策略，如提问、概括、笔记、推理等，不仅理解文字表意，还要加深对其背后涵义的理解和记忆，同时，将自己的经验和知识运用到阅读中去，以达到更好的阅读效果。主动阅读能够提高我们的阅读能力和思维水平，培养我们批判性地思考和分析问题的能力，帮助我们更好地掌握知识和信息。",
        "sort": "",
        "title": "主动阅读：成为更好的阅读者",
        "status": "Published",
        "updated": "2023-10-08 14:42:00"
      },
      "url": "https://www.notion.so/8900fb845f494f66a5fafd34b7fd832a",
      "public_url": "https://honesty-blog.notion.site/8900fb845f494f66a5fafd34b7fd832a"
    },
    {
      "object": "page",
      "id": "3a77d2b4-f1df-48cc-bdc1-3192e46e674c",
      "created_time": "2023-04-28T05:26:00.000Z",
      "last_edited_time": "2023-10-08T06:42:00.000Z",
      "created_by": {
        "object": "user",
        "id": "a62f8c60-4c10-40c8-aa57-ef88b58f25a4"
      },
      "last_edited_by": {
        "object": "user",
        "id": "a62f8c60-4c10-40c8-aa57-ef88b58f25a4"
      },
      "cover": {
        "type": "external",
        "external": {
          "url": "https://developers.redhat.com/sites/default/files/styles/article_feature/public/blog/2019/12/keycloak10.png?itok=-ExiELH9"
        }
      },
      "icon": {
        "type": "external",
        "external": {
          "url": "https://www.notion.so/icons/lock-keyhole_blue.svg"
        }
      },
      "parent": {
        "type": "database_id",
        "database_id": "e2aef2eb-e7a1-4fc0-94f9-a186b6be6efa"
      },
      "archived": false,
      "properties": {
        "password": "",
        "icon": "",
        "date": "2023-04-28",
        "type": "Post",
        "category": "技术分享",
        "urlname": "api-login-and-jwt-token-generation-using-keycloak",
        "catalog": [
          "archives"
        ],
        "tags": [
          "开发",
          "建站",
          "Java",
          "keycloak",
          "oauth"
        ],
        "summary": "Red Hat SSO (或Keycloak)是领先的Web SSO产品之一，支持SAML 2.0、OpenID Connect和OAuth 2.0等标准，强大之处在于可通过多种方式直接访问Keycloak，包括API调用生成和验证JWT令牌。操作仅限API调用，无需暴露Keycloak的UI给公众。",
        "sort": "",
        "title": "使用 Keycloak 的 API 登录和 JWT 令牌生成",
        "status": "Published",
        "updated": "2023-10-08 14:42:00"
      },
      "url": "https://www.notion.so/Keycloak-API-JWT-3a77d2b4f1df48ccbdc13192e46e674c",
      "public_url": "https://honesty-blog.notion.site/Keycloak-API-JWT-3a77d2b4f1df48ccbdc13192e46e674c"
    },
    {
      "object": "page",
      "id": "7d7311ba-8ed2-4a90-91ac-5d2b4a503c4c",
      "created_time": "2023-04-28T05:06:00.000Z",
      "last_edited_time": "2023-10-08T06:42:00.000Z",
      "created_by": {
        "object": "user",
        "id": "a62f8c60-4c10-40c8-aa57-ef88b58f25a4"
      },
      "last_edited_by": {
        "object": "user",
        "id": "a62f8c60-4c10-40c8-aa57-ef88b58f25a4"
      },
      "cover": {
        "type": "external",
        "external": {
          "url": "https://www.keycloak.org/docs/latest/authorization_services/images/getting-started/hello-world/create-realm.png"
        }
      },
      "icon": {
        "type": "external",
        "external": {
          "url": "https://www.notion.so/icons/lock-keyhole_blue.svg"
        }
      },
      "parent": {
        "type": "database_id",
        "database_id": "e2aef2eb-e7a1-4fc0-94f9-a186b6be6efa"
      },
      "archived": false,
      "properties": {
        "password": "",
        "icon": "",
        "date": "2023-04-28T13:13:00.000+08:00",
        "type": "Post",
        "category": "技术分享",
        "urlname": "authentication-and-authorization-using-the-keycloak-rest-api",
        "catalog": [
          "archives"
        ],
        "tags": [
          "开发",
          "建站",
          "Java",
          "oauth",
          "keycloak"
        ],
        "summary": "Keycloak REST API 可以被用来进行身份验证和授权。使用该API，开发人员可以轻松地在其应用程序中实现安全性验证和授权功能，同时使用Keycloak的内置功能进行管理和配置。Keycloak的REST API还提供了许多不同的终端点来进行用户和角色管理、认证事件和SAML元数据的访问等。 作为一个基于开源的身份和访问管理解决方案，Keycloak的REST API对于任何需要对应用程序进行认证和授权的开发人员都非常有帮助。",
        "sort": "",
        "title": "使用 Keycloak REST API 进行身份验证和授权",
        "status": "Published",
        "updated": "2023-10-08 14:42:00"
      },
      "url": "https://www.notion.so/Keycloak-REST-API-7d7311ba8ed24a9091ac5d2b4a503c4c",
      "public_url": "https://honesty-blog.notion.site/Keycloak-REST-API-7d7311ba8ed24a9091ac5d2b4a503c4c"
    },
    {
      "object": "page",
      "id": "e454567c-abfe-4f35-9826-05e8eff97c1f",
      "created_time": "2023-04-27T11:24:00.000Z",
      "last_edited_time": "2023-10-08T06:42:00.000Z",
      "created_by": {
        "object": "user",
        "id": "a62f8c60-4c10-40c8-aa57-ef88b58f25a4"
      },
      "last_edited_by": {
        "object": "user",
        "id": "a62f8c60-4c10-40c8-aa57-ef88b58f25a4"
      },
      "cover": {
        "type": "external",
        "external": {
          "url": "https://cdn.jsdelivr.net/gh/listener-He/images@default/202305102204351.png"
        }
      },
      "icon": {
        "type": "emoji",
        "emoji": "🗒️"
      },
      "parent": {
        "type": "database_id",
        "database_id": "e2aef2eb-e7a1-4fc0-94f9-a186b6be6efa"
      },
      "archived": false,
      "properties": {
        "password": "",
        "icon": "",
        "date": "2023-04-27",
        "type": "Post",
        "category": "学习思考",
        "urlname": "iot-prospect-for-java",
        "catalog": [
          "archives"
        ],
        "tags": [
          "思考",
          "物联网"
        ],
        "summary": "物联网的应用场景非常广泛，例如智能家居、智慧城市、智能医疗、智能制造等。智能家居可以通过物联网技术实现家电之间的互联互通，例如智能门锁、智能音响、智能家电等，用户可以通过手机APP或者语音控制完成对家中设备的操作。智慧城市可以通过物联网技术实现城市基础设施之间的互联互通和数据共享，例如智能交通、智能停车、智能照明等，实现城市的智能化管理和优化。智能医疗可以通过物联网技术实现医疗设备之间的互联互通和数据共享，例如智能健康监测设备、智能手环、智能康复仪等，可以有效地提高医疗设备的使用效率和医疗服务的质量。智能制造可以通过物联网技术实现工业设备之间的互联互通和数据共享，例如智能机床、智能物流、智能质检等，可以提高生产线的自动化程度和生产效率。",
        "sort": "",
        "title": "物联网发展及前景如何？物联网开发有哪些板块？java 如何驱动万物互联？",
        "status": "Published",
        "updated": "2023-10-08 14:42:00"
      },
      "url": "https://www.notion.so/java-e454567cabfe4f35982605e8eff97c1f",
      "public_url": "https://honesty-blog.notion.site/java-e454567cabfe4f35982605e8eff97c1f"
    },
    {
      "object": "page",
      "id": "4f23b21c-584b-4922-a7dc-1fa3e68e609f",
      "created_time": "2023-04-24T09:01:00.000Z",
      "last_edited_time": "2023-10-08T06:42:00.000Z",
      "created_by": {
        "object": "user",
        "id": "a62f8c60-4c10-40c8-aa57-ef88b58f25a4"
      },
      "last_edited_by": {
        "object": "user",
        "id": "a62f8c60-4c10-40c8-aa57-ef88b58f25a4"
      },
      "cover": {
        "type": "file",
        "file": {
          "url": "https://s3.us-west-2.amazonaws.com/secure.notion-static.com/bd13b709-62b4-45ef-bdf2-cb1a84350da2/WX20230424-180802.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20231031%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20231031T090551Z&X-Amz-Expires=3600&X-Amz-Signature=7112ee3f4c6b8625857badaac59d4a0d8684e84f5b2f06e9fbcdf53e0ab3e839&X-Amz-SignedHeaders=host&x-id=GetObject",
          "expiry_time": "2023-10-31T10:05:51.029Z"
        }
      },
      "icon": {
        "type": "emoji",
        "emoji": "🗒️"
      },
      "parent": {
        "type": "database_id",
        "database_id": "e2aef2eb-e7a1-4fc0-94f9-a186b6be6efa"
      },
      "archived": false,
      "properties": {
        "password": "",
        "icon": "",
        "date": "2023-04-24T00:00:00.000+08:00",
        "type": "Post",
        "category": "创作分享",
        "urlname": "notion-ai-writely",
        "catalog": [
          "archives"
        ],
        "tags": [
          "文字",
          "工具",
          "chatgpt",
          "notion"
        ],
        "summary": "作为NiotionAi的重度依赖者，最近观察到一个比NotionAI更强大而且还免费的插件。一款基于chatGPT技术的写作神器Writely。借助自然语言处理技术，Writely可以自动生成、推荐内容，并进行语法纠错优化。同时提供多种模板和主题，让写作更加高效实用。该工具适合初学者和专业写作者使用",
        "sort": "",
        "title": "Notion AI平替 Writely 基于chatGPT免费实现的写作神器",
        "status": "Published",
        "updated": "2023-10-08 14:42:00"
      },
      "url": "https://www.notion.so/Notion-AI-Writely-chatGPT-4f23b21c584b4922a7dc1fa3e68e609f",
      "public_url": "https://honesty-blog.notion.site/Notion-AI-Writely-chatGPT-4f23b21c584b4922a7dc1fa3e68e609f"
    },
    {
      "object": "page",
      "id": "387ccffe-e894-442f-9471-2460619d4e62",
      "created_time": "2023-04-15T14:42:00.000Z",
      "last_edited_time": "2023-10-08T06:42:00.000Z",
      "created_by": {
        "object": "user",
        "id": "a62f8c60-4c10-40c8-aa57-ef88b58f25a4"
      },
      "last_edited_by": {
        "object": "user",
        "id": "a62f8c60-4c10-40c8-aa57-ef88b58f25a4"
      },
      "cover": {
        "type": "external",
        "external": {
          "url": "https://cdn.jsdelivr.net/gh/listener-He/images@default/202305102210908.png"
        }
      },
      "icon": {
        "type": "emoji",
        "emoji": "🗒️"
      },
      "parent": {
        "type": "database_id",
        "database_id": "e2aef2eb-e7a1-4fc0-94f9-a186b6be6efa"
      },
      "archived": false,
      "properties": {
        "password": "",
        "icon": "",
        "date": "2023-04-15",
        "type": "Post",
        "category": "技术分享",
        "urlname": "redis-01",
        "catalog": [
          "archives"
        ],
        "tags": [
          "开发",
          "微服务",
          "分布式",
          "Redis",
          "缓存"
        ],
        "summary": "Redis是一种高性能的key-value存储系统，具有以下几个特点：\n1. 内存存储：Redis将数据存储在内存中，因此读写速度非常快，通常比基于磁盘的存储系统快几个数量级。\n2. 数据结构多样：Redis支持多种数据结构，包括字符串、哈希、列表、集合、有序集合等，丰富的数据结构使Redis可以支持更多的应用场景。\n3. 持久化：Redis支持将数据持久化到磁盘上，以保证数据的可靠性。\n4. 高并发性：Redis采用单线程模型，减少了多线程之间的竞争，从而使得Redis具有更高的并发性。\n5. 分布式：Redis提供了集群模式，可以将数据分布到不同的节点上，从而实现水平扩展。",
        "sort": "",
        "title": "Redis 入手知识点",
        "status": "Published",
        "updated": "2023-10-08 14:42:00"
      },
      "url": "https://www.notion.so/Redis-387ccffee894442f94712460619d4e62",
      "public_url": "https://honesty-blog.notion.site/Redis-387ccffee894442f94712460619d4e62"
    },
    {
      "object": "page",
      "id": "921e862a-78e5-4248-97a1-0fc04c2c558b",
      "created_time": "2023-03-30T06:41:00.000Z",
      "last_edited_time": "2023-10-08T06:42:00.000Z",
      "created_by": {
        "object": "user",
        "id": "a62f8c60-4c10-40c8-aa57-ef88b58f25a4"
      },
      "last_edited_by": {
        "object": "user",
        "id": "a62f8c60-4c10-40c8-aa57-ef88b58f25a4"
      },
      "cover": {
        "type": "external",
        "external": {
          "url": "https://www.notion.so/images/page-cover/nasa_fingerprints_of_water_on_the_sand.jpg"
        }
      },
      "icon": null,
      "parent": {
        "type": "database_id",
        "database_id": "e2aef2eb-e7a1-4fc0-94f9-a186b6be6efa"
      },
      "archived": false,
      "properties": {
        "password": "",
        "icon": "",
        "date": "2020-01-22",
        "type": "Post",
        "category": "技术分享",
        "urlname": "45",
        "catalog": [
          "archives"
        ],
        "tags": [
          "Java"
        ],
        "summary": "String对象常量池String 对象的两种创建方式：String str1 = \"abcd\";//先检查字符串常量池中有没有\"abcd\"，如果字符串常量池中",
        "sort": "",
        "title": "Java基础-String",
        "status": "Published",
        "updated": "2023-10-08 14:42:00"
      },
      "url": "https://www.notion.so/Java-String-921e862a78e5424897a10fc04c2c558b",
      "public_url": "https://honesty-blog.notion.site/Java-String-921e862a78e5424897a10fc04c2c558b"
    },
    {
      "object": "page",
      "id": "f9af0c25-a672-4cac-bc8e-43bb190216fb",
      "created_time": "2023-03-30T06:40:00.000Z",
      "last_edited_time": "2023-10-08T06:42:00.000Z",
      "created_by": {
        "object": "user",
        "id": "a62f8c60-4c10-40c8-aa57-ef88b58f25a4"
      },
      "last_edited_by": {
        "object": "user",
        "id": "a62f8c60-4c10-40c8-aa57-ef88b58f25a4"
      },
      "cover": {
        "type": "external",
        "external": {
          "url": "https://www.notion.so/images/page-cover/woodcuts_11.jpg"
        }
      },
      "icon": null,
      "parent": {
        "type": "database_id",
        "database_id": "e2aef2eb-e7a1-4fc0-94f9-a186b6be6efa"
      },
      "archived": false,
      "properties": {
        "password": "",
        "icon": "",
        "date": "2020-01-26",
        "type": "Post",
        "category": "技术分享",
        "urlname": "43",
        "catalog": [
          "archives"
        ],
        "tags": [
          "Java"
        ],
        "summary": "对象，类类加载一个类的完整生命周期如下：Class 文件需要加载到虚拟机中之后才能运行和使用，那么虚拟机是如何加载这些 Class 文件呢？系统加载 Class 类型的文件主要三步:加载->连接",
        "sort": "",
        "title": "Java基础-class",
        "status": "Published",
        "updated": "2023-10-08 14:42:00"
      },
      "url": "https://www.notion.so/Java-class-f9af0c25a6724cacbc8e43bb190216fb",
      "public_url": "https://honesty-blog.notion.site/Java-class-f9af0c25a6724cacbc8e43bb190216fb"
    },
    {
      "object": "page",
      "id": "54c4aad1-0a7a-4a4b-9906-1f54a3810362",
      "created_time": "2023-03-30T06:38:00.000Z",
      "last_edited_time": "2023-10-08T06:42:00.000Z",
      "created_by": {
        "object": "user",
        "id": "a62f8c60-4c10-40c8-aa57-ef88b58f25a4"
      },
      "last_edited_by": {
        "object": "user",
        "id": "a62f8c60-4c10-40c8-aa57-ef88b58f25a4"
      },
      "cover": {
        "type": "external",
        "external": {
          "url": "https://www.notion.so/images/page-cover/webb1.jpg"
        }
      },
      "icon": null,
      "parent": {
        "type": "database_id",
        "database_id": "e2aef2eb-e7a1-4fc0-94f9-a186b6be6efa"
      },
      "archived": false,
      "properties": {
        "password": "",
        "icon": "",
        "date": "2020-02-25",
        "type": "Post",
        "category": "技术分享",
        "urlname": "22",
        "catalog": [
          "archives"
        ],
        "tags": [
          "Java",
          "异步编程",
          "多线程"
        ],
        "summary": "线程与进程相似，但线程是一个比进程更小的执行单位。一个进程在其执行的过程中可以产生多个线程。与进程不同的是同类的多个线程共享同一块内存空间和一组系统资源，所以系统在产生一个线程，或是在各个线程之间作切换工作时，负担要比进程小得多，也正因为如此，线程也被称为轻量级进程。",
        "sort": "",
        "title": "Java基础-线程&并发",
        "status": "Published",
        "updated": "2023-10-08 14:42:00"
      },
      "url": "https://www.notion.so/Java-54c4aad10a7a4a4b99061f54a3810362",
      "public_url": "https://honesty-blog.notion.site/Java-54c4aad10a7a4a4b99061f54a3810362"
    },
    {
      "object": "page",
      "id": "ba68c9c4-7d5d-4640-80d9-c123616ea287",
      "created_time": "2023-03-30T06:37:00.000Z",
      "last_edited_time": "2023-10-08T06:42:00.000Z",
      "created_by": {
        "object": "user",
        "id": "a62f8c60-4c10-40c8-aa57-ef88b58f25a4"
      },
      "last_edited_by": {
        "object": "user",
        "id": "a62f8c60-4c10-40c8-aa57-ef88b58f25a4"
      },
      "cover": null,
      "icon": null,
      "parent": {
        "type": "database_id",
        "database_id": "e2aef2eb-e7a1-4fc0-94f9-a186b6be6efa"
      },
      "archived": false,
      "properties": {
        "password": "",
        "icon": "",
        "date": "2020-03-08",
        "type": "Post",
        "category": "技术分享",
        "urlname": "44",
        "catalog": [
          "archives"
        ],
        "tags": [
          "Java",
          "数据结构"
        ],
        "summary": "这篇文章讲解了Java集合中的ArrayList，它是一种动态数组，可以快速随机访问。相比于Vector，ArrayList不是线程安全的，但是LinkedList则不支持高效的随机元素访问。此外，ArrayList实现了多个接口，包括List、RandomAccess、Cloneable和Serializable。因此，ArrayList在实际应用中非常常见。\n除了ArrayList，文章还介绍了Java中的其他集合类型，如LinkedList和HashMap。LinkedList是一种双向链表，可以在任意位置进行插入和删除操作，但是不支持随机访问。而HashMap是一种基于哈希表实现的Map，可以用于存储键值对。HashMap使用数组和链表结合的方式，来解决哈希冲突的问题，JDK1.8之后还引入了红黑树来优化性能。\n文章还介绍了一些Java集合中的底层实现细节，例如HashMap中的扰动函数、loadFactor加载因子、threshold临界值等。这些细节对于理解集合的工作原理非常重要，也可以帮助我们更好地使用Java集合。\n总之，Java集合是Java编程中非常常用的一部分，掌握好集合的使用方法和底层实现细节，可以帮助我们编写更高效、更易维护的Java代码。",
        "sort": "",
        "title": "Java基础-集合",
        "status": "Published",
        "updated": "2023-10-08 14:42:00"
      },
      "url": "https://www.notion.so/Java-ba68c9c47d5d464080d9c123616ea287",
      "public_url": "https://honesty-blog.notion.site/Java-ba68c9c47d5d464080d9c123616ea287"
    },
    {
      "object": "page",
      "id": "f2eefe59-c1ef-4dba-847d-6525d1ddc518",
      "created_time": "2023-03-30T06:35:00.000Z",
      "last_edited_time": "2023-10-08T06:42:00.000Z",
      "created_by": {
        "object": "user",
        "id": "a62f8c60-4c10-40c8-aa57-ef88b58f25a4"
      },
      "last_edited_by": {
        "object": "user",
        "id": "a62f8c60-4c10-40c8-aa57-ef88b58f25a4"
      },
      "cover": {
        "type": "external",
        "external": {
          "url": "https://cdn.jsdelivr.net/gh/listener-He/images@default/202305102223244.png"
        }
      },
      "icon": null,
      "parent": {
        "type": "database_id",
        "database_id": "e2aef2eb-e7a1-4fc0-94f9-a186b6be6efa"
      },
      "archived": false,
      "properties": {
        "password": "",
        "icon": "",
        "date": "2020-03-12",
        "type": "Post",
        "category": "技术分享",
        "urlname": "42",
        "catalog": [
          "archives"
        ],
        "tags": [
          "Java",
          "Jvm"
        ],
        "summary": "Java基础-JVM是Java开发者必须要掌握的重要知识点之一，JVM全称为Java Virtual Machine（Java虚拟机），它是Java程序运行的环境，在Java编程中具有极其重要的作用。\n作为Java语言的核心，JVM能够通过将Java代码编译为字节码，再通过字节码的解释器实现Java程序的运行。JVM负责管理应用程序的内存、多线程、垃圾回收等操作，是实现跨平台、自动垃圾回收、安全性高等特性的关键。\n要深入理解JVM，需要掌握JVM的内部机制，包括虚拟机类加载器、运行时数据区域、字节码执行引擎等方面。此外，对于JVM的性能调优及故障排查也非常重要，能够帮助开发者优化应用程序的运行效率。\n总之，掌握Java基础-JVM是Java开发者必须要具备的知识技能，它可以帮助你更好地理解Java程序的运行机制，提高开发效率，提升应用程序的性能以及稳定性。",
        "sort": "",
        "title": "Java基础-JVM",
        "status": "Published",
        "updated": "2023-10-08 14:42:00"
      },
      "url": "https://www.notion.so/Java-JVM-f2eefe59c1ef4dba847d6525d1ddc518",
      "public_url": "https://honesty-blog.notion.site/Java-JVM-f2eefe59c1ef4dba847d6525d1ddc518"
    },
    {
      "object": "page",
      "id": "bcd4f723-5f6a-4d94-90a1-63bb8c6dbfcf",
      "created_time": "2023-03-30T06:32:00.000Z",
      "last_edited_time": "2023-10-08T06:42:00.000Z",
      "created_by": {
        "object": "user",
        "id": "a62f8c60-4c10-40c8-aa57-ef88b58f25a4"
      },
      "last_edited_by": {
        "object": "user",
        "id": "a62f8c60-4c10-40c8-aa57-ef88b58f25a4"
      },
      "cover": {
        "type": "external",
        "external": {
          "url": "https://cdn.jsdelivr.net/gh/listener-He/images@default/202305102220554.png"
        }
      },
      "icon": null,
      "parent": {
        "type": "database_id",
        "database_id": "e2aef2eb-e7a1-4fc0-94f9-a186b6be6efa"
      },
      "archived": false,
      "properties": {
        "password": "",
        "icon": "",
        "date": "2020-06-02",
        "type": "Post",
        "category": "技术分享",
        "urlname": "18",
        "catalog": [
          "archives"
        ],
        "tags": [
          "Spring",
          "缓存"
        ],
        "summary": "Spring Cache是Spring框架用于支持缓存的模块。它提供了一组缓存抽象，使得我们可以将不同的缓存技术集成到应用程序中，从而提高性能和可扩展性。Spring Cache通过使用轻量级的注释来定义缓存的行为，从而减少了缓存操作的复杂性。此外，Spring Cache还支持事务性缓存，这样可以保证缓存与数据库之间的一致性。总体来说，Spring Cache是一个强大的工具，可以极大地提高应用程序的性能和可用性。",
        "sort": "",
        "title": "spring cache",
        "status": "Published",
        "updated": "2023-10-08 14:42:00"
      },
      "url": "https://www.notion.so/spring-cache-bcd4f7235f6a4d9490a163bb8c6dbfcf",
      "public_url": "https://honesty-blog.notion.site/spring-cache-bcd4f7235f6a4d9490a163bb8c6dbfcf"
    },
    {
      "object": "page",
      "id": "76be662e-68e2-4e75-8d51-c66197e850d3",
      "created_time": "2023-03-30T06:30:00.000Z",
      "last_edited_time": "2023-10-08T06:42:00.000Z",
      "created_by": {
        "object": "user",
        "id": "a62f8c60-4c10-40c8-aa57-ef88b58f25a4"
      },
      "last_edited_by": {
        "object": "user",
        "id": "a62f8c60-4c10-40c8-aa57-ef88b58f25a4"
      },
      "cover": {
        "type": "external",
        "external": {
          "url": "https://cdn.jsdelivr.net/gh/listener-He/images@default/202305102218981.png"
        }
      },
      "icon": null,
      "parent": {
        "type": "database_id",
        "database_id": "e2aef2eb-e7a1-4fc0-94f9-a186b6be6efa"
      },
      "archived": false,
      "properties": {
        "password": "",
        "icon": "",
        "date": "2020-07-23",
        "type": "Post",
        "category": "技术分享",
        "urlname": "9",
        "catalog": [
          "archives"
        ],
        "tags": [
          "Spring",
          "Java",
          "Redis"
        ],
        "summary": "记录一次scan和keys的使用,scan和key都是redis搜索key的值函数,但实现却完全不同。生产环境用key的同学准备好跑路吧~keysWarning: consider KEYS as a",
        "sort": "",
        "title": "RedisOperations scan 用法",
        "status": "Published",
        "updated": "2023-10-08 14:42:00"
      },
      "url": "https://www.notion.so/RedisOperations-scan-76be662e68e24e758d51c66197e850d3",
      "public_url": "https://honesty-blog.notion.site/RedisOperations-scan-76be662e68e24e758d51c66197e850d3"
    },
    {
      "object": "page",
      "id": "a1eb98de-4986-46c0-b1ac-6846629d0819",
      "created_time": "2023-03-30T06:28:00.000Z",
      "last_edited_time": "2023-10-08T06:42:00.000Z",
      "created_by": {
        "object": "user",
        "id": "a62f8c60-4c10-40c8-aa57-ef88b58f25a4"
      },
      "last_edited_by": {
        "object": "user",
        "id": "a62f8c60-4c10-40c8-aa57-ef88b58f25a4"
      },
      "cover": {
        "type": "external",
        "external": {
          "url": "https://www.notion.so/images/page-cover/nasa_tim_peake_spacewalk.jpg"
        }
      },
      "icon": null,
      "parent": {
        "type": "database_id",
        "database_id": "e2aef2eb-e7a1-4fc0-94f9-a186b6be6efa"
      },
      "archived": false,
      "properties": {
        "password": "",
        "icon": "",
        "date": "2021-04-10",
        "type": "Post",
        "category": "技术分享",
        "urlname": "21",
        "catalog": [
          "archives"
        ],
        "tags": [
          "Spring",
          "Java",
          "响应式",
          "异步编程",
          "WebFlux"
        ],
        "summary": "webFlux 初识LambdaLambda 表达式，有时候也称为匿名函数或箭头函数，几乎在当前的各种主流的编程语言中都有它的身影。Java8 中引入 Lambda 表达式，使原本需要用匿名类实现接口",
        "sort": "",
        "title": "响应式开发之webFlux & Reactor",
        "status": "Published",
        "updated": "2023-10-08 14:42:00"
      },
      "url": "https://www.notion.so/webFlux-Reactor-a1eb98de498646c0b1ac6846629d0819",
      "public_url": "https://honesty-blog.notion.site/webFlux-Reactor-a1eb98de498646c0b1ac6846629d0819"
    },
    {
      "object": "page",
      "id": "44263dbd-e5d6-4365-a72e-b0ff92bd0220",
      "created_time": "2023-03-30T06:26:00.000Z",
      "last_edited_time": "2023-10-08T06:42:00.000Z",
      "created_by": {
        "object": "user",
        "id": "a62f8c60-4c10-40c8-aa57-ef88b58f25a4"
      },
      "last_edited_by": {
        "object": "user",
        "id": "a62f8c60-4c10-40c8-aa57-ef88b58f25a4"
      },
      "cover": {
        "type": "external",
        "external": {
          "url": "https://www.notion.so/images/page-cover/webb2.jpg"
        }
      },
      "icon": {
        "type": "emoji",
        "emoji": "🥅"
      },
      "parent": {
        "type": "database_id",
        "database_id": "e2aef2eb-e7a1-4fc0-94f9-a186b6be6efa"
      },
      "archived": false,
      "properties": {
        "password": "",
        "icon": "",
        "date": "2021-08-27",
        "type": "Post",
        "category": "技术分享",
        "urlname": "20",
        "catalog": [
          "archives"
        ],
        "tags": [
          "Spring",
          "微服务",
          "分布式",
          "Redis"
        ],
        "summary": "在SpringCloud体系中，我们知道服务之间的调用是通过http协议进行调用的。而注册中心的主要目的就是维护这些服务的服务列表。我们知道，在Spring中，提供了RestTemplate。RestTemplate是Spring提供的用于访问Rest服务的客户端。而在SpringCloud中也是使用此服务进行服务调用的。",
        "sort": "",
        "title": "RestTemplate与OpenFeign",
        "status": "Published",
        "updated": "2023-10-08 14:42:00"
      },
      "url": "https://www.notion.so/RestTemplate-OpenFeign-44263dbde5d64365a72eb0ff92bd0220",
      "public_url": "https://honesty-blog.notion.site/RestTemplate-OpenFeign-44263dbde5d64365a72eb0ff92bd0220"
    },
    {
      "object": "page",
      "id": "f81c2940-d22e-4782-9d95-d16b57f6d3f4",
      "created_time": "2023-03-30T06:25:00.000Z",
      "last_edited_time": "2023-10-08T06:42:00.000Z",
      "created_by": {
        "object": "user",
        "id": "a62f8c60-4c10-40c8-aa57-ef88b58f25a4"
      },
      "last_edited_by": {
        "object": "user",
        "id": "a62f8c60-4c10-40c8-aa57-ef88b58f25a4"
      },
      "cover": {
        "type": "external",
        "external": {
          "url": "https://cdn.jsdelivr.net/gh/listener-He/images@default/202305102218848.png"
        }
      },
      "icon": null,
      "parent": {
        "type": "database_id",
        "database_id": "e2aef2eb-e7a1-4fc0-94f9-a186b6be6efa"
      },
      "archived": false,
      "properties": {
        "password": "",
        "icon": "",
        "date": "2021-12-18",
        "type": "Post",
        "category": "技术分享",
        "urlname": "23",
        "catalog": [
          "archives"
        ],
        "tags": [
          "微服务",
          "分布式",
          "Redis"
        ],
        "summary": "一致性hash在Redis 集群模式Cluster中，Redis采用的是分片Sharding的方式，也就是将数据采用一定的分区策略，分发到相应的集群节点中。但是我们使用上述HASH算法进行缓存时，会出",
        "sort": "",
        "title": "Redis集群与特性",
        "status": "Published",
        "updated": "2023-10-08 14:42:00"
      },
      "url": "https://www.notion.so/Redis-f81c2940d22e47829d95d16b57f6d3f4",
      "public_url": "https://honesty-blog.notion.site/Redis-f81c2940d22e47829d95d16b57f6d3f4"
    },
    {
      "object": "page",
      "id": "807ec69b-8970-4659-8443-d224934a8e7f",
      "created_time": "2023-03-30T06:22:00.000Z",
      "last_edited_time": "2023-10-08T06:42:00.000Z",
      "created_by": {
        "object": "user",
        "id": "a62f8c60-4c10-40c8-aa57-ef88b58f25a4"
      },
      "last_edited_by": {
        "object": "user",
        "id": "a62f8c60-4c10-40c8-aa57-ef88b58f25a4"
      },
      "cover": {
        "type": "external",
        "external": {
          "url": "https://cdn.jsdelivr.net/gh/listener-He/images@default/202305102216883.png"
        }
      },
      "icon": null,
      "parent": {
        "type": "database_id",
        "database_id": "e2aef2eb-e7a1-4fc0-94f9-a186b6be6efa"
      },
      "archived": false,
      "properties": {
        "password": "",
        "icon": "",
        "date": "2021-12-27",
        "type": "Post",
        "category": "技术分享",
        "urlname": "24",
        "catalog": [
          "archives"
        ],
        "tags": [
          "Spring",
          "Java"
        ],
        "summary": "springBoot 三板斧AOPaop是一种面向切面编程 能够将那些与业务无关，却为业务模块所共同调用的逻辑或责任（缓存，锁） 封装起来，便于减少系统的重复代码，降低模块间的耦合度，并有利于未来的可",
        "sort": "",
        "title": "springBoot三剑客",
        "status": "Published",
        "updated": "2023-10-08 14:42:00"
      },
      "url": "https://www.notion.so/springBoot-807ec69b897046598443d224934a8e7f",
      "public_url": "https://honesty-blog.notion.site/springBoot-807ec69b897046598443d224934a8e7f"
    },
    {
      "object": "page",
      "id": "747841e4-eeac-4349-88b0-63fc6421cf0c",
      "created_time": "2023-03-30T06:18:00.000Z",
      "last_edited_time": "2023-10-08T06:42:00.000Z",
      "created_by": {
        "object": "user",
        "id": "a62f8c60-4c10-40c8-aa57-ef88b58f25a4"
      },
      "last_edited_by": {
        "object": "user",
        "id": "a62f8c60-4c10-40c8-aa57-ef88b58f25a4"
      },
      "cover": {
        "type": "external",
        "external": {
          "url": "https://blog-file.hehouhui.cn/image-1647847160115.png"
        }
      },
      "icon": null,
      "parent": {
        "type": "database_id",
        "database_id": "e2aef2eb-e7a1-4fc0-94f9-a186b6be6efa"
      },
      "archived": false,
      "properties": {
        "password": "",
        "icon": "",
        "date": "2022-02-08",
        "type": "Post",
        "category": "技术分享",
        "urlname": "37",
        "catalog": [
          "archives"
        ],
        "tags": [
          "Java"
        ],
        "summary": "synchronized 关键字解决的是多个线程之间访问资源的同步性，synchronized关键字可以保证被它修饰的方法或者代码块在任意时刻只能有一个线程执行。另外，在 Ja",
        "sort": "",
        "title": "Java基础-锁",
        "status": "Published",
        "updated": "2023-10-08 14:42:00"
      },
      "url": "https://www.notion.so/Java-747841e4eeac434988b063fc6421cf0c",
      "public_url": "https://honesty-blog.notion.site/Java-747841e4eeac434988b063fc6421cf0c"
    },
    {
      "object": "page",
      "id": "397d5281-9f11-4c6f-8430-399176cc403d",
      "created_time": "2023-03-30T06:16:00.000Z",
      "last_edited_time": "2023-10-08T06:42:00.000Z",
      "created_by": {
        "object": "user",
        "id": "a62f8c60-4c10-40c8-aa57-ef88b58f25a4"
      },
      "last_edited_by": {
        "object": "user",
        "id": "a62f8c60-4c10-40c8-aa57-ef88b58f25a4"
      },
      "cover": {
        "type": "external",
        "external": {
          "url": "https://cdn.jsdelivr.net/gh/listener-He/images@default/202305102214388.png"
        }
      },
      "icon": null,
      "parent": {
        "type": "database_id",
        "database_id": "e2aef2eb-e7a1-4fc0-94f9-a186b6be6efa"
      },
      "archived": false,
      "properties": {
        "password": "",
        "icon": "",
        "date": "2022-02-21",
        "type": "Post",
        "category": "技术分享",
        "urlname": "38",
        "catalog": [
          "archives"
        ],
        "tags": [
          "mysql"
        ],
        "summary": "Mysql数据库存储引擎MyISAM引擎 不支持事务支持表级锁（MySql支持两种表级锁，表共享读锁和表独占写锁），但不支持行级锁存储表的总行数一个MyISAM表有三个文件：索引文件（.MYI",
        "sort": "",
        "title": "Mysql数据结构&锁",
        "status": "Published",
        "updated": "2023-10-08 14:42:00"
      },
      "url": "https://www.notion.so/Mysql-397d52819f114c6f8430399176cc403d",
      "public_url": "https://honesty-blog.notion.site/Mysql-397d52819f114c6f8430399176cc403d"
    },
    {
      "object": "page",
      "id": "07c3aa88-08b7-4093-9ac6-0a495a187163",
      "created_time": "2023-03-30T06:14:00.000Z",
      "last_edited_time": "2023-10-08T06:42:00.000Z",
      "created_by": {
        "object": "user",
        "id": "a62f8c60-4c10-40c8-aa57-ef88b58f25a4"
      },
      "last_edited_by": {
        "object": "user",
        "id": "a62f8c60-4c10-40c8-aa57-ef88b58f25a4"
      },
      "cover": {
        "type": "external",
        "external": {
          "url": "https://blog-file.hehouhui.cn/image-1647847511877.png"
        }
      },
      "icon": {
        "type": "emoji",
        "emoji": "🗒️"
      },
      "parent": {
        "type": "database_id",
        "database_id": "e2aef2eb-e7a1-4fc0-94f9-a186b6be6efa"
      },
      "archived": false,
      "properties": {
        "password": "",
        "icon": "",
        "date": "2022-03-15",
        "type": "Post",
        "category": "技术分享",
        "urlname": "39",
        "catalog": [
          "archives"
        ],
        "tags": [
          "图像处理"
        ],
        "summary": "wkhtmltopdf精讲(原文) 作者：JSON_NULL术语定义文档对象“文档对象”是指PDF文档中的文档对象，共有三种类型的“文档对象”，他们分别是“页面对象”，“封面对象”和“目录对象”。页面",
        "sort": "",
        "title": "wkhtmltopdf详细使用",
        "status": "Published",
        "updated": "2023-10-08 14:42:00"
      },
      "url": "https://www.notion.so/wkhtmltopdf-07c3aa8808b740939ac60a495a187163",
      "public_url": "https://honesty-blog.notion.site/wkhtmltopdf-07c3aa8808b740939ac60a495a187163"
    },
    {
      "object": "page",
      "id": "a60e468f-c55d-4add-a043-970fc03927d0",
      "created_time": "2023-03-30T06:09:00.000Z",
      "last_edited_time": "2023-10-08T06:42:00.000Z",
      "created_by": {
        "object": "user",
        "id": "a62f8c60-4c10-40c8-aa57-ef88b58f25a4"
      },
      "last_edited_by": {
        "object": "user",
        "id": "a62f8c60-4c10-40c8-aa57-ef88b58f25a4"
      },
      "cover": {
        "type": "external",
        "external": {
          "url": "https://cdn.jsdelivr.net/gh/listener-He/images@default/202305102132809.jpeg"
        }
      },
      "icon": {
        "type": "emoji",
        "emoji": "🗒️"
      },
      "parent": {
        "type": "database_id",
        "database_id": "e2aef2eb-e7a1-4fc0-94f9-a186b6be6efa"
      },
      "archived": false,
      "properties": {
        "password": "",
        "icon": "",
        "date": "2022-03-15",
        "type": "Post",
        "category": "技术分享",
        "urlname": "40",
        "catalog": [
          "archives"
        ],
        "tags": [
          "图像处理"
        ],
        "summary": "“wkhtmltopdf\"，是一个能够把网页/文件转换成PDF的工具。工具全名叫 \"wkhtmltopdf\" ; 是一个使用 Qt WebKit 引擎做渲染的，能够把html 文档转换成 pdf 文档 或 图片(image) 的**“命令行工具”**。\n支持多个平台，可在win，linux，os x 等系统下运行。",
        "sort": "",
        "title": "wkhtmltopdf 安装",
        "status": "Published",
        "updated": "2023-10-08 14:42:00"
      },
      "url": "https://www.notion.so/wkhtmltopdf-a60e468fc55d4adda043970fc03927d0",
      "public_url": "https://honesty-blog.notion.site/wkhtmltopdf-a60e468fc55d4adda043970fc03927d0"
    },
    {
      "object": "page",
      "id": "6a6aca31-1871-49f3-a600-5062cb673260",
      "created_time": "2023-03-27T03:18:00.000Z",
      "last_edited_time": "2023-10-08T06:42:00.000Z",
      "created_by": {
        "object": "user",
        "id": "a62f8c60-4c10-40c8-aa57-ef88b58f25a4"
      },
      "last_edited_by": {
        "object": "user",
        "id": "a62f8c60-4c10-40c8-aa57-ef88b58f25a4"
      },
      "cover": {
        "type": "external",
        "external": {
          "url": "https://cdn.jsdelivr.net/gh/listener-He/images@default/202305102212132.png"
        }
      },
      "icon": {
        "type": "emoji",
        "emoji": "🗒️"
      },
      "parent": {
        "type": "database_id",
        "database_id": "e2aef2eb-e7a1-4fc0-94f9-a186b6be6efa"
      },
      "archived": false,
      "properties": {
        "password": "",
        "icon": "",
        "date": "2022-03-17",
        "type": "Post",
        "category": "技术分享",
        "urlname": "41",
        "catalog": [
          "archives"
        ],
        "tags": [
          "Python",
          "健康"
        ],
        "summary": "之前一直使用ffmpeg来进行格式转换，但是将微信的amr转为mp3后语音质量不理想（也可能是我参数没有调正确🤪）。\n于是就继续想解决办法，后来在github瞎逛时看到可以使用silk-v3-decoder来做这件事情。虽然本质上还是使用的ffmpeg来转的，只是封装了一下。",
        "sort": "",
        "title": "silk-v3-decoder 一款微信音频转码的工具",
        "status": "Published",
        "updated": "2023-10-08 14:42:00"
      },
      "url": "https://www.notion.so/silk-v3-decoder-6a6aca31187149f3a6005062cb673260",
      "public_url": "https://honesty-blog.notion.site/silk-v3-decoder-6a6aca31187149f3a6005062cb673260"
    },
    {
      "object": "page",
      "id": "0d2fc9f8-00d3-40d8-b7bb-cc4dbbb10251",
      "created_time": "2023-03-27T03:18:00.000Z",
      "last_edited_time": "2023-10-08T06:42:00.000Z",
      "created_by": {
        "object": "user",
        "id": "a62f8c60-4c10-40c8-aa57-ef88b58f25a4"
      },
      "last_edited_by": {
        "object": "user",
        "id": "a62f8c60-4c10-40c8-aa57-ef88b58f25a4"
      },
      "cover": {
        "type": "external",
        "external": {
          "url": "https://cdn.jsdelivr.net/gh/listener-He/images@default/202305102209640.png"
        }
      },
      "icon": null,
      "parent": {
        "type": "database_id",
        "database_id": "e2aef2eb-e7a1-4fc0-94f9-a186b6be6efa"
      },
      "archived": false,
      "properties": {
        "password": "",
        "icon": "",
        "date": "2022-03-05",
        "type": "Post",
        "category": "技术分享",
        "urlname": "46",
        "catalog": [
          "archives"
        ],
        "tags": [
          "Spring",
          "微服务",
          "分布式"
        ],
        "summary": "API 网关是一个服务器，是系统对外的唯一入口。API 网关封装了系统内部架构，为每个客户端提供定制的 API。所有的客户端和消费端都通过统一的网关接入微服务，在网关层处理所有非业务功能。API 网关并不是微服务场景中必须的组件，如下图，不管有没有 API 网关，后端微服务都可以通过 API 很好地支持客户端的访问",
        "sort": "",
        "title": "API网关之Gateway",
        "status": "Published",
        "updated": "2023-10-08 14:42:00"
      },
      "url": "https://www.notion.so/API-Gateway-0d2fc9f800d340d8b7bbcc4dbbb10251",
      "public_url": "https://honesty-blog.notion.site/API-Gateway-0d2fc9f800d340d8b7bbcc4dbbb10251"
    }
  ]
}